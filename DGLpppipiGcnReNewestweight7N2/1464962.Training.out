0: gpu012.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-b4a35fc2-ff11-b9e9-de14-36f4bbc00b10)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Fri Jul  8 14:07:02 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:1F:00.0 Off |                    0 |
| N/A   38C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b5d80782910> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	1m28.847s
user	0m3.290s
sys	0m2.315s
[14:08:31] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.1560],
        [ 0.1037],
        [ 0.4745],
        ...,
        [-2.1939],
        [-0.5906],
        [-0.1953]], device='cuda:0', requires_grad=True) 
node features sum: tensor(113.0962, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (2000, 6796) (2000, 6796)
sum 189931 265535
shape torch.Size([2000, 6796]) torch.Size([2000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0207,  0.1513,  0.1202, -0.0390, -0.0116, -0.1017,  0.0110, -0.0980,
         -0.0659, -0.0393,  0.0567, -0.1337, -0.0626, -0.0460, -0.0345, -0.0825,
         -0.0480,  0.0270,  0.0822, -0.1284, -0.1431,  0.0092,  0.1396,  0.0004,
          0.1503,  0.0662, -0.1318,  0.0307, -0.0329,  0.0941,  0.1492,  0.0504,
          0.0147, -0.1421,  0.0051,  0.0794,  0.0967,  0.1379,  0.1262, -0.0955,
          0.0500, -0.1439, -0.0078, -0.1458, -0.0820, -0.0076, -0.0931, -0.1019,
          0.0312, -0.1104,  0.0954, -0.0655,  0.1331,  0.0131,  0.0466, -0.1116,
         -0.1108,  0.0132, -0.0914, -0.0030,  0.0006, -0.1303, -0.1494,  0.0053,
         -0.1379, -0.0691,  0.1314, -0.1522, -0.0717,  0.1431, -0.1393,  0.1245,
         -0.0330,  0.0947,  0.0114, -0.0896,  0.0427,  0.1170, -0.0860, -0.0852,
          0.0694, -0.1379, -0.1439,  0.0228,  0.0399,  0.0035,  0.0801, -0.0177,
          0.0543, -0.1322, -0.1148, -0.0687, -0.0118,  0.1300, -0.0162, -0.0686,
         -0.0381, -0.0476,  0.0856, -0.0489, -0.0163, -0.0959, -0.0609,  0.1367,
          0.0048,  0.1399, -0.0044, -0.1319, -0.0992,  0.0072,  0.0890,  0.1216,
         -0.0665, -0.0632, -0.1468, -0.1336, -0.0425, -0.0541, -0.1093, -0.1237,
          0.1073,  0.0725, -0.1006,  0.1243,  0.0306,  0.0608,  0.0617, -0.0998,
         -0.1095, -0.0030,  0.1273, -0.1306,  0.1497,  0.1281,  0.1012,  0.1347,
         -0.1477, -0.0789,  0.0564, -0.0206,  0.0119, -0.1191,  0.0047,  0.0709,
         -0.0421, -0.1351, -0.1311,  0.0922, -0.0427, -0.0604,  0.1507, -0.0132,
         -0.1043,  0.0747,  0.0624,  0.0530,  0.1150, -0.0910, -0.1126,  0.1528,
          0.0114,  0.0300,  0.0473,  0.1140, -0.0046,  0.0426,  0.0420,  0.0024,
         -0.1260, -0.0635,  0.0976, -0.0371,  0.0141,  0.0463,  0.0485,  0.0461,
         -0.1409, -0.1267, -0.0747, -0.0109,  0.0286,  0.0644, -0.1091,  0.0105,
          0.0015,  0.0231,  0.0947,  0.0887,  0.0096,  0.0699, -0.1301, -0.0871,
         -0.0106,  0.0684,  0.0505,  0.1157, -0.1288, -0.0436, -0.0651,  0.0064,
         -0.0839,  0.0267,  0.0223, -0.0164,  0.1187, -0.0278, -0.0913,  0.0682,
         -0.0257, -0.1213, -0.1389, -0.1142,  0.1230,  0.0506, -0.1210, -0.0074,
         -0.0267,  0.1497,  0.1379, -0.0281,  0.0504, -0.1161,  0.1120,  0.0008,
         -0.0868,  0.1090, -0.1079,  0.0023,  0.1215, -0.0088, -0.0389, -0.0535,
         -0.1310,  0.0923, -0.0219, -0.0468,  0.1283, -0.1240, -0.0360, -0.0187,
         -0.1072,  0.1467,  0.0766,  0.0208,  0.1249, -0.0996, -0.0224, -0.1052,
         -0.1495, -0.1419, -0.0571,  0.1126, -0.0953,  0.0457, -0.0918,  0.1299]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0207,  0.1513,  0.1202, -0.0390, -0.0116, -0.1017,  0.0110, -0.0980,
         -0.0659, -0.0393,  0.0567, -0.1337, -0.0626, -0.0460, -0.0345, -0.0825,
         -0.0480,  0.0270,  0.0822, -0.1284, -0.1431,  0.0092,  0.1396,  0.0004,
          0.1503,  0.0662, -0.1318,  0.0307, -0.0329,  0.0941,  0.1492,  0.0504,
          0.0147, -0.1421,  0.0051,  0.0794,  0.0967,  0.1379,  0.1262, -0.0955,
          0.0500, -0.1439, -0.0078, -0.1458, -0.0820, -0.0076, -0.0931, -0.1019,
          0.0312, -0.1104,  0.0954, -0.0655,  0.1331,  0.0131,  0.0466, -0.1116,
         -0.1108,  0.0132, -0.0914, -0.0030,  0.0006, -0.1303, -0.1494,  0.0053,
         -0.1379, -0.0691,  0.1314, -0.1522, -0.0717,  0.1431, -0.1393,  0.1245,
         -0.0330,  0.0947,  0.0114, -0.0896,  0.0427,  0.1170, -0.0860, -0.0852,
          0.0694, -0.1379, -0.1439,  0.0228,  0.0399,  0.0035,  0.0801, -0.0177,
          0.0543, -0.1322, -0.1148, -0.0687, -0.0118,  0.1300, -0.0162, -0.0686,
         -0.0381, -0.0476,  0.0856, -0.0489, -0.0163, -0.0959, -0.0609,  0.1367,
          0.0048,  0.1399, -0.0044, -0.1319, -0.0992,  0.0072,  0.0890,  0.1216,
         -0.0665, -0.0632, -0.1468, -0.1336, -0.0425, -0.0541, -0.1093, -0.1237,
          0.1073,  0.0725, -0.1006,  0.1243,  0.0306,  0.0608,  0.0617, -0.0998,
         -0.1095, -0.0030,  0.1273, -0.1306,  0.1497,  0.1281,  0.1012,  0.1347,
         -0.1477, -0.0789,  0.0564, -0.0206,  0.0119, -0.1191,  0.0047,  0.0709,
         -0.0421, -0.1351, -0.1311,  0.0922, -0.0427, -0.0604,  0.1507, -0.0132,
         -0.1043,  0.0747,  0.0624,  0.0530,  0.1150, -0.0910, -0.1126,  0.1528,
          0.0114,  0.0300,  0.0473,  0.1140, -0.0046,  0.0426,  0.0420,  0.0024,
         -0.1260, -0.0635,  0.0976, -0.0371,  0.0141,  0.0463,  0.0485,  0.0461,
         -0.1409, -0.1267, -0.0747, -0.0109,  0.0286,  0.0644, -0.1091,  0.0105,
          0.0015,  0.0231,  0.0947,  0.0887,  0.0096,  0.0699, -0.1301, -0.0871,
         -0.0106,  0.0684,  0.0505,  0.1157, -0.1288, -0.0436, -0.0651,  0.0064,
         -0.0839,  0.0267,  0.0223, -0.0164,  0.1187, -0.0278, -0.0913,  0.0682,
         -0.0257, -0.1213, -0.1389, -0.1142,  0.1230,  0.0506, -0.1210, -0.0074,
         -0.0267,  0.1497,  0.1379, -0.0281,  0.0504, -0.1161,  0.1120,  0.0008,
         -0.0868,  0.1090, -0.1079,  0.0023,  0.1215, -0.0088, -0.0389, -0.0535,
         -0.1310,  0.0923, -0.0219, -0.0468,  0.1283, -0.1240, -0.0360, -0.0187,
         -0.1072,  0.1467,  0.0766,  0.0208,  0.1249, -0.0996, -0.0224, -0.1052,
         -0.1495, -0.1419, -0.0571,  0.1126, -0.0953,  0.0457, -0.0918,  0.1299]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.1165, -0.0347,  0.0321,  ...,  0.0108,  0.1240,  0.0749],
        [ 0.0214,  0.0995, -0.1073,  ..., -0.0495,  0.1047, -0.0296],
        [-0.0958,  0.1127,  0.0030,  ...,  0.1042, -0.0917, -0.0790],
        ...,
        [ 0.0592,  0.0239, -0.0279,  ..., -0.0118, -0.0685, -0.1005],
        [-0.0118,  0.0559, -0.0776,  ...,  0.0453, -0.1080, -0.0309],
        [ 0.0169, -0.0713, -0.0764,  ...,  0.0097,  0.0142,  0.0578]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1165, -0.0347,  0.0321,  ...,  0.0108,  0.1240,  0.0749],
        [ 0.0214,  0.0995, -0.1073,  ..., -0.0495,  0.1047, -0.0296],
        [-0.0958,  0.1127,  0.0030,  ...,  0.1042, -0.0917, -0.0790],
        ...,
        [ 0.0592,  0.0239, -0.0279,  ..., -0.0118, -0.0685, -0.1005],
        [-0.0118,  0.0559, -0.0776,  ...,  0.0453, -0.1080, -0.0309],
        [ 0.0169, -0.0713, -0.0764,  ...,  0.0097,  0.0142,  0.0578]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0531,  0.1232, -0.1658,  ...,  0.0482,  0.0726, -0.0048],
        [-0.0697,  0.0102, -0.0144,  ...,  0.1444,  0.0334,  0.1294],
        [-0.1424, -0.0331, -0.0985,  ...,  0.0601,  0.1282,  0.0114],
        ...,
        [-0.0889, -0.1719,  0.0025,  ..., -0.1096, -0.1617, -0.0640],
        [-0.0269,  0.0365, -0.1697,  ..., -0.1474,  0.0906,  0.1685],
        [ 0.0061, -0.1499, -0.0074,  ...,  0.0525, -0.1251,  0.0988]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0531,  0.1232, -0.1658,  ...,  0.0482,  0.0726, -0.0048],
        [-0.0697,  0.0102, -0.0144,  ...,  0.1444,  0.0334,  0.1294],
        [-0.1424, -0.0331, -0.0985,  ...,  0.0601,  0.1282,  0.0114],
        ...,
        [-0.0889, -0.1719,  0.0025,  ..., -0.1096, -0.1617, -0.0640],
        [-0.0269,  0.0365, -0.1697,  ..., -0.1474,  0.0906,  0.1685],
        [ 0.0061, -0.1499, -0.0074,  ...,  0.0525, -0.1251,  0.0988]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0330,  0.2086, -0.0282,  ...,  0.1292,  0.0766, -0.1741],
        [-0.2002,  0.2116,  0.2205,  ...,  0.1794, -0.0141, -0.0287],
        [ 0.0575,  0.1469, -0.0761,  ...,  0.2462,  0.0303,  0.0680],
        ...,
        [-0.1060,  0.0905,  0.2268,  ...,  0.2306,  0.1414,  0.0795],
        [ 0.1886, -0.1010,  0.0636,  ...,  0.2012,  0.2127, -0.0306],
        [ 0.2390,  0.1066, -0.2223,  ...,  0.1491,  0.0955,  0.1275]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0330,  0.2086, -0.0282,  ...,  0.1292,  0.0766, -0.1741],
        [-0.2002,  0.2116,  0.2205,  ...,  0.1794, -0.0141, -0.0287],
        [ 0.0575,  0.1469, -0.0761,  ...,  0.2462,  0.0303,  0.0680],
        ...,
        [-0.1060,  0.0905,  0.2268,  ...,  0.2306,  0.1414,  0.0795],
        [ 0.1886, -0.1010,  0.0636,  ...,  0.2012,  0.2127, -0.0306],
        [ 0.2390,  0.1066, -0.2223,  ...,  0.1491,  0.0955,  0.1275]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2106],
        [ 0.2498],
        [ 0.0155],
        [-0.1812],
        [ 0.1230],
        [ 0.3283],
        [ 0.0494],
        [ 0.0446],
        [-0.3107],
        [ 0.0720],
        [-0.2534],
        [ 0.2841],
        [-0.1770],
        [-0.2808],
        [-0.1821],
        [-0.2554],
        [ 0.4211],
        [-0.4142],
        [ 0.1914],
        [ 0.2066],
        [-0.2569],
        [-0.0792],
        [ 0.3722],
        [-0.0087],
        [ 0.3095],
        [-0.0554],
        [-0.3124],
        [ 0.3003],
        [-0.0778],
        [ 0.1163],
        [-0.1705],
        [ 0.0774]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2106],
        [ 0.2498],
        [ 0.0155],
        [-0.1812],
        [ 0.1230],
        [ 0.3283],
        [ 0.0494],
        [ 0.0446],
        [-0.3107],
        [ 0.0720],
        [-0.2534],
        [ 0.2841],
        [-0.1770],
        [-0.2808],
        [-0.1821],
        [-0.2554],
        [ 0.4211],
        [-0.4142],
        [ 0.1914],
        [ 0.2066],
        [-0.2569],
        [-0.0792],
        [ 0.3722],
        [-0.0087],
        [ 0.3095],
        [-0.0554],
        [-0.3124],
        [ 0.3003],
        [-0.0778],
        [ 0.1163],
        [-0.1705],
        [ 0.0774]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1051,  0.0948,  0.1077,  0.1153, -0.0628,  0.1045,  0.0134,  0.0541,
         -0.0081,  0.0632, -0.1389, -0.0276,  0.1151, -0.1208, -0.1412,  0.1077,
         -0.0871, -0.0930,  0.1314,  0.0510,  0.0990,  0.1306, -0.1493,  0.1213,
         -0.0252,  0.1122, -0.0488,  0.1425,  0.0167,  0.1423,  0.0252, -0.0638,
          0.0749, -0.1122, -0.0989,  0.1202,  0.1183,  0.0484, -0.0083,  0.0901,
         -0.1084, -0.0912,  0.1191, -0.0380, -0.0541, -0.0653,  0.0556,  0.0074,
         -0.0632, -0.0255,  0.0719,  0.1023,  0.0098,  0.1074,  0.0795,  0.0047,
          0.0665, -0.0300,  0.1092, -0.0391,  0.0896, -0.0158,  0.0143,  0.0881,
         -0.0627, -0.0491, -0.0806, -0.0873, -0.0622,  0.0036, -0.0228,  0.0536,
          0.0432, -0.0372, -0.0936,  0.1443,  0.1126, -0.1074,  0.1485,  0.1106,
         -0.0200, -0.0827,  0.1224, -0.0688,  0.0543,  0.0660, -0.0446, -0.1485,
         -0.0346,  0.0242, -0.0335, -0.1432, -0.0710, -0.0608, -0.0445,  0.1272,
          0.1064, -0.0630, -0.1507, -0.0612,  0.0503, -0.0201,  0.1289,  0.0815,
          0.0353, -0.0647,  0.0340,  0.1167, -0.1508, -0.1076, -0.0803,  0.0734,
          0.0931,  0.1408, -0.0243,  0.1445,  0.0961, -0.0711, -0.0475, -0.1076,
          0.0545,  0.0510,  0.1358, -0.0128,  0.1057, -0.0033,  0.0886,  0.0422,
          0.0144,  0.0195, -0.0174, -0.0885, -0.1319,  0.0701,  0.1338, -0.0190,
          0.1282, -0.0734,  0.0494, -0.0717, -0.0866, -0.0588,  0.0043,  0.0660,
          0.0654, -0.0517,  0.0264, -0.1019,  0.1170,  0.0984, -0.1189, -0.0346,
          0.1007,  0.0397, -0.0337, -0.1356,  0.1475,  0.0362, -0.0767, -0.0190,
          0.1335,  0.1265, -0.0520, -0.0494, -0.0618, -0.1240, -0.0990, -0.0872,
          0.1408,  0.1105, -0.0399, -0.0450, -0.0095, -0.0095,  0.0685,  0.1363,
         -0.1223, -0.0181,  0.1377, -0.0396, -0.0363, -0.0644,  0.1430, -0.0870,
          0.0858, -0.0091, -0.1500, -0.0308, -0.1127,  0.1027, -0.1370,  0.1057,
          0.1519, -0.0701, -0.1244, -0.1061,  0.0629, -0.1074, -0.0178, -0.0917,
         -0.0660, -0.0122,  0.0392, -0.0286,  0.0296, -0.0347, -0.0563, -0.1017,
         -0.0895,  0.0681,  0.1263, -0.0455, -0.0141,  0.0749,  0.0991, -0.1440,
         -0.0695, -0.0747, -0.0623,  0.0348, -0.1425,  0.0078,  0.1222,  0.0897,
          0.0377, -0.0955,  0.0541, -0.0118, -0.0298, -0.0159,  0.1379, -0.0378,
         -0.1084, -0.0477,  0.1336, -0.1524, -0.0374,  0.1061, -0.0755,  0.1086,
          0.0943, -0.1341, -0.1093,  0.0442, -0.0439, -0.1104,  0.0847, -0.0631,
         -0.1502, -0.1084, -0.0857, -0.0269,  0.0042,  0.0696,  0.1131,  0.1311]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1051,  0.0948,  0.1077,  0.1153, -0.0628,  0.1045,  0.0134,  0.0541,
         -0.0081,  0.0632, -0.1389, -0.0276,  0.1151, -0.1208, -0.1412,  0.1077,
         -0.0871, -0.0930,  0.1314,  0.0510,  0.0990,  0.1306, -0.1493,  0.1213,
         -0.0252,  0.1122, -0.0488,  0.1425,  0.0167,  0.1423,  0.0252, -0.0638,
          0.0749, -0.1122, -0.0989,  0.1202,  0.1183,  0.0484, -0.0083,  0.0901,
         -0.1084, -0.0912,  0.1191, -0.0380, -0.0541, -0.0653,  0.0556,  0.0074,
         -0.0632, -0.0255,  0.0719,  0.1023,  0.0098,  0.1074,  0.0795,  0.0047,
          0.0665, -0.0300,  0.1092, -0.0391,  0.0896, -0.0158,  0.0143,  0.0881,
         -0.0627, -0.0491, -0.0806, -0.0873, -0.0622,  0.0036, -0.0228,  0.0536,
          0.0432, -0.0372, -0.0936,  0.1443,  0.1126, -0.1074,  0.1485,  0.1106,
         -0.0200, -0.0827,  0.1224, -0.0688,  0.0543,  0.0660, -0.0446, -0.1485,
         -0.0346,  0.0242, -0.0335, -0.1432, -0.0710, -0.0608, -0.0445,  0.1272,
          0.1064, -0.0630, -0.1507, -0.0612,  0.0503, -0.0201,  0.1289,  0.0815,
          0.0353, -0.0647,  0.0340,  0.1167, -0.1508, -0.1076, -0.0803,  0.0734,
          0.0931,  0.1408, -0.0243,  0.1445,  0.0961, -0.0711, -0.0475, -0.1076,
          0.0545,  0.0510,  0.1358, -0.0128,  0.1057, -0.0033,  0.0886,  0.0422,
          0.0144,  0.0195, -0.0174, -0.0885, -0.1319,  0.0701,  0.1338, -0.0190,
          0.1282, -0.0734,  0.0494, -0.0717, -0.0866, -0.0588,  0.0043,  0.0660,
          0.0654, -0.0517,  0.0264, -0.1019,  0.1170,  0.0984, -0.1189, -0.0346,
          0.1007,  0.0397, -0.0337, -0.1356,  0.1475,  0.0362, -0.0767, -0.0190,
          0.1335,  0.1265, -0.0520, -0.0494, -0.0618, -0.1240, -0.0990, -0.0872,
          0.1408,  0.1105, -0.0399, -0.0450, -0.0095, -0.0095,  0.0685,  0.1363,
         -0.1223, -0.0181,  0.1377, -0.0396, -0.0363, -0.0644,  0.1430, -0.0870,
          0.0858, -0.0091, -0.1500, -0.0308, -0.1127,  0.1027, -0.1370,  0.1057,
          0.1519, -0.0701, -0.1244, -0.1061,  0.0629, -0.1074, -0.0178, -0.0917,
         -0.0660, -0.0122,  0.0392, -0.0286,  0.0296, -0.0347, -0.0563, -0.1017,
         -0.0895,  0.0681,  0.1263, -0.0455, -0.0141,  0.0749,  0.0991, -0.1440,
         -0.0695, -0.0747, -0.0623,  0.0348, -0.1425,  0.0078,  0.1222,  0.0897,
          0.0377, -0.0955,  0.0541, -0.0118, -0.0298, -0.0159,  0.1379, -0.0378,
         -0.1084, -0.0477,  0.1336, -0.1524, -0.0374,  0.1061, -0.0755,  0.1086,
          0.0943, -0.1341, -0.1093,  0.0442, -0.0439, -0.1104,  0.0847, -0.0631,
         -0.1502, -0.1084, -0.0857, -0.0269,  0.0042,  0.0696,  0.1131,  0.1311]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0064, -0.1076,  0.0458,  ..., -0.0491, -0.0706,  0.0283],
        [-0.0302,  0.0496,  0.0272,  ...,  0.0773, -0.0626,  0.0793],
        [-0.0577, -0.0270, -0.0287,  ...,  0.0972, -0.0538,  0.0264],
        ...,
        [-0.0573,  0.0610,  0.0567,  ...,  0.0656, -0.0672,  0.0046],
        [-0.0221,  0.1040, -0.0615,  ...,  0.1005,  0.0041, -0.0981],
        [-0.0112,  0.1055, -0.1051,  ..., -0.0012, -0.0371, -0.1163]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0064, -0.1076,  0.0458,  ..., -0.0491, -0.0706,  0.0283],
        [-0.0302,  0.0496,  0.0272,  ...,  0.0773, -0.0626,  0.0793],
        [-0.0577, -0.0270, -0.0287,  ...,  0.0972, -0.0538,  0.0264],
        ...,
        [-0.0573,  0.0610,  0.0567,  ...,  0.0656, -0.0672,  0.0046],
        [-0.0221,  0.1040, -0.0615,  ...,  0.1005,  0.0041, -0.0981],
        [-0.0112,  0.1055, -0.1051,  ..., -0.0012, -0.0371, -0.1163]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1364, -0.0141,  0.0209,  ...,  0.1014, -0.0156,  0.1059],
        [ 0.0774, -0.1462,  0.0412,  ...,  0.0416,  0.1758, -0.0643],
        [-0.1164, -0.1491, -0.1601,  ...,  0.0771,  0.0793, -0.0804],
        ...,
        [-0.1286,  0.1490,  0.1503,  ..., -0.1069, -0.1541, -0.0972],
        [-0.1527,  0.1214, -0.0680,  ..., -0.1420, -0.0194, -0.1674],
        [ 0.1544, -0.0636,  0.0710,  ..., -0.0360, -0.0954,  0.0363]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1364, -0.0141,  0.0209,  ...,  0.1014, -0.0156,  0.1059],
        [ 0.0774, -0.1462,  0.0412,  ...,  0.0416,  0.1758, -0.0643],
        [-0.1164, -0.1491, -0.1601,  ...,  0.0771,  0.0793, -0.0804],
        ...,
        [-0.1286,  0.1490,  0.1503,  ..., -0.1069, -0.1541, -0.0972],
        [-0.1527,  0.1214, -0.0680,  ..., -0.1420, -0.0194, -0.1674],
        [ 0.1544, -0.0636,  0.0710,  ..., -0.0360, -0.0954,  0.0363]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0020, -0.0763, -0.1695,  ...,  0.0484, -0.0703,  0.1710],
        [-0.1406, -0.1458,  0.2251,  ..., -0.1398,  0.1394, -0.1748],
        [ 0.0944,  0.2169,  0.0763,  ...,  0.1940, -0.0764, -0.1157],
        ...,
        [-0.1713,  0.0205,  0.2218,  ...,  0.1441,  0.1131,  0.2266],
        [ 0.2413,  0.0774,  0.1588,  ...,  0.2033,  0.1764, -0.1247],
        [ 0.0114,  0.0380, -0.2303,  ..., -0.0315,  0.1546,  0.1429]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0020, -0.0763, -0.1695,  ...,  0.0484, -0.0703,  0.1710],
        [-0.1406, -0.1458,  0.2251,  ..., -0.1398,  0.1394, -0.1748],
        [ 0.0944,  0.2169,  0.0763,  ...,  0.1940, -0.0764, -0.1157],
        ...,
        [-0.1713,  0.0205,  0.2218,  ...,  0.1441,  0.1131,  0.2266],
        [ 0.2413,  0.0774,  0.1588,  ...,  0.2033,  0.1764, -0.1247],
        [ 0.0114,  0.0380, -0.2303,  ..., -0.0315,  0.1546,  0.1429]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.4143],
        [ 0.0438],
        [ 0.3667],
        [ 0.3738],
        [ 0.4144],
        [-0.2587],
        [ 0.1400],
        [ 0.0008],
        [-0.2175],
        [ 0.2376],
        [-0.3100],
        [ 0.1822],
        [-0.1440],
        [ 0.1101],
        [ 0.1445],
        [-0.2265],
        [-0.2011],
        [ 0.0894],
        [ 0.2334],
        [ 0.1921],
        [ 0.2199],
        [-0.3902],
        [-0.2216],
        [ 0.0364],
        [-0.0774],
        [ 0.0300],
        [ 0.1254],
        [-0.3706],
        [-0.3446],
        [ 0.0405],
        [-0.2430],
        [-0.0115]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.4143],
        [ 0.0438],
        [ 0.3667],
        [ 0.3738],
        [ 0.4144],
        [-0.2587],
        [ 0.1400],
        [ 0.0008],
        [-0.2175],
        [ 0.2376],
        [-0.3100],
        [ 0.1822],
        [-0.1440],
        [ 0.1101],
        [ 0.1445],
        [-0.2265],
        [-0.2011],
        [ 0.0894],
        [ 0.2334],
        [ 0.1921],
        [ 0.2199],
        [-0.3902],
        [-0.2216],
        [ 0.0364],
        [-0.0774],
        [ 0.0300],
        [ 0.1254],
        [-0.3706],
        [-0.3446],
        [ 0.0405],
        [-0.2430],
        [-0.0115]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057, -0.0020, -0.0024,  ..., -0.0043, -0.0033,  0.0008],
        [ 0.0190, -0.0065, -0.0078,  ..., -0.0142, -0.0111,  0.0025],
        [ 0.0126, -0.0043, -0.0052,  ..., -0.0094, -0.0074,  0.0017],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-86.3939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.8988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5177, device='cuda:0')



h[100].sum tensor(14.9595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5417, device='cuda:0')



h[200].sum tensor(11.7497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2071, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0436, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0058],
        [0.0383, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0051],
        [0.0527, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0070],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(18965.7598, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1550, 0.2241, 0.0664,  ..., 0.1265, 0.0000, 0.0000],
        [0.1519, 0.2197, 0.0651,  ..., 0.1241, 0.0000, 0.0000],
        [0.1527, 0.2208, 0.0655,  ..., 0.1247, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(125074.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2490.2310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(158.8731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-182.9739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-132.7286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1922],
        [0.1850],
        [0.1661],
        ...,
        [0.0014],
        [0.0015],
        [0.0015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(9882.5957, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.1922],
        [0.1850],
        [0.1661],
        ...,
        [0.0014],
        [0.0015],
        [0.0015]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0028, -0.0039, -0.0086,  ...,  0.0026, -0.0034, -0.0079],
        [-0.0092, -0.0128, -0.0286,  ...,  0.0086, -0.0114, -0.0262],
        [-0.0061, -0.0085, -0.0190,  ...,  0.0057, -0.0076, -0.0174],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-80.0541, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6693, device='cuda:0')



h[100].sum tensor(-37.8827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-39.3007, device='cuda:0')



h[200].sum tensor(37.3436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.7414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(27587.9355, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0548, 0.2347,  ..., 0.0045, 0.0938, 0.0000],
        [0.0000, 0.0537, 0.2301,  ..., 0.0044, 0.0919, 0.0000],
        [0.0000, 0.0540, 0.2312,  ..., 0.0044, 0.0924, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(152453.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.6723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(236.5555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-8.4118, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[3.2989e-01],
        [3.1757e-01],
        [2.8518e-01],
        ...,
        [1.1066e-05],
        [1.8272e-04],
        [1.2246e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(23131.7598, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.1922],
        [0.1850],
        [0.1661],
        ...,
        [0.0014],
        [0.0015],
        [0.0015]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



load_model False 
TraEvN 1998 
BatchSize 15 
EpochNum 10 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.1451, -0.0623, -0.1057,  0.0899,  0.1185, -0.0681,  0.0827,  0.1419,
         -0.0362, -0.1390,  0.0100, -0.1389,  0.0352,  0.0159, -0.1174,  0.0425,
         -0.1146, -0.1330,  0.0598, -0.0810, -0.0830,  0.1202,  0.1511,  0.0006,
          0.0557,  0.0682,  0.0451, -0.1444, -0.0475,  0.1473, -0.0263,  0.1047,
         -0.0778,  0.0619,  0.0061, -0.1431, -0.0194,  0.0093,  0.0030, -0.1346,
          0.1047, -0.1509,  0.0751,  0.1141,  0.1088,  0.0550,  0.1071,  0.0691,
          0.0875, -0.0619,  0.0953,  0.1147, -0.1029,  0.0067,  0.0537,  0.0424,
         -0.0048,  0.0746, -0.0817, -0.0875, -0.1356, -0.1070,  0.0675,  0.0611,
         -0.0500, -0.0135, -0.1306, -0.0475, -0.0897,  0.1312,  0.0157, -0.0609,
         -0.1041,  0.0132, -0.1067,  0.0379,  0.0299, -0.1102,  0.0817,  0.1383,
          0.0916,  0.0909, -0.0636, -0.0765, -0.0312,  0.0802, -0.1334,  0.0990,
          0.0360,  0.0726, -0.0552, -0.0540,  0.0947,  0.1442, -0.0679, -0.0388,
         -0.0063, -0.1309, -0.0837,  0.0648, -0.0522,  0.0730, -0.0773,  0.0197,
         -0.0348, -0.0093, -0.0092, -0.0276,  0.1272,  0.0130,  0.0977,  0.0159,
          0.1493,  0.1321,  0.1252, -0.1455,  0.0145, -0.0820, -0.0605, -0.0805,
         -0.0925, -0.1310, -0.1107,  0.0677, -0.1322,  0.0963, -0.1066,  0.0941,
          0.1221,  0.0477,  0.0940,  0.1178, -0.0452, -0.1094, -0.0284, -0.0162,
          0.1219,  0.0876,  0.0226, -0.1450, -0.0526, -0.0047,  0.0988,  0.0863,
         -0.0504, -0.0532,  0.0878,  0.0458, -0.1463, -0.0107, -0.0958, -0.0423,
         -0.1052,  0.0672, -0.0099,  0.0822, -0.1440, -0.1253, -0.1076, -0.0937,
          0.0633, -0.0073, -0.0013,  0.0306,  0.1128, -0.0426,  0.0686, -0.0650,
          0.1372, -0.0469,  0.0091,  0.1271,  0.0560, -0.0080, -0.0886,  0.1417,
          0.0782,  0.0853, -0.0783, -0.1164,  0.1358, -0.1511, -0.0051, -0.1348,
          0.0776, -0.0836, -0.0759, -0.1192, -0.0678,  0.1378, -0.0264, -0.0309,
         -0.1340, -0.1271, -0.0829,  0.0947, -0.0296, -0.1087, -0.0565, -0.1095,
         -0.1436, -0.0598,  0.0669,  0.0519, -0.1203,  0.0308,  0.0488,  0.1200,
         -0.0743,  0.0317,  0.0816, -0.1142,  0.0893, -0.0259, -0.1265,  0.1411,
         -0.0729, -0.0085, -0.0127, -0.1191, -0.1124, -0.0352,  0.0508,  0.0227,
          0.0331, -0.0280, -0.1181,  0.1075,  0.1033, -0.0246,  0.1073, -0.0882,
         -0.0416,  0.1478,  0.0976, -0.0426, -0.0471,  0.0679,  0.0317, -0.1414,
          0.1375, -0.0323,  0.1282,  0.1204,  0.0879, -0.1144,  0.1098,  0.0507,
         -0.1447, -0.0029, -0.0913, -0.0994,  0.1127, -0.0457,  0.0702,  0.0194]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1118,  0.1189, -0.0164,  ...,  0.0440,  0.0239,  0.0092],
        [ 0.0471,  0.0256,  0.1131,  ..., -0.1129,  0.0092,  0.0725],
        [-0.0044, -0.1151,  0.1160,  ...,  0.0843, -0.0429, -0.0862],
        ...,
        [ 0.1159, -0.0424, -0.0754,  ..., -0.0188,  0.0299, -0.0456],
        [ 0.0146,  0.0661, -0.0794,  ..., -0.0725,  0.0230,  0.1163],
        [ 0.0028, -0.0471, -0.0054,  ...,  0.0035,  0.0533, -0.0830]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1230, -0.0075,  0.0804,  ..., -0.1454,  0.1627, -0.0438],
        [ 0.0185,  0.1206, -0.1636,  ...,  0.1285,  0.1704,  0.0575],
        [ 0.0669,  0.0230,  0.1461,  ..., -0.0602, -0.1674,  0.0382],
        ...,
        [-0.1467,  0.0810, -0.0305,  ..., -0.0965,  0.1754,  0.0839],
        [ 0.1091, -0.1518, -0.1230,  ...,  0.0616,  0.0323,  0.1139],
        [ 0.0175, -0.0412,  0.1245,  ..., -0.0589,  0.0977,  0.0621]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0413, -0.0554, -0.1230,  ...,  0.2244,  0.1599,  0.0968],
        [-0.1490,  0.0438,  0.1152,  ..., -0.1926, -0.0076, -0.2259],
        [ 0.0239, -0.0437,  0.0129,  ..., -0.0978, -0.1190, -0.2098],
        ...,
        [ 0.1798, -0.0237,  0.1615,  ..., -0.0499,  0.2032,  0.1142],
        [-0.2409,  0.1229,  0.2219,  ..., -0.0128, -0.1688, -0.1156],
        [ 0.1424,  0.1927, -0.2310,  ..., -0.2027, -0.0667, -0.0304]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2000],
        [-0.0394],
        [-0.3795],
        [ 0.0812],
        [-0.1273],
        [ 0.0877],
        [ 0.2873],
        [ 0.0500],
        [ 0.0388],
        [-0.3914],
        [ 0.2324],
        [ 0.4126],
        [ 0.0225],
        [ 0.4092],
        [-0.1553],
        [ 0.0704],
        [ 0.2619],
        [ 0.2819],
        [ 0.1787],
        [ 0.3787],
        [ 0.3684],
        [-0.1709],
        [ 0.3805],
        [ 0.1128],
        [ 0.2870],
        [ 0.3753],
        [ 0.3263],
        [-0.0097],
        [-0.0991],
        [ 0.1157],
        [ 0.3038],
        [ 0.1971]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.1451, -0.0623, -0.1057,  0.0899,  0.1185, -0.0681,  0.0827,  0.1419,
         -0.0362, -0.1390,  0.0100, -0.1389,  0.0352,  0.0159, -0.1174,  0.0425,
         -0.1146, -0.1330,  0.0598, -0.0810, -0.0830,  0.1202,  0.1511,  0.0006,
          0.0557,  0.0682,  0.0451, -0.1444, -0.0475,  0.1473, -0.0263,  0.1047,
         -0.0778,  0.0619,  0.0061, -0.1431, -0.0194,  0.0093,  0.0030, -0.1346,
          0.1047, -0.1509,  0.0751,  0.1141,  0.1088,  0.0550,  0.1071,  0.0691,
          0.0875, -0.0619,  0.0953,  0.1147, -0.1029,  0.0067,  0.0537,  0.0424,
         -0.0048,  0.0746, -0.0817, -0.0875, -0.1356, -0.1070,  0.0675,  0.0611,
         -0.0500, -0.0135, -0.1306, -0.0475, -0.0897,  0.1312,  0.0157, -0.0609,
         -0.1041,  0.0132, -0.1067,  0.0379,  0.0299, -0.1102,  0.0817,  0.1383,
          0.0916,  0.0909, -0.0636, -0.0765, -0.0312,  0.0802, -0.1334,  0.0990,
          0.0360,  0.0726, -0.0552, -0.0540,  0.0947,  0.1442, -0.0679, -0.0388,
         -0.0063, -0.1309, -0.0837,  0.0648, -0.0522,  0.0730, -0.0773,  0.0197,
         -0.0348, -0.0093, -0.0092, -0.0276,  0.1272,  0.0130,  0.0977,  0.0159,
          0.1493,  0.1321,  0.1252, -0.1455,  0.0145, -0.0820, -0.0605, -0.0805,
         -0.0925, -0.1310, -0.1107,  0.0677, -0.1322,  0.0963, -0.1066,  0.0941,
          0.1221,  0.0477,  0.0940,  0.1178, -0.0452, -0.1094, -0.0284, -0.0162,
          0.1219,  0.0876,  0.0226, -0.1450, -0.0526, -0.0047,  0.0988,  0.0863,
         -0.0504, -0.0532,  0.0878,  0.0458, -0.1463, -0.0107, -0.0958, -0.0423,
         -0.1052,  0.0672, -0.0099,  0.0822, -0.1440, -0.1253, -0.1076, -0.0937,
          0.0633, -0.0073, -0.0013,  0.0306,  0.1128, -0.0426,  0.0686, -0.0650,
          0.1372, -0.0469,  0.0091,  0.1271,  0.0560, -0.0080, -0.0886,  0.1417,
          0.0782,  0.0853, -0.0783, -0.1164,  0.1358, -0.1511, -0.0051, -0.1348,
          0.0776, -0.0836, -0.0759, -0.1192, -0.0678,  0.1378, -0.0264, -0.0309,
         -0.1340, -0.1271, -0.0829,  0.0947, -0.0296, -0.1087, -0.0565, -0.1095,
         -0.1436, -0.0598,  0.0669,  0.0519, -0.1203,  0.0308,  0.0488,  0.1200,
         -0.0743,  0.0317,  0.0816, -0.1142,  0.0893, -0.0259, -0.1265,  0.1411,
         -0.0729, -0.0085, -0.0127, -0.1191, -0.1124, -0.0352,  0.0508,  0.0227,
          0.0331, -0.0280, -0.1181,  0.1075,  0.1033, -0.0246,  0.1073, -0.0882,
         -0.0416,  0.1478,  0.0976, -0.0426, -0.0471,  0.0679,  0.0317, -0.1414,
          0.1375, -0.0323,  0.1282,  0.1204,  0.0879, -0.1144,  0.1098,  0.0507,
         -0.1447, -0.0029, -0.0913, -0.0994,  0.1127, -0.0457,  0.0702,  0.0194]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1118,  0.1189, -0.0164,  ...,  0.0440,  0.0239,  0.0092],
        [ 0.0471,  0.0256,  0.1131,  ..., -0.1129,  0.0092,  0.0725],
        [-0.0044, -0.1151,  0.1160,  ...,  0.0843, -0.0429, -0.0862],
        ...,
        [ 0.1159, -0.0424, -0.0754,  ..., -0.0188,  0.0299, -0.0456],
        [ 0.0146,  0.0661, -0.0794,  ..., -0.0725,  0.0230,  0.1163],
        [ 0.0028, -0.0471, -0.0054,  ...,  0.0035,  0.0533, -0.0830]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1230, -0.0075,  0.0804,  ..., -0.1454,  0.1627, -0.0438],
        [ 0.0185,  0.1206, -0.1636,  ...,  0.1285,  0.1704,  0.0575],
        [ 0.0669,  0.0230,  0.1461,  ..., -0.0602, -0.1674,  0.0382],
        ...,
        [-0.1467,  0.0810, -0.0305,  ..., -0.0965,  0.1754,  0.0839],
        [ 0.1091, -0.1518, -0.1230,  ...,  0.0616,  0.0323,  0.1139],
        [ 0.0175, -0.0412,  0.1245,  ..., -0.0589,  0.0977,  0.0621]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0413, -0.0554, -0.1230,  ...,  0.2244,  0.1599,  0.0968],
        [-0.1490,  0.0438,  0.1152,  ..., -0.1926, -0.0076, -0.2259],
        [ 0.0239, -0.0437,  0.0129,  ..., -0.0978, -0.1190, -0.2098],
        ...,
        [ 0.1798, -0.0237,  0.1615,  ..., -0.0499,  0.2032,  0.1142],
        [-0.2409,  0.1229,  0.2219,  ..., -0.0128, -0.1688, -0.1156],
        [ 0.1424,  0.1927, -0.2310,  ..., -0.2027, -0.0667, -0.0304]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2000],
        [-0.0394],
        [-0.3795],
        [ 0.0812],
        [-0.1273],
        [ 0.0877],
        [ 0.2873],
        [ 0.0500],
        [ 0.0388],
        [-0.3914],
        [ 0.2324],
        [ 0.4126],
        [ 0.0225],
        [ 0.4092],
        [-0.1553],
        [ 0.0704],
        [ 0.2619],
        [ 0.2819],
        [ 0.1787],
        [ 0.3787],
        [ 0.3684],
        [-0.1709],
        [ 0.3805],
        [ 0.1128],
        [ 0.2870],
        [ 0.3753],
        [ 0.3263],
        [-0.0097],
        [-0.0991],
        [ 0.1157],
        [ 0.3038],
        [ 0.1971]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-870.7832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(219.0316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-71.7778, device='cuda:0')



h[100].sum tensor(-78.8429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-222.6572, device='cuda:0')



h[200].sum tensor(-216.7706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(219.4884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0253, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160184.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0309, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(726476.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(368.5486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17848.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5.8949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10867.2100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(474.3642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[2.3179e-01],
        [2.0349e-01],
        [1.9958e-01],
        ...,
        [9.7492e-07],
        [1.2809e-07],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(85666.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-912.1051, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(229.6631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-75.1043, device='cuda:0')



h[100].sum tensor(-82.5689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-232.9763, device='cuda:0')



h[200].sum tensor(-227.2907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(229.6608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164485.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0168, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0181, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(738427.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(378.7222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18159.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5.8502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11033.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(486.5427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1077],
        [0.0864],
        [0.0912],
        ...,
        [0.0763],
        [0.0991],
        [0.1216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(87009.6328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0153, -0.0066, -0.0111,  ..., -0.0048,  0.0074,  0.0020],
        [ 0.0340, -0.0146, -0.0248,  ..., -0.0107,  0.0164,  0.0045],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-947.5225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(238.8289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-78.2993, device='cuda:0')



h[100].sum tensor(-85.7592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-242.8872, device='cuda:0')



h[200].sum tensor(-236.3602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(239.4306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1359, 0.0000, 0.0000,  ..., 0.0000, 0.0656, 0.0180],
        [0.0684, 0.0000, 0.0000,  ..., 0.0000, 0.0330, 0.0091],
        [0.0769, 0.0000, 0.0000,  ..., 0.0000, 0.0371, 0.0102],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(173669.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1152, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0879, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0810, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(777902.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(400.1648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19148.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5.9591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(11611.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(513.1188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.6581],
        [0.5692],
        [0.4966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(90159.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097462.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3881e-05,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
         -6.3881e-05,  6.3881e-05],
        [ 2.4685e-02, -1.0600e-02, -1.8006e-02,  ..., -7.7563e-03,
          1.1880e-02,  3.3617e-03],
        [-6.3881e-05,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
         -6.3881e-05,  6.3881e-05],
        ...,
        [-6.3881e-05,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
         -6.3881e-05,  6.3881e-05],
        [-6.3881e-05,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
         -6.3881e-05,  6.3881e-05],
        [-6.3881e-05,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
         -6.3881e-05,  6.3881e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-913.4468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(203.4902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-68.6206, device='cuda:0')



h[100].sum tensor(-75.2969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-212.8636, device='cuda:0')



h[200].sum tensor(-207.7784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(209.8343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1009, 0.0000, 0.0000,  ..., 0.0000, 0.0486, 0.0137],
        [0.0201, 0.0000, 0.0000,  ..., 0.0000, 0.0097, 0.0029],
        [0.0247, 0.0000, 0.0000,  ..., 0.0000, 0.0119, 0.0036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0092, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156345.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0558, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0278, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0181, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(711571.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(343.8563, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18220.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5.4842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12261.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(497.2892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1217],
        [ 0.0829],
        [ 0.0465],
        ...,
        [-0.0013],
        [ 0.0105],
        [ 0.0317]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(10331.3584, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097462.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097354.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001,  0.0000,  0.0000,  ...,  0.0000, -0.0001,  0.0001],
        [ 0.0094, -0.0041, -0.0069,  ..., -0.0030,  0.0045,  0.0014],
        [-0.0001,  0.0000,  0.0000,  ...,  0.0000, -0.0001,  0.0001],
        ...,
        [-0.0001,  0.0000,  0.0000,  ...,  0.0000, -0.0001,  0.0001],
        [-0.0001,  0.0000,  0.0000,  ...,  0.0000, -0.0001,  0.0001],
        [-0.0001,  0.0000,  0.0000,  ...,  0.0000, -0.0001,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1100.1683, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(230.8968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-79.3561, device='cuda:0')



h[100].sum tensor(-86.9007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-246.1655, device='cuda:0')



h[200].sum tensor(-240.0919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(242.6622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0479, 0.0000, 0.0000,  ..., 0.0000, 0.0229, 0.0069],
        [0.0145, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0024],
        [0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0017],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(181332.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0248, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0195, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0244, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(823605.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(388.1011, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21590.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6.1716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15377.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(600.9407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0348],
        [-0.0152],
        [ 0.0187],
        ...,
        [-0.0029],
        [-0.0029],
        [-0.0028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34391.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097354.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097246.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        ...,
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1145.9091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(226.0781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-79.5403, device='cuda:0')



h[100].sum tensor(-86.6495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-246.7367, device='cuda:0')



h[200].sum tensor(-239.6913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(243.2253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(184161.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0014,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(836997.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(384.0040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22323.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(6.0370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16677.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(634.0668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0203],
        [-0.0308],
        [-0.0471],
        ...,
        [-0.0031],
        [-0.0031],
        [-0.0031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-62741.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097246.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097139.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        ...,
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1218.5288, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(231.9027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-82.3335, device='cuda:0')



h[100].sum tensor(-89.9704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-255.4013, device='cuda:0')



h[200].sum tensor(-249.1837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(251.7666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(193750.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.8420e-03, 5.7046e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6217e-05, 1.0469e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4565e-04, 1.3305e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.6815e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6815e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6815e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(893972.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(396.9188, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24185.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5.3811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18643.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(684.4316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0913],
        [-0.0694],
        [-0.0573],
        ...,
        [-0.0039],
        [-0.0039],
        [-0.0039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-89696.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097139.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097032., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [ 0.0362, -0.0155, -0.0264,  ..., -0.0113,  0.0173,  0.0051],
        [ 0.0159, -0.0069, -0.0117,  ..., -0.0050,  0.0075,  0.0024],
        ...,
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002],
        [-0.0002,  0.0000,  0.0000,  ...,  0.0000, -0.0002,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1040.2913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(169.9938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-63.4463, device='cuda:0')



h[100].sum tensor(-68.9735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-196.8127, device='cuda:0')



h[200].sum tensor(-191.2656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(194.0118, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0893, 0.0000, 0.0000,  ..., 0.0000, 0.0427, 0.0130],
        [0.0870, 0.0000, 0.0000,  ..., 0.0000, 0.0416, 0.0127],
        [0.1279, 0.0000, 0.0000,  ..., 0.0000, 0.0611, 0.0183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(151204.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0766, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0863, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0933, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(701533.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.7542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19438.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4.1184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16229.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(575.9310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1483],
        [-0.1535],
        [-0.1628],
        ...,
        [-0.0049],
        [-0.0048],
        [-0.0047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-93113.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097032., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096924.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [ 0.0091, -0.0040, -0.0068,  ..., -0.0029,  0.0043,  0.0015],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        ...,
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1206.7742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(207.1016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-76.3140, device='cuda:0')



h[100].sum tensor(-83.1136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-236.7287, device='cuda:0')



h[200].sum tensor(-230.7614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(233.3598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0330, 0.0000, 0.0000,  ..., 0.0000, 0.0154, 0.0056],
        [0.0074, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0021],
        [0.0091, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0023],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(180924.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0022,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(833428.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.6542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23168.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.9987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19256.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(681.9178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2168],
        [-0.2187],
        [-0.2262],
        ...,
        [-0.0055],
        [-0.0055],
        [-0.0055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-116516.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096924.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096817.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0197, -0.0085, -0.0145,  ..., -0.0062,  0.0094,  0.0030],
        [ 0.0425, -0.0182, -0.0310,  ..., -0.0133,  0.0203,  0.0061],
        [ 0.0402, -0.0172, -0.0293,  ..., -0.0125,  0.0192,  0.0058],
        ...,
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1102.3160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(167.6760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-64.7265, device='cuda:0')



h[100].sum tensor(-69.9154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-200.7840, device='cuda:0')



h[200].sum tensor(-194.3573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(197.9266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1335, 0.0000, 0.0000,  ..., 0.0000, 0.0637, 0.0193],
        [0.1495, 0.0000, 0.0000,  ..., 0.0000, 0.0715, 0.0215],
        [0.1719, 0.0000, 0.0000,  ..., 0.0000, 0.0822, 0.0245],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157840.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1207, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1272, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(749571.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.1173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21047.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.8387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18299.1191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(626.6731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3164],
        [-0.3226],
        [-0.3232],
        ...,
        [-0.0117],
        [-0.0073],
        [-0.0062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-106937.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096817.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096817.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0362, -0.0155, -0.0265,  ..., -0.0113,  0.0173,  0.0052],
        [ 0.0375, -0.0161, -0.0274,  ..., -0.0117,  0.0179,  0.0054],
        [ 0.0360, -0.0154, -0.0263,  ..., -0.0112,  0.0172,  0.0052],
        ...,
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1074.5537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(159.4296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-61.7947, device='cuda:0')



h[100].sum tensor(-66.9881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-191.6895, device='cuda:0')



h[200].sum tensor(-186.2198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(188.9615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1552, 0.0000, 0.0000,  ..., 0.0000, 0.0742, 0.0223],
        [0.1310, 0.0000, 0.0000,  ..., 0.0000, 0.0625, 0.0190],
        [0.1343, 0.0000, 0.0000,  ..., 0.0000, 0.0641, 0.0194],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(148749.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1164, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1029, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0024,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(696738.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.1766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19662.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.7723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(17398.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(599.4943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2391],
        [-0.2362],
        [-0.2294],
        ...,
        [-0.0060],
        [-0.0060],
        [-0.0060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-105617.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096817.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096817.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        ...,
        [ 0.0230, -0.0099, -0.0169,  ..., -0.0072,  0.0110,  0.0034],
        [ 0.0206, -0.0089, -0.0151,  ..., -0.0065,  0.0098,  0.0031],
        [ 0.0204, -0.0088, -0.0150,  ..., -0.0064,  0.0097,  0.0031]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1130.8955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(176.1650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-67.3907, device='cuda:0')



h[100].sum tensor(-72.9288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-209.0484, device='cuda:0')



h[200].sum tensor(-202.7342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(206.0733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0342, 0.0000, 0.0000,  ..., 0.0000, 0.0163, 0.0058],
        ...,
        [0.0699, 0.0000, 0.0000,  ..., 0.0000, 0.0331, 0.0107],
        [0.0765, 0.0000, 0.0000,  ..., 0.0000, 0.0362, 0.0116],
        [0.0732, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.0112]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160397.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0019,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0012,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0236, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0419, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0485, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0501, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(745719.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.7054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20988.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.7405, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(18309.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(634.8190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0651],
        [-0.1043],
        [-0.1474],
        ...,
        [-0.1388],
        [-0.1481],
        [-0.1542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-113259.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096817.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096710., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0187, -0.0081, -0.0138,  ..., -0.0059,  0.0089,  0.0029],
        [ 0.0073, -0.0032, -0.0055,  ..., -0.0023,  0.0033,  0.0013],
        [ 0.0112, -0.0049, -0.0083,  ..., -0.0035,  0.0052,  0.0019],
        ...,
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1213.2814, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(193.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-73.7244, device='cuda:0')



h[100].sum tensor(-79.8156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-228.6958, device='cuda:0')



h[200].sum tensor(-222.1541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(225.4412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0305, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.0055],
        [0.0709, 0.0000, 0.0000,  ..., 0.0000, 0.0335, 0.0110],
        [0.0309, 0.0000, 0.0000,  ..., 0.0000, 0.0145, 0.0055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(175200.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0267, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0364, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0240, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(817377.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.4534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23114.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.3016, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20201.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(692.8643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2712],
        [-0.2555],
        [-0.2276],
        ...,
        [-0.0066],
        [-0.0066],
        [-0.0066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-123337.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096710., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096710., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        ...,
        [ 0.0115, -0.0050, -0.0085,  ..., -0.0036,  0.0054,  0.0019],
        [ 0.0108, -0.0047, -0.0080,  ..., -0.0034,  0.0050,  0.0018],
        [ 0.0217, -0.0093, -0.0159,  ..., -0.0068,  0.0103,  0.0033]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1151.6357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(174.7935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-67.3894, device='cuda:0')



h[100].sum tensor(-73.1531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-209.0444, device='cuda:0')



h[200].sum tensor(-203.6100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(206.0694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0101, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        ...,
        [0.0647, 0.0000, 0.0000,  ..., 0.0000, 0.0305, 0.0101],
        [0.0509, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.0083],
        [0.0454, 0.0000, 0.0000,  ..., 0.0000, 0.0212, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164042.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0466, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0451, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0441, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(767742.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.8897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21790.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.2146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19337.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(659.3617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1428],
        [-0.0995],
        [-0.0655],
        ...,
        [-0.2039],
        [-0.2015],
        [-0.1975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-120503.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096710., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096602.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0222, -0.0095, -0.0163,  ..., -0.0069,  0.0105,  0.0034],
        [ 0.0261, -0.0112, -0.0191,  ..., -0.0082,  0.0124,  0.0039],
        [ 0.0114, -0.0050, -0.0085,  ..., -0.0036,  0.0053,  0.0019],
        ...,
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003],
        [-0.0003,  0.0000,  0.0000,  ...,  0.0000, -0.0003,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1203.2251, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(184.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-71.4227, device='cuda:0')



h[100].sum tensor(-77.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-221.5556, device='cuda:0')



h[200].sum tensor(-215.2244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(218.4026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0837, 0.0000, 0.0000,  ..., 0.0000, 0.0396, 0.0128],
        [0.0737, 0.0000, 0.0000,  ..., 0.0000, 0.0348, 0.0115],
        [0.0780, 0.0000, 0.0000,  ..., 0.0000, 0.0369, 0.0121],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(170760.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0573, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0481, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0440, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(800321.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.6450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22707.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.7063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20250.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(690.4852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3137],
        [-0.3023],
        [-0.2940],
        ...,
        [-0.0071],
        [-0.0071],
        [-0.0071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-121746.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096602.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096495.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0434, -0.0185, -0.0316,  ..., -0.0135,  0.0207,  0.0063],
        [ 0.0076, -0.0033, -0.0057,  ..., -0.0024,  0.0035,  0.0014],
        [ 0.0103, -0.0045, -0.0077,  ..., -0.0033,  0.0048,  0.0018],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [ 0.0182, -0.0078, -0.0134,  ..., -0.0057,  0.0086,  0.0029],
        [ 0.0079, -0.0035, -0.0059,  ..., -0.0025,  0.0036,  0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1363.9385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(231.4646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-87.4655, device='cuda:0')



h[100].sum tensor(-94.3222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-271.3210, device='cuda:0')



h[200].sum tensor(-263.1847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(267.4598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0829, 0.0000, 0.0000,  ..., 0.0000, 0.0394, 0.0128],
        [0.1086, 0.0000, 0.0000,  ..., 0.0000, 0.0516, 0.0163],
        [0.0301, 0.0000, 0.0000,  ..., 0.0000, 0.0141, 0.0056],
        ...,
        [0.0311, 0.0000, 0.0000,  ..., 0.0000, 0.0146, 0.0057],
        [0.0354, 0.0000, 0.0000,  ..., 0.0000, 0.0165, 0.0063],
        [0.0575, 0.0000, 0.0000,  ..., 0.0000, 0.0270, 0.0094]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(209253.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0803, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0712, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0389, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0237, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0295, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0313, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(976717., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(397.0453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(27723.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.0475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24081.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(818.6616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4537],
        [-0.4345],
        [-0.4057],
        ...,
        [-0.1731],
        [-0.1882],
        [-0.1773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-158557.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096495.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096388.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [ 0.0076, -0.0033, -0.0057,  ..., -0.0024,  0.0035,  0.0014],
        [ 0.0076, -0.0033, -0.0057,  ..., -0.0024,  0.0035,  0.0014],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1194.9371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(172.9449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-68.9020, device='cuda:0')



h[100].sum tensor(-74.1328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-213.7364, device='cuda:0')



h[200].sum tensor(-207.1091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(210.6946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.0059],
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0034],
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167668.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0175, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0170, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0030,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(785069.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.7028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22550.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.2638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20833.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(701.5680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2253],
        [-0.2608],
        [-0.3085],
        ...,
        [-0.0082],
        [-0.0082],
        [-0.0083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-124102.5078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096388.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096280.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0473, -0.0201, -0.0344,  ..., -0.0146,  0.0226,  0.0069],
        [ 0.0155, -0.0067, -0.0115,  ..., -0.0049,  0.0073,  0.0025],
        [ 0.0592, -0.0251, -0.0430,  ..., -0.0183,  0.0283,  0.0085],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1196.0284, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(170.1718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-68.7467, device='cuda:0')



h[100].sum tensor(-73.5677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-213.2546, device='cuda:0')



h[200].sum tensor(-205.7876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(210.2197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0923, 0.0000, 0.0000,  ..., 0.0000, 0.0436, 0.0143],
        [0.2003, 0.0000, 0.0000,  ..., 0.0000, 0.0957, 0.0289],
        [0.1861, 0.0000, 0.0000,  ..., 0.0000, 0.0888, 0.0270],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165047.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0981, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1449, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1611, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(780857.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.2502, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22597.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.6859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21247.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(703.2924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5361],
        [-0.5830],
        [-0.6154],
        ...,
        [-0.0085],
        [-0.0085],
        [-0.0085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-134124.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096280.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096173.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [ 0.0070, -0.0031, -0.0054,  ..., -0.0023,  0.0032,  0.0014],
        [ 0.0249, -0.0107, -0.0183,  ..., -0.0077,  0.0118,  0.0038],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1141.3918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(148.6043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-62.1256, device='cuda:0')



h[100].sum tensor(-66.3526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-192.7159, device='cuda:0')



h[200].sum tensor(-185.8377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(189.9733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0034],
        [0.0436, 0.0000, 0.0000,  ..., 0.0000, 0.0204, 0.0077],
        [0.0663, 0.0000, 0.0000,  ..., 0.0000, 0.0311, 0.0108],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(155063.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0334, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0476, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0033,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(745892.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.3937, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21673.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.1756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20781.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(679.8389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2600],
        [-0.3287],
        [-0.3769],
        ...,
        [-0.0089],
        [-0.0089],
        [-0.0089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-125578.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096173.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096066.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [ 0.0082, -0.0036, -0.0062,  ..., -0.0026,  0.0038,  0.0016],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.6268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(154.2154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-64.1980, device='cuda:0')



h[100].sum tensor(-68.6422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-199.1446, device='cuda:0')



h[200].sum tensor(-192.4918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(196.3105, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0149, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0038],
        [0.0207, 0.0000, 0.0000,  ..., 0.0000, 0.0093, 0.0046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159919.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0025, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0017,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(767055.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.8445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22360.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.8214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21531.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(701.9882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1467],
        [-0.2028],
        [-0.2592],
        ...,
        [-0.0205],
        [-0.0443],
        [-0.0905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-130051.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096066.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095958.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0093, -0.0041, -0.0070,  ..., -0.0029,  0.0042,  0.0017],
        [ 0.0404, -0.0171, -0.0294,  ..., -0.0124,  0.0192,  0.0060],
        [ 0.0160, -0.0069, -0.0118,  ..., -0.0050,  0.0075,  0.0026],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1137.5525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(143.6499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-61.1715, device='cuda:0')



h[100].sum tensor(-65.2222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-189.7563, device='cuda:0')



h[200].sum tensor(-183.1314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(187.0558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1374, 0.0000, 0.0000,  ..., 0.0000, 0.0653, 0.0206],
        [0.0931, 0.0000, 0.0000,  ..., 0.0000, 0.0440, 0.0146],
        [0.1100, 0.0000, 0.0000,  ..., 0.0000, 0.0521, 0.0169],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(155384.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0870, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0924, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0992, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0034,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(757088.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.0632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22196.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.4390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21685.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(695.2134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6283],
        [-0.6898],
        [-0.7319],
        ...,
        [-0.0104],
        [-0.0103],
        [-0.0102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-133038.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095958.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 300 loss: tensor(0.0786, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095851.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0529, -0.0224, -0.0384,  ..., -0.0162,  0.0252,  0.0077],
        [ 0.0487, -0.0206, -0.0353,  ..., -0.0149,  0.0232,  0.0071],
        [ 0.0389, -0.0165, -0.0283,  ..., -0.0119,  0.0185,  0.0058],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1185.1801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(161.4677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-67.2522, device='cuda:0')



h[100].sum tensor(-71.6869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-208.6188, device='cuda:0')



h[200].sum tensor(-201.5369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(205.6499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.2092, 0.0000, 0.0000,  ..., 0.0000, 0.0998, 0.0304],
        [0.1947, 0.0000, 0.0000,  ..., 0.0000, 0.0928, 0.0285],
        [0.1645, 0.0000, 0.0000,  ..., 0.0000, 0.0783, 0.0243],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165549.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1724, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1636, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1482, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(786446.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(279.8871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23008.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.5721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22409.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(731.5111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0774],
        [-1.0752],
        [-1.0636],
        ...,
        [-0.0098],
        [-0.0098],
        [-0.0098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-134563.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095851.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095743.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [ 0.0199, -0.0085, -0.0146,  ..., -0.0062,  0.0093,  0.0032],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1167.5061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(154.9777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-65.3815, device='cuda:0')



h[100].sum tensor(-69.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-202.8158, device='cuda:0')



h[200].sum tensor(-195.9791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(199.9295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0272, 0.0000, 0.0000,  ..., 0.0000, 0.0126, 0.0056],
        [0.0248, 0.0000, 0.0000,  ..., 0.0000, 0.0112, 0.0053],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166755.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0034, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0181, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(815260.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.4238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23903.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.1907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23217.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(741., device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1646],
        [-0.2316],
        [-0.2983],
        ...,
        [-0.0101],
        [-0.0101],
        [-0.0101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-147002.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095743.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095636.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        ...,
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004],
        [-0.0004,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1128.8918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(139.9435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-60.7540, device='cuda:0')



h[100].sum tensor(-64.5623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-188.4610, device='cuda:0')



h[200].sum tensor(-181.9664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(185.7790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(155383.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0036,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(755470.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(253.2819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22347.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.9278, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22387.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(711.6996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0583],
        [-0.0890],
        [-0.1322],
        ...,
        [-0.0104],
        [-0.0103],
        [-0.0103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-131578.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095636.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095528.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1221.5690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(181.4508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-74.9367, device='cuda:0')



h[100].sum tensor(-79.1222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-232.4562, device='cuda:0')



h[200].sum tensor(-223.2857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(229.1481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(182997.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0018,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0017,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(872646.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.1424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25613.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.0599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24726.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(799.8131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1803],
        [-0.1602],
        [-0.1890],
        ...,
        [-0.0121],
        [-0.0124],
        [-0.0126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-154979.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095528.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095421.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0255, -0.0108, -0.0186,  ..., -0.0078,  0.0120,  0.0040],
        [ 0.0519, -0.0218, -0.0375,  ..., -0.0158,  0.0247,  0.0076],
        [ 0.0659, -0.0276, -0.0475,  ..., -0.0200,  0.0315,  0.0095],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1271.1565, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(207.4317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-83.3108, device='cuda:0')



h[100].sum tensor(-88.2212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-258.4332, device='cuda:0')



h[200].sum tensor(-249.2798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(254.7553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1246, 0.0000, 0.0000,  ..., 0.0000, 0.0590, 0.0191],
        [0.1748, 0.0000, 0.0000,  ..., 0.0000, 0.0832, 0.0259],
        [0.2212, 0.0000, 0.0000,  ..., 0.0000, 0.1055, 0.0323],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(203888.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1450, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1725, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(970961., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.1632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(28444.2012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.4637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26899.9004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(868.1451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0091],
        [-1.0970],
        [-1.1724],
        ...,
        [-0.0108],
        [-0.0108],
        [-0.0108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-177984.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095421.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095313.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0097, -0.0042, -0.0073,  ..., -0.0031,  0.0044,  0.0019],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1136.9825, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(149.6211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-64.6757, device='cuda:0')



h[100].sum tensor(-68.2859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-200.6264, device='cuda:0')



h[200].sum tensor(-193.1957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(197.7713, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0424, 0.0000, 0.0000,  ..., 0.0000, 0.0194, 0.0079],
        [0.0078, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0030],
        [0.0241, 0.0000, 0.0000,  ..., 0.0000, 0.0109, 0.0054],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165047.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0181, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(808319.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(270.4887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23924.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.6124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23891.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(753.3988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3296],
        [-0.3420],
        [-0.3486],
        ...,
        [-0.0110],
        [-0.0109],
        [-0.0109]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-145781.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095313.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095206.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0076, -0.0034, -0.0058,  ..., -0.0024,  0.0034,  0.0016],
        [ 0.0076, -0.0034, -0.0058,  ..., -0.0024,  0.0034,  0.0016],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1056.4576, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(113.6697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-53.2097, device='cuda:0')



h[100].sum tensor(-55.9513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-165.0585, device='cuda:0')



h[200].sum tensor(-158.5003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(162.7095, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0207, 0.0000, 0.0000,  ..., 0.0000, 0.0095, 0.0049],
        [0.0273, 0.0000, 0.0000,  ..., 0.0000, 0.0124, 0.0058],
        [0.0203, 0.0000, 0.0000,  ..., 0.0000, 0.0090, 0.0049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(138294.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(681311., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.0026, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20272.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.2825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21252.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(672.9046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2121],
        [-0.2388],
        [-0.2440],
        ...,
        [-0.0111],
        [-0.0111],
        [-0.0111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-116451.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095206.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095098.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0154, -0.0066, -0.0113,  ..., -0.0047,  0.0071,  0.0026],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1145.3226, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(162.0000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-69.1658, device='cuda:0')



h[100].sum tensor(-72.6782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-214.5548, device='cuda:0')



h[200].sum tensor(-206.1476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(211.5014, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0250, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.0055],
        [0.0124, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0037],
        [0.0555, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(173653.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0180, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0038,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(836964.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.4049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24664.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.8329, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24441.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(784.0971, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2354],
        [-0.2356],
        [-0.2459],
        ...,
        [-0.0113],
        [-0.0113],
        [-0.0113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-146925.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095098.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094991., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0076, -0.0033, -0.0058,  ..., -0.0024,  0.0034,  0.0016],
        [ 0.0076, -0.0033, -0.0058,  ..., -0.0024,  0.0034,  0.0016],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1107.2888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(147.0995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-64.6119, device='cuda:0')



h[100].sum tensor(-67.5840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-200.4284, device='cuda:0')



h[200].sum tensor(-191.9435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(197.5760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0039],
        [0.0137, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0039],
        [0.0331, 0.0000, 0.0000,  ..., 0.0000, 0.0152, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160359.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(771321.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.5458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23021.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.5516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23596.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(748.2117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1207],
        [-0.1431],
        [-0.1782],
        ...,
        [-0.0114],
        [-0.0114],
        [-0.0114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-136962.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094991., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094883.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0173, -0.0074, -0.0127,  ..., -0.0053,  0.0081,  0.0029],
        [ 0.0343, -0.0144, -0.0248,  ..., -0.0104,  0.0162,  0.0052],
        [ 0.0460, -0.0192, -0.0331,  ..., -0.0138,  0.0219,  0.0068],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1108.3772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(153.2833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-66.7800, device='cuda:0')



h[100].sum tensor(-69.7286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-207.1539, device='cuda:0')



h[200].sum tensor(-198.2881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(204.2058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1292, 0.0000, 0.0000,  ..., 0.0000, 0.0611, 0.0199],
        [0.1232, 0.0000, 0.0000,  ..., 0.0000, 0.0582, 0.0190],
        [0.1434, 0.0000, 0.0000,  ..., 0.0000, 0.0680, 0.0218],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167243.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0923, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0914, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0897, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0027,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(806739.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(270.8008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23839.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.1214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24009.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(769.3300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6802],
        [-0.6825],
        [-0.6505],
        ...,
        [-0.0409],
        [-0.0399],
        [-0.0583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-141079.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094883.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094883.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0175, -0.0074, -0.0128,  ..., -0.0054,  0.0082,  0.0029],
        [ 0.0305, -0.0128, -0.0221,  ..., -0.0092,  0.0144,  0.0047],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1084.0341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(139.5192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-61.8381, device='cuda:0')



h[100].sum tensor(-65.0058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-191.8239, device='cuda:0')



h[200].sum tensor(-184.8577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(189.0940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1197, 0.0000, 0.0000,  ..., 0.0000, 0.0566, 0.0186],
        [0.0635, 0.0000, 0.0000,  ..., 0.0000, 0.0298, 0.0108],
        [0.0437, 0.0000, 0.0000,  ..., 0.0000, 0.0205, 0.0081],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157214.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0585, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0460, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0298, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(761516.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(247.7465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22591.9121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.2779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23106.9434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(738.4935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4934],
        [-0.4894],
        [-0.4500],
        ...,
        [-0.0115],
        [-0.0115],
        [-0.0115]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-133226.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094883.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094775.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0232, -0.0098, -0.0169,  ..., -0.0070,  0.0109,  0.0037],
        [ 0.0404, -0.0168, -0.0291,  ..., -0.0121,  0.0192,  0.0061],
        [ 0.0442, -0.0184, -0.0318,  ..., -0.0133,  0.0210,  0.0066],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1111.4463, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(161.7300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-69.5638, device='cuda:0')



h[100].sum tensor(-72.6254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-215.7894, device='cuda:0')



h[200].sum tensor(-206.7907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(212.7184, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0933, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.0150],
        [0.1286, 0.0000, 0.0000,  ..., 0.0000, 0.0608, 0.0198],
        [0.1470, 0.0000, 0.0000,  ..., 0.0000, 0.0697, 0.0223],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(178892.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0581, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0762, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0852, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(882905.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.6786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(26030.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.3245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25678.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(808.1397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4486],
        [-0.4765],
        [-0.4745],
        ...,
        [-0.0117],
        [-0.0116],
        [-0.0116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-157817.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094775.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094668.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0556, -0.0231, -0.0399,  ..., -0.0166,  0.0265,  0.0082],
        [ 0.0630, -0.0261, -0.0452,  ..., -0.0188,  0.0301,  0.0092],
        [ 0.0446, -0.0186, -0.0321,  ..., -0.0134,  0.0212,  0.0067],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0175, -0.0074, -0.0128,  ..., -0.0053,  0.0082,  0.0030]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1054.1069, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(133.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-60.2197, device='cuda:0')



h[100].sum tensor(-62.9887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-186.8037, device='cuda:0')



h[200].sum tensor(-179.5822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(184.1452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1583, 0.0000, 0.0000,  ..., 0.0000, 0.0751, 0.0239],
        [0.1828, 0.0000, 0.0000,  ..., 0.0000, 0.0869, 0.0272],
        [0.1912, 0.0000, 0.0000,  ..., 0.0000, 0.0910, 0.0284],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0325, 0.0000, 0.0000,  ..., 0.0000, 0.0151, 0.0066],
        [0.0562, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.0099]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158323.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0952, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1148, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0295, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0457, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(782973., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.6143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23240.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.0422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23705.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(746.3162, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5404],
        [-0.6106],
        [-0.6419],
        ...,
        [-0.1966],
        [-0.2889],
        [-0.3509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-139242.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094668.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094560.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0195, -0.0082, -0.0143,  ..., -0.0059,  0.0091,  0.0032],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1046.6605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(135.6989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-61.2632, device='cuda:0')



h[100].sum tensor(-63.7047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-190.0405, device='cuda:0')



h[200].sum tensor(-181.8575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(187.3359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0268, 0.0000, 0.0000,  ..., 0.0000, 0.0124, 0.0058],
        [0.0306, 0.0000, 0.0000,  ..., 0.0000, 0.0139, 0.0064],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160095., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0040, 0.0015,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0151, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0237, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(790138.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(251.9228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23416.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.3945, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23846.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(753.0063, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2393],
        [-0.3396],
        [-0.4521],
        ...,
        [-0.0119],
        [-0.0118],
        [-0.0118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-139490.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094560.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094452.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0114, -0.0049, -0.0085,  ..., -0.0035,  0.0052,  0.0021],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1013.4996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(120.2951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-56.1463, device='cuda:0')



h[100].sum tensor(-58.4408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-174.1679, device='cuda:0')



h[200].sum tensor(-167.0459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(171.6893, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0465, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.0085],
        [0.0406, 0.0000, 0.0000,  ..., 0.0000, 0.0187, 0.0078],
        [0.0388, 0.0000, 0.0000,  ..., 0.0000, 0.0181, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0136, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(146561.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0506, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0460, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0404, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0020,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(720373.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(220.4409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21459.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.7702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22517.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(713.1999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6610],
        [-0.6773],
        [-0.6718],
        ...,
        [-0.0442],
        [-0.0910],
        [-0.1643]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-123995.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094452.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094345., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0176, -0.0074, -0.0129,  ..., -0.0053,  0.0082,  0.0030],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1105.0231, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(197.1485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-81.7034, device='cuda:0')



h[100].sum tensor(-84.5336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-253.4468, device='cuda:0')



h[200].sum tensor(-241.9410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(249.8399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0176, 0.0000, 0.0000,  ..., 0.0000, 0.0082, 0.0045],
        [0.0142, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0040],
        [0.0636, 0.0000, 0.0000,  ..., 0.0000, 0.0295, 0.0110],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(199649.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(951662.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.7527, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(28162.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.4606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(27498.8789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(879.3394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2229],
        [-0.2023],
        [-0.1919],
        ...,
        [-0.0120],
        [-0.0120],
        [-0.0120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-176382.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094345., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094237.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0218, -0.0091, -0.0158,  ..., -0.0066,  0.0102,  0.0036],
        [ 0.0320, -0.0133, -0.0230,  ..., -0.0095,  0.0151,  0.0050],
        [ 0.0810, -0.0333, -0.0578,  ..., -0.0239,  0.0387,  0.0117],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-993.5610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(121.4825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-56.8221, device='cuda:0')



h[100].sum tensor(-58.7959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-176.2641, device='cuda:0')



h[200].sum tensor(-168.4955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(173.7556, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0739, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.0123],
        [0.1793, 0.0000, 0.0000,  ..., 0.0000, 0.0852, 0.0268],
        [0.2072, 0.0000, 0.0000,  ..., 0.0000, 0.0986, 0.0306],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(146104.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0809, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1193, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1460, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(718573.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(218.5044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21500.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.5359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22828.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(716.2117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9603],
        [-1.0271],
        [-1.0864],
        ...,
        [-0.0121],
        [-0.0121],
        [-0.0120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-125452.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094237.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094129.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0117, -0.0050, -0.0087,  ..., -0.0036,  0.0054,  0.0022],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1011.2275, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(146.7435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-65.3405, device='cuda:0')



h[100].sum tensor(-67.3075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-202.6885, device='cuda:0')



h[200].sum tensor(-193.1379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(199.8040, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0034],
        [0.0117, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166720.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(819434., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(265.1701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24464.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.8188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25042.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(781.5654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2075],
        [-0.2218],
        [-0.2492],
        ...,
        [-0.0124],
        [-0.0124],
        [-0.0123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-148898.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094129.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094021.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0086, -0.0037, -0.0064,  ..., -0.0027,  0.0039,  0.0018],
        [ 0.0223, -0.0093, -0.0162,  ..., -0.0067,  0.0105,  0.0036],
        [ 0.0314, -0.0130, -0.0226,  ..., -0.0093,  0.0149,  0.0049],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1005.9984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(154.9801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-68.2571, device='cuda:0')



h[100].sum tensor(-70.0278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-211.7361, device='cuda:0')



h[200].sum tensor(-201.2046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(208.7228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0590, 0.0000, 0.0000,  ..., 0.0000, 0.0273, 0.0104],
        [0.0716, 0.0000, 0.0000,  ..., 0.0000, 0.0334, 0.0121],
        [0.1258, 0.0000, 0.0000,  ..., 0.0000, 0.0594, 0.0195],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(172355.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0481, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0651, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0904, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(838653.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.3854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24989.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.8552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25413.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(799.5188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6904],
        [-0.8070],
        [-0.9126],
        ...,
        [-0.0125],
        [-0.0124],
        [-0.0123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-151869.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094021.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093914., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0231, -0.0096, -0.0167,  ..., -0.0069,  0.0109,  0.0038],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0182, -0.0076, -0.0132,  ..., -0.0055,  0.0085,  0.0031],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-951.7145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(114.7850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-55.1025, device='cuda:0')



h[100].sum tensor(-56.4053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-170.9301, device='cuda:0')



h[200].sum tensor(-162.2750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(168.4975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0187, 0.0000, 0.0000,  ..., 0.0000, 0.0087, 0.0047],
        [0.0558, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.0099],
        [0.0547, 0.0000, 0.0000,  ..., 0.0000, 0.0255, 0.0098],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(142728.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0280, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0457, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0564, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(704457.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.7908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20894.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4.8080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22140.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(705.8857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7811],
        [-0.8025],
        [-0.8255],
        ...,
        [-0.0123],
        [-0.0122],
        [-0.0122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-118734.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093914., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 600 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093806.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0074, -0.0032, -0.0056,  ..., -0.0023,  0.0033,  0.0016],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-978.2399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(157.8930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-69.2525, device='cuda:0')



h[100].sum tensor(-70.8540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-214.8236, device='cuda:0')



h[200].sum tensor(-204.1084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(211.7663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0354, 0.0000, 0.0000,  ..., 0.0000, 0.0162, 0.0071],
        [0.0205, 0.0000, 0.0000,  ..., 0.0000, 0.0093, 0.0050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(179550.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0230, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(886750.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.7409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(26146.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3.0383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26052.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(821.3537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2918],
        [-0.2681],
        [-0.2353],
        ...,
        [-0.0123],
        [-0.0123],
        [-0.0123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-156043.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093806.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093806.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0534, -0.0219, -0.0381,  ..., -0.0157,  0.0254,  0.0079],
        [ 0.0590, -0.0242, -0.0420,  ..., -0.0173,  0.0281,  0.0087],
        [ 0.0649, -0.0266, -0.0462,  ..., -0.0190,  0.0309,  0.0095],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-975.5913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(154.9146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-68.2429, device='cuda:0')



h[100].sum tensor(-69.8520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-211.6918, device='cuda:0')



h[200].sum tensor(-201.2221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(208.6791, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1509, 0.0000, 0.0000,  ..., 0.0000, 0.0715, 0.0230],
        [0.2083, 0.0000, 0.0000,  ..., 0.0000, 0.0991, 0.0308],
        [0.1831, 0.0000, 0.0000,  ..., 0.0000, 0.0870, 0.0274],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167627.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1150, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1278, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1175, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(807566.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(265.9784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24179.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.2285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25011.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(787.8687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8316],
        [-0.8317],
        [-0.8054],
        ...,
        [-0.0123],
        [-0.0123],
        [-0.0123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-146381.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093806.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093698.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-932.0761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(120.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-56.6577, device='cuda:0')



h[100].sum tensor(-58.0638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-175.7543, device='cuda:0')



h[200].sum tensor(-167.4818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(173.2531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0193, 0.0000, 0.0000,  ..., 0.0000, 0.0090, 0.0048],
        [0.0596, 0.0000, 0.0000,  ..., 0.0000, 0.0281, 0.0104],
        ...,
        [0.0278, 0.0000, 0.0000,  ..., 0.0000, 0.0128, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(148527.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0091, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0264, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0538, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0236, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0009,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0020,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(734658.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.6140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21984.5371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.9053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23276.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(728.2899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3252],
        [-0.4558],
        [-0.5876],
        ...,
        [-0.2765],
        [-0.1881],
        [-0.1069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-129295.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093698.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093590.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0332, -0.0137, -0.0238,  ..., -0.0098,  0.0157,  0.0051],
        [ 0.0169, -0.0071, -0.0123,  ..., -0.0051,  0.0079,  0.0029],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-930.5761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(135.8039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-61.9530, device='cuda:0')



h[100].sum tensor(-63.2731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-192.1803, device='cuda:0')



h[200].sum tensor(-182.7462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(189.4453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1416, 0.0000, 0.0000,  ..., 0.0000, 0.0670, 0.0217],
        [0.0591, 0.0000, 0.0000,  ..., 0.0000, 0.0279, 0.0103],
        [0.0539, 0.0000, 0.0000,  ..., 0.0000, 0.0251, 0.0097],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157924.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0891, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0587, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0487, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(771808.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(242.7938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23044.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.6741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24049.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(757.7498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8824],
        [-0.7957],
        [-0.7223],
        ...,
        [-0.0124],
        [-0.0124],
        [-0.0123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-136479.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093590.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093482.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0391, -0.0160, -0.0279,  ..., -0.0114,  0.0185,  0.0059],
        [ 0.0364, -0.0149, -0.0260,  ..., -0.0107,  0.0172,  0.0056],
        [ 0.0539, -0.0220, -0.0384,  ..., -0.0157,  0.0257,  0.0080],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-931.0428, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(159.3755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-69.7988, device='cuda:0')



h[100].sum tensor(-71.0729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-216.5182, device='cuda:0')



h[200].sum tensor(-205.5422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(213.4369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0964, 0.0000, 0.0000,  ..., 0.0000, 0.0453, 0.0156],
        [0.1956, 0.0000, 0.0000,  ..., 0.0000, 0.0930, 0.0291],
        [0.1868, 0.0000, 0.0000,  ..., 0.0000, 0.0887, 0.0279],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(174797.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0858, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1204, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1259, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(841151.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.7249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24943.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.9434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25321.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(809.2201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9990],
        [-1.0598],
        [-1.1066],
        ...,
        [-0.0144],
        [-0.0223],
        [-0.0439]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-147391.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093482.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093374.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-892.7323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(119.1788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-56.5721, device='cuda:0')



h[100].sum tensor(-57.5567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-175.4886, device='cuda:0')



h[200].sum tensor(-166.6715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(172.9911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(149550.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0040,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(744180.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(223.2833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22316.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.2898, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23641.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(733.9255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1703],
        [-0.1591],
        [-0.1470],
        ...,
        [-0.0124],
        [-0.0124],
        [-0.0124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-131568.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093374.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093266.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [ 0.0070, -0.0030, -0.0053,  ..., -0.0022,  0.0031,  0.0015],
        ...,
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0005,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-900.9272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(166.3634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-72.4567, device='cuda:0')



h[100].sum tensor(-73.1923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-224.7634, device='cuda:0')



h[200].sum tensor(-212.2265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(221.5647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0125, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0039],
        [0.0271, 0.0000, 0.0000,  ..., 0.0000, 0.0122, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(179596.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0185, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0039,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(865543.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.1493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25749.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.1562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26019.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(826.0557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3593],
        [-0.4167],
        [-0.4953],
        ...,
        [-0.0125],
        [-0.0125],
        [-0.0124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-157533.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093266.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093158.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        ...,
        [ 0.0127, -0.0054, -0.0094,  ..., -0.0038,  0.0059,  0.0024],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-900.3408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(119.5941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-58.8047, device='cuda:0')



h[100].sum tensor(-59.4178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-182.4144, device='cuda:0')



h[200].sum tensor(-172.5127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(179.8184, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        ...,
        [0.0247, 0.0000, 0.0000,  ..., 0.0000, 0.0114, 0.0057],
        [0.0127, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(152725.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 4.5453e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2383e-03, 2.5079e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.9723e-03, 1.2384e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.2345e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.6510e-03, 1.1180e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.3828e-04, 2.2557e-03,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(754480., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.8306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23434.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(38.1895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23427.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(752.0541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0341],
        [-0.0582],
        [-0.0898],
        ...,
        [-0.1072],
        [-0.0757],
        [-0.0455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-120272.0859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093158.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093050.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        ...,
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0006,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-920.1921, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(131.2216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-64.4283, device='cuda:0')



h[100].sum tensor(-64.8999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-199.8588, device='cuda:0')



h[200].sum tensor(-188.6772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(197.0146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0394, 0.0000, 0.0000,  ..., 0.0000, 0.0186, 0.0078],
        [0.0535, 0.0000, 0.0000,  ..., 0.0000, 0.0254, 0.0098],
        [0.0391, 0.0000, 0.0000,  ..., 0.0000, 0.0185, 0.0078],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163314.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0498, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0529, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0430, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0050,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0050,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0050,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(801017.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.0779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25524.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(69.5999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(24063.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(793.2085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0384],
        [-0.9938],
        [-0.9350],
        ...,
        [-0.0096],
        [-0.0096],
        [-0.0096]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-115793.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093050.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092942.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        ...,
        [-0.0007,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0007,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-926.4955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(103.7057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-56.7220, device='cuda:0')



h[100].sum tensor(-57.2341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-175.9537, device='cuda:0')



h[200].sum tensor(-166.6103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(173.4496, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(144130.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0047,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0035,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0021,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0055,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0055,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0055,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(697537.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.7236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23113.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.0997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21568.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(740.1269, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0694],
        [-0.0864],
        [-0.1104],
        ...,
        [-0.0069],
        [-0.0070],
        [-0.0072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-80108.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092942.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 750 loss: tensor(0.0830, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092835., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0335, -0.0137, -0.0240,  ..., -0.0098,  0.0160,  0.0053],
        [ 0.0641, -0.0260, -0.0454,  ..., -0.0185,  0.0308,  0.0095],
        [ 0.0472, -0.0192, -0.0336,  ..., -0.0137,  0.0226,  0.0072],
        ...,
        [ 0.0284, -0.0117, -0.0204,  ..., -0.0083,  0.0136,  0.0046],
        [ 0.0094, -0.0041, -0.0071,  ..., -0.0029,  0.0044,  0.0020],
        [ 0.0090, -0.0039, -0.0068,  ..., -0.0028,  0.0042,  0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-940.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(127.0767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-65.7747, device='cuda:0')



h[100].sum tensor(-66.2852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-204.0355, device='cuda:0')



h[200].sum tensor(-193.2130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(201.1318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.2124, 0.0000, 0.0000,  ..., 0.0000, 0.1019, 0.0320],
        [0.1785, 0.0000, 0.0000,  ..., 0.0000, 0.0856, 0.0273],
        [0.2039, 0.0000, 0.0000,  ..., 0.0000, 0.0978, 0.0308],
        ...,
        [0.0473, 0.0000, 0.0000,  ..., 0.0000, 0.0223, 0.0092],
        [0.0673, 0.0000, 0.0000,  ..., 0.0000, 0.0320, 0.0120],
        [0.0248, 0.0000, 0.0000,  ..., 0.0000, 0.0117, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165272.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1412, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1360, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1346, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0397, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0330, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(794335.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.2732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(26597.5371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.8063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23251.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(812.3419, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9014],
        [-0.9244],
        [-0.9327],
        ...,
        [-0.3165],
        [-0.2365],
        [-0.1509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-85283.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092835., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092727.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0323, -0.0132, -0.0232,  ..., -0.0094,  0.0155,  0.0052],
        [ 0.0372, -0.0152, -0.0266,  ..., -0.0108,  0.0179,  0.0058],
        [ 0.0171, -0.0071, -0.0125,  ..., -0.0051,  0.0081,  0.0031],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-946.9713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(162.8392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-78.8623, device='cuda:0')



h[100].sum tensor(-79.2663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-244.6337, device='cuda:0')



h[200].sum tensor(-231.3565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(241.1523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1613, 0.0000, 0.0000,  ..., 0.0000, 0.0774, 0.0250],
        [0.1438, 0.0000, 0.0000,  ..., 0.0000, 0.0690, 0.0226],
        [0.1424, 0.0000, 0.0000,  ..., 0.0000, 0.0683, 0.0224],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(204498.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0934, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0959, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0949, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1003617., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.0213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(33224.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.5901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26795.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(936.9270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6930],
        [-0.7042],
        [-0.6845],
        ...,
        [-0.0036],
        [-0.0037],
        [-0.0037]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-108809.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092727.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092619.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0005,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-949.3181, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(120.2447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-66.2838, device='cuda:0')



h[100].sum tensor(-66.2523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-205.6148, device='cuda:0')



h[200].sum tensor(-193.6280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(202.6886, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167783.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0068,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(815778.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(269.1839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(28188.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(173.1692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22951.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(829.5471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0006],
        [-0.0033],
        [-0.0078],
        ...,
        [-0.0013],
        [-0.0013],
        [-0.0014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-69058.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092619.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092511.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0080, -0.0035, -0.0062,  ..., -0.0025,  0.0038,  0.0018],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        ...,
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [-0.0008,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-952.2667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(107.9632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-63.1580, device='cuda:0')



h[100].sum tensor(-63.1514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-195.9182, device='cuda:0')



h[200].sum tensor(-184.8099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(193.1300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0294, 0.0000, 0.0000,  ..., 0.0000, 0.0140, 0.0069],
        [0.0144, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160220.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0167, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0031,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(778987.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(253.7010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(27699.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(191.2250, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(22306.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(814.6305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2342],
        [-0.1644],
        [-0.1059],
        ...,
        [ 0.0006],
        [ 0.0006],
        [ 0.0006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-53559.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092511.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092404.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0276, -0.0113, -0.0199,  ..., -0.0080,  0.0133,  0.0046],
        [ 0.0100, -0.0043, -0.0076,  ..., -0.0031,  0.0048,  0.0021],
        [ 0.0276, -0.0113, -0.0199,  ..., -0.0080,  0.0133,  0.0046],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-956.0649, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(91.3934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-58.4824, device='cuda:0')



h[100].sum tensor(-58.5552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-181.4145, device='cuda:0')



h[200].sum tensor(-171.5868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(178.8327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0589, 0.0000, 0.0000,  ..., 0.0000, 0.0283, 0.0111],
        [0.1081, 0.0000, 0.0000,  ..., 0.0000, 0.0521, 0.0179],
        [0.0322, 0.0000, 0.0000,  ..., 0.0000, 0.0155, 0.0072],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(151882.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0319, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0403, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0245, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0073,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0073,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0073,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(745435.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(233.8824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(26830.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(212.0781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21083.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(789.2759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2805],
        [-0.2839],
        [-0.2593],
        ...,
        [ 0.0025],
        [ 0.0023],
        [ 0.0022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-41156.6328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092404.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092296.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [ 0.0103, -0.0045, -0.0078,  ..., -0.0032,  0.0050,  0.0022],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-942.0531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(135.4868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-73.9202, device='cuda:0')



h[100].sum tensor(-73.7747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-229.3032, device='cuda:0')



h[200].sum tensor(-216.4722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(226.0399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0441, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.0091],
        [0.0215, 0.0000, 0.0000,  ..., 0.0000, 0.0104, 0.0058],
        [0.0165, 0.0000, 0.0000,  ..., 0.0000, 0.0080, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(184410.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(878586., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.3967, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(31223.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(227.3035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23490.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(894.9542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1312],
        [-0.1168],
        [-0.0982],
        ...,
        [ 0.0048],
        [ 0.0048],
        [ 0.0047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-46804.0430, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092296.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092189., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0123, -0.0052, -0.0092,  ..., -0.0037,  0.0060,  0.0025],
        [ 0.0085, -0.0037, -0.0066,  ..., -0.0027,  0.0041,  0.0020],
        [ 0.0085, -0.0037, -0.0066,  ..., -0.0027,  0.0041,  0.0020],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-931.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(146.4978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-78.5221, device='cuda:0')



h[100].sum tensor(-78.0309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-243.5784, device='cuda:0')



h[200].sum tensor(-229.2654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(240.1120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0691, 0.0000, 0.0000,  ..., 0.0000, 0.0334, 0.0126],
        [0.0575, 0.0000, 0.0000,  ..., 0.0000, 0.0278, 0.0110],
        [0.0261, 0.0000, 0.0000,  ..., 0.0000, 0.0127, 0.0066],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(199226.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0436, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0331, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0184, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(969489.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.5921, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34340.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(242.1861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(25004.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(944.1320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2853],
        [-0.2221],
        [-0.1585],
        ...,
        [ 0.0064],
        [ 0.0063],
        [ 0.0063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-56301.9492, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092189., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092081.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [ 0.0088, -0.0039, -0.0068,  ..., -0.0027,  0.0043,  0.0020],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-948.2419, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(94.2240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-61.9167, device='cuda:0')



h[100].sum tensor(-61.5278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-192.0677, device='cuda:0')



h[200].sum tensor(-181.0179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(189.3344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0087, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0040],
        [0.0449, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(161959.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0273, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0080,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0080,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0080,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(797856.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.7526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29287.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(256.9342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21529.0020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(832.1794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1885],
        [-0.2110],
        [-0.2343],
        ...,
        [ 0.0077],
        [ 0.0076],
        [ 0.0075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-26258.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092081.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091973.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0183, -0.0076, -0.0134,  ..., -0.0054,  0.0089,  0.0033],
        [ 0.0194, -0.0081, -0.0142,  ..., -0.0057,  0.0094,  0.0035],
        [ 0.0236, -0.0097, -0.0171,  ..., -0.0069,  0.0114,  0.0041],
        ...,
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0009,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-942.7906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(94.2071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-62.7820, device='cuda:0')



h[100].sum tensor(-62.0554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-194.7520, device='cuda:0')



h[200].sum tensor(-182.8139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(191.9804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0390, 0.0000, 0.0000,  ..., 0.0000, 0.0190, 0.0084],
        [0.0779, 0.0000, 0.0000,  ..., 0.0000, 0.0378, 0.0139],
        [0.0876, 0.0000, 0.0000,  ..., 0.0000, 0.0425, 0.0153],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160603.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0259, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0350, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0367, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0082,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0082,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(783014.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(257.2288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29125.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(269.2653, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21123.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(831.1323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1320],
        [-0.1432],
        [-0.1473],
        ...,
        [ 0.0089],
        [ 0.0088],
        [ 0.0088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-19095.5117, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091973.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091866.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-936.3563, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(93.7492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-63.0481, device='cuda:0')



h[100].sum tensor(-62.3768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-195.5775, device='cuda:0')



h[200].sum tensor(-184.0063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(192.7941, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(162566.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0044,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0072,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(798770.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.8293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29905.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(280.9693, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21263.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(839.0135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1323],
        [-0.0853],
        [-0.0604],
        ...,
        [ 0.0098],
        [ 0.0098],
        [ 0.0097]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-16892.0371, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091866.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 900 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091758.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-924.1447, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(99.7465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-65.6013, device='cuda:0')



h[100].sum tensor(-64.7381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-203.4977, device='cuda:0')



h[200].sum tensor(-191.2271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(200.6016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167371.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0085,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0086,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0029,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0052,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0086,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(817944.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(272.2098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30455.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(293.1882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21286.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(853.7689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0018],
        [ 0.0064],
        [ 0.0074],
        ...,
        [-0.0773],
        [-0.0323],
        [-0.0055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-13651.0215, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091758.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091651.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-926.3567, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(85.5501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-61.6064, device='cuda:0')



h[100].sum tensor(-60.4952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-191.1053, device='cuda:0')



h[200].sum tensor(-178.9337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(188.3856, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157252.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0047,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(773839.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.1263, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29484.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(301.7858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20549.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(827.1669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0379],
        [-0.0254],
        [-0.0239],
        ...,
        [ 0.0112],
        [ 0.0111],
        [ 0.0111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-4818.0503, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091651.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091543.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0340, -0.0138, -0.0243,  ..., -0.0097,  0.0165,  0.0055],
        [ 0.0676, -0.0270, -0.0477,  ..., -0.0191,  0.0328,  0.0102],
        [ 0.0680, -0.0272, -0.0480,  ..., -0.0192,  0.0330,  0.0103],
        ...,
        [ 0.0171, -0.0071, -0.0126,  ..., -0.0050,  0.0084,  0.0032],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-912.4500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(90.9288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-63.6557, device='cuda:0')



h[100].sum tensor(-62.5553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-197.4621, device='cuda:0')



h[200].sum tensor(-185.2754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(194.6519, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1537, 0.0000, 0.0000,  ..., 0.0000, 0.0747, 0.0246],
        [0.1993, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.0309],
        [0.2581, 0.0000, 0.0000,  ..., 0.0000, 0.1252, 0.0391],
        ...,
        [0.0348, 0.0000, 0.0000,  ..., 0.0000, 0.0171, 0.0080],
        [0.0300, 0.0000, 0.0000,  ..., 0.0000, 0.0147, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163508.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0900, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1218, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1470, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0314, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0256, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(804356.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.5449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30518.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(312.0728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(20956.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(847.7178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5743],
        [-0.6087],
        [-0.6207],
        ...,
        [-0.2449],
        [-0.2246],
        [-0.1937]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-5319.8657, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091543.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091436.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [ 0.0109, -0.0047, -0.0083,  ..., -0.0033,  0.0054,  0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-926.4474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(66.1821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-55.8426, device='cuda:0')



h[100].sum tensor(-54.8407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-173.2256, device='cuda:0')



h[200].sum tensor(-162.6445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(170.7604, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0108, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0044],
        [0.0194, 0.0000, 0.0000,  ..., 0.0000, 0.0096, 0.0057]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(146107.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0054,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0022,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(729745.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(222.3887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(28172.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(323.3584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(19315.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(793.9960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.5433e-03],
        [ 6.4572e-06],
        [ 6.5751e-03],
        ...,
        [ 8.8053e-04],
        [-1.0824e-02],
        [-2.6866e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(5026.1699, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091436.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091328.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0569, -0.0227, -0.0402,  ..., -0.0160,  0.0276,  0.0087],
        [ 0.0541, -0.0216, -0.0382,  ..., -0.0153,  0.0263,  0.0083],
        [ 0.0774, -0.0308, -0.0544,  ..., -0.0217,  0.0376,  0.0116],
        ...,
        [ 0.0218, -0.0089, -0.0158,  ..., -0.0063,  0.0106,  0.0039],
        [ 0.0104, -0.0045, -0.0079,  ..., -0.0032,  0.0051,  0.0023],
        [ 0.0120, -0.0051, -0.0091,  ..., -0.0036,  0.0059,  0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-871.8745, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(108.8853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-70.4953, device='cuda:0')



h[100].sum tensor(-68.8508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-218.6788, device='cuda:0')



h[200].sum tensor(-204.4699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(215.5667, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.2231, 0.0000, 0.0000,  ..., 0.0000, 0.1084, 0.0343],
        [0.2813, 0.0000, 0.0000,  ..., 0.0000, 0.1365, 0.0423],
        [0.1592, 0.0000, 0.0000,  ..., 0.0000, 0.0774, 0.0254],
        ...,
        [0.0651, 0.0000, 0.0000,  ..., 0.0000, 0.0320, 0.0124],
        [0.0620, 0.0000, 0.0000,  ..., 0.0000, 0.0305, 0.0119],
        [0.0278, 0.0000, 0.0000,  ..., 0.0000, 0.0138, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(176711.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1364, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1521, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.1154, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0395, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0370, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0295, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(860949.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.8848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32693.9473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(329.2169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(21923.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(893.5797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6150],
        [-0.6039],
        [-0.5429],
        ...,
        [-0.2194],
        [-0.2039],
        [-0.1872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-4575.8965, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091328.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091328.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062, -0.0028, -0.0050,  ..., -0.0020,  0.0031,  0.0017],
        [ 0.0253, -0.0103, -0.0183,  ..., -0.0073,  0.0124,  0.0043],
        [ 0.0095, -0.0041, -0.0073,  ..., -0.0029,  0.0047,  0.0022],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-958.6945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-43.2803, device='cuda:0')



h[100].sum tensor(-42.6479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-134.2572, device='cuda:0')



h[200].sum tensor(-126.6537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(132.3465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0927, 0.0000, 0.0000,  ..., 0.0000, 0.0453, 0.0162],
        [0.0423, 0.0000, 0.0000,  ..., 0.0000, 0.0209, 0.0092],
        [0.0443, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.0093],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(121024.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0341, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0276, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0259, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0091,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(632324.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.6744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24595.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(338.7172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16793.0488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(709.1271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1388],
        [-0.1414],
        [-0.1386],
        ...,
        [ 0.0125],
        [ 0.0124],
        [ 0.0123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(15001.5244, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091328.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091221.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1105.8999, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-105.8248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31458.9473, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(279915.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.3272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12140.7822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(361.5402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8836.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(415.4303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0267],
        [0.0271],
        [0.0277],
        ...,
        [0.0128],
        [0.0127],
        [0.0127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(44801.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091221.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091113.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0010,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1111.2510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-106.7452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31496.2715, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(281011.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.0567, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12223.6455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(369.4856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8804.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(416.6718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0272],
        [0.0276],
        [0.0283],
        ...,
        [0.0131],
        [0.0130],
        [0.0130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(45676.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091113.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091006.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1116.1058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-107.5800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31529.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(281997.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.8109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12298.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(376.6899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8774.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(417.7942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0277],
        [0.0281],
        [0.0288],
        ...,
        [0.0134],
        [0.0133],
        [0.0132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(46465.5391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091006.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090898.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1120.5105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-108.3376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31560., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(282884.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.5875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12366.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(383.2209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8747.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(418.8083, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0282],
        [0.0286],
        [0.0292],
        ...,
        [0.0136],
        [0.0135],
        [0.0135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(47176.8477, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090898.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 1050 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090791.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1124.5065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-109.0246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31587.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(283681.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.3844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12427.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(389.1408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8722.8145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(419.7243, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0286],
        [0.0290],
        [0.0297],
        ...,
        [0.0139],
        [0.0137],
        [0.0137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(47817.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090791.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090683.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1128.1301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-109.6479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31612.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(284387.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.2891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12483.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(394.6196, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8699.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(420.6189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0289],
        [0.0293],
        [0.0300],
        ...,
        [0.0141],
        [0.0139],
        [0.0139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(48378.0859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090683.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090576.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1131.4170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-110.2132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31639.9941, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(284996.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.5428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12537.6533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(400.0177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8673.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(421.6854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0292],
        [0.0296],
        [0.0303],
        ...,
        [0.0142],
        [0.0141],
        [0.0141]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(48804.6445, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090576.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090468.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1134.3979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-110.7258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31664.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(285542.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.7723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12586.2793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(404.9084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8650.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(422.6484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0294],
        [0.0298],
        [0.0305],
        ...,
        [0.0143],
        [0.0142],
        [0.0142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(49182.0977, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090468.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090361., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1137.1006, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-111.1906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31686.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(286034.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.9799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12630.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(409.3386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8628.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(423.5176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0296],
        [0.0300],
        [0.0307],
        ...,
        [0.0145],
        [0.0143],
        [0.0143]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(49512.4414, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090361., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090253.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1139.5503, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-111.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31706.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0098,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(286473.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.1675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12669.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(413.3506, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8609.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(424.3017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0298],
        [0.0302],
        [0.0309],
        ...,
        [0.0146],
        [0.0145],
        [0.0144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(49803.9570, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090253.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090146., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1141.7725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-111.9940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31724.6309, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(286863., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.2566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12705.8193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(416.9551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8591.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(425.0542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0300],
        [0.0304],
        [0.0311],
        ...,
        [0.0147],
        [0.0146],
        [0.0145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(50067.2773, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090146., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090038.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1143.7864, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-112.3403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31743.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287200.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.0841, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12740.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(420.1295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8575.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(425.8760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0302],
        [0.0306],
        [0.0313],
        ...,
        [0.0148],
        [0.0147],
        [0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(50313.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090038.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089931., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1145.6115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-112.6543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31760.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287499.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.9272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12770.8291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(423.0026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8560.4854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(426.6169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0303],
        [0.0307],
        [0.0314],
        ...,
        [0.0149],
        [0.0147],
        [0.0147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(50532.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089931., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089823.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1147.2659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-112.9387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31775.5254, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287764.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.7845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12798.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(425.6019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8546.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(427.2843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0304],
        [0.0309],
        [0.0316],
        ...,
        [0.0149],
        [0.0148],
        [0.0148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(50722.0742, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089823.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 1200 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089715.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1148.7649, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-113.1966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31788.8379, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287996.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.6547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12823.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(427.9531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8534.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(427.8850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0306],
        [0.0310],
        [0.0317],
        ...,
        [0.0150],
        [0.0149],
        [0.0148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(50889.4102, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089715.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089608.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1150.1233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-113.4302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31800.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288199.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.5366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12845.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(430.0794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8523.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(428.4254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0307],
        [0.0311],
        [0.0318],
        ...,
        [0.0151],
        [0.0150],
        [0.0149]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51038.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089608.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089500.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1151.3546, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-113.6420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31810.9707, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288375.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.4292, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12865.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(432.0017, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8512.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(428.9108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0308],
        [0.0312],
        [0.0319],
        ...,
        [0.0151],
        [0.0150],
        [0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51169.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089500.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089393.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1152.4697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-113.8338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31820.0645, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288527.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.3313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12883.4189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(433.7389, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8502.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(429.3466, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0309],
        [0.0313],
        [0.0320],
        ...,
        [0.0152],
        [0.0151],
        [0.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51285.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089393.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089285.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1153.4805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.0076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31828.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288657.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.2421, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12899.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(435.3084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8493.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(429.7373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0309],
        [0.0314],
        [0.0321],
        ...,
        [0.0152],
        [0.0151],
        [0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51387.7617, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089285.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089178., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1154.3959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.1649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31834.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288767.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.1608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12913.5527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(436.7260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8485.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.0873, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0310],
        [0.0314],
        [0.0322],
        ...,
        [0.0153],
        [0.0151],
        [0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51477.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089178., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089070.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1155.2252, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.3076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31840.8457, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288859.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.0867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12926.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(438.0056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8478.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.4001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0315],
        [0.0322],
        ...,
        [0.0153],
        [0.0152],
        [0.0151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51556.6992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089070.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088962.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1155.9753, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.4368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31845.9355, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288934.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.0191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12937.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(439.1603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8471.1143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.6793, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0316],
        [0.0323],
        ...,
        [0.0153],
        [0.0152],
        [0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51625.5664, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088962.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088855.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1156.6558, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.5537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31850.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288995.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.9574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12947.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(440.2018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8464.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.9282, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0316],
        [0.0323],
        ...,
        [0.0154],
        [0.0152],
        [0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51685.3320, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088855.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088747.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1157.2717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.6597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31853.8496, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289043.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.9010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12956.0518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(441.1407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8458.5371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.1495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0316],
        [0.0324],
        ...,
        [0.0154],
        [0.0153],
        [0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51736.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088747.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 1350 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088639.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1157.8301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.7555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31856.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289078.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.8495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12963.7402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(441.9867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8452.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.3459, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0317],
        [0.0324],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51781.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088639.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088532.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1158.3347, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.8424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31859.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289102.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.8022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12970.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(442.7483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8447.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.5195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0317],
        [0.0325],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51818.6484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088532.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088424.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1158.7920, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.9210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31861.0645, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289117.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.7591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12976.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(443.4337, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8442.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.6728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0317],
        [0.0325],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51850.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088424.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088317., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1159.2061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-114.9923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31862.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289122.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.7195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12981.3965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(444.0500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8438.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.8074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51876.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088317., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088209.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1159.5811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.0567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31863.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289119.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.6831, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12985.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(444.6036, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8433.8271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.9251, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51898.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088209.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088101.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1159.9205, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.1151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31863.9707, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289109.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.6496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12989.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(445.1005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8429.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.0277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51914.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088101.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087994.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1160.2278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.1679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31864.1816, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289092.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.6190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12992.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(445.5460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8425.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.1165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51927.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087994.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087886.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1160.5056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.2157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31864.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289069.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.5906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12995.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(445.9451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8422.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.1927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51936.9102, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087886.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087778.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1160.7571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.2590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31863.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289040.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.5645, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12997.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(446.3019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8418.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.2578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51942.6445, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087778.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087671.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1160.9851, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.2981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31863.0195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(289007.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.5402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12998.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(446.6207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8415.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.3125, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51945.3008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087671.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 1500 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087563.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1161.1909, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.3335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31862.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288969.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.5179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13000.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(446.9048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8412.5576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.3578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51945.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087563.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087455.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1161.3777, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.3657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31861.0098, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288926.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13001.2021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.1577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8409.6035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.3949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51942.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087455.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087348., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1161.5461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.3947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31859.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288880.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13001.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.3823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8406.7764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4243, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51937.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087348., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087240.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1161.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.4209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31858.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288831.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4599, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13002.1299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.5811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8404.0654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51930.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087240.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087132.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1161.8373, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.4447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31856.5762, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288779.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13002.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.7569, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8401.4590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51921.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087132.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087025., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1161.9622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.4663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31854.7871, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288723.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13001.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.9116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8398.9482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51911.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087025., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086917.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.0754, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.4857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31852.8691, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288666.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4129, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13001.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.0474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8396.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51899.0234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086917.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086809.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.1770, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31850.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288606.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13000.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.1659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8394.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0326],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51885.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086809.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086701.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.2700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31848.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288544.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13000.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.2689, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8391.8975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51870.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086701.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086594.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.3539, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31846.4473, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288480.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12999.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.3578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8389.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0314],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51854.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086594.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 1650 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086486.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.4294, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31844.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288414.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12998.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.4338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8387.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51837.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086486.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086378.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.4980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31841.7090, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288347.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12996.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.4984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8385.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51819.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086378.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086271., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.5596, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31839.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288279.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12995.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.5525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8383.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51800.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086271., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086163.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.6157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31836.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288209.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12994.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.5972, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8381.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.4104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0318],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51781.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086163.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086055.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.6670, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31834.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288139.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12992.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8379.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.3892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0317],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51760.8242, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086055.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085947.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.7126, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.5954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31831.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(288067.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12991.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8377.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.3658, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0317],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51739.9180, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085947.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085840., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.7545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31828.7207, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287994.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12989.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8375.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.3406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0317],
        [0.0325],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51718.4727, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085840., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085732.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.7916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31825.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287920.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12987.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8373.6465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.3138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0317],
        [0.0324],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51696.5117, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085732.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085624.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.8254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31823.1816, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287846.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12985.7480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.7077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8371.7920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.2853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0313],
        [0.0317],
        [0.0324],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51674.0977, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085624.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085516.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.8556, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31820.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287771.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12983.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.7119, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8369.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.2555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0317],
        [0.0324],
        ...,
        [0.0155],
        [0.0154],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51651.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085516.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 1800 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085409., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.8835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31817.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287696.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12981.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.7113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8368.1514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.2244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0317],
        [0.0324],
        ...,
        [0.0155],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51628.0508, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085409., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085301.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.9087, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31814.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287619.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12979.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.7067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8366.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.1922, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0317],
        [0.0324],
        ...,
        [0.0155],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51604.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085301.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085301.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.9087, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31814.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287619.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12979.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.7067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8366.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.1922, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0317],
        [0.0324],
        ...,
        [0.0155],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51604.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085301.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085193.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.9314, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31811.7051, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287543.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12977.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8364.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.1590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0316],
        [0.0324],
        ...,
        [0.0155],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51580.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085193.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085085.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.9517, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31808.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287466., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12975.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8362.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.1248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0316],
        [0.0324],
        ...,
        [0.0155],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51556.8008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085085.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084977.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.9705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31805.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287388.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12973.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8361.0840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.0898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0316],
        [0.0323],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51533., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084977.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084870.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1162.9868, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31802.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287310.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2370, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12971.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8359.3535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.0540, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0312],
        [0.0316],
        [0.0323],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51508.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084870.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084762.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1163.0022, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31799.8418, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287232.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12969.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8357.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.0175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0316],
        [0.0323],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51484.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084762.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084654.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1163.0159, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31796.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287153.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12967.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.6096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8355.9248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.9804, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0316],
        [0.0323],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51460.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084654.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084546.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1163.0281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31793.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(287074.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12964.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.5847, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8354.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.9428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0315],
        [0.0323],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51436.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084546.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 1950 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9876],
        [0.9876],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084439., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1163.0393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31790.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(286995.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2116, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12962.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.5579, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8352.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.9046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0315],
        [0.0323],
        ...,
        [0.0154],
        [0.0153],
        [0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51412.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9876],
        [0.9876],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084439., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9875],
        [0.9875],
        [0.9875],
        ...,
        [0.9875],
        [0.9875],
        [0.9875]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084331.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        ...,
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007],
        [-0.0011,  0.0000,  0.0000,  ...,  0.0000, -0.0004,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1163.0496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.6532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(31787.7207, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(286916.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12960.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(448.5293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8350.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.8660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0315],
        [0.0322],
        ...,
        [0.0154],
        [0.0153],
        [0.0152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(51387.9453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9875],
        [0.9875],
        [0.9875],
        ...,
        [0.9875],
        [0.9875],
        [0.9875]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084331.2500, device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./Training.py", line 76, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(BatchSize * 6796, 1)
RuntimeError: shape '[101940, 1]' is invalid for input of size 33980

real	1m37.387s
user	0m14.657s
sys	0m11.251s
