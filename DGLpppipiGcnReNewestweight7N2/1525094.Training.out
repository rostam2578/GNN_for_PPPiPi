0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 01:26:39 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   35C    P0    45W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b8a48ea9910> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m3.604s
user	0m2.220s
sys	0m0.725s
[01:26:44] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.8523],
        [ 1.6059],
        [ 2.1897],
        ...,
        [-1.0490],
        [ 0.2632],
        [ 0.0596]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-31.3679, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0453, -0.1121,  0.1126, -0.0596,  0.1370,  0.1178, -0.1515, -0.0409,
         -0.0157, -0.0009, -0.0981, -0.1135, -0.1145, -0.0566,  0.0134, -0.0824,
          0.1189,  0.0781,  0.0024, -0.1258,  0.0113,  0.0623,  0.0817, -0.1045,
          0.0743,  0.0888, -0.0516,  0.1091,  0.1171, -0.0560, -0.1206, -0.0966,
         -0.0878, -0.0144,  0.1180,  0.1285, -0.0210,  0.0154, -0.0107,  0.0270,
          0.1331, -0.1131, -0.0203,  0.0804,  0.1318,  0.1114, -0.0943,  0.0741,
         -0.0633, -0.1108, -0.0320,  0.0118,  0.0780, -0.0366, -0.0146, -0.0033,
          0.1429, -0.0408,  0.1387, -0.1080,  0.0562, -0.0196,  0.1318, -0.0038,
          0.0056,  0.0929, -0.0755,  0.1360, -0.0048,  0.0041,  0.1120,  0.0851,
         -0.0691, -0.0722, -0.1421,  0.1044,  0.1108,  0.0388, -0.0508,  0.0199,
         -0.0566, -0.1155, -0.0893,  0.1035, -0.1525,  0.1240, -0.0119,  0.1342,
          0.0432,  0.1090,  0.1442,  0.0961, -0.1322,  0.0735,  0.1151, -0.1135,
         -0.0082, -0.1322,  0.1294, -0.1488, -0.0167, -0.0120, -0.1479, -0.0877,
         -0.0239, -0.0373, -0.1055, -0.1159, -0.1394,  0.0695,  0.0387, -0.0559,
         -0.1379, -0.0116, -0.0510,  0.0765, -0.1161,  0.0002,  0.0869,  0.0864,
          0.1487, -0.1077,  0.0923,  0.0630, -0.1320, -0.1251,  0.1004,  0.0739,
         -0.0572,  0.0999, -0.0221,  0.0070, -0.0197, -0.0080, -0.1464,  0.0176,
          0.0639, -0.1052, -0.1383, -0.0564,  0.1271,  0.0203, -0.0773,  0.0261,
          0.0766,  0.0853,  0.0211, -0.0496,  0.0846,  0.0777, -0.0209, -0.0951,
          0.0837,  0.0336, -0.0403, -0.1136,  0.0745,  0.0250, -0.1055,  0.0875,
         -0.1508, -0.1062, -0.0439, -0.1044,  0.0911, -0.0534,  0.1345,  0.0321,
         -0.0129,  0.0165,  0.0934, -0.0660, -0.0562,  0.0183,  0.1495, -0.0239,
          0.0466, -0.1001, -0.0856, -0.0997, -0.1051,  0.0557,  0.0163, -0.1431,
         -0.0813,  0.1322, -0.0355,  0.0167, -0.1285, -0.0331, -0.0326, -0.0722,
          0.1331, -0.1096, -0.1067,  0.0176, -0.0870, -0.0634,  0.1216, -0.0107,
         -0.0242,  0.0893, -0.0289, -0.0777, -0.1463, -0.0835, -0.0275, -0.1248,
         -0.0740,  0.1312,  0.1174, -0.0865, -0.0025, -0.0900, -0.0776,  0.1402,
         -0.0218,  0.0030, -0.0570, -0.0223, -0.0180,  0.1147,  0.1134,  0.1014,
         -0.0220,  0.0289, -0.1328, -0.0162,  0.0876,  0.1310, -0.0003, -0.0500,
          0.0736,  0.0083, -0.0772, -0.0143,  0.1081,  0.1342,  0.0819, -0.1342,
          0.1512, -0.1027, -0.1137,  0.0174,  0.0257, -0.1099,  0.1039, -0.0298,
          0.1369,  0.0388, -0.1051,  0.1077,  0.0032,  0.0903,  0.1319, -0.1149]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0453, -0.1121,  0.1126, -0.0596,  0.1370,  0.1178, -0.1515, -0.0409,
         -0.0157, -0.0009, -0.0981, -0.1135, -0.1145, -0.0566,  0.0134, -0.0824,
          0.1189,  0.0781,  0.0024, -0.1258,  0.0113,  0.0623,  0.0817, -0.1045,
          0.0743,  0.0888, -0.0516,  0.1091,  0.1171, -0.0560, -0.1206, -0.0966,
         -0.0878, -0.0144,  0.1180,  0.1285, -0.0210,  0.0154, -0.0107,  0.0270,
          0.1331, -0.1131, -0.0203,  0.0804,  0.1318,  0.1114, -0.0943,  0.0741,
         -0.0633, -0.1108, -0.0320,  0.0118,  0.0780, -0.0366, -0.0146, -0.0033,
          0.1429, -0.0408,  0.1387, -0.1080,  0.0562, -0.0196,  0.1318, -0.0038,
          0.0056,  0.0929, -0.0755,  0.1360, -0.0048,  0.0041,  0.1120,  0.0851,
         -0.0691, -0.0722, -0.1421,  0.1044,  0.1108,  0.0388, -0.0508,  0.0199,
         -0.0566, -0.1155, -0.0893,  0.1035, -0.1525,  0.1240, -0.0119,  0.1342,
          0.0432,  0.1090,  0.1442,  0.0961, -0.1322,  0.0735,  0.1151, -0.1135,
         -0.0082, -0.1322,  0.1294, -0.1488, -0.0167, -0.0120, -0.1479, -0.0877,
         -0.0239, -0.0373, -0.1055, -0.1159, -0.1394,  0.0695,  0.0387, -0.0559,
         -0.1379, -0.0116, -0.0510,  0.0765, -0.1161,  0.0002,  0.0869,  0.0864,
          0.1487, -0.1077,  0.0923,  0.0630, -0.1320, -0.1251,  0.1004,  0.0739,
         -0.0572,  0.0999, -0.0221,  0.0070, -0.0197, -0.0080, -0.1464,  0.0176,
          0.0639, -0.1052, -0.1383, -0.0564,  0.1271,  0.0203, -0.0773,  0.0261,
          0.0766,  0.0853,  0.0211, -0.0496,  0.0846,  0.0777, -0.0209, -0.0951,
          0.0837,  0.0336, -0.0403, -0.1136,  0.0745,  0.0250, -0.1055,  0.0875,
         -0.1508, -0.1062, -0.0439, -0.1044,  0.0911, -0.0534,  0.1345,  0.0321,
         -0.0129,  0.0165,  0.0934, -0.0660, -0.0562,  0.0183,  0.1495, -0.0239,
          0.0466, -0.1001, -0.0856, -0.0997, -0.1051,  0.0557,  0.0163, -0.1431,
         -0.0813,  0.1322, -0.0355,  0.0167, -0.1285, -0.0331, -0.0326, -0.0722,
          0.1331, -0.1096, -0.1067,  0.0176, -0.0870, -0.0634,  0.1216, -0.0107,
         -0.0242,  0.0893, -0.0289, -0.0777, -0.1463, -0.0835, -0.0275, -0.1248,
         -0.0740,  0.1312,  0.1174, -0.0865, -0.0025, -0.0900, -0.0776,  0.1402,
         -0.0218,  0.0030, -0.0570, -0.0223, -0.0180,  0.1147,  0.1134,  0.1014,
         -0.0220,  0.0289, -0.1328, -0.0162,  0.0876,  0.1310, -0.0003, -0.0500,
          0.0736,  0.0083, -0.0772, -0.0143,  0.1081,  0.1342,  0.0819, -0.1342,
          0.1512, -0.1027, -0.1137,  0.0174,  0.0257, -0.1099,  0.1039, -0.0298,
          0.1369,  0.0388, -0.1051,  0.1077,  0.0032,  0.0903,  0.1319, -0.1149]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0833, -0.0727, -0.0576,  ...,  0.0544,  0.0952, -0.0747],
        [-0.0589, -0.0304, -0.0942,  ..., -0.0075,  0.1182,  0.0373],
        [-0.0116,  0.0466, -0.0335,  ...,  0.1177, -0.1105,  0.0886],
        ...,
        [-0.0061, -0.1019, -0.0611,  ..., -0.0160,  0.0956, -0.0434],
        [ 0.0064, -0.0306, -0.0631,  ..., -0.1186,  0.0154, -0.0040],
        [-0.0667,  0.0833, -0.0484,  ..., -0.0738,  0.0469, -0.1094]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0833, -0.0727, -0.0576,  ...,  0.0544,  0.0952, -0.0747],
        [-0.0589, -0.0304, -0.0942,  ..., -0.0075,  0.1182,  0.0373],
        [-0.0116,  0.0466, -0.0335,  ...,  0.1177, -0.1105,  0.0886],
        ...,
        [-0.0061, -0.1019, -0.0611,  ..., -0.0160,  0.0956, -0.0434],
        [ 0.0064, -0.0306, -0.0631,  ..., -0.1186,  0.0154, -0.0040],
        [-0.0667,  0.0833, -0.0484,  ..., -0.0738,  0.0469, -0.1094]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1696,  0.0429, -0.1718,  ...,  0.0882, -0.0455, -0.1267],
        [-0.0450, -0.0494, -0.0174,  ..., -0.0491, -0.0480, -0.0514],
        [-0.1030,  0.0223, -0.1469,  ..., -0.0515, -0.1663,  0.0353],
        ...,
        [ 0.0010, -0.0770,  0.1111,  ..., -0.1525, -0.1217, -0.0368],
        [ 0.1691, -0.0823,  0.1370,  ...,  0.0355, -0.0552, -0.0379],
        [-0.1428, -0.0514,  0.0230,  ..., -0.0166,  0.0545, -0.0674]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1696,  0.0429, -0.1718,  ...,  0.0882, -0.0455, -0.1267],
        [-0.0450, -0.0494, -0.0174,  ..., -0.0491, -0.0480, -0.0514],
        [-0.1030,  0.0223, -0.1469,  ..., -0.0515, -0.1663,  0.0353],
        ...,
        [ 0.0010, -0.0770,  0.1111,  ..., -0.1525, -0.1217, -0.0368],
        [ 0.1691, -0.0823,  0.1370,  ...,  0.0355, -0.0552, -0.0379],
        [-0.1428, -0.0514,  0.0230,  ..., -0.0166,  0.0545, -0.0674]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0867, -0.0523, -0.0739,  ..., -0.2283,  0.1026, -0.2221],
        [ 0.0262,  0.0203,  0.1417,  ...,  0.1103,  0.1253, -0.1590],
        [ 0.1985,  0.0353,  0.1054,  ...,  0.0502,  0.0176,  0.1418],
        ...,
        [ 0.1064,  0.1341, -0.1045,  ..., -0.2261, -0.1529,  0.0483],
        [-0.1020,  0.1436,  0.0895,  ...,  0.2106, -0.1646, -0.1397],
        [-0.0550,  0.1787, -0.0789,  ..., -0.0368,  0.1533,  0.0216]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0867, -0.0523, -0.0739,  ..., -0.2283,  0.1026, -0.2221],
        [ 0.0262,  0.0203,  0.1417,  ...,  0.1103,  0.1253, -0.1590],
        [ 0.1985,  0.0353,  0.1054,  ...,  0.0502,  0.0176,  0.1418],
        ...,
        [ 0.1064,  0.1341, -0.1045,  ..., -0.2261, -0.1529,  0.0483],
        [-0.1020,  0.1436,  0.0895,  ...,  0.2106, -0.1646, -0.1397],
        [-0.0550,  0.1787, -0.0789,  ..., -0.0368,  0.1533,  0.0216]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2140],
        [ 0.0130],
        [-0.1134],
        [ 0.0217],
        [-0.3522],
        [ 0.2904],
        [ 0.0063],
        [-0.0845],
        [-0.2012],
        [ 0.0794],
        [ 0.1230],
        [ 0.3741],
        [-0.0025],
        [ 0.2839],
        [-0.2997],
        [-0.3738],
        [-0.2289],
        [-0.2545],
        [ 0.1246],
        [ 0.2456],
        [ 0.2670],
        [-0.3746],
        [-0.1293],
        [-0.1273],
        [ 0.2160],
        [ 0.1807],
        [ 0.3289],
        [-0.0007],
        [ 0.0212],
        [ 0.4249],
        [-0.4097],
        [-0.2832]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2140],
        [ 0.0130],
        [-0.1134],
        [ 0.0217],
        [-0.3522],
        [ 0.2904],
        [ 0.0063],
        [-0.0845],
        [-0.2012],
        [ 0.0794],
        [ 0.1230],
        [ 0.3741],
        [-0.0025],
        [ 0.2839],
        [-0.2997],
        [-0.3738],
        [-0.2289],
        [-0.2545],
        [ 0.1246],
        [ 0.2456],
        [ 0.2670],
        [-0.3746],
        [-0.1293],
        [-0.1273],
        [ 0.2160],
        [ 0.1807],
        [ 0.3289],
        [-0.0007],
        [ 0.0212],
        [ 0.4249],
        [-0.4097],
        [-0.2832]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0451, -0.0571, -0.1325, -0.1367, -0.0196,  0.1169, -0.0806,  0.0004,
         -0.1526,  0.0869, -0.0238, -0.0392, -0.0745,  0.1035,  0.0686, -0.1218,
          0.1464, -0.0225,  0.1306,  0.0869,  0.0230,  0.0529,  0.0286, -0.1209,
         -0.0970,  0.0210, -0.1221, -0.0083,  0.0487, -0.1380,  0.0197, -0.0646,
          0.0925,  0.0328, -0.0327,  0.0924,  0.1060, -0.0625, -0.0170,  0.0806,
         -0.1131, -0.0938,  0.1041, -0.0491,  0.1257, -0.0013, -0.1488, -0.0296,
         -0.0042, -0.0023, -0.0047, -0.0857, -0.0086,  0.0588,  0.0602, -0.0270,
         -0.0019,  0.0161, -0.1146,  0.1209,  0.1338,  0.0988,  0.0664,  0.0582,
          0.0712,  0.1285,  0.1342,  0.1489,  0.1437, -0.1301,  0.0314, -0.1173,
         -0.0225, -0.0827,  0.1411, -0.0235,  0.0368, -0.1034, -0.0857, -0.0419,
          0.0738,  0.0745, -0.0152,  0.0249, -0.1011, -0.1451,  0.0173, -0.0184,
          0.0933, -0.0563,  0.0190,  0.1234, -0.1349, -0.0152,  0.0694,  0.0187,
          0.1357,  0.1456, -0.0468,  0.0250, -0.1088, -0.0015, -0.0265, -0.0717,
         -0.0719,  0.0555,  0.1444,  0.1335,  0.1021, -0.0232, -0.0493, -0.0130,
          0.0474, -0.1345, -0.0927,  0.0317,  0.1202, -0.0264, -0.0573, -0.0239,
         -0.0531,  0.1412,  0.0449,  0.0683,  0.1377,  0.1439,  0.1225, -0.0251,
          0.0542,  0.1107,  0.0944,  0.1517, -0.0323,  0.0769, -0.0031,  0.0030,
         -0.0988,  0.0093,  0.1092, -0.0606,  0.0926,  0.0612,  0.0460,  0.0691,
          0.0256,  0.0263, -0.0833, -0.0152,  0.0028,  0.0666,  0.1135,  0.1178,
          0.1034,  0.0729, -0.0328,  0.0920, -0.0266,  0.1507,  0.0147,  0.0103,
          0.0445, -0.0281,  0.0814,  0.1305,  0.0503, -0.0871, -0.0921, -0.0322,
          0.1226, -0.0620, -0.1318,  0.1426,  0.0212,  0.1062,  0.0398, -0.1441,
         -0.1371, -0.0672,  0.0468, -0.0331,  0.1270,  0.0524,  0.1372, -0.1165,
         -0.0541,  0.1228,  0.1431, -0.0350,  0.0047, -0.1422,  0.0866, -0.0731,
          0.0468,  0.0841, -0.1052,  0.0065,  0.1463,  0.0266, -0.1018, -0.0304,
         -0.1340, -0.1141, -0.0683,  0.0834, -0.0506,  0.0519, -0.0125,  0.1064,
         -0.1092, -0.1263,  0.0219, -0.0229, -0.0989,  0.1315, -0.0216, -0.1418,
         -0.0366, -0.0669,  0.0260,  0.1018,  0.0172, -0.0043, -0.0648,  0.0710,
          0.1445, -0.1022, -0.1012,  0.0554,  0.0688, -0.0772, -0.1143, -0.1205,
          0.1369,  0.0109, -0.0996, -0.1060,  0.0374,  0.1232, -0.0868, -0.0533,
         -0.0481, -0.0749, -0.0456, -0.1067,  0.1217,  0.0976, -0.0569, -0.0169,
          0.0216, -0.0147, -0.1369, -0.1528, -0.1080, -0.0495,  0.1260, -0.0328]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0451, -0.0571, -0.1325, -0.1367, -0.0196,  0.1169, -0.0806,  0.0004,
         -0.1526,  0.0869, -0.0238, -0.0392, -0.0745,  0.1035,  0.0686, -0.1218,
          0.1464, -0.0225,  0.1306,  0.0869,  0.0230,  0.0529,  0.0286, -0.1209,
         -0.0970,  0.0210, -0.1221, -0.0083,  0.0487, -0.1380,  0.0197, -0.0646,
          0.0925,  0.0328, -0.0327,  0.0924,  0.1060, -0.0625, -0.0170,  0.0806,
         -0.1131, -0.0938,  0.1041, -0.0491,  0.1257, -0.0013, -0.1488, -0.0296,
         -0.0042, -0.0023, -0.0047, -0.0857, -0.0086,  0.0588,  0.0602, -0.0270,
         -0.0019,  0.0161, -0.1146,  0.1209,  0.1338,  0.0988,  0.0664,  0.0582,
          0.0712,  0.1285,  0.1342,  0.1489,  0.1437, -0.1301,  0.0314, -0.1173,
         -0.0225, -0.0827,  0.1411, -0.0235,  0.0368, -0.1034, -0.0857, -0.0419,
          0.0738,  0.0745, -0.0152,  0.0249, -0.1011, -0.1451,  0.0173, -0.0184,
          0.0933, -0.0563,  0.0190,  0.1234, -0.1349, -0.0152,  0.0694,  0.0187,
          0.1357,  0.1456, -0.0468,  0.0250, -0.1088, -0.0015, -0.0265, -0.0717,
         -0.0719,  0.0555,  0.1444,  0.1335,  0.1021, -0.0232, -0.0493, -0.0130,
          0.0474, -0.1345, -0.0927,  0.0317,  0.1202, -0.0264, -0.0573, -0.0239,
         -0.0531,  0.1412,  0.0449,  0.0683,  0.1377,  0.1439,  0.1225, -0.0251,
          0.0542,  0.1107,  0.0944,  0.1517, -0.0323,  0.0769, -0.0031,  0.0030,
         -0.0988,  0.0093,  0.1092, -0.0606,  0.0926,  0.0612,  0.0460,  0.0691,
          0.0256,  0.0263, -0.0833, -0.0152,  0.0028,  0.0666,  0.1135,  0.1178,
          0.1034,  0.0729, -0.0328,  0.0920, -0.0266,  0.1507,  0.0147,  0.0103,
          0.0445, -0.0281,  0.0814,  0.1305,  0.0503, -0.0871, -0.0921, -0.0322,
          0.1226, -0.0620, -0.1318,  0.1426,  0.0212,  0.1062,  0.0398, -0.1441,
         -0.1371, -0.0672,  0.0468, -0.0331,  0.1270,  0.0524,  0.1372, -0.1165,
         -0.0541,  0.1228,  0.1431, -0.0350,  0.0047, -0.1422,  0.0866, -0.0731,
          0.0468,  0.0841, -0.1052,  0.0065,  0.1463,  0.0266, -0.1018, -0.0304,
         -0.1340, -0.1141, -0.0683,  0.0834, -0.0506,  0.0519, -0.0125,  0.1064,
         -0.1092, -0.1263,  0.0219, -0.0229, -0.0989,  0.1315, -0.0216, -0.1418,
         -0.0366, -0.0669,  0.0260,  0.1018,  0.0172, -0.0043, -0.0648,  0.0710,
          0.1445, -0.1022, -0.1012,  0.0554,  0.0688, -0.0772, -0.1143, -0.1205,
          0.1369,  0.0109, -0.0996, -0.1060,  0.0374,  0.1232, -0.0868, -0.0533,
         -0.0481, -0.0749, -0.0456, -0.1067,  0.1217,  0.0976, -0.0569, -0.0169,
          0.0216, -0.0147, -0.1369, -0.1528, -0.1080, -0.0495,  0.1260, -0.0328]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0877, -0.1201,  0.0350,  ..., -0.0333,  0.1164,  0.1203],
        [-0.0158,  0.0621,  0.0591,  ...,  0.0906, -0.0228,  0.1052],
        [ 0.0792,  0.0083, -0.0889,  ..., -0.0931,  0.0905, -0.0443],
        ...,
        [-0.0936,  0.0448,  0.0494,  ..., -0.1142,  0.0201,  0.0090],
        [ 0.0801,  0.0737,  0.0828,  ..., -0.0012,  0.1104, -0.0092],
        [-0.0161,  0.0110,  0.0181,  ..., -0.0500,  0.0481,  0.0663]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0877, -0.1201,  0.0350,  ..., -0.0333,  0.1164,  0.1203],
        [-0.0158,  0.0621,  0.0591,  ...,  0.0906, -0.0228,  0.1052],
        [ 0.0792,  0.0083, -0.0889,  ..., -0.0931,  0.0905, -0.0443],
        ...,
        [-0.0936,  0.0448,  0.0494,  ..., -0.1142,  0.0201,  0.0090],
        [ 0.0801,  0.0737,  0.0828,  ..., -0.0012,  0.1104, -0.0092],
        [-0.0161,  0.0110,  0.0181,  ..., -0.0500,  0.0481,  0.0663]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0766, -0.0436, -0.1361,  ...,  0.0707,  0.0587, -0.0454],
        [-0.0984,  0.0742, -0.0414,  ..., -0.0350,  0.1215,  0.1502],
        [ 0.1025, -0.0409, -0.1728,  ...,  0.0529, -0.1540,  0.1270],
        ...,
        [-0.1376, -0.1310, -0.0582,  ..., -0.0630,  0.1701, -0.0735],
        [ 0.0919,  0.0815, -0.1724,  ..., -0.0022,  0.1754,  0.0581],
        [-0.0040,  0.1489, -0.1167,  ..., -0.0809,  0.0184, -0.1510]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0766, -0.0436, -0.1361,  ...,  0.0707,  0.0587, -0.0454],
        [-0.0984,  0.0742, -0.0414,  ..., -0.0350,  0.1215,  0.1502],
        [ 0.1025, -0.0409, -0.1728,  ...,  0.0529, -0.1540,  0.1270],
        ...,
        [-0.1376, -0.1310, -0.0582,  ..., -0.0630,  0.1701, -0.0735],
        [ 0.0919,  0.0815, -0.1724,  ..., -0.0022,  0.1754,  0.0581],
        [-0.0040,  0.1489, -0.1167,  ..., -0.0809,  0.0184, -0.1510]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1218, -0.0980, -0.0473,  ..., -0.1266,  0.2433, -0.1669],
        [-0.1132,  0.2253,  0.1693,  ..., -0.2325, -0.2234, -0.0511],
        [ 0.1193, -0.1561,  0.1677,  ...,  0.0944, -0.1383, -0.1879],
        ...,
        [ 0.2335,  0.2083,  0.1116,  ...,  0.1422,  0.0110, -0.0740],
        [ 0.0452,  0.1754,  0.0484,  ..., -0.2034,  0.0220, -0.0373],
        [-0.1585,  0.1132,  0.0679,  ..., -0.2156, -0.0214, -0.2055]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1218, -0.0980, -0.0473,  ..., -0.1266,  0.2433, -0.1669],
        [-0.1132,  0.2253,  0.1693,  ..., -0.2325, -0.2234, -0.0511],
        [ 0.1193, -0.1561,  0.1677,  ...,  0.0944, -0.1383, -0.1879],
        ...,
        [ 0.2335,  0.2083,  0.1116,  ...,  0.1422,  0.0110, -0.0740],
        [ 0.0452,  0.1754,  0.0484,  ..., -0.2034,  0.0220, -0.0373],
        [-0.1585,  0.1132,  0.0679,  ..., -0.2156, -0.0214, -0.2055]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.1818],
        [ 0.1082],
        [ 0.0715],
        [ 0.2989],
        [-0.0355],
        [ 0.2903],
        [-0.0853],
        [ 0.2540],
        [ 0.2762],
        [-0.0499],
        [ 0.1126],
        [ 0.1598],
        [-0.1901],
        [ 0.1758],
        [ 0.2246],
        [-0.2883],
        [ 0.1419],
        [ 0.3354],
        [ 0.3447],
        [ 0.1491],
        [ 0.0202],
        [-0.1693],
        [ 0.0203],
        [-0.2112],
        [ 0.3288],
        [-0.1995],
        [-0.2495],
        [ 0.0588],
        [ 0.1310],
        [ 0.2808],
        [ 0.2313],
        [ 0.2030]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.1818],
        [ 0.1082],
        [ 0.0715],
        [ 0.2989],
        [-0.0355],
        [ 0.2903],
        [-0.0853],
        [ 0.2540],
        [ 0.2762],
        [-0.0499],
        [ 0.1126],
        [ 0.1598],
        [-0.1901],
        [ 0.1758],
        [ 0.2246],
        [-0.2883],
        [ 0.1419],
        [ 0.3354],
        [ 0.3447],
        [ 0.1491],
        [ 0.0202],
        [-0.1693],
        [ 0.0203],
        [-0.2112],
        [ 0.3288],
        [-0.1995],
        [-0.2495],
        [ 0.0588],
        [ 0.1310],
        [ 0.2808],
        [ 0.2313],
        [ 0.2030]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-51.4923, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.0730, device='cuda:0')



h[100].sum tensor(-4.7393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-4.8640, device='cuda:0')



h[200].sum tensor(4.6853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(4.8087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2623.9216, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0014, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0000],
        [0.0073, 0.0000, 0.0022,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(10730.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(286.1970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(22.8950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-20.3520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(198.7377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(15.8985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0841],
        [-0.1030],
        [-0.1485],
        ...,
        [-0.0238],
        [-0.0238],
        [-0.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-2122.8430, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.0841],
        [-0.1030],
        [-0.1485],
        ...,
        [-0.0238],
        [-0.0238],
        [-0.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(126.6925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.4557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(7.3953, device='cuda:0')



h[100].sum tensor(11.8329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7370, device='cuda:0')



h[200].sum tensor(15.2073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0841, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15055.4277, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0107, 0.0000,  ..., 0.0310, 0.0026, 0.0000],
        [0.0008, 0.0022, 0.0000,  ..., 0.0065, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(70811.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(315.9367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(22.2276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(756.7202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(53.2388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-79.3311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2087],
        [-0.1280],
        [-0.0783],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-12259.7598, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0841],
        [-0.1030],
        [-0.1485],
        ...,
        [-0.0238],
        [-0.0238],
        [-0.0188]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 2 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0490,  0.1264,  0.0459, -0.0398, -0.1095, -0.0225,  0.1426,  0.1503,
         -0.0671, -0.0566, -0.0283,  0.1011, -0.1239, -0.0013, -0.0612,  0.0563,
         -0.0461, -0.0494, -0.1014,  0.0101, -0.1342, -0.0630, -0.0315,  0.1453,
          0.0497,  0.0373,  0.1039,  0.0322, -0.0891, -0.0676, -0.0177,  0.0669,
          0.0693,  0.1093, -0.0424, -0.1029,  0.1028,  0.0512,  0.0744,  0.1329,
         -0.0067,  0.0013,  0.0659,  0.0838,  0.0676,  0.0677, -0.0261,  0.0284,
          0.0393,  0.0666,  0.0688,  0.0915, -0.1222,  0.0809,  0.0096,  0.0434,
         -0.0965,  0.0211,  0.0396,  0.1303, -0.0081, -0.1056,  0.0659, -0.0903,
          0.1035,  0.0779,  0.0272, -0.1060, -0.0653,  0.0943, -0.0202, -0.1190,
         -0.0225,  0.0362, -0.0560, -0.0092, -0.0355,  0.0743,  0.0643, -0.0511,
          0.1326, -0.0898,  0.0989, -0.0479,  0.0487,  0.0305, -0.0760, -0.0129,
         -0.0301,  0.0384, -0.1137, -0.0894, -0.0099, -0.1151,  0.1014, -0.0849,
          0.1010,  0.0151, -0.1443, -0.0948, -0.0259, -0.1506, -0.0831, -0.0232,
          0.0185,  0.0903, -0.1372, -0.0623,  0.0799,  0.0487,  0.0853, -0.0849,
         -0.0575, -0.1189,  0.1388, -0.1161, -0.1279,  0.0921,  0.0857,  0.0987,
          0.0707,  0.0215, -0.0932, -0.0878,  0.1376,  0.0945,  0.0965, -0.0431,
         -0.0657,  0.0389,  0.0706, -0.1292, -0.1114,  0.1059,  0.0245, -0.0258,
          0.0712, -0.0733,  0.1470, -0.0268, -0.0586,  0.0259, -0.1133,  0.1424,
         -0.0522, -0.1446,  0.1018,  0.0541, -0.0356, -0.0835, -0.0490,  0.1438,
          0.0356, -0.1118,  0.0223, -0.0572, -0.1125,  0.0285,  0.1126,  0.1521,
         -0.0150,  0.0277, -0.1432,  0.0135, -0.0315, -0.0871,  0.1182,  0.0948,
          0.0601, -0.0860,  0.0561,  0.0851, -0.1352, -0.0346,  0.1106,  0.0345,
          0.0510, -0.1370,  0.0431,  0.0195, -0.0246, -0.0948, -0.1454,  0.0210,
          0.0436,  0.1277,  0.0260,  0.1299, -0.0568, -0.0520, -0.0103,  0.0644,
         -0.1453, -0.0999, -0.0965, -0.1225, -0.0409, -0.0138, -0.0470,  0.0071,
          0.0015, -0.0942,  0.0524,  0.1271,  0.0066, -0.0421,  0.1219, -0.0862,
         -0.0011,  0.0135,  0.0381,  0.0880,  0.1029,  0.0892,  0.0902, -0.0713,
         -0.0418,  0.0412, -0.0494,  0.0019,  0.1067, -0.1248, -0.1134,  0.1065,
         -0.1355, -0.0769,  0.0745, -0.1209,  0.1352, -0.0966, -0.0246, -0.0776,
          0.1294, -0.1309, -0.0563,  0.1250, -0.1174,  0.0043,  0.0555,  0.1461,
          0.1120,  0.0360,  0.0555,  0.1080,  0.0308,  0.0842,  0.0050,  0.1349,
          0.0984, -0.0824,  0.0021,  0.0661, -0.0776,  0.0178,  0.1264,  0.1104]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1083, -0.0322,  0.0876,  ..., -0.1124, -0.0361,  0.1179],
        [-0.1225, -0.0497, -0.0933,  ...,  0.0892,  0.0148,  0.0360],
        [-0.0515, -0.0438, -0.0598,  ..., -0.0886, -0.0916,  0.0100],
        ...,
        [ 0.0468,  0.0251,  0.0707,  ..., -0.0325, -0.0051,  0.0345],
        [ 0.1004, -0.0189, -0.0370,  ...,  0.0870, -0.1244,  0.0763],
        [ 0.0112, -0.0105, -0.0019,  ..., -0.0941,  0.0472,  0.0514]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0507, -0.0929, -0.0984,  ..., -0.0781, -0.0153, -0.1168],
        [-0.0269,  0.1264, -0.1430,  ...,  0.1715,  0.1402,  0.1744],
        [-0.1639, -0.0756,  0.0317,  ...,  0.1737,  0.0777,  0.1700],
        ...,
        [-0.1630,  0.0698, -0.1047,  ...,  0.1177,  0.0827,  0.1220],
        [-0.1674, -0.0619,  0.1251,  ..., -0.1632, -0.0164,  0.0860],
        [ 0.1197,  0.0306, -0.1048,  ...,  0.1599, -0.0915,  0.1246]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0321,  0.0432, -0.2249,  ...,  0.1977,  0.0282,  0.2064],
        [ 0.1349, -0.1877, -0.2248,  ...,  0.1251, -0.1017,  0.1563],
        [-0.0635, -0.1307, -0.2444,  ...,  0.0548, -0.1733,  0.0353],
        ...,
        [ 0.2442,  0.2371,  0.0189,  ...,  0.1884,  0.1911,  0.2384],
        [-0.0967, -0.2131, -0.1909,  ..., -0.1708,  0.1352, -0.1005],
        [-0.0341,  0.0909,  0.2081,  ..., -0.0738,  0.0206, -0.1810]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1938],
        [-0.2791],
        [ 0.3306],
        [ 0.0506],
        [ 0.1906],
        [-0.2190],
        [ 0.1110],
        [ 0.0115],
        [ 0.2668],
        [-0.1569],
        [-0.1487],
        [-0.1656],
        [-0.1353],
        [ 0.1618],
        [-0.2373],
        [-0.2870],
        [-0.0763],
        [-0.1510],
        [ 0.1413],
        [ 0.0843],
        [-0.0432],
        [ 0.0805],
        [ 0.1738],
        [-0.4019],
        [-0.3525],
        [-0.1229],
        [-0.0865],
        [ 0.3437],
        [-0.1431],
        [-0.3159],
        [-0.0280],
        [ 0.1868]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0490,  0.1264,  0.0459, -0.0398, -0.1095, -0.0225,  0.1426,  0.1503,
         -0.0671, -0.0566, -0.0283,  0.1011, -0.1239, -0.0013, -0.0612,  0.0563,
         -0.0461, -0.0494, -0.1014,  0.0101, -0.1342, -0.0630, -0.0315,  0.1453,
          0.0497,  0.0373,  0.1039,  0.0322, -0.0891, -0.0676, -0.0177,  0.0669,
          0.0693,  0.1093, -0.0424, -0.1029,  0.1028,  0.0512,  0.0744,  0.1329,
         -0.0067,  0.0013,  0.0659,  0.0838,  0.0676,  0.0677, -0.0261,  0.0284,
          0.0393,  0.0666,  0.0688,  0.0915, -0.1222,  0.0809,  0.0096,  0.0434,
         -0.0965,  0.0211,  0.0396,  0.1303, -0.0081, -0.1056,  0.0659, -0.0903,
          0.1035,  0.0779,  0.0272, -0.1060, -0.0653,  0.0943, -0.0202, -0.1190,
         -0.0225,  0.0362, -0.0560, -0.0092, -0.0355,  0.0743,  0.0643, -0.0511,
          0.1326, -0.0898,  0.0989, -0.0479,  0.0487,  0.0305, -0.0760, -0.0129,
         -0.0301,  0.0384, -0.1137, -0.0894, -0.0099, -0.1151,  0.1014, -0.0849,
          0.1010,  0.0151, -0.1443, -0.0948, -0.0259, -0.1506, -0.0831, -0.0232,
          0.0185,  0.0903, -0.1372, -0.0623,  0.0799,  0.0487,  0.0853, -0.0849,
         -0.0575, -0.1189,  0.1388, -0.1161, -0.1279,  0.0921,  0.0857,  0.0987,
          0.0707,  0.0215, -0.0932, -0.0878,  0.1376,  0.0945,  0.0965, -0.0431,
         -0.0657,  0.0389,  0.0706, -0.1292, -0.1114,  0.1059,  0.0245, -0.0258,
          0.0712, -0.0733,  0.1470, -0.0268, -0.0586,  0.0259, -0.1133,  0.1424,
         -0.0522, -0.1446,  0.1018,  0.0541, -0.0356, -0.0835, -0.0490,  0.1438,
          0.0356, -0.1118,  0.0223, -0.0572, -0.1125,  0.0285,  0.1126,  0.1521,
         -0.0150,  0.0277, -0.1432,  0.0135, -0.0315, -0.0871,  0.1182,  0.0948,
          0.0601, -0.0860,  0.0561,  0.0851, -0.1352, -0.0346,  0.1106,  0.0345,
          0.0510, -0.1370,  0.0431,  0.0195, -0.0246, -0.0948, -0.1454,  0.0210,
          0.0436,  0.1277,  0.0260,  0.1299, -0.0568, -0.0520, -0.0103,  0.0644,
         -0.1453, -0.0999, -0.0965, -0.1225, -0.0409, -0.0138, -0.0470,  0.0071,
          0.0015, -0.0942,  0.0524,  0.1271,  0.0066, -0.0421,  0.1219, -0.0862,
         -0.0011,  0.0135,  0.0381,  0.0880,  0.1029,  0.0892,  0.0902, -0.0713,
         -0.0418,  0.0412, -0.0494,  0.0019,  0.1067, -0.1248, -0.1134,  0.1065,
         -0.1355, -0.0769,  0.0745, -0.1209,  0.1352, -0.0966, -0.0246, -0.0776,
          0.1294, -0.1309, -0.0563,  0.1250, -0.1174,  0.0043,  0.0555,  0.1461,
          0.1120,  0.0360,  0.0555,  0.1080,  0.0308,  0.0842,  0.0050,  0.1349,
          0.0984, -0.0824,  0.0021,  0.0661, -0.0776,  0.0178,  0.1264,  0.1104]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1083, -0.0322,  0.0876,  ..., -0.1124, -0.0361,  0.1179],
        [-0.1225, -0.0497, -0.0933,  ...,  0.0892,  0.0148,  0.0360],
        [-0.0515, -0.0438, -0.0598,  ..., -0.0886, -0.0916,  0.0100],
        ...,
        [ 0.0468,  0.0251,  0.0707,  ..., -0.0325, -0.0051,  0.0345],
        [ 0.1004, -0.0189, -0.0370,  ...,  0.0870, -0.1244,  0.0763],
        [ 0.0112, -0.0105, -0.0019,  ..., -0.0941,  0.0472,  0.0514]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0507, -0.0929, -0.0984,  ..., -0.0781, -0.0153, -0.1168],
        [-0.0269,  0.1264, -0.1430,  ...,  0.1715,  0.1402,  0.1744],
        [-0.1639, -0.0756,  0.0317,  ...,  0.1737,  0.0777,  0.1700],
        ...,
        [-0.1630,  0.0698, -0.1047,  ...,  0.1177,  0.0827,  0.1220],
        [-0.1674, -0.0619,  0.1251,  ..., -0.1632, -0.0164,  0.0860],
        [ 0.1197,  0.0306, -0.1048,  ...,  0.1599, -0.0915,  0.1246]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0321,  0.0432, -0.2249,  ...,  0.1977,  0.0282,  0.2064],
        [ 0.1349, -0.1877, -0.2248,  ...,  0.1251, -0.1017,  0.1563],
        [-0.0635, -0.1307, -0.2444,  ...,  0.0548, -0.1733,  0.0353],
        ...,
        [ 0.2442,  0.2371,  0.0189,  ...,  0.1884,  0.1911,  0.2384],
        [-0.0967, -0.2131, -0.1909,  ..., -0.1708,  0.1352, -0.1005],
        [-0.0341,  0.0909,  0.2081,  ..., -0.0738,  0.0206, -0.1810]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1938],
        [-0.2791],
        [ 0.3306],
        [ 0.0506],
        [ 0.1906],
        [-0.2190],
        [ 0.1110],
        [ 0.0115],
        [ 0.2668],
        [-0.1569],
        [-0.1487],
        [-0.1656],
        [-0.1353],
        [ 0.1618],
        [-0.2373],
        [-0.2870],
        [-0.0763],
        [-0.1510],
        [ 0.1413],
        [ 0.0843],
        [-0.0432],
        [ 0.0805],
        [ 0.1738],
        [-0.4019],
        [-0.3525],
        [-0.1229],
        [-0.0865],
        [ 0.3437],
        [-0.1431],
        [-0.3159],
        [-0.0280],
        [ 0.1868]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0072,  0.0185,  0.0067,  ...,  0.0026,  0.0185,  0.0162],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(287.5433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4687, device='cuda:0')



h[100].sum tensor(-7.5782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2829, device='cuda:0')



h[200].sum tensor(0.4333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0701, 0.0255,  ..., 0.0099, 0.0701, 0.0612],
        [0.0000, 0.0576, 0.0209,  ..., 0.0081, 0.0577, 0.0504],
        [0.0000, 0.0135, 0.0049,  ..., 0.0019, 0.0135, 0.0118],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30029.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0000, 0.0000,  ..., 0.2136, 0.1974, 0.0878],
        [0.0161, 0.0000, 0.0000,  ..., 0.1830, 0.1692, 0.0752],
        [0.0129, 0.0000, 0.0000,  ..., 0.1469, 0.1358, 0.0604],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145151.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(262.1655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.9533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3.9346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(249.8092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0542e+00],
        [-1.1407e+00],
        [-1.2587e+00],
        ...,
        [-1.2665e-06],
        [-1.6640e-07],
        [ 0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19647.2676, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(78.1938, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365911.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0000e-04, -1.0000e-04,  ...,  1.0000e-04,
          1.0000e-04, -1.0000e-04],
        [ 0.0000e+00,  1.0000e-04, -1.0000e-04,  ...,  1.0000e-04,
          1.0000e-04, -1.0000e-04],
        [ 0.0000e+00,  1.0000e-04, -1.0000e-04,  ...,  1.0000e-04,
          1.0000e-04, -1.0000e-04],
        ...,
        [ 0.0000e+00,  1.0000e-04, -1.0000e-04,  ...,  1.0000e-04,
          1.0000e-04, -1.0000e-04],
        [ 0.0000e+00,  1.0000e-04, -1.0000e-04,  ...,  1.0000e-04,
          1.0000e-04, -1.0000e-04],
        [ 0.0000e+00,  1.0000e-04, -1.0000e-04,  ...,  1.0000e-04,
          1.0000e-04, -1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(199.9422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6695, device='cuda:0')



h[100].sum tensor(-6.9632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0200, device='cuda:0')



h[200].sum tensor(-3.0253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0004, 0.0004, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0004, 0.0004, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0004, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0004, 0.0004, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0004, 0.0004, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0004, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30255.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0004, 0.0005,  ..., 0.0026, 0.0011, 0.0023],
        [0.0016, 0.0004, 0.0005,  ..., 0.0026, 0.0011, 0.0023],
        [0.0016, 0.0003, 0.0003,  ..., 0.0028, 0.0013, 0.0024],
        ...,
        [0.0016, 0.0004, 0.0004,  ..., 0.0026, 0.0011, 0.0022],
        [0.0016, 0.0004, 0.0004,  ..., 0.0026, 0.0011, 0.0022],
        [0.0016, 0.0004, 0.0004,  ..., 0.0026, 0.0011, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155538.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(666.8863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.9453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(0.3057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.2019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0126],
        [0.0102],
        [0.0055],
        ...,
        [0.0073],
        [0.0072],
        [0.0072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-3613.5144, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365911.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365886.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0002,  ...,  0.0002,  0.0001, -0.0002],
        [ 0.0000,  0.0002, -0.0002,  ...,  0.0002,  0.0001, -0.0002],
        [ 0.0000,  0.0002, -0.0002,  ...,  0.0002,  0.0001, -0.0002],
        ...,
        [ 0.0000,  0.0002, -0.0002,  ...,  0.0002,  0.0001, -0.0002],
        [ 0.0000,  0.0002, -0.0002,  ...,  0.0002,  0.0001, -0.0002],
        [ 0.0000,  0.0002, -0.0002,  ...,  0.0002,  0.0001, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(0.6289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6886, device='cuda:0')



h[100].sum tensor(-4.9312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1461, device='cuda:0')



h[200].sum tensor(-5.6645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0631, 0.0220,  ..., 0.0097, 0.0629, 0.0538],
        [0.0000, 0.0421, 0.0146,  ..., 0.0067, 0.0419, 0.0357],
        [0.0000, 0.0349, 0.0120,  ..., 0.0057, 0.0348, 0.0295],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0008, 0.0005, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0008, 0.0005, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0008, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24038.3145, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0256, 0.0000, 0.0000,  ..., 0.1888, 0.1733, 0.0868],
        [0.0214, 0.0000, 0.0000,  ..., 0.1523, 0.1396, 0.0704],
        [0.0174, 0.0000, 0.0000,  ..., 0.1167, 0.1068, 0.0547],
        ...,
        [0.0045, 0.0004, 0.0009,  ..., 0.0041, 0.0030, 0.0035],
        [0.0045, 0.0004, 0.0009,  ..., 0.0041, 0.0030, 0.0035],
        [0.0045, 0.0004, 0.0009,  ..., 0.0041, 0.0030, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(132690.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1381.5383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.3028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.5149, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(241.2620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1676],
        [-0.1452],
        [-0.1081],
        ...,
        [ 0.0074],
        [ 0.0073],
        [ 0.0073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(4411.7109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [1.0001],
        [1.0001],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365886.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365862.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0070,  0.0022,  ...,  0.0012,  0.0070,  0.0057],
        [-0.0064,  0.0168,  0.0057,  ...,  0.0026,  0.0168,  0.0143],
        [-0.0047,  0.0124,  0.0041,  ...,  0.0020,  0.0123,  0.0104],
        ...,
        [ 0.0000,  0.0001, -0.0003,  ...,  0.0003,  0.0001, -0.0003],
        [ 0.0000,  0.0001, -0.0003,  ...,  0.0003,  0.0001, -0.0003],
        [ 0.0000,  0.0001, -0.0003,  ...,  0.0003,  0.0001, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-100.8401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1480, device='cuda:0')



h[100].sum tensor(-5.3935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3594, device='cuda:0')



h[200].sum tensor(-7.6168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0602, 0.0204,  ..., 0.0096, 0.0601, 0.0508],
        [0.0000, 0.0390, 0.0127,  ..., 0.0066, 0.0390, 0.0324],
        [0.0000, 0.0328, 0.0108,  ..., 0.0057, 0.0328, 0.0273],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0011, 0.0004, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0011, 0.0004, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0011, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25088.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0237, 0.0000, 0.0000,  ..., 0.1318, 0.1222, 0.0642],
        [0.0220, 0.0000, 0.0000,  ..., 0.1184, 0.1098, 0.0584],
        [0.0200, 0.0000, 0.0000,  ..., 0.1025, 0.0951, 0.0507],
        ...,
        [0.0073, 0.0005, 0.0015,  ..., 0.0045, 0.0051, 0.0035],
        [0.0073, 0.0005, 0.0015,  ..., 0.0045, 0.0051, 0.0035],
        [0.0073, 0.0005, 0.0015,  ..., 0.0045, 0.0051, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(135659.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2140.3088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.3922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-0.1503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(16.8611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(267.4283, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0678],
        [-0.0595],
        [-0.0441],
        ...,
        [ 0.0044],
        [ 0.0044],
        [ 0.0044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(7142.8643, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9998],
        [0.9997],
        ...,
        [1.0000],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365862.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365842.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  6.5739e-05, -3.7199e-04,  ...,  3.1246e-04,
          5.8146e-05, -3.4196e-04],
        [ 0.0000e+00,  6.5739e-05, -3.7199e-04,  ...,  3.1246e-04,
          5.8146e-05, -3.4196e-04],
        [ 0.0000e+00,  6.5739e-05, -3.7199e-04,  ...,  3.1246e-04,
          5.8146e-05, -3.4196e-04],
        ...,
        [ 0.0000e+00,  6.5739e-05, -3.7199e-04,  ...,  3.1246e-04,
          5.8146e-05, -3.4196e-04],
        [ 0.0000e+00,  6.5739e-05, -3.7199e-04,  ...,  3.1246e-04,
          5.8146e-05, -3.4196e-04],
        [ 0.0000e+00,  6.5739e-05, -3.7199e-04,  ...,  3.1246e-04,
          5.8146e-05, -3.4196e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-235.6822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0358, device='cuda:0')



h[100].sum tensor(-4.9847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1969, device='cuda:0')



h[200].sum tensor(-9.2642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0012, 0.0002, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0012, 0.0002, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0012, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0012, 0.0002, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0012, 0.0002, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0012, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23465.4473, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0014, 0.0025,  ..., 0.0048, 0.0067, 0.0029],
        [0.0098, 0.0011, 0.0019,  ..., 0.0062, 0.0079, 0.0036],
        [0.0107, 0.0007, 0.0012,  ..., 0.0120, 0.0133, 0.0066],
        ...,
        [0.0095, 0.0014, 0.0025,  ..., 0.0048, 0.0066, 0.0028],
        [0.0095, 0.0014, 0.0025,  ..., 0.0048, 0.0066, 0.0028],
        [0.0095, 0.0014, 0.0025,  ..., 0.0048, 0.0066, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(128227.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2718.3311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.2320, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2.6702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(96.0041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(265.0463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0329],
        [0.0358],
        [0.0395],
        ...,
        [0.0037],
        [0.0015],
        [0.0008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(6605.6338, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365842.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365825.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.4942e-03,  1.1756e-02,  3.7613e-03,  ...,  1.9750e-03,
          1.1757e-02,  9.7955e-03],
        [ 0.0000e+00, -4.1106e-06, -4.5173e-04,  ...,  3.0521e-04,
          6.0637e-07, -4.0183e-04],
        [ 0.0000e+00, -4.1106e-06, -4.5173e-04,  ...,  3.0521e-04,
          6.0637e-07, -4.0183e-04],
        ...,
        [ 0.0000e+00, -4.1106e-06, -4.5173e-04,  ...,  3.0521e-04,
          6.0637e-07, -4.0183e-04],
        [ 0.0000e+00, -4.1106e-06, -4.5173e-04,  ...,  3.0521e-04,
          6.0637e-07, -4.0183e-04],
        [ 0.0000e+00, -4.1106e-06, -4.5173e-04,  ...,  3.0521e-04,
          6.0637e-07, -4.0183e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-251.5175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5460, device='cuda:0')



h[100].sum tensor(-7.2170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1481, device='cuda:0')



h[200].sum tensor(-10.5468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.3869e-02, 1.4816e-02,  ..., 7.4505e-03, 4.3868e-02,
         3.7244e-02],
        [0.0000e+00, 1.1752e-02, 3.7599e-03,  ..., 2.8896e-03, 1.1755e-02,
         9.7920e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2205e-03, 2.4247e-06,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2203e-03, 2.4243e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2203e-03, 2.4243e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2203e-03, 2.4243e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34692.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.0266e-02, 0.0000e+00, 0.0000e+00,  ..., 1.2154e-01, 1.1603e-01,
         5.8424e-02],
        [2.0035e-02, 1.1102e-04, 0.0000e+00,  ..., 5.7262e-02, 5.6452e-02,
         2.7533e-02],
        [1.3883e-02, 1.2168e-03, 1.7509e-03,  ..., 1.9338e-02, 2.1312e-02,
         8.9788e-03],
        ...,
        [1.1381e-02, 2.4029e-03, 3.4663e-03,  ..., 4.7570e-03, 7.7514e-03,
         1.8057e-03],
        [1.1381e-02, 2.4029e-03, 3.4663e-03,  ..., 4.7570e-03, 7.7514e-03,
         1.8057e-03],
        [1.1381e-02, 2.4029e-03, 3.4663e-03,  ..., 4.7570e-03, 7.7514e-03,
         1.8057e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(196467.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3488.8740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3.6549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(177.0646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(371.7837, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0602],
        [ 0.0551],
        [ 0.0436],
        ...,
        [-0.0036],
        [-0.0036],
        [-0.0036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(8753.6523, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365825.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365810.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.1331e-05, -5.2350e-04,  ...,  2.9683e-04,
         -5.9920e-05, -4.5331e-04],
        [ 0.0000e+00, -6.1331e-05, -5.2350e-04,  ...,  2.9683e-04,
         -5.9920e-05, -4.5331e-04],
        [ 0.0000e+00, -6.1331e-05, -5.2350e-04,  ...,  2.9683e-04,
         -5.9920e-05, -4.5331e-04],
        ...,
        [ 0.0000e+00, -6.1331e-05, -5.2350e-04,  ...,  2.9683e-04,
         -5.9920e-05, -4.5331e-04],
        [ 0.0000e+00, -6.1331e-05, -5.2350e-04,  ...,  2.9683e-04,
         -5.9920e-05, -4.5331e-04],
        [ 0.0000e+00, -6.1331e-05, -5.2350e-04,  ...,  2.9683e-04,
         -5.9920e-05, -4.5331e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-381.2755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0152, device='cuda:0')



h[100].sum tensor(-6.3968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7783, device='cuda:0')



h[200].sum tensor(-11.7864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31395.7441, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0171, 0.0016, 0.0016,  ..., 0.0248, 0.0272, 0.0112],
        [0.0141, 0.0025, 0.0027,  ..., 0.0088, 0.0123, 0.0031],
        [0.0133, 0.0036, 0.0041,  ..., 0.0046, 0.0084, 0.0008],
        ...,
        [0.0132, 0.0035, 0.0041,  ..., 0.0046, 0.0083, 0.0008],
        [0.0132, 0.0035, 0.0041,  ..., 0.0046, 0.0083, 0.0008],
        [0.0132, 0.0035, 0.0041,  ..., 0.0046, 0.0083, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177209.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3878.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4.1646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(218.3206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(352.5903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0619],
        [ 0.0465],
        [ 0.0321],
        ...,
        [-0.0081],
        [-0.0081],
        [-0.0081]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(5510.8472, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365810.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365798.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0082,  0.0216,  0.0071,  ...,  0.0033,  0.0215,  0.0182],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        ...,
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-527.1556, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0962, device='cuda:0')



h[100].sum tensor(-4.9859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2057, device='cuda:0')



h[200].sum tensor(-12.9120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0387, 0.0127,  ..., 0.0066, 0.0386, 0.0326],
        [0.0000, 0.0215, 0.0071,  ..., 0.0041, 0.0215, 0.0182],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25658.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.4688e-02, 0.0000e+00, 0.0000e+00,  ..., 1.7027e-01, 1.6351e-01,
         8.0436e-02],
        [2.9834e-02, 0.0000e+00, 0.0000e+00,  ..., 8.7246e-02, 8.6109e-02,
         4.0327e-02],
        [2.0180e-02, 4.3152e-04, 0.0000e+00,  ..., 3.1678e-02, 3.4369e-02,
         1.3834e-02],
        ...,
        [1.4707e-02, 4.5800e-03, 4.5481e-03,  ..., 4.2758e-03, 8.6677e-03,
         2.6245e-05],
        [1.4707e-02, 4.5800e-03, 4.5480e-03,  ..., 4.2758e-03, 8.6676e-03,
         2.6244e-05],
        [1.4707e-02, 4.5800e-03, 4.5480e-03,  ..., 4.2758e-03, 8.6676e-03,
         2.6244e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153556.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4228.9985, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.4529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3.2939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(208.6285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(317.0822, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0914],
        [ 0.0921],
        [ 0.0972],
        ...,
        [-0.0132],
        [-0.0131],
        [-0.0131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(6306.6538, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365798.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365798.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        ...,
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005],
        [ 0.0000, -0.0001, -0.0006,  ...,  0.0003, -0.0001, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-565.7198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.4578, device='cuda:0')



h[100].sum tensor(-4.0668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8202, device='cuda:0')



h[200].sum tensor(-12.9492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0305, 0.0098,  ..., 0.0054, 0.0305, 0.0256],
        [0.0000, 0.0152, 0.0049,  ..., 0.0032, 0.0152, 0.0128],
        [0.0000, 0.0266, 0.0089,  ..., 0.0049, 0.0266, 0.0226],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(21931.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.9569e-02, 0.0000e+00, 0.0000e+00,  ..., 8.0219e-02, 7.9836e-02,
         3.8455e-02],
        [2.7909e-02, 0.0000e+00, 0.0000e+00,  ..., 7.2658e-02, 7.2706e-02,
         3.4331e-02],
        [3.0313e-02, 0.0000e+00, 0.0000e+00,  ..., 8.7512e-02, 8.6491e-02,
         4.1036e-02],
        ...,
        [1.4707e-02, 4.5800e-03, 4.5481e-03,  ..., 4.2758e-03, 8.6677e-03,
         2.6245e-05],
        [1.4707e-02, 4.5800e-03, 4.5480e-03,  ..., 4.2758e-03, 8.6676e-03,
         2.6244e-05],
        [1.4707e-02, 4.5800e-03, 4.5480e-03,  ..., 4.2758e-03, 8.6676e-03,
         2.6244e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(136125.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4109.4673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.0892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3.1125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(217.0034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.8633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0838],
        [ 0.0866],
        [ 0.0899],
        ...,
        [-0.0134],
        [-0.0133],
        [-0.0133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(3873.4255, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365798.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365789.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002, -0.0006,  ...,  0.0002, -0.0002, -0.0005],
        [-0.0033,  0.0085,  0.0024,  ...,  0.0015,  0.0085,  0.0069],
        [ 0.0000, -0.0002, -0.0006,  ...,  0.0002, -0.0002, -0.0005],
        ...,
        [ 0.0000, -0.0002, -0.0006,  ...,  0.0002, -0.0002, -0.0005],
        [ 0.0000, -0.0002, -0.0006,  ...,  0.0002, -0.0002, -0.0005],
        [ 0.0000, -0.0002, -0.0006,  ...,  0.0002, -0.0002, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-663.6821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.1847, device='cuda:0')



h[100].sum tensor(-3.6722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.6342, device='cuda:0')



h[200].sum tensor(-13.9031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0024,  ..., 0.0022, 0.0085, 0.0069],
        [0.0000, 0.0121, 0.0037,  ..., 0.0027, 0.0120, 0.0100],
        [0.0000, 0.0411, 0.0122,  ..., 0.0068, 0.0410, 0.0338],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(21030.8730, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.3609e-02, 9.2535e-04, 0.0000e+00,  ..., 3.5418e-02, 3.8813e-02,
         1.6521e-02],
        [2.7144e-02, 6.1215e-05, 0.0000e+00,  ..., 5.0790e-02, 5.3339e-02,
         2.4789e-02],
        [3.3058e-02, 0.0000e+00, 0.0000e+00,  ..., 7.6706e-02, 7.7781e-02,
         3.8621e-02],
        ...,
        [1.6018e-02, 5.6515e-03, 5.0104e-03,  ..., 3.7715e-03, 8.8335e-03,
         0.0000e+00],
        [1.6018e-02, 5.6515e-03, 5.0104e-03,  ..., 3.7715e-03, 8.8335e-03,
         0.0000e+00],
        [1.6018e-02, 5.6515e-03, 5.0104e-03,  ..., 3.7715e-03, 8.8335e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(137190.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4469.3818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.0034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2.5297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(161.2843, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.7372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0926],
        [ 0.1131],
        [ 0.1310],
        ...,
        [-0.0177],
        [-0.0177],
        [-0.0177]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(2312.9915, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365789.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365784.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [-0.0108,  0.0284,  0.0094,  ...,  0.0042,  0.0283,  0.0241],
        [-0.0109,  0.0287,  0.0096,  ...,  0.0043,  0.0287,  0.0244],
        ...,
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-666.7225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1166, device='cuda:0')



h[100].sum tensor(-5.2852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3548, device='cuda:0')



h[200].sum tensor(-14.6749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0371, 0.0119,  ..., 0.0062, 0.0371, 0.0312],
        [0.0000, 0.0516, 0.0165,  ..., 0.0082, 0.0516, 0.0433],
        [0.0000, 0.1086, 0.0361,  ..., 0.0163, 0.1084, 0.0921],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27156.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0422, 0.0000, 0.0000,  ..., 0.1185, 0.1173, 0.0573],
        [0.0550, 0.0000, 0.0000,  ..., 0.1838, 0.1784, 0.0892],
        [0.0734, 0.0000, 0.0000,  ..., 0.2772, 0.2658, 0.1346],
        ...,
        [0.0173, 0.0069, 0.0054,  ..., 0.0030, 0.0088, 0.0000],
        [0.0173, 0.0069, 0.0054,  ..., 0.0030, 0.0088, 0.0000],
        [0.0173, 0.0069, 0.0054,  ..., 0.0030, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166239.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4926.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.6002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3.9872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(126.7064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(354.8936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2039],
        [ 0.2180],
        [ 0.2334],
        ...,
        [-0.0223],
        [-0.0222],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(1423.8029, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365784.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365784.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0038,  0.0099,  0.0029,  ...,  0.0016,  0.0099,  0.0081],
        [-0.0046,  0.0120,  0.0036,  ...,  0.0019,  0.0120,  0.0100],
        [-0.0076,  0.0199,  0.0064,  ...,  0.0030,  0.0199,  0.0168],
        ...,
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-572.6293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6501, device='cuda:0')



h[100].sum tensor(-7.4632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3094, device='cuda:0')



h[200].sum tensor(-14.5927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0518, 0.0159,  ..., 0.0083, 0.0518, 0.0431],
        [0.0000, 0.0559, 0.0173,  ..., 0.0088, 0.0558, 0.0466],
        [0.0000, 0.0384, 0.0111,  ..., 0.0064, 0.0384, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34729.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0399, 0.0000, 0.0000,  ..., 0.1026, 0.1027, 0.0507],
        [0.0460, 0.0000, 0.0000,  ..., 0.1307, 0.1290, 0.0649],
        [0.0522, 0.0000, 0.0000,  ..., 0.1597, 0.1563, 0.0795],
        ...,
        [0.0173, 0.0069, 0.0054,  ..., 0.0030, 0.0088, 0.0000],
        [0.0173, 0.0069, 0.0054,  ..., 0.0030, 0.0088, 0.0000],
        [0.0173, 0.0069, 0.0054,  ..., 0.0030, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194571.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5050.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4.4024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(126.0631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(418.8331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1549],
        [ 0.1773],
        [ 0.2002],
        ...,
        [-0.0223],
        [-0.0222],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(2344.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365784.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365781.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        ...,
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006],
        [ 0.0000, -0.0002, -0.0007,  ...,  0.0002, -0.0002, -0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-511.3505, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.5512, device='cuda:0')



h[100].sum tensor(-10.2393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6099, device='cuda:0')



h[200].sum tensor(-15.2386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0042,  ..., 0.0028, 0.0138, 0.0115],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45521.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0191, 0.0077, 0.0048,  ..., 0.0046, 0.0110, 0.0007],
        [0.0212, 0.0058, 0.0030,  ..., 0.0132, 0.0191, 0.0049],
        [0.0271, 0.0024, 0.0002,  ..., 0.0377, 0.0422, 0.0170],
        ...,
        [0.0184, 0.0081, 0.0059,  ..., 0.0024, 0.0089, 0.0000],
        [0.0184, 0.0081, 0.0059,  ..., 0.0024, 0.0089, 0.0000],
        [0.0184, 0.0081, 0.0059,  ..., 0.0024, 0.0089, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(249617.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5617.7065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6.5349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(113.6689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(524.8865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0060],
        [ 0.0388],
        [ 0.0789],
        ...,
        [-0.0267],
        [-0.0266],
        [-0.0265]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(3455.8816, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365781.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9997],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365779., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0108,  0.0284,  0.0094,  ...,  0.0042,  0.0284,  0.0241],
        [-0.0157,  0.0415,  0.0140,  ...,  0.0061,  0.0414,  0.0354],
        [-0.0112,  0.0296,  0.0098,  ...,  0.0044,  0.0295,  0.0251],
        ...,
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-677.4067, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0775, device='cuda:0')



h[100].sum tensor(-7.4507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3718, device='cuda:0')



h[200].sum tensor(-16.0106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1275, 0.0425,  ..., 0.0188, 0.1274, 0.1083],
        [0.0000, 0.1491, 0.0502,  ..., 0.0219, 0.1489, 0.1269],
        [0.0000, 0.1234, 0.0411,  ..., 0.0183, 0.1233, 0.1048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36384.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0799, 0.0000, 0.0000,  ..., 0.2979, 0.2849, 0.1438],
        [0.0927, 0.0000, 0.0000,  ..., 0.3623, 0.3449, 0.1749],
        [0.0907, 0.0000, 0.0000,  ..., 0.3501, 0.3337, 0.1695],
        ...,
        [0.0193, 0.0088, 0.0065,  ..., 0.0018, 0.0088, 0.0000],
        [0.0193, 0.0088, 0.0065,  ..., 0.0018, 0.0088, 0.0000],
        [0.0193, 0.0088, 0.0065,  ..., 0.0018, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(212208.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5556.6299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7.8223, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(155.0295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(454.5970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2093],
        [ 0.2374],
        [ 0.2590],
        ...,
        [-0.0308],
        [-0.0307],
        [-0.0306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-1439.0404, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9997],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365779., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9997],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365779., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        ...,
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-689.2499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8862, device='cuda:0')



h[100].sum tensor(-7.1843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1978, device='cuda:0')



h[200].sum tensor(-16.0201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36182.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0194, 0.0089, 0.0065,  ..., 0.0018, 0.0089, 0.0000],
        [0.0194, 0.0089, 0.0065,  ..., 0.0018, 0.0089, 0.0000],
        [0.0195, 0.0089, 0.0065,  ..., 0.0018, 0.0089, 0.0000],
        ...,
        [0.0193, 0.0088, 0.0065,  ..., 0.0018, 0.0088, 0.0000],
        [0.0193, 0.0088, 0.0065,  ..., 0.0018, 0.0088, 0.0000],
        [0.0193, 0.0088, 0.0065,  ..., 0.0018, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217843., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5587.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4712, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7.8862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(155.5490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(452.5875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0357],
        [-0.0303],
        [-0.0173],
        ...,
        [-0.0209],
        [-0.0290],
        [-0.0305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-589.6105, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9997],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365779., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9997],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365778.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0047,  0.0123,  0.0037,  ...,  0.0019,  0.0123,  0.0102],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [-0.0081,  0.0211,  0.0069,  ...,  0.0032,  0.0211,  0.0178],
        ...,
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0003, -0.0008,  ...,  0.0002, -0.0003, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-753.2142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8589, device='cuda:0')



h[100].sum tensor(-6.4177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9016, device='cuda:0')



h[200].sum tensor(-16.6486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0029,  ..., 0.0021, 0.0100, 0.0082],
        [0.0000, 0.0533, 0.0163,  ..., 0.0084, 0.0534, 0.0445],
        [0.0000, 0.0296, 0.0085,  ..., 0.0050, 0.0296, 0.0244],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33635.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0306, 0.0026, 0.0000,  ..., 0.0469, 0.0512, 0.0223],
        [0.0418, 0.0000, 0.0000,  ..., 0.0945, 0.0954, 0.0470],
        [0.0432, 0.0000, 0.0000,  ..., 0.0966, 0.0974, 0.0488],
        ...,
        [0.0197, 0.0093, 0.0072,  ..., 0.0013, 0.0088, 0.0000],
        [0.0196, 0.0093, 0.0072,  ..., 0.0013, 0.0088, 0.0000],
        [0.0196, 0.0093, 0.0072,  ..., 0.0013, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206380.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5656.1821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.2034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.5373, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(241.0793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(438.4909, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0593],
        [ 0.0972],
        [ 0.1216],
        ...,
        [-0.0336],
        [-0.0334],
        [-0.0334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-1821.3632, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9996],
        [0.9997],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365778.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9997],
        [0.9998],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365779.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0042,  0.0108,  0.0032,  ...,  0.0017,  0.0108,  0.0089],
        [ 0.0000, -0.0004, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [-0.0042,  0.0108,  0.0032,  ...,  0.0017,  0.0108,  0.0089],
        ...,
        [ 0.0000, -0.0004, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0004, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0004, -0.0008,  ...,  0.0002, -0.0003, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-845.8234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3110, device='cuda:0')



h[100].sum tensor(-4.9215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2371, device='cuda:0')



h[200].sum tensor(-17.2413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0025,  ..., 0.0020, 0.0088, 0.0072],
        [0.0000, 0.0391, 0.0114,  ..., 0.0064, 0.0392, 0.0322],
        [0.0000, 0.0087, 0.0025,  ..., 0.0020, 0.0088, 0.0072],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26891.4629, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0274, 0.0039, 0.0000,  ..., 0.0317, 0.0370, 0.0146],
        [0.0326, 0.0003, 0.0000,  ..., 0.0538, 0.0572, 0.0261],
        [0.0275, 0.0039, 0.0000,  ..., 0.0317, 0.0370, 0.0146],
        ...,
        [0.0200, 0.0096, 0.0078,  ..., 0.0011, 0.0088, 0.0000],
        [0.0200, 0.0096, 0.0078,  ..., 0.0011, 0.0088, 0.0000],
        [0.0200, 0.0096, 0.0078,  ..., 0.0011, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175204.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5384.1187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.5336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-11.5991, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(342.5189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(378.5337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0215],
        [-0.0153],
        [-0.0185],
        ...,
        [-0.0362],
        [-0.0361],
        [-0.0360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-7623.7314, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9997],
        [0.9998],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365779.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [0.9999],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365782.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0091,  0.0238,  0.0079,  ...,  0.0036,  0.0238,  0.0202],
        [-0.0084,  0.0219,  0.0072,  ...,  0.0033,  0.0220,  0.0186],
        [-0.0129,  0.0340,  0.0115,  ...,  0.0050,  0.0341,  0.0291],
        ...,
        [ 0.0000, -0.0004, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0004, -0.0008,  ...,  0.0002, -0.0003, -0.0007],
        [ 0.0000, -0.0004, -0.0008,  ...,  0.0002, -0.0003, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-822.1042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4009, device='cuda:0')



h[100].sum tensor(-5.8682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6885, device='cuda:0')



h[200].sum tensor(-17.6983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0946, 0.0312,  ..., 0.0142, 0.0949, 0.0804],
        [0.0000, 0.1232, 0.0414,  ..., 0.0182, 0.1234, 0.1051],
        [0.0000, 0.1490, 0.0506,  ..., 0.0218, 0.1492, 0.1273],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29801.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0747, 0.0000, 0.0000,  ..., 0.2765, 0.2622, 0.1323],
        [0.0845, 0.0000, 0.0000,  ..., 0.3315, 0.3131, 0.1579],
        [0.0957, 0.0000, 0.0000,  ..., 0.3945, 0.3713, 0.1870],
        ...,
        [0.0203, 0.0098, 0.0084,  ..., 0.0009, 0.0088, 0.0000],
        [0.0203, 0.0098, 0.0084,  ..., 0.0009, 0.0088, 0.0000],
        [0.0203, 0.0098, 0.0084,  ..., 0.0009, 0.0088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185454.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5471.9971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.8070, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-12.0724, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(392.7781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(404.2586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1059],
        [ 0.1045],
        [ 0.1039],
        ...,
        [-0.0398],
        [-0.0396],
        [-0.0396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-8266.5664, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [0.9999],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365782.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [0.9999],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365786.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032,  0.0081,  0.0023,  ...,  0.0014,  0.0082,  0.0066],
        [-0.0030,  0.0075,  0.0021,  ...,  0.0013,  0.0076,  0.0061],
        [ 0.0000, -0.0005, -0.0008,  ...,  0.0002, -0.0003, -0.0008],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ...,  0.0002, -0.0003, -0.0008],
        [ 0.0000, -0.0005, -0.0008,  ...,  0.0002, -0.0003, -0.0008],
        [ 0.0000, -0.0005, -0.0008,  ...,  0.0002, -0.0003, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-678.3026, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7335, device='cuda:0')



h[100].sum tensor(-9.2187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1982, device='cuda:0')



h[200].sum tensor(-18.0318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0640, 0.0205,  ..., 0.0099, 0.0644, 0.0539],
        [0.0000, 0.0141, 0.0038,  ..., 0.0028, 0.0143, 0.0115],
        [0.0000, 0.0312, 0.0093,  ..., 0.0052, 0.0315, 0.0259],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46039.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0517, 0.0000, 0.0000,  ..., 0.1556, 0.1489, 0.0750],
        [0.0401, 0.0000, 0.0000,  ..., 0.0921, 0.0907, 0.0448],
        [0.0421, 0.0000, 0.0000,  ..., 0.1015, 0.0992, 0.0496],
        ...,
        [0.0208, 0.0100, 0.0089,  ..., 0.0008, 0.0089, 0.0000],
        [0.0208, 0.0100, 0.0089,  ..., 0.0008, 0.0089, 0.0000],
        [0.0208, 0.0100, 0.0089,  ..., 0.0008, 0.0089, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(273293.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5957.7256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-11.9694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(408.1812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(545.3165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0891],
        [ 0.0969],
        [ 0.1041],
        ...,
        [-0.0397],
        [-0.0395],
        [-0.0397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-3537.3267, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [0.9999],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365786.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [1.0000],
        ...,
        [0.9995],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365790.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008],
        ...,
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-795.8811, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6314, device='cuda:0')



h[100].sum tensor(-6.9124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1605, device='cuda:0')



h[200].sum tensor(-18.5089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34861.9023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0213, 0.0103, 0.0094,  ..., 0.0007, 0.0089, 0.0000],
        [0.0213, 0.0103, 0.0094,  ..., 0.0007, 0.0089, 0.0000],
        [0.0214, 0.0103, 0.0094,  ..., 0.0007, 0.0089, 0.0000],
        ...,
        [0.0212, 0.0102, 0.0093,  ..., 0.0007, 0.0089, 0.0000],
        [0.0212, 0.0102, 0.0093,  ..., 0.0007, 0.0089, 0.0000],
        [0.0212, 0.0102, 0.0093,  ..., 0.0007, 0.0089, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215783.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5750.1938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.2826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.5344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(411.6266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(450.2313, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0584],
        [-0.0592],
        [-0.0594],
        ...,
        [-0.0449],
        [-0.0435],
        [-0.0421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-5633.0986, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9997],
        [1.0000],
        ...,
        [0.9995],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365790.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0001],
        ...,
        [0.9995],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365794.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040,  0.0101,  0.0031,  ...,  0.0017,  0.0103,  0.0084],
        [-0.0041,  0.0106,  0.0032,  ...,  0.0018,  0.0108,  0.0088],
        [-0.0040,  0.0101,  0.0031,  ...,  0.0017,  0.0103,  0.0084],
        ...,
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-743.7008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5290, device='cuda:0')



h[100].sum tensor(-8.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7300, device='cuda:0')



h[200].sum tensor(-18.8371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0362, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0471, 0.0146,  ..., 0.0077, 0.0477, 0.0392],
        [0.0000, 0.0456, 0.0141,  ..., 0.0075, 0.0462, 0.0380],
        [0.0000, 0.0188, 0.0056,  ..., 0.0036, 0.0191, 0.0155],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41280.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0382, 0.0000, 0.0000,  ..., 0.0886, 0.0846, 0.0433],
        [0.0384, 0.0000, 0.0000,  ..., 0.0896, 0.0852, 0.0439],
        [0.0351, 0.0010, 0.0000,  ..., 0.0697, 0.0676, 0.0342],
        ...,
        [0.0216, 0.0106, 0.0099,  ..., 0.0008, 0.0089, 0.0000],
        [0.0216, 0.0106, 0.0099,  ..., 0.0008, 0.0089, 0.0000],
        [0.0216, 0.0106, 0.0099,  ..., 0.0008, 0.0089, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(250048.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5857.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.7196, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(381.0785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(500.6156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0541],
        [ 0.0626],
        [ 0.0679],
        ...,
        [-0.0465],
        [-0.0461],
        [-0.0463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-8030.0596, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0001],
        ...,
        [0.9995],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365794.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(1012.2387, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365800.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030,  0.0075,  0.0021,  ...,  0.0013,  0.0077,  0.0061],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        ...,
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-867.8295, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7474, device='cuda:0')



h[100].sum tensor(-5.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5931, device='cuda:0')



h[200].sum tensor(-19.2470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0299, 0.0096,  ..., 0.0052, 0.0302, 0.0251],
        [0.0000, 0.0075, 0.0021,  ..., 0.0020, 0.0077, 0.0061],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29528.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0394, 0.0014, 0.0000,  ..., 0.1065, 0.1008, 0.0505],
        [0.0310, 0.0046, 0.0019,  ..., 0.0542, 0.0548, 0.0254],
        [0.0251, 0.0088, 0.0053,  ..., 0.0181, 0.0237, 0.0080],
        ...,
        [0.0220, 0.0111, 0.0105,  ..., 0.0012, 0.0090, 0.0000],
        [0.0220, 0.0111, 0.0105,  ..., 0.0012, 0.0090, 0.0000],
        [0.0220, 0.0111, 0.0105,  ..., 0.0012, 0.0090, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189584.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5599.0830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.7417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-9.5725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(354.5648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(398.0240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0287],
        [ 0.0192],
        [ 0.0021],
        ...,
        [-0.0518],
        [-0.0516],
        [-0.0515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-11184.9072, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365800.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0002],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365807.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        ...,
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0005, -0.0007,  ...,  0.0002, -0.0004, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-908.1757, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1503, device='cuda:0')



h[100].sum tensor(-4.7486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2136, device='cuda:0')



h[200].sum tensor(-19.5716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0188, 0.0062,  ..., 0.0036, 0.0190, 0.0159],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25986.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.3609e-02, 1.0768e-02, 8.7065e-03,  ..., 6.7384e-03, 1.3176e-02,
         2.6162e-03],
        [2.5245e-02, 9.4132e-03, 5.2718e-03,  ..., 1.8040e-02, 2.3346e-02,
         7.9027e-03],
        [3.0472e-02, 6.0036e-03, 1.1566e-03,  ..., 5.2961e-02, 5.4225e-02,
         2.4321e-02],
        ...,
        [2.2609e-02, 1.1556e-02, 1.1163e-02,  ..., 1.5510e-03, 9.0191e-03,
         5.6445e-05],
        [2.2609e-02, 1.1556e-02, 1.1163e-02,  ..., 1.5510e-03, 9.0191e-03,
         5.6384e-05],
        [2.2609e-02, 1.1556e-02, 1.1163e-02,  ..., 1.5509e-03, 9.0189e-03,
         5.6368e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171314.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5555.7759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.3948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.7984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(418.0439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(367.2849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0081],
        [ 0.0047],
        [ 0.0024],
        ...,
        [-0.0554],
        [-0.0553],
        [-0.0552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14516.9785, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0002],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365807.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0002],
        ...,
        [0.9997],
        [0.9996],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365815.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        ...,
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-801.4041, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0207, device='cuda:0')



h[100].sum tensor(-6.9205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2174, device='cuda:0')



h[200].sum tensor(-19.7786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36716.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0271, 0.0082, 0.0053,  ..., 0.0274, 0.0308, 0.0126],
        [0.0245, 0.0108, 0.0085,  ..., 0.0088, 0.0149, 0.0037],
        [0.0238, 0.0116, 0.0112,  ..., 0.0035, 0.0103, 0.0011],
        ...,
        [0.0234, 0.0118, 0.0118,  ..., 0.0019, 0.0090, 0.0003],
        [0.0234, 0.0118, 0.0118,  ..., 0.0019, 0.0090, 0.0003],
        [0.0234, 0.0118, 0.0118,  ..., 0.0019, 0.0090, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(227215.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5925.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-9.3497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(505.2268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(468.3293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0145],
        [-0.0176],
        [-0.0125],
        ...,
        [-0.0596],
        [-0.0594],
        [-0.0594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-10555.8574, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0002],
        ...,
        [0.9997],
        [0.9996],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365815.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0003],
        ...,
        [0.9997],
        [0.9996],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365825.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0026,  0.0064,  0.0017,  ...,  0.0011,  0.0065,  0.0050],
        [-0.0033,  0.0082,  0.0024,  ...,  0.0014,  0.0084,  0.0067],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        ...,
        [-0.0042,  0.0108,  0.0033,  ...,  0.0017,  0.0110,  0.0089],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0002, -0.0004, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-842.7093, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7759, device='cuda:0')



h[100].sum tensor(-6.1451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8894, device='cuda:0')



h[200].sum tensor(-20.0506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0632, 0.0205,  ..., 0.0098, 0.0640, 0.0530],
        [0.0000, 0.0251, 0.0079,  ..., 0.0043, 0.0255, 0.0209],
        [0.0000, 0.0082, 0.0024,  ..., 0.0019, 0.0084, 0.0067],
        ...,
        [0.0000, 0.0804, 0.0272,  ..., 0.0121, 0.0810, 0.0683],
        [0.0000, 0.0711, 0.0239,  ..., 0.0108, 0.0716, 0.0602],
        [0.0000, 0.0511, 0.0172,  ..., 0.0079, 0.0515, 0.0434]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32089.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0429, 0.0000, 0.0000,  ..., 0.1434, 0.1284, 0.0691],
        [0.0368, 0.0020, 0.0000,  ..., 0.0962, 0.0878, 0.0465],
        [0.0308, 0.0054, 0.0028,  ..., 0.0499, 0.0485, 0.0242],
        ...,
        [0.0671, 0.0000, 0.0000,  ..., 0.3302, 0.2982, 0.1537],
        [0.0586, 0.0000, 0.0000,  ..., 0.2655, 0.2402, 0.1240],
        [0.0481, 0.0000, 0.0000,  ..., 0.1852, 0.1691, 0.0869]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203059.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5970.6748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.9986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-11.5450, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(699.9065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(433.2047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0299],
        [ 0.0258],
        [ 0.0159],
        ...,
        [-0.1184],
        [-0.0845],
        [-0.0534]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-15686.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9998],
        [1.0003],
        ...,
        [0.9997],
        [0.9996],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365825.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0003],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365835.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0006, -0.0007,  ...,  0.0001, -0.0004, -0.0010],
        [-0.0049,  0.0129,  0.0041,  ...,  0.0020,  0.0130,  0.0107],
        [-0.0049,  0.0129,  0.0041,  ...,  0.0020,  0.0130,  0.0107],
        ...,
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0006, -0.0007,  ...,  0.0001, -0.0004, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-889.2473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0298, device='cuda:0')



h[100].sum tensor(-5.2831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4882, device='cuda:0')



h[200].sum tensor(-20.3024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0232, 0.0072,  ..., 0.0039, 0.0236, 0.0192],
        [0.0000, 0.0336, 0.0104,  ..., 0.0054, 0.0341, 0.0277],
        [0.0000, 0.0496, 0.0161,  ..., 0.0076, 0.0502, 0.0416],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29461.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0389, 0.0007, 0.0000,  ..., 0.1131, 0.1029, 0.0547],
        [0.0448, 0.0002, 0.0000,  ..., 0.1606, 0.1445, 0.0770],
        [0.0522, 0.0000, 0.0000,  ..., 0.2192, 0.1969, 0.1041],
        ...,
        [0.0251, 0.0118, 0.0132,  ..., 0.0027, 0.0092, 0.0015],
        [0.0251, 0.0118, 0.0132,  ..., 0.0027, 0.0092, 0.0015],
        [0.0251, 0.0118, 0.0132,  ..., 0.0027, 0.0092, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193568.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6088.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.7479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-13.0420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(918.0665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(417.4612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0753],
        [-0.1065],
        [-0.1367],
        ...,
        [-0.0714],
        [-0.0712],
        [-0.0711]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19541.8398, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0003],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365835.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0003],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365846.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.5309e-04, -7.4850e-04,  ...,  6.1718e-05,
         -3.7011e-04, -9.6110e-04],
        [ 0.0000e+00, -5.5309e-04, -7.4850e-04,  ...,  6.1718e-05,
         -3.7011e-04, -9.6110e-04],
        [-3.0833e-03,  7.8259e-03,  2.2529e-03,  ...,  1.2257e-03,
          8.0107e-03,  6.3035e-03],
        ...,
        [ 0.0000e+00, -5.5309e-04, -7.4850e-04,  ...,  6.1718e-05,
         -3.7011e-04, -9.6110e-04],
        [ 0.0000e+00, -5.5309e-04, -7.4850e-04,  ...,  6.1718e-05,
         -3.7011e-04, -9.6110e-04],
        [ 0.0000e+00, -5.5309e-04, -7.4850e-04,  ...,  6.1718e-05,
         -3.7011e-04, -9.6110e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-880.5499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8833, device='cuda:0')



h[100].sum tensor(-5.4615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6129, device='cuda:0')



h[200].sum tensor(-20.5028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0023,  ..., 0.0014, 0.0080, 0.0063],
        [0.0000, 0.0181, 0.0054,  ..., 0.0029, 0.0185, 0.0147],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31662.2754, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.7625e-02, 9.0243e-03, 7.2223e-03,  ..., 1.6669e-02, 1.9169e-02,
         9.2330e-03],
        [3.1084e-02, 4.6743e-03, 3.4498e-03,  ..., 4.6664e-02, 4.3615e-02,
         2.4023e-02],
        [3.5833e-02, 1.7794e-03, 9.6038e-05,  ..., 8.7059e-02, 7.7341e-02,
         4.3700e-02],
        ...,
        [2.5967e-02, 1.1639e-02, 1.3937e-02,  ..., 3.0973e-03, 9.3259e-03,
         2.0039e-03],
        [2.5967e-02, 1.1639e-02, 1.3937e-02,  ..., 3.0973e-03, 9.3260e-03,
         2.0038e-03],
        [2.5966e-02, 1.1639e-02, 1.3937e-02,  ..., 3.0972e-03, 9.3257e-03,
         2.0036e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208321.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6314.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.9715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.0682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1087.0874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(448.7124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0181],
        [ 0.0243],
        [ 0.0251],
        ...,
        [-0.0772],
        [-0.0769],
        [-0.0769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14825.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0003],
        ...,
        [0.9998],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365846.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0003],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365857.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.5963e-03,  2.2869e-02,  7.6258e-03,  ...,  3.2507e-03,
          2.3049e-02,  1.9335e-02],
        [-8.9006e-03,  2.3698e-02,  7.9226e-03,  ...,  3.3657e-03,
          2.3877e-02,  2.0054e-02],
        [-7.5146e-03,  1.9923e-02,  6.5704e-03,  ...,  2.8420e-03,
          2.0102e-02,  1.6781e-02],
        ...,
        [ 0.0000e+00, -5.4021e-04, -7.6080e-04,  ...,  2.4096e-06,
         -3.6673e-04, -9.6435e-04],
        [ 0.0000e+00, -5.4021e-04, -7.6080e-04,  ...,  2.4096e-06,
         -3.6673e-04, -9.6435e-04],
        [ 0.0000e+00, -5.4021e-04, -7.6080e-04,  ...,  2.4096e-06,
         -3.6673e-04, -9.6435e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-919.6631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2353, device='cuda:0')



h[100].sum tensor(-4.6871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2260, device='cuda:0')



h[200].sum tensor(-20.7127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.5954e-02, 2.8526e-02,  ..., 1.2236e-02, 8.6669e-02,
         7.2552e-02],
        [0.0000e+00, 9.2696e-02, 3.0941e-02,  ..., 1.3172e-02, 9.3414e-02,
         7.8399e-02],
        [0.0000e+00, 1.1491e-01, 3.8899e-02,  ..., 1.6255e-02, 1.1564e-01,
         9.7663e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.6347e-06, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.6352e-06, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.6350e-06, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27193.9707, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0514, 0.0000, 0.0000,  ..., 0.2274, 0.2009, 0.1102],
        [0.0537, 0.0000, 0.0000,  ..., 0.2477, 0.2185, 0.1198],
        [0.0562, 0.0000, 0.0000,  ..., 0.2695, 0.2377, 0.1300],
        ...,
        [0.0267, 0.0116, 0.0147,  ..., 0.0036, 0.0095, 0.0024],
        [0.0267, 0.0116, 0.0147,  ..., 0.0036, 0.0095, 0.0024],
        [0.0267, 0.0116, 0.0147,  ..., 0.0036, 0.0095, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185271.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6362.3325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.5429, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.4399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1312.2231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(418.1594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0215],
        [ 0.0203],
        [ 0.0221],
        ...,
        [-0.0846],
        [-0.0843],
        [-0.0842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19662.1543, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0003],
        ...,
        [0.9999],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365857.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365870.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.2585e-04, -7.7328e-04,  ..., -5.4884e-05,
         -3.6272e-04, -9.6829e-04],
        [-3.3002e-03,  8.4801e-03,  2.4534e-03,  ...,  1.1934e-03,
          8.6456e-03,  6.8422e-03],
        [ 0.0000e+00, -5.2585e-04, -7.7328e-04,  ..., -5.4884e-05,
         -3.6272e-04, -9.6829e-04],
        ...,
        [ 0.0000e+00, -5.2585e-04, -7.7328e-04,  ..., -5.4884e-05,
         -3.6272e-04, -9.6829e-04],
        [ 0.0000e+00, -5.2585e-04, -7.7328e-04,  ..., -5.4884e-05,
         -3.6272e-04, -9.6829e-04],
        [ 0.0000e+00, -5.2585e-04, -7.7328e-04,  ..., -5.4884e-05,
         -3.6272e-04, -9.6829e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-878.7278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5778, device='cuda:0')



h[100].sum tensor(-5.3878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5683, device='cuda:0')



h[200].sum tensor(-20.8628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0370, 0.0109,  ..., 0.0052, 0.0376, 0.0300],
        [0.0000, 0.0068, 0.0019,  ..., 0.0010, 0.0070, 0.0054],
        [0.0000, 0.0085, 0.0025,  ..., 0.0012, 0.0086, 0.0068],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30737.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0366, 0.0000, 0.0000,  ..., 0.0967, 0.0830, 0.0499],
        [0.0330, 0.0029, 0.0019,  ..., 0.0601, 0.0535, 0.0315],
        [0.0309, 0.0054, 0.0047,  ..., 0.0372, 0.0351, 0.0201],
        ...,
        [0.0274, 0.0117, 0.0157,  ..., 0.0041, 0.0097, 0.0029],
        [0.0274, 0.0117, 0.0157,  ..., 0.0041, 0.0097, 0.0029],
        [0.0274, 0.0117, 0.0157,  ..., 0.0041, 0.0097, 0.0029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203158.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6546.3833, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.8948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-7.9083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1453.7845, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(459.2013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0618],
        [ 0.0599],
        [ 0.0554],
        ...,
        [-0.0909],
        [-0.0907],
        [-0.0906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-18629.9805, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365870.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365883.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-820.2997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8433, device='cuda:0')



h[100].sum tensor(-6.3606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0454, device='cuda:0')



h[200].sum tensor(-20.9901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0147, 0.0041,  ..., 0.0020, 0.0150, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0033,  ..., 0.0015, 0.0109, 0.0088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35143.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0286, 0.0108, 0.0140,  ..., 0.0097, 0.0134, 0.0061],
        [0.0308, 0.0065, 0.0076,  ..., 0.0324, 0.0318, 0.0174],
        [0.0374, 0.0026, 0.0020,  ..., 0.1002, 0.0886, 0.0506],
        ...,
        [0.0282, 0.0115, 0.0156,  ..., 0.0066, 0.0112, 0.0043],
        [0.0299, 0.0082, 0.0083,  ..., 0.0246, 0.0261, 0.0133],
        [0.0335, 0.0039, 0.0030,  ..., 0.0631, 0.0576, 0.0326]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(224240.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6712.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5.1534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1598.3083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(507.2029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0197],
        [-0.0333],
        [-0.0675],
        ...,
        [-0.0527],
        [-0.0201],
        [ 0.0053]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14732.4883, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365883.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365883.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-930.7125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1855, device='cuda:0')



h[100].sum tensor(-4.3421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0726, device='cuda:0')



h[200].sum tensor(-21.0483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26149.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0283, 0.0116, 0.0158,  ..., 0.0063, 0.0110, 0.0042],
        [0.0282, 0.0120, 0.0166,  ..., 0.0048, 0.0098, 0.0035],
        [0.0283, 0.0120, 0.0166,  ..., 0.0048, 0.0098, 0.0035],
        ...,
        [0.0281, 0.0119, 0.0165,  ..., 0.0047, 0.0097, 0.0034],
        [0.0281, 0.0119, 0.0165,  ..., 0.0047, 0.0097, 0.0034],
        [0.0281, 0.0119, 0.0165,  ..., 0.0047, 0.0097, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182635.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6583.9888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.4495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-8.6226, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1654.6924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(426.8250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0335],
        [-0.0648],
        [-0.0897],
        ...,
        [-0.0971],
        [-0.0968],
        [-0.0966]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-22691.9727, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365883.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365883.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0001, -0.0004, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-871.1447, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8411, device='cuda:0')



h[100].sum tensor(-5.4311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6067, device='cuda:0')



h[200].sum tensor(-21.0169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31170.4902, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0283, 0.0116, 0.0157,  ..., 0.0063, 0.0105, 0.0045],
        [0.0283, 0.0119, 0.0163,  ..., 0.0053, 0.0100, 0.0038],
        [0.0283, 0.0120, 0.0166,  ..., 0.0048, 0.0098, 0.0035],
        ...,
        [0.0281, 0.0119, 0.0165,  ..., 0.0047, 0.0097, 0.0034],
        [0.0281, 0.0119, 0.0165,  ..., 0.0047, 0.0097, 0.0034],
        [0.0281, 0.0119, 0.0165,  ..., 0.0047, 0.0097, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(205917.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6656.3433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.9410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6.1404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1620.9526, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(472.2061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0031],
        [-0.0178],
        [-0.0386],
        ...,
        [-0.0964],
        [-0.0959],
        [-0.0957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17471.1426, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0000],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365883.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365896.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0004, -0.0010],
        [-0.0036,  0.0094,  0.0027,  ...,  0.0012,  0.0095,  0.0076],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0004, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0004, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-825.5255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8387, device='cuda:0')



h[100].sum tensor(-6.1694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8986, device='cuda:0')



h[200].sum tensor(-21.1371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0164, 0.0046,  ..., 0.0021, 0.0167, 0.0132],
        [0.0000, 0.0421, 0.0132,  ..., 0.0055, 0.0425, 0.0349],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35648.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0317, 0.0055, 0.0067,  ..., 0.0380, 0.0351, 0.0208],
        [0.0365, 0.0021, 0.0016,  ..., 0.0904, 0.0782, 0.0469],
        [0.0436, 0.0000, 0.0000,  ..., 0.1639, 0.1412, 0.0824],
        ...,
        [0.0288, 0.0121, 0.0175,  ..., 0.0053, 0.0099, 0.0037],
        [0.0288, 0.0121, 0.0175,  ..., 0.0053, 0.0099, 0.0037],
        [0.0288, 0.0121, 0.0175,  ..., 0.0053, 0.0099, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(233407.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6856.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-5.4182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1782.3939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(518.5943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0481],
        [ 0.0608],
        [ 0.0604],
        ...,
        [-0.0934],
        [-0.1007],
        [-0.1024]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-20218.0039, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365896.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365909.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-924.8842, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1110, device='cuda:0')



h[100].sum tensor(-4.3090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0617, device='cuda:0')



h[200].sum tensor(-21.3193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26679.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0123, 0.0187,  ..., 0.0057, 0.0103, 0.0039],
        [0.0297, 0.0123, 0.0187,  ..., 0.0057, 0.0103, 0.0040],
        [0.0299, 0.0123, 0.0188,  ..., 0.0058, 0.0103, 0.0040],
        ...,
        [0.0296, 0.0122, 0.0187,  ..., 0.0057, 0.0102, 0.0039],
        [0.0296, 0.0122, 0.0187,  ..., 0.0057, 0.0102, 0.0039],
        [0.0296, 0.0122, 0.0187,  ..., 0.0057, 0.0102, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189299.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6907.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.5067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3.5352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1945.2831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(451.2353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1394],
        [-0.1472],
        [-0.1532],
        ...,
        [-0.1106],
        [-0.1101],
        [-0.1100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-23168.4043, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0001],
        [0.9999],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365909.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365921.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0132,  0.0359,  0.0122,  ...,  0.0048,  0.0360,  0.0306],
        [-0.0141,  0.0383,  0.0131,  ...,  0.0051,  0.0385,  0.0327],
        [-0.0113,  0.0305,  0.0103,  ...,  0.0040,  0.0307,  0.0259],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0002, -0.0003, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-729.7892, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0712, device='cuda:0')



h[100].sum tensor(-7.6898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6631, device='cuda:0')



h[200].sum tensor(-21.3401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1495, 0.0510,  ..., 0.0199, 0.1501, 0.1275],
        [0.0000, 0.1542, 0.0526,  ..., 0.0206, 0.1547, 0.1315],
        [0.0000, 0.1691, 0.0580,  ..., 0.0226, 0.1697, 0.1444],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44273.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0000, 0.0000,  ..., 0.4235, 0.3725, 0.2041],
        [0.0725, 0.0000, 0.0000,  ..., 0.4605, 0.4057, 0.2211],
        [0.0749, 0.0000, 0.0000,  ..., 0.4841, 0.4270, 0.2317],
        ...,
        [0.0305, 0.0123, 0.0199,  ..., 0.0061, 0.0105, 0.0040],
        [0.0305, 0.0123, 0.0199,  ..., 0.0061, 0.0105, 0.0040],
        [0.0305, 0.0123, 0.0199,  ..., 0.0061, 0.0105, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287760.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7305.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1.8851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2046.5194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(606.8565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0417],
        [-0.0595],
        [-0.0626],
        ...,
        [-0.1184],
        [-0.1179],
        [-0.1178]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-26457.2695, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365921.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365932.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035,  0.0091,  0.0026,  ...,  0.0010,  0.0093,  0.0073],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-918.8947, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5383, device='cuda:0')



h[100].sum tensor(-4.3615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1242, device='cuda:0')



h[200].sum tensor(-21.5418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0206, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0020,  ..., 0.0008, 0.0075, 0.0058],
        [0.0000, 0.0091, 0.0026,  ..., 0.0010, 0.0093, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27529.4121, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0337, 0.0056, 0.0076,  ..., 0.0349, 0.0323, 0.0191],
        [0.0333, 0.0072, 0.0115,  ..., 0.0285, 0.0277, 0.0156],
        [0.0322, 0.0109, 0.0180,  ..., 0.0133, 0.0162, 0.0077],
        ...,
        [0.0314, 0.0124, 0.0213,  ..., 0.0063, 0.0109, 0.0039],
        [0.0314, 0.0124, 0.0213,  ..., 0.0063, 0.0109, 0.0039],
        [0.0314, 0.0124, 0.0213,  ..., 0.0063, 0.0109, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(200083.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7307.8794, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.5909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-0.1826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2145.6655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(472.3495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0873],
        [-0.0939],
        [-0.0927],
        ...,
        [-0.1270],
        [-0.1265],
        [-0.1264]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30486.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365932.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365944.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0171,  0.0055,  ...,  0.0021,  0.0172,  0.0142],
        [-0.0073,  0.0196,  0.0064,  ...,  0.0025,  0.0198,  0.0164],
        [-0.0203,  0.0558,  0.0193,  ...,  0.0075,  0.0559,  0.0478],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-784.0264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6788, device='cuda:0')



h[100].sum tensor(-6.5997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1675, device='cuda:0')



h[200].sum tensor(-21.5756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1352, 0.0457,  ..., 0.0177, 0.1358, 0.1148],
        [0.0000, 0.1536, 0.0523,  ..., 0.0202, 0.1541, 0.1308],
        [0.0000, 0.0928, 0.0305,  ..., 0.0118, 0.0933, 0.0780],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39873.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0662, 0.0000, 0.0000,  ..., 0.3906, 0.3418, 0.1886],
        [0.0688, 0.0000, 0.0000,  ..., 0.4179, 0.3661, 0.2011],
        [0.0625, 0.0000, 0.0000,  ..., 0.3490, 0.3049, 0.1693],
        ...,
        [0.0321, 0.0126, 0.0226,  ..., 0.0067, 0.0112, 0.0039],
        [0.0321, 0.0126, 0.0226,  ..., 0.0067, 0.0112, 0.0039],
        [0.0321, 0.0126, 0.0226,  ..., 0.0067, 0.0112, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(268510.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7590.7026, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.9049, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2185.2944, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(582.2269, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0262],
        [-0.0306],
        [-0.0196],
        ...,
        [-0.1353],
        [-0.1348],
        [-0.1347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-32909.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365944.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365956.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0011],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0011],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0011],
        ...,
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0011],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0011],
        [ 0.0000, -0.0005, -0.0008,  ..., -0.0003, -0.0003, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-795.6957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1020, device='cuda:0')



h[100].sum tensor(-6.2487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0832, device='cuda:0')



h[200].sum tensor(-21.6741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0230, 0.0069,  ..., 0.0026, 0.0232, 0.0186],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37312.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0336, 0.0109, 0.0195,  ..., 0.0164, 0.0189, 0.0086],
        [0.0344, 0.0084, 0.0145,  ..., 0.0265, 0.0261, 0.0141],
        [0.0378, 0.0032, 0.0052,  ..., 0.0675, 0.0576, 0.0354],
        ...,
        [0.0327, 0.0129, 0.0238,  ..., 0.0070, 0.0115, 0.0038],
        [0.0327, 0.0129, 0.0238,  ..., 0.0070, 0.0115, 0.0038],
        [0.0327, 0.0129, 0.0238,  ..., 0.0070, 0.0115, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255247.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7686.6084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(5.4588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2245.6006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(566.1214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0310],
        [ 0.0043],
        [ 0.0538],
        ...,
        [-0.1435],
        [-0.1430],
        [-0.1428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-32077.5078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365956.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365968., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036,  0.0094,  0.0027,  ...,  0.0010,  0.0096,  0.0075],
        [-0.0023,  0.0059,  0.0014,  ...,  0.0005,  0.0060,  0.0044],
        [-0.0058,  0.0158,  0.0050,  ...,  0.0019,  0.0159,  0.0130],
        ...,
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0011],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0011],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-898.2882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4661, device='cuda:0')



h[100].sum tensor(-4.2964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1136, device='cuda:0')



h[200].sum tensor(-21.8101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0187, 0.0053,  ..., 0.0020, 0.0190, 0.0148],
        [0.0000, 0.0521, 0.0159,  ..., 0.0060, 0.0526, 0.0423],
        [0.0000, 0.0312, 0.0091,  ..., 0.0034, 0.0316, 0.0249],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28511.4121, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0395, 0.0016, 0.0028,  ..., 0.0865, 0.0728, 0.0445],
        [0.0425, 0.0000, 0.0000,  ..., 0.1246, 0.1034, 0.0637],
        [0.0417, 0.0000, 0.0000,  ..., 0.1137, 0.0942, 0.0584],
        ...,
        [0.0333, 0.0132, 0.0249,  ..., 0.0074, 0.0117, 0.0039],
        [0.0333, 0.0132, 0.0249,  ..., 0.0074, 0.0117, 0.0039],
        [0.0333, 0.0132, 0.0249,  ..., 0.0074, 0.0117, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215707.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7654.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.6809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.0995, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2405.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(488.9240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0858],
        [ 0.0949],
        [ 0.0935],
        ...,
        [-0.1431],
        [-0.1365],
        [-0.1310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45460.1328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365968., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365979.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        ...,
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-629.7266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2594, device='cuda:0')



h[100].sum tensor(-8.4460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1289, device='cuda:0')



h[200].sum tensor(-21.7661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0010,  ..., 0.0003, 0.0049, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46044.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0336, 0.0135, 0.0258,  ..., 0.0080, 0.0119, 0.0040],
        [0.0336, 0.0135, 0.0258,  ..., 0.0080, 0.0119, 0.0040],
        [0.0339, 0.0131, 0.0248,  ..., 0.0104, 0.0138, 0.0052],
        ...,
        [0.0338, 0.0124, 0.0236,  ..., 0.0123, 0.0145, 0.0064],
        [0.0342, 0.0108, 0.0205,  ..., 0.0187, 0.0184, 0.0099],
        [0.0349, 0.0081, 0.0155,  ..., 0.0291, 0.0248, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(294201.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7829.9033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(4.9439, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2452.7100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(638.7172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1853],
        [-0.1528],
        [-0.1019],
        ...,
        [-0.1293],
        [-0.1039],
        [-0.0799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42892.0664, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0004],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365979.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0005],
        ...,
        [1.0005],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365991.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        ...,
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0012]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-782.1986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3863, device='cuda:0')



h[100].sum tensor(-5.6215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8325, device='cuda:0')



h[200].sum tensor(-21.9128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0188, 0.0060,  ..., 0.0022, 0.0190, 0.0155],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34364.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.0031, 0.0038,  ..., 0.0762, 0.0666, 0.0374],
        [0.0364, 0.0068, 0.0111,  ..., 0.0448, 0.0410, 0.0219],
        [0.0353, 0.0096, 0.0170,  ..., 0.0295, 0.0280, 0.0146],
        ...,
        [0.0336, 0.0138, 0.0264,  ..., 0.0086, 0.0118, 0.0040],
        [0.0336, 0.0138, 0.0264,  ..., 0.0086, 0.0118, 0.0040],
        [0.0336, 0.0138, 0.0264,  ..., 0.0086, 0.0118, 0.0040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244319.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7696.2158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.2538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(8.6075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2590.7180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(540.3096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0598],
        [ 0.0449],
        [ 0.0334],
        ...,
        [-0.1648],
        [-0.1642],
        [-0.1640]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45539.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0005],
        ...,
        [1.0005],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365991.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(1030.7114, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0005],
        ...,
        [1.0005],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366002.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024,  0.0062,  0.0015,  ...,  0.0005,  0.0064,  0.0045],
        [-0.0024,  0.0062,  0.0015,  ...,  0.0005,  0.0064,  0.0045],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0013],
        ...,
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0013],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0013],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0004, -0.0003, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-794.7408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3015, device='cuda:0')



h[100].sum tensor(-5.0576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5279, device='cuda:0')



h[200].sum tensor(-21.9895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0026,  ..., 0.0008, 0.0116, 0.0079],
        [0.0000, 0.0112, 0.0026,  ..., 0.0008, 0.0116, 0.0079],
        [0.0000, 0.0112, 0.0026,  ..., 0.0008, 0.0116, 0.0079],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31962.5762, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0370, 0.0026, 0.0041,  ..., 0.0611, 0.0441, 0.0319],
        [0.0362, 0.0049, 0.0088,  ..., 0.0491, 0.0361, 0.0256],
        [0.0357, 0.0073, 0.0133,  ..., 0.0389, 0.0299, 0.0201],
        ...,
        [0.0336, 0.0141, 0.0272,  ..., 0.0093, 0.0116, 0.0041],
        [0.0336, 0.0141, 0.0272,  ..., 0.0093, 0.0116, 0.0041],
        [0.0336, 0.0141, 0.0272,  ..., 0.0093, 0.0116, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232581.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7620.9756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(11.8752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2718.4644, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(519.3197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0470],
        [-0.0080],
        [-0.0732],
        ...,
        [-0.1715],
        [-0.1709],
        [-0.1707]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-45534.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9999],
        [1.0005],
        ...,
        [1.0005],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366002.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [1.0000],
        [1.0005],
        ...,
        [1.0005],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366013.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0003, -0.0013],
        [-0.0108,  0.0298,  0.0100,  ...,  0.0037,  0.0300,  0.0248],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0003, -0.0013],
        ...,
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0003, -0.0013],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0003, -0.0013],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0003, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-647.7021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6149, device='cuda:0')



h[100].sum tensor(-7.0027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4503, device='cuda:0')



h[200].sum tensor(-21.9897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0465, 0.0153,  ..., 0.0056, 0.0469, 0.0384],
        [0.0000, 0.0582, 0.0194,  ..., 0.0072, 0.0586, 0.0485],
        [0.0000, 0.1253, 0.0421,  ..., 0.0156, 0.1261, 0.1047],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42008.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0015, 0.0010,  ..., 0.1349, 0.1139, 0.0643],
        [0.0462, 0.0000, 0.0000,  ..., 0.1763, 0.1486, 0.0840],
        [0.0493, 0.0000, 0.0000,  ..., 0.2138, 0.1810, 0.1016],
        ...,
        [0.0336, 0.0144, 0.0276,  ..., 0.0101, 0.0114, 0.0041],
        [0.0336, 0.0144, 0.0276,  ..., 0.0101, 0.0114, 0.0041],
        [0.0336, 0.0144, 0.0276,  ..., 0.0101, 0.0114, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(283940.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7667.4971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(17.4695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2799.7380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(605.1440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0010],
        [ 0.0060],
        [ 0.0005],
        ...,
        [-0.1772],
        [-0.1766],
        [-0.1764]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44909.1445, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [1.0000],
        [1.0005],
        ...,
        [1.0005],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366013.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0005],
        ...,
        [1.0005],
        [1.0003],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366025.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0176,  0.0489,  0.0168,  ...,  0.0063,  0.0491,  0.0413],
        [-0.0166,  0.0462,  0.0158,  ...,  0.0059,  0.0464,  0.0390],
        [-0.0168,  0.0467,  0.0160,  ...,  0.0060,  0.0470,  0.0395],
        ...,
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0014],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0014],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0014]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-690.7523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2490, device='cuda:0')



h[100].sum tensor(-5.8899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9586, device='cuda:0')



h[200].sum tensor(-22.0715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1844, 0.0632,  ..., 0.0237, 0.1853, 0.1557],
        [0.0000, 0.2010, 0.0692,  ..., 0.0260, 0.2019, 0.1701],
        [0.0000, 0.1971, 0.0678,  ..., 0.0254, 0.1980, 0.1667],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36677.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0742, 0.0000, 0.0000,  ..., 0.5436, 0.4634, 0.2539],
        [0.0756, 0.0000, 0.0000,  ..., 0.5611, 0.4790, 0.2618],
        [0.0734, 0.0000, 0.0000,  ..., 0.5330, 0.4538, 0.2492],
        ...,
        [0.0336, 0.0147, 0.0279,  ..., 0.0108, 0.0111, 0.0041],
        [0.0336, 0.0147, 0.0279,  ..., 0.0108, 0.0111, 0.0041],
        [0.0336, 0.0147, 0.0279,  ..., 0.0108, 0.0111, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(257453.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7544.5107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4873, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(22.9184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2931.3628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(559.3089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1104],
        [-0.1092],
        [-0.0955],
        ...,
        [-0.1823],
        [-0.1816],
        [-0.1814]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42696.1367, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0005],
        ...,
        [1.0005],
        [1.0003],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366025.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0006],
        ...,
        [1.0006],
        [1.0003],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366037.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0015],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0015],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0015],
        ...,
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0015],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0015],
        [ 0.0000, -0.0004, -0.0009,  ..., -0.0005, -0.0002, -0.0015]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-607.9776, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3194, device='cuda:0')



h[100].sum tensor(-6.6727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4072, device='cuda:0')



h[200].sum tensor(-22.0950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40528.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0342, 0.0112, 0.0189,  ..., 0.0310, 0.0233, 0.0142],
        [0.0334, 0.0140, 0.0258,  ..., 0.0164, 0.0126, 0.0068],
        [0.0334, 0.0145, 0.0270,  ..., 0.0142, 0.0113, 0.0056],
        ...,
        [0.0331, 0.0148, 0.0279,  ..., 0.0119, 0.0106, 0.0042],
        [0.0331, 0.0148, 0.0279,  ..., 0.0119, 0.0106, 0.0042],
        [0.0331, 0.0148, 0.0279,  ..., 0.0119, 0.0106, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(277427.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7378.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(24.6253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3092.7700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(584.0488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0128],
        [-0.0412],
        [-0.0968],
        ...,
        [-0.1863],
        [-0.1856],
        [-0.1854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50044.2305, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0006],
        ...,
        [1.0006],
        [1.0003],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366037.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0006],
        ...,
        [1.0006],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366048.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0008,  ..., -0.0005, -0.0002, -0.0016],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0005, -0.0002, -0.0016],
        [-0.0047,  0.0128,  0.0039,  ...,  0.0013,  0.0130,  0.0099],
        ...,
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0005, -0.0002, -0.0016],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0005, -0.0002, -0.0016],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0005, -0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-718.1529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7727, device='cuda:0')



h[100].sum tensor(-4.5362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3045, device='cuda:0')



h[200].sum tensor(-22.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0226, 0.0074,  ..., 0.0026, 0.0229, 0.0184],
        [0.0000, 0.0128, 0.0039,  ..., 0.0013, 0.0130, 0.0099],
        [0.0000, 0.0381, 0.0122,  ..., 0.0043, 0.0385, 0.0305],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31135.7402, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.0006, 0.0000,  ..., 0.1142, 0.0895, 0.0538],
        [0.0384, 0.0005, 0.0000,  ..., 0.1083, 0.0842, 0.0512],
        [0.0411, 0.0002, 0.0000,  ..., 0.1454, 0.1153, 0.0686],
        ...,
        [0.0327, 0.0149, 0.0280,  ..., 0.0130, 0.0101, 0.0045],
        [0.0327, 0.0149, 0.0280,  ..., 0.0130, 0.0101, 0.0045],
        [0.0327, 0.0149, 0.0280,  ..., 0.0130, 0.0101, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234754.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7137.6143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.9556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(31.9666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3216.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(500.4974, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0460],
        [ 0.0444],
        [ 0.0387],
        ...,
        [-0.1904],
        [-0.1897],
        [-0.1895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-49701.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0006],
        ...,
        [1.0006],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366048.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0006],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366060.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058,  0.0160,  0.0050,  ...,  0.0017,  0.0162,  0.0125],
        [-0.0028,  0.0075,  0.0020,  ...,  0.0005,  0.0078,  0.0052],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0016],
        ...,
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0016],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0016],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-520.6204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8119, device='cuda:0')



h[100].sum tensor(-7.1741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6252, device='cuda:0')



h[200].sum tensor(-22.1599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0684, 0.0217,  ..., 0.0074, 0.0693, 0.0541],
        [0.0000, 0.0411, 0.0133,  ..., 0.0047, 0.0416, 0.0331],
        [0.0000, 0.0075, 0.0020,  ..., 0.0005, 0.0078, 0.0052],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43141.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0439, 0.0000, 0.0000,  ..., 0.2111, 0.1636, 0.1007],
        [0.0409, 0.0000, 0.0000,  ..., 0.1599, 0.1230, 0.0759],
        [0.0376, 0.0012, 0.0002,  ..., 0.1063, 0.0791, 0.0504],
        ...,
        [0.0324, 0.0150, 0.0283,  ..., 0.0139, 0.0097, 0.0047],
        [0.0324, 0.0150, 0.0283,  ..., 0.0139, 0.0097, 0.0047],
        [0.0324, 0.0150, 0.0283,  ..., 0.0139, 0.0097, 0.0047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(286109.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7097.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(43.5025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3167.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(601.8875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0452],
        [-0.0141],
        [ 0.0131],
        ...,
        [-0.1953],
        [-0.1946],
        [-0.1944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42207.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0006],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366060.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366072.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0017],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0017],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0017],
        ...,
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0017],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0017],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-675.0415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0072, device='cuda:0')



h[100].sum tensor(-4.5798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3388, device='cuda:0')



h[200].sum tensor(-22.2680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30805.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0367, 0.0065, 0.0116,  ..., 0.0835, 0.0649, 0.0381],
        [0.0337, 0.0118, 0.0197,  ..., 0.0336, 0.0240, 0.0144],
        [0.0329, 0.0143, 0.0267,  ..., 0.0191, 0.0119, 0.0077],
        ...,
        [0.0325, 0.0151, 0.0287,  ..., 0.0145, 0.0095, 0.0050],
        [0.0325, 0.0151, 0.0287,  ..., 0.0145, 0.0095, 0.0050],
        [0.0325, 0.0151, 0.0287,  ..., 0.0145, 0.0095, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(228096.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7001.7119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.9220, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(47.8369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3314.0361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(494.3402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0306],
        [ 0.0372],
        [ 0.0449],
        ...,
        [-0.2013],
        [-0.2008],
        [-0.2007]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-51003.5586, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366072.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366085.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0017],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0017],
        [-0.0033,  0.0089,  0.0025,  ...,  0.0007,  0.0091,  0.0063],
        ...,
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0017],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0017],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-611.5317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0224, device='cuda:0')



h[100].sum tensor(-5.3690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7793, device='cuda:0')



h[200].sum tensor(-22.2766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0175, 0.0056,  ..., 0.0019, 0.0178, 0.0138],
        [0.0000, 0.0089, 0.0025,  ..., 0.0007, 0.0091, 0.0063],
        [0.0000, 0.0197, 0.0057,  ..., 0.0016, 0.0202, 0.0142],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34422.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0360, 0.0053, 0.0063,  ..., 0.0707, 0.0496, 0.0335],
        [0.0360, 0.0046, 0.0046,  ..., 0.0737, 0.0499, 0.0354],
        [0.0374, 0.0025, 0.0020,  ..., 0.1006, 0.0688, 0.0490],
        ...,
        [0.0331, 0.0151, 0.0292,  ..., 0.0149, 0.0093, 0.0055],
        [0.0331, 0.0151, 0.0292,  ..., 0.0149, 0.0093, 0.0055],
        [0.0331, 0.0151, 0.0292,  ..., 0.0149, 0.0093, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(246879.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7133.2793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.2746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(54.2487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3338.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(524.7599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0690],
        [ 0.0758],
        [ 0.0821],
        ...,
        [-0.2098],
        [-0.2091],
        [-0.2089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55295.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366085.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366098.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        ...,
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        [ 0.0000, -0.0004, -0.0008,  ..., -0.0006, -0.0001, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-643.4780, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9333, device='cuda:0')



h[100].sum tensor(-4.7799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4741, device='cuda:0')



h[200].sum tensor(-22.3217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0136, 0.0042,  ..., 0.0013, 0.0139, 0.0103],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31863.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0340, 0.0138, 0.0268,  ..., 0.0210, 0.0125, 0.0093],
        [0.0345, 0.0124, 0.0229,  ..., 0.0290, 0.0190, 0.0131],
        [0.0361, 0.0080, 0.0134,  ..., 0.0581, 0.0414, 0.0273],
        ...,
        [0.0337, 0.0149, 0.0296,  ..., 0.0151, 0.0091, 0.0061],
        [0.0337, 0.0149, 0.0296,  ..., 0.0151, 0.0091, 0.0061],
        [0.0337, 0.0149, 0.0296,  ..., 0.0151, 0.0091, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(237950.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7248.7207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(61.9154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3345.1768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(503.9321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0546],
        [-0.0825],
        [-0.0822],
        ...,
        [-0.2179],
        [-0.2172],
        [-0.2169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56170.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0007],
        ...,
        [1.0007],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366098.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366111.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        [-0.0065,  0.0183,  0.0059,  ...,  0.0020,  0.0186,  0.0143],
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        ...,
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0018],
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-646.5417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5752, device='cuda:0')



h[100].sum tensor(-4.6397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4218, device='cuda:0')



h[200].sum tensor(-22.3514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0259, 0.0079,  ..., 0.0024, 0.0264, 0.0194],
        [0.0000, 0.0149, 0.0046,  ..., 0.0015, 0.0151, 0.0114],
        [0.0000, 0.0860, 0.0280,  ..., 0.0096, 0.0870, 0.0684],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32774.0430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0385, 0.0016, 0.0008,  ..., 0.0969, 0.0677, 0.0478],
        [0.0388, 0.0023, 0.0013,  ..., 0.0972, 0.0705, 0.0472],
        [0.0426, 0.0000, 0.0000,  ..., 0.1605, 0.1223, 0.0773],
        ...,
        [0.0344, 0.0147, 0.0298,  ..., 0.0153, 0.0090, 0.0067],
        [0.0344, 0.0147, 0.0298,  ..., 0.0153, 0.0090, 0.0067],
        [0.0344, 0.0147, 0.0298,  ..., 0.0153, 0.0090, 0.0067]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(250351.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7402.6118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.1079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(67.9530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3313.3198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(512.4290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0650],
        [ 0.0509],
        [ 0.0344],
        ...,
        [-0.2253],
        [-0.2245],
        [-0.2243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56318.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366111.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(573.5436, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366125.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0019],
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0019],
        [-0.0040,  0.0111,  0.0033,  ...,  0.0010,  0.0114,  0.0081],
        ...,
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0019],
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0019],
        [ 0.0000, -0.0003, -0.0008,  ..., -0.0006, -0.0001, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-177.2845, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.4630, device='cuda:0')



h[100].sum tensor(-11.2262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7659, device='cuda:0')



h[200].sum tensor(-22.1852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0170, 0.0047,  ..., 0.0012, 0.0175, 0.0116],
        [0.0000, 0.0402, 0.0123,  ..., 0.0038, 0.0409, 0.0300],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65554.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0364, 0.0083, 0.0154,  ..., 0.0456, 0.0285, 0.0231],
        [0.0387, 0.0040, 0.0072,  ..., 0.0922, 0.0623, 0.0466],
        [0.0421, 0.0005, 0.0000,  ..., 0.1540, 0.1100, 0.0769],
        ...,
        [0.0351, 0.0142, 0.0299,  ..., 0.0156, 0.0088, 0.0074],
        [0.0351, 0.0142, 0.0299,  ..., 0.0156, 0.0088, 0.0074],
        [0.0351, 0.0142, 0.0299,  ..., 0.0156, 0.0088, 0.0074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(412245.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7761.2407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(79.0968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3043.5139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(793.6166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0413],
        [ 0.0489],
        [ 0.0514],
        ...,
        [-0.2315],
        [-0.2307],
        [-0.2305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-48986.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0000],
        [1.0008],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366125.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0008],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366139.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.9704e-04, -8.2108e-04,  ..., -6.3218e-04,
         -8.0666e-05, -1.8968e-03],
        [ 0.0000e+00, -2.9704e-04, -8.2108e-04,  ..., -6.3218e-04,
         -8.0666e-05, -1.8968e-03],
        [ 0.0000e+00, -2.9704e-04, -8.2108e-04,  ..., -6.3218e-04,
         -8.0666e-05, -1.8968e-03],
        ...,
        [ 0.0000e+00, -2.9704e-04, -8.2108e-04,  ..., -6.3218e-04,
         -8.0666e-05, -1.8968e-03],
        [ 0.0000e+00, -2.9704e-04, -8.2108e-04,  ..., -6.3218e-04,
         -8.0666e-05, -1.8968e-03],
        [ 0.0000e+00, -2.9704e-04, -8.2108e-04,  ..., -6.3218e-04,
         -8.0666e-05, -1.8968e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-545.8261, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6649, device='cuda:0')



h[100].sum tensor(-5.8189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0193, device='cuda:0')



h[200].sum tensor(-22.3618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41143.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0360, 0.0109, 0.0230,  ..., 0.0297, 0.0166, 0.0158],
        [0.0358, 0.0118, 0.0249,  ..., 0.0260, 0.0145, 0.0137],
        [0.0357, 0.0133, 0.0285,  ..., 0.0191, 0.0105, 0.0099],
        ...,
        [0.0355, 0.0140, 0.0300,  ..., 0.0158, 0.0087, 0.0081],
        [0.0355, 0.0140, 0.0300,  ..., 0.0158, 0.0087, 0.0081],
        [0.0355, 0.0140, 0.0300,  ..., 0.0158, 0.0087, 0.0081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(304811.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7642.2803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(76.0923, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3289.2510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(580.0791, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0115],
        [-0.0414],
        [-0.0787],
        ...,
        [-0.2374],
        [-0.2366],
        [-0.2364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69398.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0008],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366139.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366152.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.0482e-03,  2.2799e-02,  7.4470e-03,  ...,  2.5342e-03,
          2.3048e-02,  1.8038e-02],
        [-4.0376e-03,  1.1299e-02,  3.3291e-03,  ...,  9.5222e-04,
          1.1531e-02,  8.0882e-03],
        [-4.0322e-03,  1.1284e-02,  3.3236e-03,  ...,  9.5009e-04,
          1.1515e-02,  8.0748e-03],
        ...,
        [ 0.0000e+00, -2.7790e-04, -8.1655e-04,  ..., -6.4045e-04,
         -6.3997e-05, -1.9284e-03],
        [ 0.0000e+00, -2.7790e-04, -8.1655e-04,  ..., -6.4045e-04,
         -6.3997e-05, -1.9284e-03],
        [ 0.0000e+00, -2.7790e-04, -8.1655e-04,  ..., -6.4045e-04,
         -6.3997e-05, -1.9284e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-571.0207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2620, device='cuda:0')



h[100].sum tensor(-5.3477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8143, device='cuda:0')



h[200].sum tensor(-22.3945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0598, 0.0185,  ..., 0.0058, 0.0607, 0.0450],
        [0.0000, 0.0618, 0.0193,  ..., 0.0061, 0.0628, 0.0468],
        [0.0000, 0.0205, 0.0059,  ..., 0.0016, 0.0209, 0.0143],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37085.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0418, 0.0000, 0.0000,  ..., 0.1512, 0.1091, 0.0770],
        [0.0412, 0.0000, 0.0000,  ..., 0.1376, 0.0986, 0.0702],
        [0.0391, 0.0027, 0.0041,  ..., 0.0916, 0.0619, 0.0477],
        ...,
        [0.0359, 0.0139, 0.0304,  ..., 0.0159, 0.0087, 0.0087],
        [0.0359, 0.0139, 0.0304,  ..., 0.0159, 0.0087, 0.0087],
        [0.0359, 0.0139, 0.0304,  ..., 0.0159, 0.0087, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276109.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7680.0225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5351, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.0497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3212.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(548.2518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0811],
        [ 0.0790],
        [ 0.0706],
        ...,
        [-0.2443],
        [-0.2437],
        [-0.2438]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-60075.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366152.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366152.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.2636e-03,  2.3417e-02,  7.6682e-03,  ...,  2.6192e-03,
          2.3666e-02,  1.8572e-02],
        [-5.7267e-03,  1.6143e-02,  5.0635e-03,  ...,  1.6185e-03,
          1.6381e-02,  1.2279e-02],
        [-8.0944e-03,  2.2932e-02,  7.4945e-03,  ...,  2.5525e-03,
          2.3181e-02,  1.8153e-02],
        ...,
        [ 0.0000e+00, -2.7790e-04, -8.1655e-04,  ..., -6.4045e-04,
         -6.3997e-05, -1.9284e-03],
        [ 0.0000e+00, -2.7790e-04, -8.1655e-04,  ..., -6.4045e-04,
         -6.3997e-05, -1.9284e-03],
        [ 0.0000e+00, -2.7790e-04, -8.1655e-04,  ..., -6.4045e-04,
         -6.3997e-05, -1.9284e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-583.4686, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7741, device='cuda:0')



h[100].sum tensor(-5.1748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7431, device='cuda:0')



h[200].sum tensor(-22.3995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0576, 0.0178,  ..., 0.0055, 0.0586, 0.0431],
        [0.0000, 0.0648, 0.0203,  ..., 0.0065, 0.0657, 0.0493],
        [0.0000, 0.0403, 0.0123,  ..., 0.0037, 0.0410, 0.0298],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35434.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0440, 0.0000, 0.0000,  ..., 0.2005, 0.1449, 0.1018],
        [0.0439, 0.0000, 0.0000,  ..., 0.1953, 0.1421, 0.0990],
        [0.0425, 0.0000, 0.0000,  ..., 0.1654, 0.1184, 0.0844],
        ...,
        [0.0359, 0.0139, 0.0304,  ..., 0.0159, 0.0087, 0.0087],
        [0.0359, 0.0139, 0.0304,  ..., 0.0159, 0.0087, 0.0087],
        [0.0359, 0.0139, 0.0304,  ..., 0.0159, 0.0087, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(264818.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7664.4585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.3875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3224.9351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(534.5532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0632],
        [ 0.0688],
        [ 0.0725],
        ...,
        [-0.2456],
        [-0.2447],
        [-0.2445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62589.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0008],
        [1.0005],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366152.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0008],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366165.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5271e-04, -8.1164e-04,  ..., -6.4850e-04,
         -4.3388e-05, -1.9570e-03],
        [ 0.0000e+00, -2.5271e-04, -8.1164e-04,  ..., -6.4850e-04,
         -4.3388e-05, -1.9570e-03],
        [ 0.0000e+00, -2.5271e-04, -8.1164e-04,  ..., -6.4850e-04,
         -4.3388e-05, -1.9570e-03],
        ...,
        [ 0.0000e+00, -2.5271e-04, -8.1164e-04,  ..., -6.4850e-04,
         -4.3388e-05, -1.9570e-03],
        [ 0.0000e+00, -2.5271e-04, -8.1164e-04,  ..., -6.4850e-04,
         -4.3388e-05, -1.9570e-03],
        [ 0.0000e+00, -2.5271e-04, -8.1164e-04,  ..., -6.4850e-04,
         -4.3388e-05, -1.9570e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-619.0270, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1134, device='cuda:0')



h[100].sum tensor(-4.4966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3543, device='cuda:0')



h[200].sum tensor(-22.4365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32400.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0362, 0.0139, 0.0308,  ..., 0.0162, 0.0088, 0.0092],
        [0.0362, 0.0139, 0.0308,  ..., 0.0162, 0.0088, 0.0092],
        [0.0363, 0.0139, 0.0309,  ..., 0.0163, 0.0088, 0.0093],
        ...,
        [0.0362, 0.0139, 0.0308,  ..., 0.0162, 0.0088, 0.0092],
        [0.0362, 0.0139, 0.0308,  ..., 0.0162, 0.0088, 0.0092],
        [0.0362, 0.0139, 0.0308,  ..., 0.0162, 0.0088, 0.0092]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254404.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7661.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.4481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3296.9731, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(505.5628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2864],
        [-0.2717],
        [-0.2410],
        ...,
        [-0.2521],
        [-0.2513],
        [-0.2510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71636.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0008],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366165.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0009],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366179.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.2090e-04, -8.0738e-04,  ..., -6.5579e-04,
         -1.7846e-05, -1.9854e-03],
        [ 0.0000e+00, -2.2090e-04, -8.0738e-04,  ..., -6.5579e-04,
         -1.7846e-05, -1.9854e-03],
        [ 0.0000e+00, -2.2090e-04, -8.0738e-04,  ..., -6.5579e-04,
         -1.7846e-05, -1.9854e-03],
        ...,
        [ 0.0000e+00, -2.2090e-04, -8.0738e-04,  ..., -6.5579e-04,
         -1.7846e-05, -1.9854e-03],
        [ 0.0000e+00, -2.2090e-04, -8.0738e-04,  ..., -6.5579e-04,
         -1.7846e-05, -1.9854e-03],
        [ 0.0000e+00, -2.2090e-04, -8.0738e-04,  ..., -6.5579e-04,
         -1.7846e-05, -1.9854e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-532.1055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8044, device='cuda:0')



h[100].sum tensor(-5.4300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8936, device='cuda:0')



h[200].sum tensor(-22.4251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35483.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0362, 0.0139, 0.0310,  ..., 0.0165, 0.0088, 0.0097],
        [0.0363, 0.0139, 0.0311,  ..., 0.0165, 0.0088, 0.0097],
        [0.0368, 0.0123, 0.0255,  ..., 0.0271, 0.0170, 0.0151],
        ...,
        [0.0363, 0.0139, 0.0311,  ..., 0.0165, 0.0088, 0.0097],
        [0.0363, 0.0139, 0.0311,  ..., 0.0165, 0.0088, 0.0097],
        [0.0363, 0.0139, 0.0311,  ..., 0.0165, 0.0088, 0.0097]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(261954.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7638.2256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(85.3088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3305.8735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(528.5208, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0699],
        [-0.1043],
        [-0.1088],
        ...,
        [-0.2570],
        [-0.2561],
        [-0.2558]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74085., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0009],
        ...,
        [1.0009],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366179.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366193.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.8594e-04, -8.0407e-04,  ..., -6.6090e-04,
          1.1045e-05, -2.0144e-03],
        [ 0.0000e+00, -1.8594e-04, -8.0407e-04,  ..., -6.6090e-04,
          1.1045e-05, -2.0144e-03],
        [ 0.0000e+00, -1.8594e-04, -8.0407e-04,  ..., -6.6090e-04,
          1.1045e-05, -2.0144e-03],
        ...,
        [ 0.0000e+00, -1.8594e-04, -8.0407e-04,  ..., -6.6090e-04,
          1.1045e-05, -2.0144e-03],
        [ 0.0000e+00, -1.8594e-04, -8.0407e-04,  ..., -6.6090e-04,
          1.1045e-05, -2.0144e-03],
        [ 0.0000e+00, -1.8594e-04, -8.0407e-04,  ..., -6.6090e-04,
          1.1045e-05, -2.0144e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-367.0739, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7880, device='cuda:0')



h[100].sum tensor(-7.3783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9140, device='cuda:0')



h[200].sum tensor(-22.3823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.4152e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.4171e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.4195e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.4320e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.4321e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.4322e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47103.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0365, 0.0090, 0.0172,  ..., 0.0428, 0.0246, 0.0251],
        [0.0364, 0.0106, 0.0219,  ..., 0.0341, 0.0190, 0.0202],
        [0.0365, 0.0119, 0.0253,  ..., 0.0281, 0.0157, 0.0167],
        ...,
        [0.0363, 0.0141, 0.0313,  ..., 0.0167, 0.0089, 0.0102],
        [0.0363, 0.0141, 0.0313,  ..., 0.0167, 0.0089, 0.0102],
        [0.0363, 0.0141, 0.0313,  ..., 0.0167, 0.0089, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322445., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7603.6807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(93.7014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3106.1599, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(628.6586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0613],
        [ 0.0329],
        [-0.0059],
        ...,
        [-0.2605],
        [-0.2596],
        [-0.2593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-57883.3203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0010],
        ...,
        [1.0009],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366193.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0010],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366207.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1060e-02,  3.1838e-02,  1.0655e-02,  ...,  3.7386e-03,
          3.2077e-02,  2.5625e-02],
        [-3.0888e-03,  8.7775e-03,  2.4008e-03,  ...,  5.6404e-04,
          8.9773e-03,  5.6885e-03],
        [-9.6801e-03,  2.7847e-02,  9.2262e-03,  ...,  3.1892e-03,
          2.8079e-02,  2.2174e-02],
        ...,
        [ 0.0000e+00, -1.5854e-04, -7.9769e-04,  ..., -6.6612e-04,
          2.6084e-05, -2.0368e-03],
        [ 0.0000e+00, -1.5854e-04, -7.9769e-04,  ..., -6.6612e-04,
          2.6084e-05, -2.0368e-03],
        [ 0.0000e+00, -1.5854e-04, -7.9769e-04,  ..., -6.6612e-04,
          2.6084e-05, -2.0368e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-198.9234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9936, device='cuda:0')



h[100].sum tensor(-9.3696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.9668, device='cuda:0')



h[200].sum tensor(-22.3365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.7148e-02, 1.4654e-02,  ..., 4.5587e-03, 4.7808e-02,
         3.5064e-02],
        [0.0000e+00, 1.1660e-01, 3.8769e-02,  ..., 1.3474e-02, 1.1753e-01,
         9.3200e-02],
        [0.0000e+00, 7.3978e-02, 2.3513e-02,  ..., 7.6058e-03, 7.4842e-02,
         5.6352e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0468e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0469e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0469e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60656.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0408, 0.0000, 0.0000,  ..., 0.2070, 0.1546, 0.1076],
        [0.0441, 0.0000, 0.0000,  ..., 0.3031, 0.2349, 0.1538],
        [0.0443, 0.0000, 0.0000,  ..., 0.3124, 0.2418, 0.1587],
        ...,
        [0.0364, 0.0144, 0.0319,  ..., 0.0169, 0.0090, 0.0107],
        [0.0365, 0.0133, 0.0278,  ..., 0.0242, 0.0145, 0.0145],
        [0.0371, 0.0102, 0.0160,  ..., 0.0460, 0.0315, 0.0255]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409546.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7648.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.6335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3040.5991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(742.0882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0306],
        [ 0.0169],
        [ 0.0077],
        ...,
        [-0.1847],
        [-0.1145],
        [-0.0473]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59077.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0010],
        ...,
        [1.0010],
        [1.0006],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366207.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0011],
        ...,
        [1.0010],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366221.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3248e-04, -7.9008e-04,  ..., -6.7146e-04,
          3.4538e-05, -2.0537e-03],
        [ 0.0000e+00, -1.3248e-04, -7.9008e-04,  ..., -6.7146e-04,
          3.4538e-05, -2.0537e-03],
        [ 0.0000e+00, -1.3248e-04, -7.9008e-04,  ..., -6.7146e-04,
          3.4538e-05, -2.0537e-03],
        ...,
        [ 0.0000e+00, -1.3248e-04, -7.9008e-04,  ..., -6.7146e-04,
          3.4538e-05, -2.0537e-03],
        [ 0.0000e+00, -1.3248e-04, -7.9008e-04,  ..., -6.7146e-04,
          3.4538e-05, -2.0537e-03],
        [ 0.0000e+00, -1.3248e-04, -7.9008e-04,  ..., -6.7146e-04,
          3.4538e-05, -2.0537e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-378.4509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7955, device='cuda:0')



h[100].sum tensor(-6.7896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6228, device='cuda:0')



h[200].sum tensor(-22.4236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44369.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0364, 0.0142, 0.0308,  ..., 0.0199, 0.0103, 0.0133],
        [0.0364, 0.0147, 0.0322,  ..., 0.0174, 0.0092, 0.0117],
        [0.0366, 0.0148, 0.0326,  ..., 0.0170, 0.0092, 0.0113],
        ...,
        [0.0365, 0.0148, 0.0326,  ..., 0.0169, 0.0091, 0.0112],
        [0.0365, 0.0148, 0.0326,  ..., 0.0169, 0.0091, 0.0112],
        [0.0365, 0.0148, 0.0326,  ..., 0.0169, 0.0091, 0.0112]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(317050.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7539.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.8528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3145.7805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(603.9099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1686],
        [-0.2437],
        [-0.3008],
        ...,
        [-0.2740],
        [-0.2730],
        [-0.2727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59533.7852, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0011],
        ...,
        [1.0010],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366221.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0011],
        ...,
        [1.0010],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366235.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.1090e-04, -7.8020e-04,  ..., -6.7692e-04,
          3.4857e-05, -2.0648e-03],
        [ 0.0000e+00, -1.1090e-04, -7.8020e-04,  ..., -6.7692e-04,
          3.4857e-05, -2.0648e-03],
        [ 0.0000e+00, -1.1090e-04, -7.8020e-04,  ..., -6.7692e-04,
          3.4857e-05, -2.0648e-03],
        ...,
        [ 0.0000e+00, -1.1090e-04, -7.8020e-04,  ..., -6.7692e-04,
          3.4857e-05, -2.0648e-03],
        [ 0.0000e+00, -1.1090e-04, -7.8020e-04,  ..., -6.7692e-04,
          3.4857e-05, -2.0648e-03],
        [ 0.0000e+00, -1.1090e-04, -7.8020e-04,  ..., -6.7692e-04,
          3.4857e-05, -2.0648e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-519.7756, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7102, device='cuda:0')



h[100].sum tensor(-4.7822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5876, device='cuda:0')



h[200].sum tensor(-22.4931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35709.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0366, 0.0153, 0.0332,  ..., 0.0168, 0.0093, 0.0116],
        [0.0366, 0.0153, 0.0332,  ..., 0.0168, 0.0093, 0.0116],
        [0.0367, 0.0153, 0.0333,  ..., 0.0169, 0.0094, 0.0117],
        ...,
        [0.0366, 0.0153, 0.0333,  ..., 0.0168, 0.0093, 0.0116],
        [0.0366, 0.0153, 0.0333,  ..., 0.0168, 0.0093, 0.0116],
        [0.0366, 0.0153, 0.0333,  ..., 0.0168, 0.0093, 0.0116]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(277610.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7507.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.1906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3267.1411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(532.1608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3697],
        [-0.3584],
        [-0.3353],
        ...,
        [-0.2822],
        [-0.2813],
        [-0.2810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69885.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0001],
        [1.0011],
        ...,
        [1.0010],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366235.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(510.0447, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0011],
        ...,
        [1.0011],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366249.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -8.7465e-05, -7.7021e-04,  ..., -6.8115e-04,
          3.4672e-05, -2.0739e-03],
        [ 0.0000e+00, -8.7465e-05, -7.7021e-04,  ..., -6.8115e-04,
          3.4672e-05, -2.0739e-03],
        [ 0.0000e+00, -8.7465e-05, -7.7021e-04,  ..., -6.8115e-04,
          3.4672e-05, -2.0739e-03],
        ...,
        [ 0.0000e+00, -8.7465e-05, -7.7021e-04,  ..., -6.8115e-04,
          3.4672e-05, -2.0739e-03],
        [ 0.0000e+00, -8.7465e-05, -7.7021e-04,  ..., -6.8115e-04,
          3.4672e-05, -2.0739e-03],
        [ 0.0000e+00, -8.7465e-05, -7.7021e-04,  ..., -6.8115e-04,
          3.4672e-05, -2.0739e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-432.4033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2915, device='cuda:0')



h[100].sum tensor(-5.7438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1109, device='cuda:0')



h[200].sum tensor(-22.4741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0089, 0.0025,  ..., 0.0006, 0.0092, 0.0057],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39103.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0367, 0.0154, 0.0327,  ..., 0.0185, 0.0105, 0.0131],
        [0.0366, 0.0136, 0.0259,  ..., 0.0308, 0.0185, 0.0201],
        [0.0366, 0.0096, 0.0140,  ..., 0.0570, 0.0355, 0.0351],
        ...,
        [0.0368, 0.0157, 0.0339,  ..., 0.0167, 0.0095, 0.0119],
        [0.0368, 0.0157, 0.0339,  ..., 0.0167, 0.0095, 0.0119],
        [0.0368, 0.0157, 0.0339,  ..., 0.0167, 0.0095, 0.0119]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(289928.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7509.1724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.0226, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3300.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(562.3079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1324],
        [-0.0506],
        [ 0.0207],
        ...,
        [-0.2904],
        [-0.2894],
        [-0.2891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77424.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0011],
        ...,
        [1.0011],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366249.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0011],
        ...,
        [1.0011],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366264.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.3327e-03,  2.1339e-02,  6.9014e-03,  ...,  2.2631e-03,
          2.1473e-02,  1.6419e-02],
        [-9.6902e-03,  2.8219e-02,  9.3646e-03,  ...,  3.2107e-03,
          2.8366e-02,  2.2366e-02],
        [-4.7888e-03,  1.3914e-02,  4.2435e-03,  ...,  1.2406e-03,
          1.4034e-02,  1.0001e-02],
        ...,
        [ 0.0000e+00, -6.2677e-05, -7.5988e-04,  ..., -6.8426e-04,
          3.1568e-05, -2.0793e-03],
        [ 0.0000e+00, -6.2677e-05, -7.5988e-04,  ..., -6.8426e-04,
          3.1568e-05, -2.0793e-03],
        [ 0.0000e+00, -6.2677e-05, -7.5988e-04,  ..., -6.8426e-04,
          3.1568e-05, -2.0793e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-585.5760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5684, device='cuda:0')



h[100].sum tensor(-3.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9825, device='cuda:0')



h[200].sum tensor(-22.5448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.2758e-01, 4.2722e-02,  ..., 1.4869e-02, 1.2819e-01,
         1.0217e-01],
        [0.0000e+00, 1.0650e-01, 3.5177e-02,  ..., 1.1965e-02, 1.0708e-01,
         8.3957e-02],
        [0.0000e+00, 8.8874e-02, 2.9602e-02,  ..., 1.0212e-02, 8.9355e-02,
         7.0741e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2677e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2677e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2678e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30490.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.6325e-02, 0.0000e+00, 0.0000e+00,  ..., 4.3597e-01, 3.5214e-01,
         2.1828e-01],
        [4.5165e-02, 0.0000e+00, 0.0000e+00,  ..., 3.9665e-01, 3.1875e-01,
         1.9966e-01],
        [4.3358e-02, 1.5756e-04, 0.0000e+00,  ..., 3.1610e-01, 2.5251e-01,
         1.6041e-01],
        ...,
        [3.6982e-02, 1.6099e-02, 3.4442e-02,  ..., 1.6592e-02, 9.5925e-03,
         1.2162e-02],
        [3.6983e-02, 1.6100e-02, 3.4443e-02,  ..., 1.6592e-02, 9.5927e-03,
         1.2163e-02],
        [3.6983e-02, 1.6100e-02, 3.4444e-02,  ..., 1.6593e-02, 9.5929e-03,
         1.2163e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(258715.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7489.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.8527, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.3463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3318.7642, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(492.8886, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0905],
        [-0.0834],
        [-0.0655],
        ...,
        [-0.2988],
        [-0.2978],
        [-0.2975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74023.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0011],
        ...,
        [1.0011],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366264.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0011],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366278.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3394e-03,  6.8113e-03,  1.6978e-03,  ...,  2.5701e-04,
          6.8844e-03,  3.8323e-03],
        [-5.7395e-03,  1.6757e-02,  5.2580e-03,  ...,  1.6270e-03,
          1.6849e-02,  1.2428e-02],
        [ 0.0000e+00, -3.1809e-05, -7.5188e-04,  ..., -6.8565e-04,
          2.8416e-05, -2.0823e-03],
        ...,
        [ 0.0000e+00, -3.1809e-05, -7.5188e-04,  ..., -6.8565e-04,
          2.8416e-05, -2.0823e-03],
        [ 0.0000e+00, -3.1809e-05, -7.5188e-04,  ..., -6.8565e-04,
          2.8416e-05, -2.0823e-03],
        [ 0.0000e+00, -3.1809e-05, -7.5188e-04,  ..., -6.8565e-04,
          2.8416e-05, -2.0823e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-254.9994, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1042, device='cuda:0')



h[100].sum tensor(-7.6856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.2524, device='cuda:0')



h[200].sum tensor(-22.4321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0599, 0.0185,  ..., 0.0055, 0.0603, 0.0436],
        [0.0000, 0.0262, 0.0072,  ..., 0.0016, 0.0265, 0.0165],
        [0.0000, 0.0223, 0.0065,  ..., 0.0017, 0.0225, 0.0152],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47278.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0367, 0.0010, 0.0000,  ..., 0.1390, 0.0965, 0.0797],
        [0.0364, 0.0038, 0.0017,  ..., 0.1105, 0.0738, 0.0652],
        [0.0368, 0.0071, 0.0080,  ..., 0.0864, 0.0577, 0.0517],
        ...,
        [0.0373, 0.0164, 0.0350,  ..., 0.0165, 0.0097, 0.0123],
        [0.0373, 0.0164, 0.0350,  ..., 0.0165, 0.0097, 0.0123],
        [0.0373, 0.0164, 0.0350,  ..., 0.0165, 0.0097, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(327694.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7584.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5023, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(89.9760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3366.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(632.8575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0594],
        [ 0.0597],
        [ 0.0423],
        ...,
        [-0.3046],
        [-0.2979],
        [-0.2864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92141.1484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0011],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366278.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0012],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366293., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.6432e-06, -7.5060e-04,  ..., -6.8477e-04,
          3.2039e-05, -2.0896e-03],
        [ 0.0000e+00,  8.6432e-06, -7.5060e-04,  ..., -6.8477e-04,
          3.2039e-05, -2.0896e-03],
        [ 0.0000e+00,  8.6432e-06, -7.5060e-04,  ..., -6.8477e-04,
          3.2039e-05, -2.0896e-03],
        ...,
        [ 0.0000e+00,  8.6432e-06, -7.5060e-04,  ..., -6.8477e-04,
          3.2039e-05, -2.0896e-03],
        [ 0.0000e+00,  8.6432e-06, -7.5060e-04,  ..., -6.8477e-04,
          3.2039e-05, -2.0896e-03],
        [ 0.0000e+00,  8.6432e-06, -7.5060e-04,  ..., -6.8477e-04,
          3.2039e-05, -2.0896e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-529.6965, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6905, device='cuda:0')



h[100].sum tensor(-4.1832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2925, device='cuda:0')



h[200].sum tensor(-22.5437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.4552e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2808e-04,
         0.0000e+00],
        [0.0000e+00, 3.4567e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2814e-04,
         0.0000e+00],
        [0.0000e+00, 3.4588e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2821e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.4719e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2870e-04,
         0.0000e+00],
        [0.0000e+00, 3.4720e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2870e-04,
         0.0000e+00],
        [0.0000e+00, 3.4721e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2871e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32262.6289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0373, 0.0157, 0.0329,  ..., 0.0207, 0.0119, 0.0153],
        [0.0374, 0.0162, 0.0350,  ..., 0.0171, 0.0100, 0.0130],
        [0.0376, 0.0164, 0.0353,  ..., 0.0167, 0.0098, 0.0128],
        ...,
        [0.0376, 0.0164, 0.0353,  ..., 0.0167, 0.0098, 0.0127],
        [0.0376, 0.0164, 0.0353,  ..., 0.0167, 0.0098, 0.0127],
        [0.0376, 0.0164, 0.0353,  ..., 0.0167, 0.0098, 0.0127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266445.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7552.9458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.1876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3328.1147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(509.3524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1907],
        [-0.2226],
        [-0.2444],
        ...,
        [-0.3132],
        [-0.3121],
        [-0.3118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82853.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0012],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366293., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0012],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366307.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  5.7698e-05, -7.5120e-04,  ..., -6.8254e-04,
          4.1956e-05, -2.0983e-03],
        [-1.4057e-02,  4.1372e-02,  1.4025e-02,  ...,  5.0174e-03,
          4.1436e-02,  3.3586e-02],
        [-6.6226e-03,  1.9521e-02,  6.2102e-03,  ...,  2.0028e-03,
          1.9543e-02,  1.4713e-02],
        ...,
        [-5.0592e-03,  1.4927e-02,  4.5668e-03,  ...,  1.3689e-03,
          1.4940e-02,  1.0745e-02],
        [-3.5512e-03,  1.0495e-02,  2.9817e-03,  ...,  7.5740e-04,
          1.0499e-02,  6.9165e-03],
        [-5.0592e-03,  1.4927e-02,  4.5668e-03,  ...,  1.3689e-03,
          1.4940e-02,  1.0745e-02]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-445.7274, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4652, device='cuda:0')



h[100].sum tensor(-5.1100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8440, device='cuda:0')



h[200].sum tensor(-22.5221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1386, 0.0465,  ..., 0.0164, 0.1388, 0.1112],
        [0.0000, 0.0534, 0.0168,  ..., 0.0053, 0.0535, 0.0396],
        [0.0000, 0.1399, 0.0470,  ..., 0.0165, 0.1401, 0.1123],
        ...,
        [0.0000, 0.0229, 0.0066,  ..., 0.0018, 0.0229, 0.0154],
        [0.0000, 0.0630, 0.0194,  ..., 0.0059, 0.0631, 0.0458],
        [0.0000, 0.0505, 0.0150,  ..., 0.0042, 0.0505, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35690.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0400, 0.0000, 0.0000,  ..., 0.3462, 0.2770, 0.1775],
        [0.0385, 0.0000, 0.0000,  ..., 0.2689, 0.2109, 0.1412],
        [0.0401, 0.0000, 0.0000,  ..., 0.3564, 0.2851, 0.1830],
        ...,
        [0.0367, 0.0075, 0.0065,  ..., 0.0856, 0.0593, 0.0510],
        [0.0364, 0.0026, 0.0000,  ..., 0.1284, 0.0921, 0.0737],
        [0.0364, 0.0015, 0.0000,  ..., 0.1395, 0.1002, 0.0796]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(278023.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7541.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(88.0716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3333.9304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(536.7422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0234],
        [-0.0145],
        [-0.0249],
        ...,
        [-0.0628],
        [-0.0093],
        [ 0.0093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88301.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0012],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366307.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0012],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366322.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  9.9412e-05, -7.5293e-04,  ..., -6.8022e-04,
          4.9889e-05, -2.1059e-03],
        [ 0.0000e+00,  9.9412e-05, -7.5293e-04,  ..., -6.8022e-04,
          4.9889e-05, -2.1059e-03],
        [ 0.0000e+00,  9.9412e-05, -7.5293e-04,  ..., -6.8022e-04,
          4.9889e-05, -2.1059e-03],
        ...,
        [ 0.0000e+00,  9.9412e-05, -7.5293e-04,  ..., -6.8022e-04,
          4.9889e-05, -2.1059e-03],
        [ 0.0000e+00,  9.9412e-05, -7.5293e-04,  ..., -6.8022e-04,
          4.9889e-05, -2.1059e-03],
        [ 0.0000e+00,  9.9412e-05, -7.5293e-04,  ..., -6.8022e-04,
          4.9889e-05, -2.1059e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-515.9408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8425, device='cuda:0')



h[100].sum tensor(-4.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3147, device='cuda:0')



h[200].sum tensor(-22.5557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33380.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0371, 0.0112, 0.0152,  ..., 0.0543, 0.0367, 0.0338],
        [0.0373, 0.0139, 0.0274,  ..., 0.0307, 0.0184, 0.0215],
        [0.0375, 0.0149, 0.0319,  ..., 0.0234, 0.0124, 0.0179],
        ...,
        [0.0378, 0.0157, 0.0353,  ..., 0.0176, 0.0096, 0.0138],
        [0.0378, 0.0157, 0.0353,  ..., 0.0176, 0.0096, 0.0138],
        [0.0378, 0.0157, 0.0353,  ..., 0.0176, 0.0096, 0.0138]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(275264.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7536.7134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.1383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(88.0246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3329.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(519.9329, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0144],
        [-0.0033],
        [-0.0031],
        ...,
        [-0.3222],
        [-0.3211],
        [-0.3208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87445.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0012],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366322.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366338., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.3297e-04, -7.5377e-04,  ..., -6.7785e-04,
          5.3571e-05, -2.1113e-03],
        [ 0.0000e+00,  1.3297e-04, -7.5377e-04,  ..., -6.7785e-04,
          5.3571e-05, -2.1113e-03],
        [ 0.0000e+00,  1.3297e-04, -7.5377e-04,  ..., -6.7785e-04,
          5.3571e-05, -2.1113e-03],
        ...,
        [ 0.0000e+00,  1.3297e-04, -7.5377e-04,  ..., -6.7785e-04,
          5.3571e-05, -2.1113e-03],
        [ 0.0000e+00,  1.3297e-04, -7.5377e-04,  ..., -6.7785e-04,
          5.3571e-05, -2.1113e-03],
        [ 0.0000e+00,  1.3297e-04, -7.5377e-04,  ..., -6.7785e-04,
          5.3571e-05, -2.1113e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-458.9397, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3280, device='cuda:0')



h[100].sum tensor(-4.8165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6779, device='cuda:0')



h[200].sum tensor(-22.5415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37135.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0378, 0.0132, 0.0230,  ..., 0.0382, 0.0254, 0.0246],
        [0.0379, 0.0147, 0.0318,  ..., 0.0237, 0.0137, 0.0172],
        [0.0379, 0.0144, 0.0309,  ..., 0.0254, 0.0145, 0.0184],
        ...,
        [0.0380, 0.0154, 0.0352,  ..., 0.0183, 0.0095, 0.0143],
        [0.0380, 0.0154, 0.0352,  ..., 0.0183, 0.0095, 0.0143],
        [0.0380, 0.0154, 0.0352,  ..., 0.0183, 0.0095, 0.0143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291960.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7516.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.1150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3224.2466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(557.9033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1246],
        [-0.1158],
        [-0.0685],
        ...,
        [-0.3271],
        [-0.3263],
        [-0.3262]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71849.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0002],
        [1.0012],
        ...,
        [1.0013],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366338., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0013],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366352.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.3796e-04, -7.5004e-04,  ..., -6.7444e-04,
          3.5915e-05, -2.1115e-03],
        [ 0.0000e+00,  1.3796e-04, -7.5004e-04,  ..., -6.7444e-04,
          3.5915e-05, -2.1115e-03],
        [ 0.0000e+00,  1.3796e-04, -7.5004e-04,  ..., -6.7444e-04,
          3.5915e-05, -2.1115e-03],
        ...,
        [ 0.0000e+00,  1.3796e-04, -7.5004e-04,  ..., -6.7444e-04,
          3.5915e-05, -2.1115e-03],
        [ 0.0000e+00,  1.3796e-04, -7.5004e-04,  ..., -6.7444e-04,
          3.5915e-05, -2.1115e-03],
        [ 0.0000e+00,  1.3796e-04, -7.5004e-04,  ..., -6.7444e-04,
          3.5915e-05, -2.1115e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-588.9582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.7147, device='cuda:0')



h[100].sum tensor(-3.3173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8577, device='cuda:0')



h[200].sum tensor(-22.5917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0065, 0.0014,  ..., 0.0001, 0.0061, 0.0030],
        [0.0000, 0.0124, 0.0027,  ..., 0.0003, 0.0120, 0.0060],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28575.0762, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0374, 0.0116, 0.0211,  ..., 0.0420, 0.0235, 0.0291],
        [0.0367, 0.0092, 0.0126,  ..., 0.0583, 0.0337, 0.0392],
        [0.0363, 0.0071, 0.0079,  ..., 0.0728, 0.0433, 0.0479],
        ...,
        [0.0387, 0.0154, 0.0355,  ..., 0.0185, 0.0094, 0.0143],
        [0.0387, 0.0154, 0.0355,  ..., 0.0185, 0.0094, 0.0143],
        [0.0387, 0.0154, 0.0355,  ..., 0.0185, 0.0094, 0.0143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255238.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7713.1167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.6704, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.2643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3524.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(483.0342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0504],
        [ 0.0827],
        [ 0.0976],
        ...,
        [-0.3383],
        [-0.3371],
        [-0.3368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99826.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [1.0003],
        [1.0013],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366352.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0013],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366367.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4711e-03,  1.6369e-02,  5.0489e-03,  ...,  1.5745e-03,
          1.6277e-02,  1.1895e-02],
        [-2.2595e-03,  6.8414e-03,  1.6462e-03,  ...,  2.5617e-04,
          6.7301e-03,  3.6743e-03],
        [-3.2116e-03,  9.6658e-03,  2.6549e-03,  ...,  6.4699e-04,
          9.5602e-03,  6.1114e-03],
        ...,
        [ 0.0000e+00,  1.3819e-04, -7.4788e-04,  ..., -6.7137e-04,
          1.3488e-05, -2.1098e-03],
        [ 0.0000e+00,  1.3819e-04, -7.4788e-04,  ..., -6.7137e-04,
          1.3488e-05, -2.1098e-03],
        [ 0.0000e+00,  1.3819e-04, -7.4788e-04,  ..., -6.7137e-04,
          1.3488e-05, -2.1098e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-468.6044, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5244, device='cuda:0')



h[100].sum tensor(-4.8208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7066, device='cuda:0')



h[200].sum tensor(-22.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.7397e-02, 7.3453e-03,  ..., 1.7016e-03, 2.6953e-02,
         1.6838e-02],
        [0.0000e+00, 6.1144e-02, 1.8649e-02,  ..., 5.6990e-03, 6.0767e-02,
         4.3845e-02],
        [0.0000e+00, 2.7423e-02, 8.1004e-03,  ..., 2.3749e-03, 2.6978e-02,
         1.8965e-02],
        ...,
        [0.0000e+00, 5.5552e-04, 0.0000e+00,  ..., 0.0000e+00, 5.4223e-05,
         0.0000e+00],
        [0.0000e+00, 5.5554e-04, 0.0000e+00,  ..., 0.0000e+00, 5.4225e-05,
         0.0000e+00],
        [0.0000e+00, 5.5556e-04, 0.0000e+00,  ..., 0.0000e+00, 5.4227e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36108.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0360, 0.0030, 0.0014,  ..., 0.1142, 0.0749, 0.0701],
        [0.0356, 0.0006, 0.0000,  ..., 0.1430, 0.0980, 0.0851],
        [0.0365, 0.0031, 0.0000,  ..., 0.1194, 0.0813, 0.0716],
        ...,
        [0.0395, 0.0155, 0.0360,  ..., 0.0188, 0.0094, 0.0143],
        [0.0395, 0.0155, 0.0360,  ..., 0.0188, 0.0094, 0.0143],
        [0.0395, 0.0155, 0.0360,  ..., 0.0188, 0.0094, 0.0143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(292222.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7869.8198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(82.6632, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3556.3423, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(552.8506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1149],
        [ 0.1185],
        [ 0.1163],
        ...,
        [-0.3488],
        [-0.3479],
        [-0.3477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106574.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0013],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366367.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0013],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366382.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.3353e-04, -7.4115e-04,  ..., -6.6581e-04,
         -4.1842e-06, -2.1131e-03],
        [ 0.0000e+00,  1.3353e-04, -7.4115e-04,  ..., -6.6581e-04,
         -4.1842e-06, -2.1131e-03],
        [ 0.0000e+00,  1.3353e-04, -7.4115e-04,  ..., -6.6581e-04,
         -4.1842e-06, -2.1131e-03],
        ...,
        [ 0.0000e+00,  1.3353e-04, -7.4115e-04,  ..., -6.6581e-04,
         -4.1842e-06, -2.1131e-03],
        [ 0.0000e+00,  1.3353e-04, -7.4115e-04,  ..., -6.6581e-04,
         -4.1842e-06, -2.1131e-03],
        [ 0.0000e+00,  1.3353e-04, -7.4115e-04,  ..., -6.6581e-04,
         -4.1842e-06, -2.1131e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-491.7394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5876, device='cuda:0')



h[100].sum tensor(-4.5430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5697, device='cuda:0')



h[200].sum tensor(-22.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36559.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0389, 0.0130, 0.0254,  ..., 0.0365, 0.0201, 0.0253],
        [0.0395, 0.0150, 0.0333,  ..., 0.0236, 0.0120, 0.0173],
        [0.0399, 0.0157, 0.0363,  ..., 0.0190, 0.0093, 0.0143],
        ...,
        [0.0399, 0.0158, 0.0364,  ..., 0.0190, 0.0093, 0.0143],
        [0.0399, 0.0158, 0.0364,  ..., 0.0190, 0.0093, 0.0143],
        [0.0399, 0.0158, 0.0364,  ..., 0.0190, 0.0093, 0.0143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(300765.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7885.9907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(85.7006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3423.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(566.9563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0103],
        [-0.0939],
        [-0.1906],
        ...,
        [-0.3590],
        [-0.3578],
        [-0.3575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84208.8984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0013],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366382.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(552.2358, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0014],
        ...,
        [1.0014],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366396.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.0546e-03,  2.4136e-02,  7.8388e-03,  ...,  2.6694e-03,
          2.4039e-02,  1.8588e-02],
        [-1.0748e-02,  3.2161e-02,  1.0704e-02,  ...,  3.7819e-03,
          3.2081e-02,  2.5511e-02],
        [-1.3851e-02,  4.1411e-02,  1.4006e-02,  ...,  5.0641e-03,
          4.1350e-02,  3.3488e-02],
        ...,
        [ 0.0000e+00,  1.3105e-04, -7.3161e-04,  ..., -6.5822e-04,
         -1.5668e-05, -2.1175e-03],
        [ 0.0000e+00,  1.3105e-04, -7.3161e-04,  ..., -6.5822e-04,
         -1.5668e-05, -2.1175e-03],
        [ 0.0000e+00,  1.3105e-04, -7.3161e-04,  ..., -6.5822e-04,
         -1.5668e-05, -2.1175e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-325.4915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4147, device='cuda:0')



h[100].sum tensor(-6.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5672, device='cuda:0')



h[200].sum tensor(-22.5102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1082, 0.0355,  ..., 0.0123, 0.1078, 0.0844],
        [0.0000, 0.1184, 0.0392,  ..., 0.0137, 0.1180, 0.0932],
        [0.0000, 0.1407, 0.0471,  ..., 0.0168, 0.1405, 0.1125],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44229.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0354, 0.0000, 0.0000,  ..., 0.2806, 0.2177, 0.1500],
        [0.0352, 0.0000, 0.0000,  ..., 0.3315, 0.2604, 0.1748],
        [0.0349, 0.0000, 0.0000,  ..., 0.3768, 0.2976, 0.1973],
        ...,
        [0.0402, 0.0162, 0.0366,  ..., 0.0191, 0.0092, 0.0141],
        [0.0402, 0.0162, 0.0366,  ..., 0.0191, 0.0092, 0.0141],
        [0.0402, 0.0162, 0.0366,  ..., 0.0191, 0.0092, 0.0141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331201.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7885.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.8006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3416.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(634.3900, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0524],
        [ 0.0584],
        [ 0.0649],
        ...,
        [-0.3666],
        [-0.3654],
        [-0.3650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86588., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0003],
        [1.0014],
        ...,
        [1.0014],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366396.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0014],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366410.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.1682e-04, -7.1673e-04,  ..., -6.4846e-04,
         -2.3324e-05, -2.1213e-03],
        [ 0.0000e+00,  1.1682e-04, -7.1673e-04,  ..., -6.4846e-04,
         -2.3324e-05, -2.1213e-03],
        [ 0.0000e+00,  1.1682e-04, -7.1673e-04,  ..., -6.4846e-04,
         -2.3324e-05, -2.1213e-03],
        ...,
        [ 0.0000e+00,  1.1682e-04, -7.1673e-04,  ..., -6.4846e-04,
         -2.3324e-05, -2.1213e-03],
        [ 0.0000e+00,  1.1682e-04, -7.1673e-04,  ..., -6.4846e-04,
         -2.3324e-05, -2.1213e-03],
        [ 0.0000e+00,  1.1682e-04, -7.1673e-04,  ..., -6.4846e-04,
         -2.3324e-05, -2.1213e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-432.7931, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1304, device='cuda:0')



h[100].sum tensor(-4.8985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7951, device='cuda:0')



h[200].sum tensor(-22.5570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0271, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0212, 0.0060,  ..., 0.0016, 0.0208, 0.0137],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38286.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0367, 0.0078, 0.0061,  ..., 0.0866, 0.0540, 0.0543],
        [0.0384, 0.0126, 0.0176,  ..., 0.0497, 0.0286, 0.0327],
        [0.0395, 0.0150, 0.0290,  ..., 0.0319, 0.0170, 0.0220],
        ...,
        [0.0402, 0.0168, 0.0370,  ..., 0.0191, 0.0091, 0.0139],
        [0.0402, 0.0168, 0.0370,  ..., 0.0191, 0.0091, 0.0139],
        [0.0402, 0.0168, 0.0370,  ..., 0.0191, 0.0091, 0.0139]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(311414.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7860.4854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(81.1622, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3487.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(584.9282, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 6.7367e-02],
        [-1.6620e-04],
        [-9.6589e-02],
        ...,
        [-3.7437e-01],
        [-3.7311e-01],
        [-3.7242e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88480.9766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0014],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366410.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0015],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366425.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03],
        [-4.4627e-03,  1.3462e-02,  4.0688e-03,  ...,  1.2182e-03,
          1.3363e-02,  9.3974e-03],
        [-3.2272e-03,  9.7635e-03,  2.7482e-03,  ...,  7.0423e-04,
          9.6557e-03,  6.2079e-03],
        ...,
        [ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03],
        [ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03],
        [ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-413.1420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1818, device='cuda:0')



h[100].sum tensor(-4.8861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8026, device='cuda:0')



h[200].sum tensor(-22.5598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0320, 0.0099,  ..., 0.0031, 0.0316, 0.0230],
        [0.0000, 0.0289, 0.0088,  ..., 0.0027, 0.0285, 0.0203],
        [0.0000, 0.0665, 0.0208,  ..., 0.0066, 0.0662, 0.0485],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39665.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0366, 0.0084, 0.0000,  ..., 0.1090, 0.0764, 0.0631],
        [0.0361, 0.0073, 0.0000,  ..., 0.1164, 0.0805, 0.0680],
        [0.0347, 0.0033, 0.0000,  ..., 0.1592, 0.1128, 0.0914],
        ...,
        [0.0403, 0.0175, 0.0373,  ..., 0.0191, 0.0090, 0.0137],
        [0.0403, 0.0175, 0.0373,  ..., 0.0191, 0.0090, 0.0137],
        [0.0403, 0.0175, 0.0373,  ..., 0.0191, 0.0090, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322068.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7812.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7170, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(78.9685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3497.9072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.7171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0567],
        [ 0.0628],
        [ 0.0705],
        ...,
        [-0.3805],
        [-0.3793],
        [-0.3789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91245.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0015],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366425.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0015],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366425.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03],
        [-2.1106e-03,  6.4208e-03,  1.5547e-03,  ...,  2.3977e-04,
          6.3054e-03,  3.3255e-03],
        [-2.1106e-03,  6.4208e-03,  1.5547e-03,  ...,  2.3977e-04,
          6.3054e-03,  3.3255e-03],
        ...,
        [ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03],
        [ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03],
        [ 0.0000e+00,  1.0262e-04, -7.0127e-04,  ..., -6.3813e-04,
         -2.7163e-05, -2.1228e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-392.1569, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2662, device='cuda:0')



h[100].sum tensor(-5.1211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9611, device='cuda:0')



h[200].sum tensor(-22.5525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0027,  ..., 0.0003, 0.0114, 0.0056],
        [0.0000, 0.0119, 0.0027,  ..., 0.0003, 0.0114, 0.0057],
        [0.0000, 0.0119, 0.0027,  ..., 0.0003, 0.0115, 0.0057],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38836.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0378, 0.0132, 0.0197,  ..., 0.0469, 0.0239, 0.0325],
        [0.0370, 0.0114, 0.0129,  ..., 0.0624, 0.0342, 0.0417],
        [0.0363, 0.0086, 0.0058,  ..., 0.0961, 0.0610, 0.0592],
        ...,
        [0.0403, 0.0175, 0.0373,  ..., 0.0191, 0.0090, 0.0137],
        [0.0403, 0.0175, 0.0373,  ..., 0.0191, 0.0090, 0.0137],
        [0.0403, 0.0175, 0.0373,  ..., 0.0191, 0.0090, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313154.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7816.7368, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(78.7840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3497.5806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(590.4100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1419],
        [-0.0391],
        [ 0.0281],
        ...,
        [-0.3805],
        [-0.3793],
        [-0.3789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89519.5234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0015],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366425.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0015],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366439.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0457e-04, -6.8602e-04,  ..., -6.2710e-04,
         -2.2258e-05, -2.1271e-03],
        [ 0.0000e+00,  1.0457e-04, -6.8602e-04,  ..., -6.2710e-04,
         -2.2258e-05, -2.1271e-03],
        [ 0.0000e+00,  1.0457e-04, -6.8602e-04,  ..., -6.2710e-04,
         -2.2258e-05, -2.1271e-03],
        ...,
        [ 0.0000e+00,  1.0457e-04, -6.8602e-04,  ..., -6.2710e-04,
         -2.2258e-05, -2.1271e-03],
        [ 0.0000e+00,  1.0457e-04, -6.8602e-04,  ..., -6.2710e-04,
         -2.2258e-05, -2.1271e-03],
        [ 0.0000e+00,  1.0457e-04, -6.8602e-04,  ..., -6.2710e-04,
         -2.2258e-05, -2.1271e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-465.9515, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5362, device='cuda:0')



h[100].sum tensor(-3.9588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2700, device='cuda:0')



h[200].sum tensor(-22.5910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32839.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0399, 0.0181, 0.0369,  ..., 0.0190, 0.0089, 0.0135],
        [0.0399, 0.0181, 0.0370,  ..., 0.0191, 0.0089, 0.0135],
        [0.0401, 0.0181, 0.0371,  ..., 0.0192, 0.0089, 0.0136],
        ...,
        [0.0401, 0.0182, 0.0371,  ..., 0.0191, 0.0089, 0.0136],
        [0.0401, 0.0182, 0.0371,  ..., 0.0191, 0.0089, 0.0136],
        [0.0401, 0.0182, 0.0371,  ..., 0.0191, 0.0089, 0.0136]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(285796.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7813.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(73.6199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3719.2024, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(531.0767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4997],
        [-0.4923],
        [-0.4774],
        ...,
        [-0.3835],
        [-0.3822],
        [-0.3819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116008.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0015],
        ...,
        [1.0015],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366439.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0016],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366454.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        ...,
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-375.9771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5394, device='cuda:0')



h[100].sum tensor(-4.6716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7088, device='cuda:0')



h[200].sum tensor(-22.5706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0262, 0.0079,  ..., 0.0024, 0.0258, 0.0180],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40583.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0347, 0.0109, 0.0065,  ..., 0.0975, 0.0626, 0.0601],
        [0.0373, 0.0152, 0.0182,  ..., 0.0497, 0.0279, 0.0330],
        [0.0388, 0.0171, 0.0299,  ..., 0.0303, 0.0145, 0.0215],
        ...,
        [0.0399, 0.0185, 0.0369,  ..., 0.0193, 0.0087, 0.0137],
        [0.0399, 0.0185, 0.0369,  ..., 0.0193, 0.0087, 0.0137],
        [0.0399, 0.0185, 0.0369,  ..., 0.0193, 0.0087, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339617.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7639.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(75.1931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3562.9912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(599.4103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0061],
        [-0.0851],
        [-0.1958],
        ...,
        [-0.3860],
        [-0.3847],
        [-0.3843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97974.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0016],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366454.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0016],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366454.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        ...,
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03],
        [ 0.0000e+00,  1.0782e-04, -6.7513e-04,  ..., -6.1860e-04,
         -1.4912e-05, -2.1307e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-422.1326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5221, device='cuda:0')



h[100].sum tensor(-4.1658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4140, device='cuda:0')



h[200].sum tensor(-22.5865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0038,  ..., 0.0011, 0.0125, 0.0086],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36033.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0375, 0.0147, 0.0163,  ..., 0.0618, 0.0402, 0.0374],
        [0.0390, 0.0173, 0.0296,  ..., 0.0303, 0.0165, 0.0201],
        [0.0397, 0.0183, 0.0355,  ..., 0.0214, 0.0102, 0.0150],
        ...,
        [0.0399, 0.0185, 0.0369,  ..., 0.0193, 0.0087, 0.0137],
        [0.0399, 0.0185, 0.0369,  ..., 0.0193, 0.0087, 0.0137],
        [0.0399, 0.0185, 0.0369,  ..., 0.0193, 0.0087, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(308296.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7627.2212, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(76.6690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3532.1646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(563.2856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1051],
        [-0.2309],
        [-0.3426],
        ...,
        [-0.3863],
        [-0.3851],
        [-0.3847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93372.6797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [1.0004],
        [1.0016],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366454.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0016],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366468.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0695e-04, -6.6630e-04,  ..., -6.1039e-04,
         -6.9668e-06, -2.1334e-03],
        [ 0.0000e+00,  1.0695e-04, -6.6630e-04,  ..., -6.1039e-04,
         -6.9668e-06, -2.1334e-03],
        [ 0.0000e+00,  1.0695e-04, -6.6630e-04,  ..., -6.1039e-04,
         -6.9668e-06, -2.1334e-03],
        ...,
        [ 0.0000e+00,  1.0695e-04, -6.6630e-04,  ..., -6.1039e-04,
         -6.9668e-06, -2.1334e-03],
        [ 0.0000e+00,  1.0695e-04, -6.6630e-04,  ..., -6.1039e-04,
         -6.9668e-06, -2.1334e-03],
        [ 0.0000e+00,  1.0695e-04, -6.6630e-04,  ..., -6.1039e-04,
         -6.9668e-06, -2.1334e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-448.1100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6414, device='cuda:0')



h[100].sum tensor(-3.6683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1392, device='cuda:0')



h[200].sum tensor(-22.6040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0287, 0.0087,  ..., 0.0027, 0.0283, 0.0201],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33067.0195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0391, 0.0179, 0.0320,  ..., 0.0263, 0.0133, 0.0181],
        [0.0378, 0.0160, 0.0186,  ..., 0.0503, 0.0311, 0.0315],
        [0.0345, 0.0113, 0.0059,  ..., 0.1101, 0.0754, 0.0651],
        ...,
        [0.0397, 0.0187, 0.0367,  ..., 0.0195, 0.0085, 0.0141],
        [0.0398, 0.0187, 0.0367,  ..., 0.0195, 0.0085, 0.0141],
        [0.0398, 0.0187, 0.0367,  ..., 0.0195, 0.0085, 0.0141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291509.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7563.7354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(76.4594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3603.3020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(537.4590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2148],
        [-0.0956],
        [-0.0047],
        ...,
        [-0.3893],
        [-0.3881],
        [-0.3878]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95748.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0016],
        ...,
        [1.0016],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366468.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0017],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366483.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.7083e-03,  8.2868e-03,  2.2599e-03,  ...,  5.3758e-04,
          8.2070e-03,  4.9148e-03],
        [ 0.0000e+00,  1.0568e-04, -6.6128e-04,  ..., -6.0342e-04,
          3.9315e-06, -2.1358e-03],
        [ 0.0000e+00,  1.0568e-04, -6.6128e-04,  ..., -6.0342e-04,
          3.9315e-06, -2.1358e-03],
        ...,
        [ 0.0000e+00,  1.0568e-04, -6.6128e-04,  ..., -6.0342e-04,
          3.9315e-06, -2.1358e-03],
        [ 0.0000e+00,  1.0568e-04, -6.6128e-04,  ..., -6.0342e-04,
          3.9315e-06, -2.1358e-03],
        [ 0.0000e+00,  1.0568e-04, -6.6128e-04,  ..., -6.0342e-04,
          3.9315e-06, -2.1358e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-437.6491, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5517, device='cuda:0')



h[100].sum tensor(-3.6540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1261, device='cuda:0')



h[200].sum tensor(-22.6060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0206, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.7359e-02, 8.2964e-03,  ..., 2.5507e-03, 2.7024e-02,
         1.8945e-02],
        [0.0000e+00, 8.6028e-03, 2.2596e-03,  ..., 5.3751e-04, 8.2177e-03,
         4.9142e-03],
        [0.0000e+00, 4.5041e-02, 1.4608e-02,  ..., 5.0149e-03, 4.4753e-02,
         3.4177e-02],
        ...,
        [0.0000e+00, 4.2535e-04, 0.0000e+00,  ..., 0.0000e+00, 1.5824e-05,
         0.0000e+00],
        [0.0000e+00, 4.2537e-04, 0.0000e+00,  ..., 0.0000e+00, 1.5825e-05,
         0.0000e+00],
        [0.0000e+00, 4.2539e-04, 0.0000e+00,  ..., 0.0000e+00, 1.5826e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33048.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0322, 0.0088, 0.0000,  ..., 0.1394, 0.0963, 0.0823],
        [0.0339, 0.0110, 0.0000,  ..., 0.1181, 0.0819, 0.0694],
        [0.0327, 0.0090, 0.0000,  ..., 0.1643, 0.1214, 0.0919],
        ...,
        [0.0397, 0.0186, 0.0365,  ..., 0.0199, 0.0083, 0.0147],
        [0.0397, 0.0186, 0.0365,  ..., 0.0199, 0.0083, 0.0147],
        [0.0397, 0.0186, 0.0365,  ..., 0.0199, 0.0083, 0.0147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(289613.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7493.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0607, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(79.1113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3640.6396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(540.8751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0652],
        [ 0.0573],
        [ 0.0479],
        ...,
        [-0.3909],
        [-0.3898],
        [-0.3897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93709.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0017],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366483.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0017],
        ...,
        [1.0017],
        [1.0013],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366498.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  9.0241e-05, -6.5765e-04,  ..., -5.9900e-04,
          5.7556e-06, -2.1325e-03],
        [ 0.0000e+00,  9.0241e-05, -6.5765e-04,  ..., -5.9900e-04,
          5.7556e-06, -2.1325e-03],
        [ 0.0000e+00,  9.0241e-05, -6.5765e-04,  ..., -5.9900e-04,
          5.7556e-06, -2.1325e-03],
        ...,
        [ 0.0000e+00,  9.0241e-05, -6.5765e-04,  ..., -5.9900e-04,
          5.7556e-06, -2.1325e-03],
        [ 0.0000e+00,  9.0241e-05, -6.5765e-04,  ..., -5.9900e-04,
          5.7556e-06, -2.1325e-03],
        [ 0.0000e+00,  9.0241e-05, -6.5765e-04,  ..., -5.9900e-04,
          5.7556e-06, -2.1325e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-423.7290, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3485, device='cuda:0')



h[100].sum tensor(-3.8176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2426, device='cuda:0')



h[200].sum tensor(-22.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.6077e-04, 0.0000e+00,  ..., 0.0000e+00, 2.3010e-05,
         0.0000e+00],
        [0.0000e+00, 3.6096e-04, 0.0000e+00,  ..., 0.0000e+00, 2.3022e-05,
         0.0000e+00],
        [0.0000e+00, 3.6123e-04, 0.0000e+00,  ..., 0.0000e+00, 2.3039e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.6327e-04, 0.0000e+00,  ..., 0.0000e+00, 2.3170e-05,
         0.0000e+00],
        [0.0000e+00, 3.6329e-04, 0.0000e+00,  ..., 0.0000e+00, 2.3171e-05,
         0.0000e+00],
        [0.0000e+00, 3.6331e-04, 0.0000e+00,  ..., 0.0000e+00, 2.3172e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34782.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0398, 0.0184, 0.0364,  ..., 0.0201, 0.0081, 0.0151],
        [0.0397, 0.0183, 0.0354,  ..., 0.0217, 0.0092, 0.0161],
        [0.0388, 0.0171, 0.0267,  ..., 0.0351, 0.0183, 0.0243],
        ...,
        [0.0400, 0.0186, 0.0366,  ..., 0.0202, 0.0082, 0.0152],
        [0.0400, 0.0186, 0.0366,  ..., 0.0202, 0.0082, 0.0152],
        [0.0400, 0.0186, 0.0366,  ..., 0.0202, 0.0082, 0.0152]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(301594.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7583.4243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.2384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(78.8466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3880.8896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(554.1959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3699],
        [-0.2694],
        [-0.1339],
        ...,
        [-0.4004],
        [-0.3992],
        [-0.3988]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108575.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0005],
        [1.0017],
        ...,
        [1.0017],
        [1.0013],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366498.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(436.4034, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0018],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366513.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.4778e-05, -6.5953e-04,  ..., -5.9744e-04,
          1.2405e-05, -2.1285e-03],
        [-5.6592e-03,  1.7259e-02,  5.4713e-03,  ...,  1.7992e-03,
          1.7234e-02,  1.2669e-02],
        [-2.8608e-03,  8.7666e-03,  2.4397e-03,  ...,  6.1411e-04,
          8.7184e-03,  5.3519e-03],
        ...,
        [ 0.0000e+00,  8.4778e-05, -6.5953e-04,  ..., -5.9744e-04,
          1.2405e-05, -2.1285e-03],
        [ 0.0000e+00,  8.4778e-05, -6.5953e-04,  ..., -5.9744e-04,
          1.2405e-05, -2.1285e-03],
        [ 0.0000e+00,  8.4778e-05, -6.5953e-04,  ..., -5.9744e-04,
          1.2405e-05, -2.1285e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-345.7455, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6863, device='cuda:0')



h[100].sum tensor(-4.6706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7302, device='cuda:0')



h[200].sum tensor(-22.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.4591e-02, 7.3390e-03,  ..., 2.1900e-03, 2.4369e-02,
         1.6641e-02],
        [0.0000e+00, 2.7990e-02, 7.8923e-03,  ..., 2.0663e-03, 2.7778e-02,
         1.7439e-02],
        [0.0000e+00, 1.0007e-01, 3.2962e-02,  ..., 1.1526e-02, 1.0006e-01,
         7.7409e-02],
        ...,
        [0.0000e+00, 3.4133e-04, 0.0000e+00,  ..., 0.0000e+00, 4.9944e-05,
         0.0000e+00],
        [0.0000e+00, 3.4135e-04, 0.0000e+00,  ..., 0.0000e+00, 4.9946e-05,
         0.0000e+00],
        [0.0000e+00, 3.4137e-04, 0.0000e+00,  ..., 0.0000e+00, 4.9950e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36516.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0346, 0.0108, 0.0072,  ..., 0.1018, 0.0656, 0.0636],
        [0.0315, 0.0062, 0.0011,  ..., 0.1601, 0.1103, 0.0956],
        [0.0273, 0.0018, 0.0000,  ..., 0.2744, 0.2046, 0.1535],
        ...,
        [0.0403, 0.0182, 0.0367,  ..., 0.0207, 0.0082, 0.0160],
        [0.0403, 0.0182, 0.0367,  ..., 0.0207, 0.0082, 0.0160],
        [0.0403, 0.0182, 0.0367,  ..., 0.0207, 0.0082, 0.0160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(301595.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7563.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(83.9705, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3952.9448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(577.4742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0477],
        [ 0.0714],
        [ 0.0738],
        ...,
        [-0.4069],
        [-0.4056],
        [-0.4053]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95257.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0018],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366513.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0019],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366528., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.0286e-05, -6.6067e-04,  ..., -5.9636e-04,
          1.2464e-05, -2.1226e-03],
        [ 0.0000e+00,  7.0286e-05, -6.6067e-04,  ..., -5.9636e-04,
          1.2464e-05, -2.1226e-03],
        [ 0.0000e+00,  7.0286e-05, -6.6067e-04,  ..., -5.9636e-04,
          1.2464e-05, -2.1226e-03],
        ...,
        [ 0.0000e+00,  7.0286e-05, -6.6067e-04,  ..., -5.9636e-04,
          1.2464e-05, -2.1226e-03],
        [ 0.0000e+00,  7.0286e-05, -6.6067e-04,  ..., -5.9636e-04,
          1.2464e-05, -2.1226e-03],
        [ 0.0000e+00,  7.0286e-05, -6.6067e-04,  ..., -5.9636e-04,
          1.2464e-05, -2.1226e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-276.5003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0830, device='cuda:0')



h[100].sum tensor(-5.4550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2265, device='cuda:0')



h[200].sum tensor(-22.5518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.8100e-04, 0.0000e+00,  ..., 0.0000e+00, 4.9832e-05,
         0.0000e+00],
        [0.0000e+00, 2.8115e-04, 0.0000e+00,  ..., 0.0000e+00, 4.9860e-05,
         0.0000e+00],
        [0.0000e+00, 2.8137e-04, 0.0000e+00,  ..., 0.0000e+00, 4.9897e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.8303e-04, 0.0000e+00,  ..., 0.0000e+00, 5.0192e-05,
         0.0000e+00],
        [0.0000e+00, 2.8305e-04, 0.0000e+00,  ..., 0.0000e+00, 5.0195e-05,
         0.0000e+00],
        [0.0000e+00, 2.8306e-04, 0.0000e+00,  ..., 0.0000e+00, 5.0198e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44653.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0405, 0.0178, 0.0367,  ..., 0.0210, 0.0082, 0.0164],
        [0.0406, 0.0178, 0.0368,  ..., 0.0210, 0.0082, 0.0164],
        [0.0407, 0.0179, 0.0369,  ..., 0.0211, 0.0082, 0.0165],
        ...,
        [0.0408, 0.0179, 0.0370,  ..., 0.0211, 0.0082, 0.0165],
        [0.0408, 0.0179, 0.0370,  ..., 0.0211, 0.0082, 0.0165],
        [0.0408, 0.0179, 0.0370,  ..., 0.0211, 0.0082, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(350858.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7591.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(86.7933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3989.6228, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(653.1671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3493],
        [-0.3934],
        [-0.4305],
        ...,
        [-0.4152],
        [-0.4139],
        [-0.4135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93035.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [1.0006],
        [1.0019],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366528., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0006],
        [1.0019],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366542.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  5.7342e-05, -6.5884e-04,  ..., -5.9493e-04,
          1.0593e-05, -2.1165e-03],
        [ 0.0000e+00,  5.7342e-05, -6.5884e-04,  ..., -5.9493e-04,
          1.0593e-05, -2.1165e-03],
        [ 0.0000e+00,  5.7342e-05, -6.5884e-04,  ..., -5.9493e-04,
          1.0593e-05, -2.1165e-03],
        ...,
        [ 0.0000e+00,  5.7342e-05, -6.5884e-04,  ..., -5.9493e-04,
          1.0593e-05, -2.1165e-03],
        [ 0.0000e+00,  5.7342e-05, -6.5884e-04,  ..., -5.9493e-04,
          1.0593e-05, -2.1165e-03],
        [ 0.0000e+00,  5.7342e-05, -6.5884e-04,  ..., -5.9493e-04,
          1.0593e-05, -2.1165e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-186.3866, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8089, device='cuda:0')



h[100].sum tensor(-6.3791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7709, device='cuda:0')



h[200].sum tensor(-22.5226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.4163e-03, 1.5494e-03,  ..., 2.6874e-04, 6.2468e-03,
         3.2142e-03],
        [0.0000e+00, 1.2610e-02, 3.1005e-03,  ..., 5.3779e-04, 1.2458e-02,
         6.4320e-03],
        [0.0000e+00, 6.4238e-03, 1.5512e-03,  ..., 2.6906e-04, 6.2542e-03,
         3.2180e-03],
        ...,
        [0.0000e+00, 2.3094e-04, 0.0000e+00,  ..., 0.0000e+00, 4.2662e-05,
         0.0000e+00],
        [0.0000e+00, 2.3096e-04, 0.0000e+00,  ..., 0.0000e+00, 4.2665e-05,
         0.0000e+00],
        [0.0000e+00, 2.3097e-04, 0.0000e+00,  ..., 0.0000e+00, 4.2667e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47349.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0373, 0.0125, 0.0123,  ..., 0.0589, 0.0294, 0.0430],
        [0.0368, 0.0117, 0.0086,  ..., 0.0645, 0.0325, 0.0469],
        [0.0372, 0.0123, 0.0098,  ..., 0.0629, 0.0321, 0.0454],
        ...,
        [0.0414, 0.0178, 0.0373,  ..., 0.0214, 0.0083, 0.0169],
        [0.0414, 0.0178, 0.0374,  ..., 0.0214, 0.0083, 0.0169],
        [0.0414, 0.0178, 0.0374,  ..., 0.0214, 0.0083, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(360917.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7727.5649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(86.9987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4166.3452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(677.0395, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1106],
        [ 0.1116],
        [ 0.1104],
        ...,
        [-0.4230],
        [-0.4217],
        [-0.4214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106770.1953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0006],
        [1.0019],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366542.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0020],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366556.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.3345e-05, -6.5591e-04,  ..., -5.9303e-04,
          3.0986e-07, -2.1092e-03],
        [ 0.0000e+00,  3.3345e-05, -6.5591e-04,  ..., -5.9303e-04,
          3.0986e-07, -2.1092e-03],
        [ 0.0000e+00,  3.3345e-05, -6.5591e-04,  ..., -5.9303e-04,
          3.0986e-07, -2.1092e-03],
        ...,
        [ 0.0000e+00,  3.3345e-05, -6.5591e-04,  ..., -5.9303e-04,
          3.0986e-07, -2.1092e-03],
        [ 0.0000e+00,  3.3345e-05, -6.5591e-04,  ..., -5.9303e-04,
          3.0986e-07, -2.1092e-03],
        [ 0.0000e+00,  3.3345e-05, -6.5591e-04,  ..., -5.9303e-04,
          3.0986e-07, -2.1092e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-401.7145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3678, device='cuda:0')



h[100].sum tensor(-4.2302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5376, device='cuda:0')



h[200].sum tensor(-22.5926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.3331e-04, 0.0000e+00,  ..., 0.0000e+00, 1.2388e-06,
         0.0000e+00],
        [0.0000e+00, 1.3339e-04, 0.0000e+00,  ..., 0.0000e+00, 1.2395e-06,
         0.0000e+00],
        [0.0000e+00, 1.3349e-04, 0.0000e+00,  ..., 0.0000e+00, 1.2405e-06,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.3432e-04, 0.0000e+00,  ..., 0.0000e+00, 1.2481e-06,
         0.0000e+00],
        [0.0000e+00, 1.3432e-04, 0.0000e+00,  ..., 0.0000e+00, 1.2482e-06,
         0.0000e+00],
        [0.0000e+00, 1.3433e-04, 0.0000e+00,  ..., 0.0000e+00, 1.2483e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36823.9023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0380, 0.0119, 0.0082,  ..., 0.0813, 0.0522, 0.0516],
        [0.0394, 0.0140, 0.0130,  ..., 0.0577, 0.0344, 0.0383],
        [0.0403, 0.0153, 0.0201,  ..., 0.0474, 0.0275, 0.0319],
        ...,
        [0.0420, 0.0179, 0.0380,  ..., 0.0216, 0.0084, 0.0170],
        [0.0420, 0.0179, 0.0380,  ..., 0.0216, 0.0084, 0.0170],
        [0.0420, 0.0179, 0.0380,  ..., 0.0216, 0.0084, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(315898.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7917.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(87.4470, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4296.6011, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(590.2766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0613],
        [ 0.0575],
        [ 0.0567],
        ...,
        [-0.4298],
        [-0.4283],
        [-0.4266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110281.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0020],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366556.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0020],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366571.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5845e-02,  4.8549e-02,  1.6665e-02,  ...,  6.1833e-03,
          4.8664e-02,  3.9701e-02],
        [-1.1377e-02,  3.4864e-02,  1.1782e-02,  ...,  4.2734e-03,
          3.4940e-02,  2.7914e-02],
        [-6.0027e-03,  1.8402e-02,  5.9087e-03,  ...,  1.9758e-03,
          1.8431e-02,  1.3735e-02],
        ...,
        [ 0.0000e+00,  1.5769e-05, -6.5133e-04,  ..., -5.9033e-04,
         -6.9757e-06, -2.1016e-03],
        [ 0.0000e+00,  1.5769e-05, -6.5133e-04,  ..., -5.9033e-04,
         -6.9757e-06, -2.1016e-03],
        [ 0.0000e+00,  1.5769e-05, -6.5133e-04,  ..., -5.9033e-04,
         -6.9757e-06, -2.1016e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-409.3040, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8809, device='cuda:0')



h[100].sum tensor(-4.1388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4664, device='cuda:0')



h[200].sum tensor(-22.5961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4865e-01, 5.0409e-02,  ..., 1.8377e-02, 1.4898e-01,
         1.1957e-01],
        [0.0000e+00, 1.3663e-01, 4.6121e-02,  ..., 1.6699e-02, 1.3693e-01,
         1.0922e-01],
        [0.0000e+00, 1.1314e-01, 3.7737e-02,  ..., 1.3418e-02, 1.1337e-01,
         8.8979e-02],
        ...,
        [0.0000e+00, 6.3530e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.3535e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.3538e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35085.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6560e-02, 0.0000e+00, 0.0000e+00,  ..., 3.8850e-01, 3.0498e-01,
         2.0916e-01],
        [2.8090e-02, 0.0000e+00, 0.0000e+00,  ..., 3.4295e-01, 2.6645e-01,
         1.8662e-01],
        [2.9728e-02, 3.0429e-05, 0.0000e+00,  ..., 2.8396e-01, 2.1513e-01,
         1.5877e-01],
        ...,
        [4.2603e-02, 1.7943e-02, 3.8546e-02,  ..., 2.1696e-02, 8.5343e-03,
         1.6954e-02],
        [4.2606e-02, 1.7945e-02, 3.8549e-02,  ..., 2.1698e-02, 8.5351e-03,
         1.6955e-02],
        [4.2610e-02, 1.7946e-02, 3.8552e-02,  ..., 2.1700e-02, 8.5358e-03,
         1.6957e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(306083.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8074.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.2637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(85.8432, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4441.5146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(574.5986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0485],
        [ 0.0587],
        [ 0.0719],
        ...,
        [-0.4419],
        [-0.4405],
        [-0.4401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121058.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0020],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366571.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0021],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366585.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7759e-02,  5.4526e-02,  1.8802e-02,  ...,  7.0200e-03,
          5.4665e-02,  4.4869e-02],
        [-5.4101e-03,  1.6612e-02,  5.2766e-03,  ...,  1.7289e-03,
          1.6645e-02,  1.2215e-02],
        [ 0.0000e+00,  1.9340e-06, -6.4887e-04,  ..., -5.8909e-04,
         -1.1699e-05, -2.0907e-03],
        ...,
        [ 0.0000e+00,  1.9340e-06, -6.4887e-04,  ..., -5.8909e-04,
         -1.1699e-05, -2.0907e-03],
        [ 0.0000e+00,  1.9340e-06, -6.4887e-04,  ..., -5.8909e-04,
         -1.1699e-05, -2.0907e-03],
        [ 0.0000e+00,  1.9340e-06, -6.4887e-04,  ..., -5.8909e-04,
         -1.1699e-05, -2.0907e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-412.0369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9123, device='cuda:0')



h[100].sum tensor(-4.0833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4710, device='cuda:0')



h[200].sum tensor(-22.5984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 9.1492e-02, 3.0042e-02,  ..., 1.0412e-02, 9.1693e-02,
         7.0433e-02],
        [0.0000e+00, 1.0858e-01, 3.6786e-02,  ..., 1.3384e-02, 1.0884e-01,
         8.7237e-02],
        [0.0000e+00, 4.3443e-02, 1.4197e-02,  ..., 4.8826e-03, 4.3534e-02,
         3.3225e-02],
        ...,
        [0.0000e+00, 7.7929e-06, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.7935e-06, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.7939e-06, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35888.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0000, 0.0000,  ..., 0.2913, 0.2212, 0.1631],
        [0.0301, 0.0007, 0.0000,  ..., 0.2910, 0.2218, 0.1621],
        [0.0329, 0.0021, 0.0000,  ..., 0.2205, 0.1635, 0.1262],
        ...,
        [0.0432, 0.0180, 0.0391,  ..., 0.0218, 0.0087, 0.0169],
        [0.0432, 0.0180, 0.0391,  ..., 0.0218, 0.0087, 0.0169],
        [0.0432, 0.0180, 0.0391,  ..., 0.0218, 0.0087, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313435.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8153.1357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(86.6601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4444.4136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(586.2840, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0507],
        [ 0.0592],
        [ 0.0684],
        ...,
        [-0.4506],
        [-0.4492],
        [-0.4488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117969.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0021],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366585.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0021],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366599.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.3269e-05, -6.4735e-04,  ..., -5.8653e-04,
         -5.8350e-06, -2.0871e-03],
        [ 0.0000e+00,  1.3269e-05, -6.4735e-04,  ..., -5.8653e-04,
         -5.8350e-06, -2.0871e-03],
        [ 0.0000e+00,  1.3269e-05, -6.4735e-04,  ..., -5.8653e-04,
         -5.8350e-06, -2.0871e-03],
        ...,
        [ 0.0000e+00,  1.3269e-05, -6.4735e-04,  ..., -5.8653e-04,
         -5.8350e-06, -2.0871e-03],
        [ 0.0000e+00,  1.3269e-05, -6.4735e-04,  ..., -5.8653e-04,
         -5.8350e-06, -2.0871e-03],
        [ 0.0000e+00,  1.3269e-05, -6.4735e-04,  ..., -5.8653e-04,
         -5.8350e-06, -2.0871e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-174.7401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2124, device='cuda:0')



h[100].sum tensor(-6.2361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8299, device='cuda:0')



h[200].sum tensor(-22.5280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0371, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.3052e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.3083e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.3126e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.3474e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.3478e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.3481e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47597.1758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0175, 0.0388,  ..., 0.0219, 0.0088, 0.0170],
        [0.0430, 0.0176, 0.0389,  ..., 0.0219, 0.0088, 0.0170],
        [0.0431, 0.0176, 0.0390,  ..., 0.0220, 0.0088, 0.0172],
        ...,
        [0.0433, 0.0177, 0.0391,  ..., 0.0220, 0.0089, 0.0172],
        [0.0433, 0.0177, 0.0391,  ..., 0.0220, 0.0089, 0.0172],
        [0.0433, 0.0177, 0.0391,  ..., 0.0220, 0.0089, 0.0172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370322.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8049.8579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4743, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(87.9015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4288.7114, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(689.2668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5406],
        [-0.5620],
        [-0.5733],
        ...,
        [-0.4519],
        [-0.4504],
        [-0.4499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-102924., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0007],
        [1.0021],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366599.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0022],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366613.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3977e-03,  1.9754e-02,  6.3994e-03,  ...,  2.1751e-03,
          1.9792e-02,  1.4901e-02],
        [-1.3331e-02,  4.1140e-02,  1.4029e-02,  ...,  5.1613e-03,
          4.1240e-02,  3.3317e-02],
        [-1.0652e-02,  3.2875e-02,  1.1080e-02,  ...,  4.0073e-03,
          3.2952e-02,  2.6200e-02],
        ...,
        [ 0.0000e+00,  2.0563e-05, -6.4012e-04,  ..., -5.8033e-04,
          2.2812e-06, -2.0905e-03],
        [ 0.0000e+00,  2.0563e-05, -6.4012e-04,  ..., -5.8033e-04,
          2.2812e-06, -2.0905e-03],
        [ 0.0000e+00,  2.0563e-05, -6.4012e-04,  ..., -5.8033e-04,
          2.2812e-06, -2.0905e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-212.1913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1389, device='cuda:0')



h[100].sum tensor(-5.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3808, device='cuda:0')



h[200].sum tensor(-22.5488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.0107e-01, 3.3466e-02,  ..., 1.1781e-02, 1.0129e-01,
         7.8600e-02],
        [0.0000e+00, 1.0738e-01, 3.5718e-02,  ..., 1.2661e-02, 1.0762e-01,
         8.4033e-02],
        [0.0000e+00, 1.3288e-01, 4.4809e-02,  ..., 1.6219e-02, 1.3319e-01,
         1.0598e-01],
        ...,
        [0.0000e+00, 8.2881e-05, 0.0000e+00,  ..., 0.0000e+00, 9.1946e-06,
         0.0000e+00],
        [0.0000e+00, 8.2888e-05, 0.0000e+00,  ..., 0.0000e+00, 9.1953e-06,
         0.0000e+00],
        [0.0000e+00, 8.2893e-05, 0.0000e+00,  ..., 0.0000e+00, 9.1959e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45091.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0302, 0.0017, 0.0000,  ..., 0.2434, 0.1820, 0.1392],
        [0.0277, 0.0000, 0.0000,  ..., 0.2925, 0.2213, 0.1656],
        [0.0263, 0.0000, 0.0000,  ..., 0.3280, 0.2504, 0.1840],
        ...,
        [0.0432, 0.0176, 0.0391,  ..., 0.0221, 0.0088, 0.0173],
        [0.0432, 0.0176, 0.0391,  ..., 0.0221, 0.0088, 0.0173],
        [0.0432, 0.0176, 0.0391,  ..., 0.0221, 0.0088, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353561.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7981.4810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2220, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(88.8660, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4246.5659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(668.0457, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0425],
        [ 0.0361],
        [ 0.0241],
        ...,
        [-0.4563],
        [-0.4549],
        [-0.4545]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95961.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0022],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366613.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0022],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366627.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4921e-03,  7.7350e-03,  2.1083e-03,  ...,  4.9671e-04,
          7.7320e-03,  4.5484e-03],
        [-8.2577e-03,  2.5560e-02,  8.4664e-03,  ...,  2.9855e-03,
          2.5608e-02,  1.9896e-02],
        [ 0.0000e+00,  3.0142e-05, -6.3988e-04,  ..., -5.7903e-04,
          5.2036e-06, -2.0855e-03],
        ...,
        [ 0.0000e+00,  3.0142e-05, -6.3988e-04,  ..., -5.7903e-04,
          5.2036e-06, -2.0855e-03],
        [ 0.0000e+00,  3.0142e-05, -6.3988e-04,  ..., -5.7903e-04,
          5.2036e-06, -2.0855e-03],
        [ 0.0000e+00,  3.0142e-05, -6.3988e-04,  ..., -5.7903e-04,
          5.2036e-06, -2.0855e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-30.0551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7464, device='cuda:0')



h[100].sum tensor(-7.1386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.3462, device='cuda:0')



h[200].sum tensor(-22.4978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.1984e-02, 2.6641e-02,  ..., 9.1146e-03, 8.2118e-02,
         6.2147e-02],
        [0.0000e+00, 3.3710e-02, 1.0061e-02,  ..., 2.9524e-03, 3.3706e-02,
         2.2663e-02],
        [0.0000e+00, 5.5245e-02, 1.7100e-02,  ..., 5.3780e-03, 5.5303e-02,
         3.9112e-02],
        ...,
        [0.0000e+00, 1.2151e-04, 0.0000e+00,  ..., 0.0000e+00, 2.0977e-05,
         0.0000e+00],
        [0.0000e+00, 1.2152e-04, 0.0000e+00,  ..., 0.0000e+00, 2.0978e-05,
         0.0000e+00],
        [0.0000e+00, 1.2152e-04, 0.0000e+00,  ..., 0.0000e+00, 2.0980e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52365.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8902e-02, 1.4568e-04, 0.0000e+00,  ..., 2.3803e-01, 1.7341e-01,
         1.4025e-01],
        [3.1361e-02, 2.0349e-03, 0.0000e+00,  ..., 1.8953e-01, 1.3379e-01,
         1.1477e-01],
        [3.2784e-02, 3.3391e-03, 0.0000e+00,  ..., 1.6914e-01, 1.1798e-01,
         1.0340e-01],
        ...,
        [4.3239e-02, 1.7554e-02, 3.9234e-02,  ..., 2.2128e-02, 8.9200e-03,
         1.7335e-02],
        [4.3243e-02, 1.7556e-02, 3.9237e-02,  ..., 2.2130e-02, 8.9210e-03,
         1.7337e-02],
        [4.3247e-02, 1.7557e-02, 3.9240e-02,  ..., 2.2132e-02, 8.9218e-03,
         1.7339e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383866.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7886.0708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.3290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4103.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(731.1696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0794],
        [ 0.0790],
        [ 0.0760],
        ...,
        [-0.4597],
        [-0.4583],
        [-0.4579]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93735.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [1.0008],
        [1.0022],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366627.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0023],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366641.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.3216e-05, -6.3351e-04,  ..., -5.7558e-04,
         -2.6393e-09, -2.0813e-03],
        [ 0.0000e+00,  2.3216e-05, -6.3351e-04,  ..., -5.7558e-04,
         -2.6393e-09, -2.0813e-03],
        [-2.6471e-03,  8.2262e-03,  2.2925e-03,  ...,  5.6989e-04,
          8.2264e-03,  4.9811e-03],
        ...,
        [ 0.0000e+00,  2.3216e-05, -6.3351e-04,  ..., -5.7558e-04,
         -2.6393e-09, -2.0813e-03],
        [ 0.0000e+00,  2.3216e-05, -6.3351e-04,  ..., -5.7558e-04,
         -2.6393e-09, -2.0813e-03],
        [ 0.0000e+00,  2.3216e-05, -6.3351e-04,  ..., -5.7558e-04,
         -2.6393e-09, -2.0813e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-53.2863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8523, device='cuda:0')



h[100].sum tensor(-6.6959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0695, device='cuda:0')



h[200].sum tensor(-22.5121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 9.2825e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.2980e-03, 2.2931e-03,  ..., 5.7004e-04, 8.2285e-03,
         4.9824e-03],
        [0.0000e+00, 2.7829e-02, 8.6247e-03,  ..., 2.7205e-03, 2.7815e-02,
         1.9712e-02],
        ...,
        [0.0000e+00, 9.3604e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.3612e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.3618e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51972.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0413, 0.0157, 0.0270,  ..., 0.0391, 0.0203, 0.0281],
        [0.0385, 0.0120, 0.0135,  ..., 0.0702, 0.0412, 0.0479],
        [0.0354, 0.0077, 0.0025,  ..., 0.1119, 0.0712, 0.0731],
        ...,
        [0.0432, 0.0178, 0.0395,  ..., 0.0221, 0.0088, 0.0171],
        [0.0432, 0.0178, 0.0395,  ..., 0.0221, 0.0088, 0.0171],
        [0.0432, 0.0178, 0.0395,  ..., 0.0221, 0.0088, 0.0171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389341.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7851.6392, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.9512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4100.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(727.0328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1230],
        [ 0.0015],
        [ 0.0759],
        ...,
        [-0.4656],
        [-0.4642],
        [-0.4638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98947.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0023],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366641.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(562.9045, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0023],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366654.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.2824e-05, -6.2760e-04,  ..., -5.7301e-04,
         -4.7343e-06, -2.0758e-03],
        [ 0.0000e+00,  1.2824e-05, -6.2760e-04,  ..., -5.7301e-04,
         -4.7343e-06, -2.0758e-03],
        [ 0.0000e+00,  1.2824e-05, -6.2760e-04,  ..., -5.7301e-04,
         -4.7343e-06, -2.0758e-03],
        ...,
        [ 0.0000e+00,  1.2824e-05, -6.2760e-04,  ..., -5.7301e-04,
         -4.7343e-06, -2.0758e-03],
        [ 0.0000e+00,  1.2824e-05, -6.2760e-04,  ..., -5.7301e-04,
         -4.7343e-06, -2.0758e-03],
        [ 0.0000e+00,  1.2824e-05, -6.2760e-04,  ..., -5.7301e-04,
         -4.7343e-06, -2.0758e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-266.1282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6321, device='cuda:0')



h[100].sum tensor(-4.4673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7223, device='cuda:0')



h[200].sum tensor(-22.5864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.1275e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1307e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1351e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.1713e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1718e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1721e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41900.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0171, 0.0344,  ..., 0.0292, 0.0127, 0.0221],
        [0.0421, 0.0175, 0.0366,  ..., 0.0262, 0.0105, 0.0204],
        [0.0413, 0.0164, 0.0286,  ..., 0.0377, 0.0185, 0.0276],
        ...,
        [0.0432, 0.0183, 0.0398,  ..., 0.0219, 0.0087, 0.0167],
        [0.0432, 0.0183, 0.0398,  ..., 0.0219, 0.0087, 0.0167],
        [0.0432, 0.0183, 0.0398,  ..., 0.0219, 0.0087, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353731.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7840.0796, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.5621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4134.0796, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(639.5950, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0251],
        [-0.0402],
        [-0.0127],
        ...,
        [-0.4711],
        [-0.4697],
        [-0.4693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98922.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0008],
        [1.0023],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366654.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0024],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366668.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.1004e-06, -6.2347e-04,  ..., -5.7066e-04,
         -3.9099e-06, -2.0701e-03],
        [ 0.0000e+00,  3.1004e-06, -6.2347e-04,  ..., -5.7066e-04,
         -3.9099e-06, -2.0701e-03],
        [ 0.0000e+00,  3.1004e-06, -6.2347e-04,  ..., -5.7066e-04,
         -3.9099e-06, -2.0701e-03],
        ...,
        [ 0.0000e+00,  3.1004e-06, -6.2347e-04,  ..., -5.7066e-04,
         -3.9099e-06, -2.0701e-03],
        [ 0.0000e+00,  3.1004e-06, -6.2347e-04,  ..., -5.7066e-04,
         -3.9099e-06, -2.0701e-03],
        [ 0.0000e+00,  3.1004e-06, -6.2347e-04,  ..., -5.7066e-04,
         -3.9099e-06, -2.0701e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-313.9303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5610, device='cuda:0')



h[100].sum tensor(-3.8736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4197, device='cuda:0')



h[200].sum tensor(-22.6062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.2397e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2404e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2415e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.2504e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2505e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2506e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35697.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0415, 0.0167, 0.0251,  ..., 0.0417, 0.0242, 0.0275],
        [0.0418, 0.0170, 0.0310,  ..., 0.0340, 0.0170, 0.0240],
        [0.0403, 0.0152, 0.0200,  ..., 0.0512, 0.0282, 0.0353],
        ...,
        [0.0433, 0.0184, 0.0401,  ..., 0.0219, 0.0086, 0.0165],
        [0.0433, 0.0184, 0.0401,  ..., 0.0219, 0.0086, 0.0165],
        [0.0433, 0.0184, 0.0401,  ..., 0.0219, 0.0086, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310710., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7957.1372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.3324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4373.1904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(580.6990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0709],
        [-0.0476],
        [-0.0010],
        ...,
        [-0.4750],
        [-0.4728],
        [-0.4671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119467.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0024],
        ...,
        [1.0018],
        [1.0014],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366668.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0024],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366682.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.8705e-03,  8.9631e-03,  2.5698e-03,  ...,  6.8076e-04,
          8.9837e-03,  5.6457e-03],
        [-2.2828e-03,  7.1293e-03,  1.9157e-03,  ...,  4.2472e-04,
          7.1447e-03,  4.0671e-03],
        [-5.1533e-03,  1.6086e-02,  5.1106e-03,  ...,  1.6754e-03,
          1.6127e-02,  1.1778e-02],
        ...,
        [ 0.0000e+00,  6.0428e-06, -6.2509e-04,  ..., -5.6988e-04,
          1.1163e-06, -2.0649e-03],
        [ 0.0000e+00,  6.0428e-06, -6.2509e-04,  ..., -5.6988e-04,
          1.1163e-06, -2.0649e-03],
        [ 0.0000e+00,  6.0428e-06, -6.2509e-04,  ..., -5.6988e-04,
          1.1163e-06, -2.0649e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-366.9838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.8976, device='cuda:0')



h[100].sum tensor(-3.2534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0306, device='cuda:0')



h[200].sum tensor(-22.6271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.6694e-02, 8.2631e-03,  ..., 2.5844e-03, 2.6750e-02,
         1.8830e-02],
        [0.0000e+00, 5.8372e-02, 1.8311e-02,  ..., 5.8668e-03, 5.8519e-02,
         4.1967e-02],
        [0.0000e+00, 2.6730e-02, 7.6485e-03,  ..., 2.0175e-03, 2.6786e-02,
         1.6788e-02],
        ...,
        [0.0000e+00, 2.4375e-05, 0.0000e+00,  ..., 0.0000e+00, 4.5029e-06,
         0.0000e+00],
        [0.0000e+00, 2.4377e-05, 0.0000e+00,  ..., 0.0000e+00, 4.5034e-06,
         0.0000e+00],
        [0.0000e+00, 2.4379e-05, 0.0000e+00,  ..., 0.0000e+00, 4.5038e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32855.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0350, 0.0090, 0.0029,  ..., 0.1085, 0.0687, 0.0705],
        [0.0322, 0.0055, 0.0000,  ..., 0.1441, 0.0950, 0.0915],
        [0.0342, 0.0079, 0.0013,  ..., 0.1167, 0.0735, 0.0763],
        ...,
        [0.0433, 0.0182, 0.0402,  ..., 0.0221, 0.0087, 0.0164],
        [0.0433, 0.0182, 0.0402,  ..., 0.0221, 0.0087, 0.0164],
        [0.0433, 0.0182, 0.0402,  ..., 0.0221, 0.0087, 0.0164]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(300647.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7921.4927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.2186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4375.1182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(557.1111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0745],
        [ 0.0846],
        [ 0.0718],
        ...,
        [-0.4757],
        [-0.4676],
        [-0.4421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113606.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0024],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366682.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0025],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366696.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.4580e-05, -6.3173e-04,  ..., -5.7223e-04,
          1.0230e-05, -2.0563e-03],
        [ 0.0000e+00,  1.4580e-05, -6.3173e-04,  ..., -5.7223e-04,
          1.0230e-05, -2.0563e-03],
        [-3.1444e-03,  9.8495e-03,  2.8759e-03,  ...,  8.0049e-04,
          9.8731e-03,  6.4096e-03],
        ...,
        [ 0.0000e+00,  1.4580e-05, -6.3173e-04,  ..., -5.7223e-04,
          1.0230e-05, -2.0563e-03],
        [ 0.0000e+00,  1.4580e-05, -6.3173e-04,  ..., -5.7223e-04,
          1.0230e-05, -2.0563e-03],
        [ 0.0000e+00,  1.4580e-05, -6.3173e-04,  ..., -5.7223e-04,
          1.0230e-05, -2.0563e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-157.6530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2335, device='cuda:0')



h[100].sum tensor(-5.1275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2485, device='cuda:0')



h[200].sum tensor(-22.5634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.8904e-02, 9.6562e-03,  ..., 3.4542e-03, 2.8968e-02,
         2.2775e-02],
        [0.0000e+00, 9.8967e-03, 2.8769e-03,  ..., 8.0077e-04, 9.9073e-03,
         6.4119e-03],
        [0.0000e+00, 1.7108e-02, 4.8157e-03,  ..., 1.2338e-03, 1.7140e-02,
         1.0558e-02],
        ...,
        [0.0000e+00, 5.8821e-05, 0.0000e+00,  ..., 0.0000e+00, 4.1271e-05,
         0.0000e+00],
        [0.0000e+00, 5.8827e-05, 0.0000e+00,  ..., 0.0000e+00, 4.1275e-05,
         0.0000e+00],
        [0.0000e+00, 5.8833e-05, 0.0000e+00,  ..., 0.0000e+00, 4.1279e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43313.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0373, 0.0100, 0.0032,  ..., 0.1155, 0.0829, 0.0673],
        [0.0385, 0.0116, 0.0047,  ..., 0.0822, 0.0535, 0.0516],
        [0.0369, 0.0097, 0.0059,  ..., 0.0962, 0.0614, 0.0619],
        ...,
        [0.0435, 0.0176, 0.0404,  ..., 0.0225, 0.0091, 0.0165],
        [0.0435, 0.0176, 0.0404,  ..., 0.0225, 0.0091, 0.0165],
        [0.0435, 0.0176, 0.0404,  ..., 0.0225, 0.0091, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349491.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7898.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(90.8127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4292.3564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(646.8526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0349],
        [ 0.0467],
        [ 0.0629],
        ...,
        [-0.4875],
        [-0.4861],
        [-0.4857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118098.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0025],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366696.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0025],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366710.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.2421e-05, -6.4074e-04,  ..., -5.7605e-04,
          1.8737e-05, -2.0461e-03],
        [ 0.0000e+00,  2.2421e-05, -6.4074e-04,  ..., -5.7605e-04,
          1.8737e-05, -2.0461e-03],
        [-3.2759e-03,  1.0293e-02,  3.0216e-03,  ...,  8.5678e-04,
          1.0318e-02,  6.7945e-03],
        ...,
        [ 0.0000e+00,  2.2421e-05, -6.4074e-04,  ..., -5.7605e-04,
          1.8737e-05, -2.0461e-03],
        [ 0.0000e+00,  2.2421e-05, -6.4074e-04,  ..., -5.7605e-04,
          1.8737e-05, -2.0461e-03],
        [ 0.0000e+00,  2.2421e-05, -6.4074e-04,  ..., -5.7605e-04,
          1.8737e-05, -2.0461e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-113.7301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6619, device='cuda:0')



h[100].sum tensor(-5.5038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4572, device='cuda:0')



h[200].sum tensor(-22.5502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.9651e-05, 0.0000e+00,  ..., 0.0000e+00, 7.4918e-05,
         0.0000e+00],
        [0.0000e+00, 1.0364e-02, 3.0227e-03,  ..., 8.5710e-04, 1.0378e-02,
         6.7971e-03],
        [0.0000e+00, 8.4856e-03, 2.3523e-03,  ..., 5.9456e-04, 8.4946e-03,
         5.1784e-03],
        ...,
        [0.0000e+00, 9.0467e-05, 0.0000e+00,  ..., 0.0000e+00, 7.5600e-05,
         0.0000e+00],
        [0.0000e+00, 9.0478e-05, 0.0000e+00,  ..., 0.0000e+00, 7.5608e-05,
         0.0000e+00],
        [0.0000e+00, 9.0486e-05, 0.0000e+00,  ..., 0.0000e+00, 7.5616e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47306.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0423, 0.0154, 0.0335,  ..., 0.0324, 0.0149, 0.0236],
        [0.0406, 0.0132, 0.0211,  ..., 0.0497, 0.0261, 0.0350],
        [0.0393, 0.0115, 0.0122,  ..., 0.0638, 0.0351, 0.0445],
        ...,
        [0.0439, 0.0168, 0.0407,  ..., 0.0229, 0.0094, 0.0166],
        [0.0439, 0.0168, 0.0407,  ..., 0.0229, 0.0094, 0.0166],
        [0.0439, 0.0168, 0.0407,  ..., 0.0229, 0.0094, 0.0166]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(373818.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7894.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4443, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.4766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4192.7676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(685.7359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2369],
        [-0.0936],
        [ 0.0270],
        ...,
        [-0.4938],
        [-0.4924],
        [-0.4920]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109755.8984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0025],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366710.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0026],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366724.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.5426e-05, -6.4538e-04,  ..., -5.7781e-04,
          2.5499e-05, -2.0379e-03],
        [ 0.0000e+00,  2.5426e-05, -6.4538e-04,  ..., -5.7781e-04,
          2.5499e-05, -2.0379e-03],
        [ 0.0000e+00,  2.5426e-05, -6.4538e-04,  ..., -5.7781e-04,
          2.5499e-05, -2.0379e-03],
        ...,
        [ 0.0000e+00,  2.5426e-05, -6.4538e-04,  ..., -5.7781e-04,
          2.5499e-05, -2.0379e-03],
        [ 0.0000e+00,  2.5426e-05, -6.4538e-04,  ..., -5.7781e-04,
          2.5499e-05, -2.0379e-03],
        [ 0.0000e+00,  2.5426e-05, -6.4538e-04,  ..., -5.7781e-04,
          2.5499e-05, -2.0379e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(13.8700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1293, device='cuda:0')



h[100].sum tensor(-6.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1099, device='cuda:0')



h[200].sum tensor(-22.5122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51823.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0437, 0.0162, 0.0405,  ..., 0.0230, 0.0096, 0.0165],
        [0.0438, 0.0162, 0.0406,  ..., 0.0230, 0.0096, 0.0166],
        [0.0439, 0.0163, 0.0407,  ..., 0.0231, 0.0096, 0.0167],
        ...,
        [0.0442, 0.0163, 0.0409,  ..., 0.0232, 0.0097, 0.0167],
        [0.0442, 0.0163, 0.0409,  ..., 0.0232, 0.0097, 0.0167],
        [0.0442, 0.0163, 0.0410,  ..., 0.0232, 0.0097, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386556.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7909.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(92.0880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4223.2646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(727.2867, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6138],
        [-0.6033],
        [-0.5784],
        ...,
        [-0.4956],
        [-0.4953],
        [-0.4954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101360.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0009],
        [1.0026],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366724.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0010],
        [1.0026],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366737.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6818e-03,  8.4673e-03,  2.3616e-03,  ...,  5.9652e-04,
          8.4960e-03,  5.2455e-03],
        [ 0.0000e+00,  1.9851e-05, -6.4989e-04,  ..., -5.8085e-04,
          2.4986e-05, -2.0256e-03],
        [ 0.0000e+00,  1.9851e-05, -6.4989e-04,  ..., -5.8085e-04,
          2.4986e-05, -2.0256e-03],
        ...,
        [ 0.0000e+00,  1.9851e-05, -6.4989e-04,  ..., -5.8085e-04,
          2.4986e-05, -2.0256e-03],
        [ 0.0000e+00,  1.9851e-05, -6.4989e-04,  ..., -5.8085e-04,
          2.4986e-05, -2.0256e-03],
        [ 0.0000e+00,  1.9851e-05, -6.4989e-04,  ..., -5.8085e-04,
          2.4986e-05, -2.0256e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-252.5206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3920, device='cuda:0')



h[100].sum tensor(-4.2228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6872, device='cuda:0')



h[200].sum tensor(-22.5933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.8280e-02, 8.7542e-03,  ..., 2.7693e-03, 2.8380e-02,
         2.0224e-02],
        [0.0000e+00, 8.5280e-03, 2.3619e-03,  ..., 5.9660e-04, 8.5722e-03,
         5.2461e-03],
        [0.0000e+00, 7.9503e-05, 0.0000e+00,  ..., 0.0000e+00, 1.0007e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 8.0119e-05, 0.0000e+00,  ..., 0.0000e+00, 1.0084e-04,
         0.0000e+00],
        [0.0000e+00, 8.0130e-05, 0.0000e+00,  ..., 0.0000e+00, 1.0086e-04,
         0.0000e+00],
        [0.0000e+00, 8.0138e-05, 0.0000e+00,  ..., 0.0000e+00, 1.0087e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38771.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0363, 0.0048, 0.0025,  ..., 0.1150, 0.0742, 0.0737],
        [0.0399, 0.0097, 0.0145,  ..., 0.0715, 0.0428, 0.0472],
        [0.0428, 0.0136, 0.0272,  ..., 0.0422, 0.0230, 0.0284],
        ...,
        [0.0447, 0.0160, 0.0414,  ..., 0.0235, 0.0099, 0.0167],
        [0.0447, 0.0160, 0.0414,  ..., 0.0235, 0.0099, 0.0167],
        [0.0447, 0.0160, 0.0414,  ..., 0.0235, 0.0099, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(327777.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8138.3213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.3479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4492.9658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(613.9465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0722],
        [-0.0185],
        [-0.1540],
        ...,
        [-0.5091],
        [-0.5077],
        [-0.5073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117218.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0010],
        [1.0026],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366737.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0010],
        [1.0027],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366751.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5553e-02,  4.9130e-02,  1.6844e-02,  ...,  6.2548e-03,
          4.9275e-02,  4.0254e-02],
        [ 0.0000e+00,  2.4715e-05, -6.5778e-04,  ..., -5.8492e-04,
          3.3334e-05, -2.0127e-03],
        [-9.2174e-03,  2.9128e-02,  9.7151e-03,  ...,  3.4687e-03,
          2.9217e-02,  2.3037e-02],
        ...,
        [ 0.0000e+00,  2.4715e-05, -6.5778e-04,  ..., -5.8492e-04,
          3.3334e-05, -2.0127e-03],
        [ 0.0000e+00,  2.4715e-05, -6.5778e-04,  ..., -5.8492e-04,
          3.3334e-05, -2.0127e-03],
        [ 0.0000e+00,  2.4715e-05, -6.5778e-04,  ..., -5.8492e-04,
          3.3334e-05, -2.0127e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-186.3956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6870, device='cuda:0')



h[100].sum tensor(-4.7918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0226, device='cuda:0')



h[200].sum tensor(-22.5732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.6442e-02, 2.8802e-02,  ..., 1.0272e-02, 8.6716e-02,
         6.8283e-02],
        [0.0000e+00, 1.2587e-01, 4.2195e-02,  ..., 1.5178e-02, 1.2625e-01,
         1.0020e-01],
        [0.0000e+00, 2.3887e-02, 7.8202e-03,  ..., 2.7278e-03, 2.3988e-02,
         1.8460e-02],
        ...,
        [0.0000e+00, 9.9766e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3456e-04,
         0.0000e+00],
        [0.0000e+00, 9.9779e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3458e-04,
         0.0000e+00],
        [0.0000e+00, 9.9789e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3459e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42603.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0290, 0.0000, 0.0000,  ..., 0.3122, 0.2432, 0.1716],
        [0.0318, 0.0000, 0.0000,  ..., 0.2568, 0.1981, 0.1424],
        [0.0386, 0.0052, 0.0015,  ..., 0.1324, 0.0971, 0.0761],
        ...,
        [0.0451, 0.0155, 0.0417,  ..., 0.0238, 0.0101, 0.0170],
        [0.0451, 0.0155, 0.0417,  ..., 0.0238, 0.0101, 0.0170],
        [0.0451, 0.0155, 0.0417,  ..., 0.0238, 0.0101, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(351773.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8198.8506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9962, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(92.9597, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4481.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(648.4503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0515],
        [ 0.0325],
        [-0.0290],
        ...,
        [-0.5159],
        [-0.5144],
        [-0.5139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125668.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0010],
        [1.0027],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366751.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0027],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366765., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.0667e-05, -6.6374e-04,  ..., -5.8799e-04,
          4.2298e-05, -2.0039e-03],
        [ 0.0000e+00,  3.0667e-05, -6.6374e-04,  ..., -5.8799e-04,
          4.2298e-05, -2.0039e-03],
        [-4.3504e-03,  1.3799e-02,  4.2427e-03,  ...,  1.3286e-03,
          1.3848e-02,  9.8465e-03],
        ...,
        [ 0.0000e+00,  3.0667e-05, -6.6374e-04,  ..., -5.8799e-04,
          4.2298e-05, -2.0039e-03],
        [ 0.0000e+00,  3.0667e-05, -6.6374e-04,  ..., -5.8799e-04,
          4.2298e-05, -2.0039e-03],
        [ 0.0000e+00,  3.0667e-05, -6.6374e-04,  ..., -5.8799e-04,
          4.2298e-05, -2.0039e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-86.7511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0418, device='cuda:0')



h[100].sum tensor(-5.5920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5127, device='cuda:0')



h[200].sum tensor(-22.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0139, 0.0042,  ..., 0.0013, 0.0140, 0.0099],
        [0.0000, 0.0480, 0.0157,  ..., 0.0055, 0.0482, 0.0372],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49397.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0422, 0.0108, 0.0173,  ..., 0.0584, 0.0354, 0.0378],
        [0.0383, 0.0054, 0.0082,  ..., 0.1250, 0.0882, 0.0745],
        [0.0326, 0.0019, 0.0000,  ..., 0.2383, 0.1810, 0.1345],
        ...,
        [0.0454, 0.0152, 0.0420,  ..., 0.0241, 0.0102, 0.0173],
        [0.0454, 0.0152, 0.0420,  ..., 0.0241, 0.0102, 0.0173],
        [0.0454, 0.0152, 0.0420,  ..., 0.0241, 0.0102, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(384644.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8182.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.7069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4433.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(708.1448, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0196],
        [ 0.0364],
        [ 0.0333],
        ...,
        [-0.5214],
        [-0.5199],
        [-0.5195]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116080.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0027],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366765., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366778.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.5275e-05, -6.6890e-04,  ..., -5.9057e-04,
          5.1464e-05, -1.9966e-03],
        [ 0.0000e+00,  3.5275e-05, -6.6890e-04,  ..., -5.9057e-04,
          5.1464e-05, -1.9966e-03],
        [ 0.0000e+00,  3.5275e-05, -6.6890e-04,  ..., -5.9057e-04,
          5.1464e-05, -1.9966e-03],
        ...,
        [ 0.0000e+00,  3.5275e-05, -6.6890e-04,  ..., -5.9057e-04,
          5.1464e-05, -1.9966e-03],
        [ 0.0000e+00,  3.5275e-05, -6.6890e-04,  ..., -5.9057e-04,
          5.1464e-05, -1.9966e-03],
        [ 0.0000e+00,  3.5275e-05, -6.6890e-04,  ..., -5.9057e-04,
          5.1464e-05, -1.9966e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-370.5402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5482, device='cuda:0')



h[100].sum tensor(-3.0610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9795, device='cuda:0')



h[200].sum tensor(-22.6327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0300, 0.0100,  ..., 0.0036, 0.0302, 0.0237],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32993.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0448, 0.0144, 0.0380,  ..., 0.0289, 0.0141, 0.0199],
        [0.0438, 0.0128, 0.0255,  ..., 0.0452, 0.0270, 0.0289],
        [0.0411, 0.0084, 0.0095,  ..., 0.0968, 0.0684, 0.0571],
        ...,
        [0.0455, 0.0151, 0.0422,  ..., 0.0243, 0.0103, 0.0175],
        [0.0455, 0.0151, 0.0422,  ..., 0.0243, 0.0103, 0.0175],
        [0.0455, 0.0151, 0.0422,  ..., 0.0243, 0.0103, 0.0175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(306467.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8357.2451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.1209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4706.1206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(563.7410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4168],
        [-0.2494],
        [-0.0705],
        ...,
        [-0.4689],
        [-0.5105],
        [-0.5210]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130846.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366778.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(523.9915, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366791.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [-2.6281e-03,  8.3944e-03,  2.3023e-03,  ...,  5.6848e-04,
          8.4374e-03,  5.2041e-03],
        ...,
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-343.0075, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4099, device='cuda:0')



h[100].sum tensor(-3.2674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1054, device='cuda:0')



h[200].sum tensor(-22.6251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0085, 0.0023,  ..., 0.0006, 0.0086, 0.0052],
        [0.0000, 0.0070, 0.0018,  ..., 0.0004, 0.0071, 0.0039],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34132.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0447, 0.0138, 0.0374,  ..., 0.0306, 0.0141, 0.0219],
        [0.0432, 0.0116, 0.0271,  ..., 0.0445, 0.0227, 0.0315],
        [0.0425, 0.0104, 0.0220,  ..., 0.0519, 0.0267, 0.0371],
        ...,
        [0.0459, 0.0150, 0.0425,  ..., 0.0246, 0.0104, 0.0177],
        [0.0459, 0.0150, 0.0425,  ..., 0.0246, 0.0104, 0.0177],
        [0.0458, 0.0148, 0.0420,  ..., 0.0252, 0.0107, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312060.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8384.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.1811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.0491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4676.5186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(576.2690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3836],
        [-0.2758],
        [-0.1383],
        ...,
        [-0.5252],
        [-0.5081],
        [-0.4852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127524.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366791.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366791.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.8504e-03,  9.1012e-03,  2.5541e-03,  ...,  6.6677e-04,
          9.1460e-03,  5.8123e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [-2.8504e-03,  9.1012e-03,  2.5541e-03,  ...,  6.6677e-04,
          9.1460e-03,  5.8123e-03],
        ...,
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-11.3435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1754, device='cuda:0')



h[100].sum tensor(-6.0940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9706, device='cuda:0')



h[200].sum tensor(-22.5259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0280, 0.0086,  ..., 0.0027, 0.0282, 0.0200],
        [0.0000, 0.0463, 0.0138,  ..., 0.0040, 0.0465, 0.0318],
        [0.0000, 0.0142, 0.0043,  ..., 0.0014, 0.0143, 0.0101],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52301.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0363, 0.0017, 0.0000,  ..., 0.1251, 0.0797, 0.0820],
        [0.0360, 0.0010, 0.0000,  ..., 0.1253, 0.0783, 0.0833],
        [0.0396, 0.0061, 0.0075,  ..., 0.0857, 0.0508, 0.0582],
        ...,
        [0.0459, 0.0150, 0.0425,  ..., 0.0246, 0.0104, 0.0177],
        [0.0459, 0.0150, 0.0425,  ..., 0.0246, 0.0104, 0.0177],
        [0.0459, 0.0150, 0.0425,  ..., 0.0246, 0.0104, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(396241., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8145.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.5654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4259.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(741.1427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1386],
        [ 0.1276],
        [ 0.0746],
        ...,
        [-0.5331],
        [-0.5315],
        [-0.5310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93236.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366791.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366791.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        ...,
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03],
        [ 0.0000e+00,  3.7090e-05, -6.7495e-04,  ..., -5.9366e-04,
          5.7794e-05, -1.9883e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-141.5272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0133, device='cuda:0')



h[100].sum tensor(-4.9845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2163, device='cuda:0')



h[200].sum tensor(-22.5648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0078, 0.0021,  ..., 0.0005, 0.0079, 0.0046],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44148.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0435, 0.0125, 0.0338,  ..., 0.0362, 0.0154, 0.0276],
        [0.0436, 0.0124, 0.0311,  ..., 0.0394, 0.0189, 0.0286],
        [0.0425, 0.0103, 0.0204,  ..., 0.0565, 0.0317, 0.0388],
        ...,
        [0.0457, 0.0147, 0.0411,  ..., 0.0264, 0.0116, 0.0189],
        [0.0459, 0.0150, 0.0425,  ..., 0.0246, 0.0104, 0.0177],
        [0.0459, 0.0150, 0.0425,  ..., 0.0246, 0.0104, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(356339.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8287.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.1311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4543.8120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(664.9295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1086],
        [-0.1071],
        [-0.0684],
        ...,
        [-0.4925],
        [-0.5189],
        [-0.5289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119455.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366791.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366804.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.5213e-05, -6.6640e-04,  ..., -5.9024e-04,
          3.4087e-05, -1.9877e-03],
        [-3.9892e-03,  1.2730e-02,  3.8631e-03,  ...,  1.1776e-03,
          1.2782e-02,  8.9546e-03],
        [-3.4263e-03,  1.0936e-02,  3.2240e-03,  ...,  9.2814e-04,
          1.0983e-02,  7.4105e-03],
        ...,
        [ 0.0000e+00,  1.5213e-05, -6.6640e-04,  ..., -5.9024e-04,
          3.4087e-05, -1.9877e-03],
        [ 0.0000e+00,  1.5213e-05, -6.6640e-04,  ..., -5.9024e-04,
          3.4087e-05, -1.9877e-03],
        [ 0.0000e+00,  1.5213e-05, -6.6640e-04,  ..., -5.9024e-04,
          3.4087e-05, -1.9877e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-380.0031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.6932, device='cuda:0')



h[100].sum tensor(-2.8653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8546, device='cuda:0')



h[200].sum tensor(-22.6390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.2774e-02, 3.8625e-03,  ..., 1.1774e-03, 1.2882e-02,
         8.9532e-03],
        [0.0000e+00, 3.0298e-02, 9.4381e-03,  ..., 3.0229e-03, 3.0452e-02,
         2.2044e-02],
        [0.0000e+00, 7.4887e-02, 2.3986e-02,  ..., 8.0392e-03, 7.5155e-02,
         5.6432e-02],
        ...,
        [0.0000e+00, 6.1447e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3768e-04,
         0.0000e+00],
        [0.0000e+00, 6.1455e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3770e-04,
         0.0000e+00],
        [0.0000e+00, 6.1462e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3771e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32273.6660, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0395, 0.0069, 0.0000,  ..., 0.1004, 0.0667, 0.0618],
        [0.0370, 0.0038, 0.0000,  ..., 0.1360, 0.0934, 0.0827],
        [0.0330, 0.0007, 0.0000,  ..., 0.1988, 0.1416, 0.1187],
        ...,
        [0.0458, 0.0159, 0.0429,  ..., 0.0247, 0.0102, 0.0169],
        [0.0458, 0.0159, 0.0429,  ..., 0.0247, 0.0103, 0.0169],
        [0.0458, 0.0159, 0.0429,  ..., 0.0247, 0.0103, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(306177.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8347.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.0067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.1103, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4726.7192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(560.3413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0704],
        [ 0.0767],
        [ 0.0808],
        ...,
        [-0.5441],
        [-0.5420],
        [-0.5386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136360.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0010],
        [1.0028],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366804.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0029],
        ...,
        [1.0020],
        [1.0015],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366816.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1009e-03,  1.6297e-02,  5.1461e-03,  ...,  1.6786e-03,
          1.6353e-02,  1.2036e-02],
        [-8.6587e-03,  2.7664e-02,  9.1950e-03,  ...,  3.2587e-03,
          2.7748e-02,  2.1817e-02],
        [-9.0137e-03,  2.8798e-02,  9.5990e-03,  ...,  3.4164e-03,
          2.8885e-02,  2.2793e-02],
        ...,
        [ 0.0000e+00,  7.3785e-07, -6.5909e-04,  ..., -5.8680e-04,
          1.6151e-05, -1.9879e-03],
        [ 0.0000e+00,  7.3785e-07, -6.5909e-04,  ..., -5.8680e-04,
          1.6151e-05, -1.9879e-03],
        [ 0.0000e+00,  7.3785e-07, -6.5909e-04,  ..., -5.8680e-04,
          1.6151e-05, -1.9879e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-267.1398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6005, device='cuda:0')



h[100].sum tensor(-3.6919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4255, device='cuda:0')



h[200].sum tensor(-22.6094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.5971e-02, 2.7988e-02,  ..., 9.6042e-03, 8.6246e-02,
         6.6029e-02],
        [0.0000e+00, 1.1458e-01, 3.8179e-02,  ..., 1.3580e-02, 1.1493e-01,
         9.0646e-02],
        [0.0000e+00, 1.0135e-01, 3.3463e-02,  ..., 1.1739e-02, 1.0167e-01,
         7.9253e-02],
        ...,
        [0.0000e+00, 2.9806e-06, 0.0000e+00,  ..., 0.0000e+00, 6.5242e-05,
         0.0000e+00],
        [0.0000e+00, 2.9810e-06, 0.0000e+00,  ..., 0.0000e+00, 6.5251e-05,
         0.0000e+00],
        [0.0000e+00, 2.9813e-06, 0.0000e+00,  ..., 0.0000e+00, 6.5258e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38225.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6957e-02, 0.0000e+00, 0.0000e+00,  ..., 2.9221e-01, 2.1671e-01,
         1.6859e-01],
        [2.7072e-02, 0.0000e+00, 0.0000e+00,  ..., 3.0794e-01, 2.3245e-01,
         1.7460e-01],
        [2.9954e-02, 2.2238e-04, 0.0000e+00,  ..., 2.6387e-01, 1.9751e-01,
         1.5034e-01],
        ...,
        [4.5558e-02, 1.6641e-02, 4.3204e-02,  ..., 2.4693e-02, 1.0222e-02,
         1.6524e-02],
        [4.5566e-02, 1.6644e-02, 4.3211e-02,  ..., 2.4697e-02, 1.0224e-02,
         1.6527e-02],
        [4.5572e-02, 1.6646e-02, 4.3216e-02,  ..., 2.4701e-02, 1.0226e-02,
         1.6530e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339698.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8202.7334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.2780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4553.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(613.5784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0818],
        [ 0.0775],
        [ 0.0696],
        ...,
        [-0.5503],
        [-0.5488],
        [-0.5483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132733.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0029],
        ...,
        [1.0020],
        [1.0015],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366816.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0029],
        ...,
        [1.0019],
        [1.0015],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366829.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5738e-03,  2.1044e-02,  6.8434e-03,  ...,  2.3420e-03,
          2.1106e-02,  1.6126e-02],
        [-3.8594e-03,  1.2351e-02,  3.7472e-03,  ...,  1.1338e-03,
          1.2393e-02,  8.6460e-03],
        [-1.3657e-02,  4.3726e-02,  1.4923e-02,  ...,  5.4946e-03,
          4.3843e-02,  3.5644e-02],
        ...,
        [ 0.0000e+00, -7.8983e-06, -6.5504e-04,  ..., -5.8395e-04,
          4.5341e-06, -1.9887e-03],
        [ 0.0000e+00, -7.8983e-06, -6.5504e-04,  ..., -5.8395e-04,
          4.5341e-06, -1.9887e-03],
        [ 0.0000e+00, -7.8983e-06, -6.5504e-04,  ..., -5.8395e-04,
          4.5341e-06, -1.9887e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-229.7605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4404, device='cuda:0')



h[100].sum tensor(-3.8805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5482, device='cuda:0')



h[200].sum tensor(-22.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.2040e-02, 1.9490e-02,  ..., 6.2921e-03, 6.2238e-02,
         4.5459e-02],
        [0.0000e+00, 1.0927e-01, 3.6312e-02,  ..., 1.2855e-02, 1.0958e-01,
         8.6096e-02],
        [0.0000e+00, 4.8106e-02, 1.5176e-02,  ..., 4.9354e-03, 4.8263e-02,
         3.5441e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8318e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8321e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8323e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38540.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0349, 0.0032, 0.0000,  ..., 0.1645, 0.1165, 0.0966],
        [0.0325, 0.0006, 0.0000,  ..., 0.2173, 0.1611, 0.1235],
        [0.0345, 0.0024, 0.0000,  ..., 0.1805, 0.1306, 0.1044],
        ...,
        [0.0454, 0.0172, 0.0434,  ..., 0.0248, 0.0102, 0.0163],
        [0.0454, 0.0172, 0.0434,  ..., 0.0248, 0.0102, 0.0163],
        [0.0454, 0.0172, 0.0434,  ..., 0.0248, 0.0103, 0.0163]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334528.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8093.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.6893, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4455.9155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(618.6669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0652],
        [-0.0064],
        [ 0.0279],
        ...,
        [-0.5550],
        [-0.5535],
        [-0.5531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122775.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0029],
        ...,
        [1.0019],
        [1.0015],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366829.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0030],
        ...,
        [1.0019],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366842.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        ...,
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-253.8089, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9117, device='cuda:0')



h[100].sum tensor(-3.5364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3248, device='cuda:0')



h[200].sum tensor(-22.6140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 9.8175e-03, 2.8443e-03,  ..., 7.8375e-04, 9.8526e-03,
         6.4639e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3160e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3223e-06,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3775e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3783e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3790e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36573.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0407, 0.0128, 0.0181,  ..., 0.0620, 0.0340, 0.0408],
        [0.0431, 0.0158, 0.0326,  ..., 0.0386, 0.0187, 0.0255],
        [0.0444, 0.0172, 0.0409,  ..., 0.0279, 0.0118, 0.0184],
        ...,
        [0.0451, 0.0177, 0.0435,  ..., 0.0249, 0.0103, 0.0161],
        [0.0451, 0.0177, 0.0435,  ..., 0.0249, 0.0103, 0.0161],
        [0.0451, 0.0177, 0.0435,  ..., 0.0249, 0.0103, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(323780.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8008.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4292, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.9781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4409.9224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(602.2360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0011],
        [-0.1055],
        [-0.1839],
        ...,
        [-0.5573],
        [-0.5558],
        [-0.5554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120170.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0030],
        ...,
        [1.0019],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366842.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0030],
        ...,
        [1.0019],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366842.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1381e-03,  6.8570e-03,  1.7896e-03,  ...,  3.7218e-04,
          6.8806e-03,  3.9159e-03],
        [-2.1381e-03,  6.8570e-03,  1.7896e-03,  ...,  3.7218e-04,
          6.8806e-03,  3.9159e-03],
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        ...,
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03],
        [ 0.0000e+00, -6.2872e-06, -6.5469e-04,  ..., -5.8147e-04,
          1.5783e-06, -1.9894e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(163.9808, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3783, device='cuda:0')



h[100].sum tensor(-6.9564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5846, device='cuda:0')



h[200].sum tensor(-22.4916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.3620e-02, 1.2926e-02,  ..., 3.7392e-03, 4.3751e-02,
         2.9598e-02],
        [0.0000e+00, 2.5802e-02, 7.2311e-03,  ..., 1.8426e-03, 2.5886e-02,
         1.6246e-02],
        [0.0000e+00, 5.1617e-02, 1.5769e-02,  ..., 4.8463e-03, 5.1766e-02,
         3.6464e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3775e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3783e-06,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3790e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59629.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0017, 0.0000,  ..., 0.1574, 0.1009, 0.1000],
        [0.0331, 0.0031, 0.0000,  ..., 0.1445, 0.0915, 0.0922],
        [0.0323, 0.0018, 0.0000,  ..., 0.1644, 0.1083, 0.1023],
        ...,
        [0.0451, 0.0177, 0.0435,  ..., 0.0249, 0.0103, 0.0161],
        [0.0451, 0.0177, 0.0435,  ..., 0.0249, 0.0103, 0.0161],
        [0.0451, 0.0177, 0.0435,  ..., 0.0249, 0.0103, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442081.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7881.1182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.2346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4228.2446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(798.5471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1105],
        [ 0.1088],
        [ 0.1024],
        ...,
        [-0.5573],
        [-0.5558],
        [-0.5554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125838.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0011],
        [1.0030],
        ...,
        [1.0019],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366842.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0030],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366855.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4680e-06, -6.5705e-04,  ..., -5.7970e-04,
         -1.3216e-06, -1.9888e-03],
        [ 0.0000e+00, -2.4680e-06, -6.5705e-04,  ..., -5.7970e-04,
         -1.3216e-06, -1.9888e-03],
        [ 0.0000e+00, -2.4680e-06, -6.5705e-04,  ..., -5.7970e-04,
         -1.3216e-06, -1.9888e-03],
        ...,
        [ 0.0000e+00, -2.4680e-06, -6.5705e-04,  ..., -5.7970e-04,
         -1.3216e-06, -1.9888e-03],
        [ 0.0000e+00, -2.4680e-06, -6.5705e-04,  ..., -5.7970e-04,
         -1.3216e-06, -1.9888e-03],
        [ 0.0000e+00, -2.4680e-06, -6.5705e-04,  ..., -5.7970e-04,
         -1.3216e-06, -1.9888e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-99.7678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2711, device='cuda:0')



h[100].sum tensor(-4.6979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1079, device='cuda:0')



h[200].sum tensor(-22.5718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0305, 0.0102,  ..., 0.0037, 0.0305, 0.0242],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44006.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0158, 0.0294,  ..., 0.0424, 0.0227, 0.0265],
        [0.0419, 0.0143, 0.0173,  ..., 0.0617, 0.0383, 0.0369],
        [0.0388, 0.0098, 0.0064,  ..., 0.1167, 0.0823, 0.0668],
        ...,
        [0.0451, 0.0180, 0.0436,  ..., 0.0251, 0.0103, 0.0160],
        [0.0451, 0.0180, 0.0436,  ..., 0.0251, 0.0103, 0.0160],
        [0.0451, 0.0180, 0.0436,  ..., 0.0251, 0.0103, 0.0160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358018.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7891.8579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.8626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4246.9658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(668.8582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0425],
        [ 0.0460],
        [ 0.0557],
        ...,
        [-0.5208],
        [-0.5330],
        [-0.5445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116982.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0030],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366855.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0031],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366868., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.8765e-06, -6.6228e-04,  ..., -5.7995e-04,
         -2.7013e-06, -1.9825e-03],
        [ 0.0000e+00,  3.8765e-06, -6.6228e-04,  ..., -5.7995e-04,
         -2.7013e-06, -1.9825e-03],
        [ 0.0000e+00,  3.8765e-06, -6.6228e-04,  ..., -5.7995e-04,
         -2.7013e-06, -1.9825e-03],
        ...,
        [-5.7494e-03,  1.8547e-02,  5.9400e-03,  ...,  1.9950e-03,
          1.8580e-02,  1.3972e-02],
        [ 0.0000e+00,  3.8765e-06, -6.6228e-04,  ..., -5.7995e-04,
         -2.7013e-06, -1.9825e-03],
        [ 0.0000e+00,  3.8765e-06, -6.6228e-04,  ..., -5.7995e-04,
         -2.7013e-06, -1.9825e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24.5231, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4715, device='cuda:0')



h[100].sum tensor(-5.6649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7216, device='cuda:0')



h[200].sum tensor(-22.5361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.5503e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5514e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5529e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.0256e-02, 8.7590e-03,  ..., 2.4411e-03, 3.0296e-02,
         2.0008e-02],
        [0.0000e+00, 2.6356e-02, 8.0395e-03,  ..., 2.4854e-03, 2.6390e-02,
         1.8655e-02],
        [0.0000e+00, 1.5672e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52349.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.4169e-02, 1.6967e-02, 3.8671e-02,  ..., 3.1322e-02, 1.4443e-02,
         1.9635e-02],
        [4.4812e-02, 1.7720e-02, 4.3446e-02,  ..., 2.5241e-02, 1.0317e-02,
         1.5761e-02],
        [4.4969e-02, 1.7798e-02, 4.3561e-02,  ..., 2.5376e-02, 1.0366e-02,
         1.5863e-02],
        ...,
        [3.5380e-02, 5.3083e-03, 6.6718e-05,  ..., 1.3821e-01, 9.0400e-02,
         8.4991e-02],
        [3.8324e-02, 9.0175e-03, 7.2365e-03,  ..., 1.0743e-01, 6.9435e-02,
         6.5522e-02],
        [4.2923e-02, 1.4888e-02, 2.2390e-02,  ..., 5.2982e-02, 3.0007e-02,
         3.2710e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406620.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7848.8857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.8155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4129.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(743.1090, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2280],
        [-0.3457],
        [-0.4229],
        ...,
        [ 0.0475],
        [-0.0450],
        [-0.2025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113439.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0031],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366868., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(359.6947, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0031],
        ...,
        [1.0018],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366880.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1234e-03,  6.8766e-03,  1.7765e-03,  ...,  3.7347e-04,
          6.8774e-03,  3.9307e-03],
        [-2.7232e-03,  8.8157e-03,  2.4668e-03,  ...,  6.4270e-04,
          8.8205e-03,  5.5991e-03],
        [ 0.0000e+00,  1.1777e-05, -6.6743e-04,  ..., -5.7964e-04,
         -1.4296e-06, -1.9757e-03],
        ...,
        [ 0.0000e+00,  1.1777e-05, -6.6743e-04,  ..., -5.7964e-04,
         -1.4296e-06, -1.9757e-03],
        [ 0.0000e+00,  1.1777e-05, -6.6743e-04,  ..., -5.7964e-04,
         -1.4296e-06, -1.9757e-03],
        [ 0.0000e+00,  1.1777e-05, -6.6743e-04,  ..., -5.7964e-04,
         -1.4296e-06, -1.9757e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-97.0258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3902, device='cuda:0')



h[100].sum tensor(-4.6401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1253, device='cuda:0')



h[200].sum tensor(-22.5725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.8105e-02, 2.1560e-02,  ..., 7.1311e-03, 6.8192e-02,
         5.0655e-02],
        [0.0000e+00, 2.7983e-02, 8.6102e-03,  ..., 2.7190e-03, 2.7990e-02,
         2.0083e-02],
        [0.0000e+00, 8.8618e-03, 2.4698e-03,  ..., 6.4348e-04, 8.8312e-03,
         5.6059e-03],
        ...,
        [0.0000e+00, 4.7610e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.7616e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.7621e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43657.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0325, 0.0016, 0.0000,  ..., 0.1789, 0.1229, 0.1071],
        [0.0365, 0.0065, 0.0026,  ..., 0.1279, 0.0848, 0.0773],
        [0.0406, 0.0118, 0.0157,  ..., 0.0780, 0.0478, 0.0478],
        ...,
        [0.0455, 0.0178, 0.0442,  ..., 0.0259, 0.0106, 0.0159],
        [0.0455, 0.0178, 0.0442,  ..., 0.0259, 0.0106, 0.0159],
        [0.0455, 0.0178, 0.0442,  ..., 0.0259, 0.0106, 0.0159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358364.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7921.5688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.6064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4197.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(670.3351, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0853],
        [ 0.0551],
        [-0.0265],
        ...,
        [-0.5748],
        [-0.5733],
        [-0.5729]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116586.9297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0031],
        ...,
        [1.0018],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366880.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0032],
        ...,
        [1.0018],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366893.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.6616e-05, -6.7096e-04,  ..., -5.7712e-04,
          9.9914e-07, -1.9701e-03],
        [ 0.0000e+00,  1.6616e-05, -6.7096e-04,  ..., -5.7712e-04,
          9.9914e-07, -1.9701e-03],
        [-2.1224e-03,  6.8945e-03,  1.7774e-03,  ...,  3.7774e-04,
          6.8925e-03,  3.9477e-03],
        ...,
        [ 0.0000e+00,  1.6616e-05, -6.7096e-04,  ..., -5.7712e-04,
          9.9914e-07, -1.9701e-03],
        [ 0.0000e+00,  1.6616e-05, -6.7096e-04,  ..., -5.7712e-04,
          9.9914e-07, -1.9701e-03],
        [ 0.0000e+00,  1.6616e-05, -6.7096e-04,  ..., -5.7712e-04,
          9.9914e-07, -1.9701e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68.5564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8675, device='cuda:0')



h[100].sum tensor(-5.8762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9256, device='cuda:0')



h[200].sum tensor(-22.5266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.6449e-03, 1.3150e-03,  ..., 1.9744e-04, 5.5935e-03,
         2.8300e-03],
        [0.0000e+00, 2.8962e-02, 8.2722e-03,  ..., 2.2791e-03, 2.8957e-02,
         1.8948e-02],
        [0.0000e+00, 5.0075e-02, 1.5786e-02,  ..., 5.2084e-03, 5.0112e-02,
         3.7107e-02],
        ...,
        [0.0000e+00, 6.7183e-05, 0.0000e+00,  ..., 0.0000e+00, 4.0397e-06,
         0.0000e+00],
        [0.0000e+00, 6.7191e-05, 0.0000e+00,  ..., 0.0000e+00, 4.0402e-06,
         0.0000e+00],
        [0.0000e+00, 6.7198e-05, 0.0000e+00,  ..., 0.0000e+00, 4.0406e-06,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53569.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0399, 0.0112, 0.0153,  ..., 0.0751, 0.0423, 0.0482],
        [0.0355, 0.0054, 0.0025,  ..., 0.1255, 0.0781, 0.0792],
        [0.0329, 0.0020, 0.0000,  ..., 0.1653, 0.1090, 0.1016],
        ...,
        [0.0457, 0.0178, 0.0444,  ..., 0.0262, 0.0106, 0.0159],
        [0.0457, 0.0178, 0.0444,  ..., 0.0262, 0.0106, 0.0159],
        [0.0457, 0.0178, 0.0444,  ..., 0.0262, 0.0106, 0.0159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409754.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7825.6748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.7009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3987.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(759.4517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0285],
        [ 0.0579],
        [ 0.0954],
        ...,
        [-0.5811],
        [-0.5796],
        [-0.5792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106191.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0012],
        [1.0032],
        ...,
        [1.0018],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366893.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0013],
        [1.0032],
        ...,
        [1.0018],
        [1.0013],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366906.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.0434e-05, -6.7354e-04,  ..., -5.7381e-04,
          4.1873e-06, -1.9617e-03],
        [ 0.0000e+00,  2.0434e-05, -6.7354e-04,  ..., -5.7381e-04,
          4.1873e-06, -1.9617e-03],
        [ 0.0000e+00,  2.0434e-05, -6.7354e-04,  ..., -5.7381e-04,
          4.1873e-06, -1.9617e-03],
        ...,
        [ 0.0000e+00,  2.0434e-05, -6.7354e-04,  ..., -5.7381e-04,
          4.1873e-06, -1.9617e-03],
        [ 0.0000e+00,  2.0434e-05, -6.7354e-04,  ..., -5.7381e-04,
          4.1873e-06, -1.9617e-03],
        [ 0.0000e+00,  2.0434e-05, -6.7354e-04,  ..., -5.7381e-04,
          4.1873e-06, -1.9617e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-203.7891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9364, device='cuda:0')



h[100].sum tensor(-3.6662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4746, device='cuda:0')



h[200].sum tensor(-22.6067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.1725e-05, 0.0000e+00,  ..., 0.0000e+00, 1.6747e-05,
         0.0000e+00],
        [0.0000e+00, 8.1783e-05, 0.0000e+00,  ..., 0.0000e+00, 1.6759e-05,
         0.0000e+00],
        [0.0000e+00, 8.1869e-05, 0.0000e+00,  ..., 0.0000e+00, 1.6777e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 8.2631e-05, 0.0000e+00,  ..., 0.0000e+00, 1.6933e-05,
         0.0000e+00],
        [0.0000e+00, 8.2641e-05, 0.0000e+00,  ..., 0.0000e+00, 1.6935e-05,
         0.0000e+00],
        [0.0000e+00, 8.2650e-05, 0.0000e+00,  ..., 0.0000e+00, 1.6937e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39212.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0175, 0.0440,  ..., 0.0261, 0.0106, 0.0157],
        [0.0454, 0.0175, 0.0440,  ..., 0.0262, 0.0106, 0.0157],
        [0.0455, 0.0176, 0.0441,  ..., 0.0263, 0.0106, 0.0158],
        ...,
        [0.0459, 0.0177, 0.0445,  ..., 0.0265, 0.0107, 0.0159],
        [0.0459, 0.0177, 0.0445,  ..., 0.0265, 0.0107, 0.0159],
        [0.0459, 0.0177, 0.0445,  ..., 0.0265, 0.0107, 0.0159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341571.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8071.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(93.4993, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4344.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(628.1680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6563],
        [-0.5953],
        [-0.5015],
        ...,
        [-0.5879],
        [-0.5863],
        [-0.5859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131460.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0013],
        [1.0032],
        ...,
        [1.0018],
        [1.0013],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366906.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0033],
        ...,
        [1.0018],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366919.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.0915e-05, -6.7808e-04,  ..., -5.7161e-04,
          9.7254e-06, -1.9531e-03],
        [ 0.0000e+00,  3.0915e-05, -6.7808e-04,  ..., -5.7161e-04,
          9.7254e-06, -1.9531e-03],
        [-2.5660e-03,  8.3857e-03,  2.2955e-03,  ...,  5.8811e-04,
          8.3799e-03,  5.2358e-03],
        ...,
        [ 0.0000e+00,  3.0915e-05, -6.7808e-04,  ..., -5.7161e-04,
          9.7254e-06, -1.9531e-03],
        [ 0.0000e+00,  3.0915e-05, -6.7808e-04,  ..., -5.7161e-04,
          9.7254e-06, -1.9531e-03],
        [ 0.0000e+00,  3.0915e-05, -6.7808e-04,  ..., -5.7161e-04,
          9.7254e-06, -1.9531e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-89.9249, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8790, device='cuda:0')



h[100].sum tensor(-4.4682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0506, device='cuda:0')



h[200].sum tensor(-22.5766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.2364e-04, 0.0000e+00,  ..., 0.0000e+00, 3.8897e-05,
         0.0000e+00],
        [0.0000e+00, 8.4848e-03, 2.2973e-03,  ..., 5.8855e-04, 8.4154e-03,
         5.2397e-03],
        [0.0000e+00, 1.2594e-02, 3.7590e-03,  ..., 1.1583e-03, 1.2532e-02,
         8.7732e-03],
        ...,
        [0.0000e+00, 1.2503e-04, 0.0000e+00,  ..., 0.0000e+00, 3.9333e-05,
         0.0000e+00],
        [0.0000e+00, 1.2505e-04, 0.0000e+00,  ..., 0.0000e+00, 3.9338e-05,
         0.0000e+00],
        [0.0000e+00, 1.2506e-04, 0.0000e+00,  ..., 0.0000e+00, 3.9342e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44568.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0444, 0.0158, 0.0357,  ..., 0.0371, 0.0175, 0.0227],
        [0.0419, 0.0125, 0.0201,  ..., 0.0624, 0.0341, 0.0392],
        [0.0396, 0.0095, 0.0090,  ..., 0.0861, 0.0495, 0.0548],
        ...,
        [0.0461, 0.0175, 0.0446,  ..., 0.0269, 0.0108, 0.0160],
        [0.0461, 0.0175, 0.0446,  ..., 0.0269, 0.0108, 0.0160],
        [0.0461, 0.0175, 0.0446,  ..., 0.0269, 0.0108, 0.0160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(366471.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7964.9175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.6242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4093.7366, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(679.6292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3818],
        [-0.2040],
        [-0.0459],
        ...,
        [-0.5936],
        [-0.5920],
        [-0.5916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105938.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0013],
        [1.0033],
        ...,
        [1.0018],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366919.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0014],
        [1.0034],
        ...,
        [1.0019],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366932.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.5923e-05, -6.8555e-04,  ..., -5.7009e-04,
          1.0261e-05, -1.9412e-03],
        [ 0.0000e+00,  3.5923e-05, -6.8555e-04,  ..., -5.7009e-04,
          1.0261e-05, -1.9412e-03],
        [ 0.0000e+00,  3.5923e-05, -6.8555e-04,  ..., -5.7009e-04,
          1.0261e-05, -1.9412e-03],
        ...,
        [ 0.0000e+00,  3.5923e-05, -6.8555e-04,  ..., -5.7009e-04,
          1.0261e-05, -1.9412e-03],
        [ 0.0000e+00,  3.5923e-05, -6.8555e-04,  ..., -5.7009e-04,
          1.0261e-05, -1.9412e-03],
        [ 0.0000e+00,  3.5923e-05, -6.8555e-04,  ..., -5.7009e-04,
          1.0261e-05, -1.9412e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-130.1757, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2729, device='cuda:0')



h[100].sum tensor(-4.1104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8159, device='cuda:0')



h[200].sum tensor(-22.5891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4368e-04, 0.0000e+00,  ..., 0.0000e+00, 4.1041e-05,
         0.0000e+00],
        [0.0000e+00, 1.4378e-04, 0.0000e+00,  ..., 0.0000e+00, 4.1070e-05,
         0.0000e+00],
        [0.0000e+00, 1.5820e-02, 4.2040e-03,  ..., 1.0325e-03, 1.5744e-02,
         9.5980e-03],
        ...,
        [0.0000e+00, 1.4531e-04, 0.0000e+00,  ..., 0.0000e+00, 4.1507e-05,
         0.0000e+00],
        [0.0000e+00, 1.4533e-04, 0.0000e+00,  ..., 0.0000e+00, 4.1511e-05,
         0.0000e+00],
        [0.0000e+00, 1.4534e-04, 0.0000e+00,  ..., 0.0000e+00, 4.1516e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40882.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0163, 0.0407,  ..., 0.0315, 0.0136, 0.0189],
        [0.0439, 0.0145, 0.0297,  ..., 0.0453, 0.0226, 0.0280],
        [0.0406, 0.0098, 0.0125,  ..., 0.0803, 0.0454, 0.0509],
        ...,
        [0.0464, 0.0173, 0.0448,  ..., 0.0273, 0.0109, 0.0160],
        [0.0464, 0.0173, 0.0448,  ..., 0.0273, 0.0109, 0.0160],
        [0.0464, 0.0173, 0.0448,  ..., 0.0273, 0.0109, 0.0160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347100.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8102.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8466, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(93.9910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4219.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(644.3222, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4204],
        [-0.2531],
        [-0.0834],
        ...,
        [-0.5896],
        [-0.5851],
        [-0.5834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119076.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0014],
        [1.0034],
        ...,
        [1.0019],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366932.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0014],
        [1.0034],
        ...,
        [1.0019],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366945.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3658e-02,  4.4716e-02,  1.5207e-02,  ...,  5.6299e-03,
          4.4764e-02,  3.6524e-02],
        [-3.4213e-03,  1.1224e-02,  3.2917e-03,  ...,  9.8439e-04,
          1.1218e-02,  7.7048e-03],
        [-1.1100e-02,  3.6348e-02,  1.2230e-02,  ...,  4.4691e-03,
          3.6382e-02,  2.9323e-02],
        ...,
        [ 0.0000e+00,  3.0006e-05, -6.9060e-04,  ..., -5.6823e-04,
          6.2614e-06, -1.9272e-03],
        [ 0.0000e+00,  3.0006e-05, -6.9060e-04,  ..., -5.6823e-04,
          6.2614e-06, -1.9272e-03],
        [ 0.0000e+00,  3.0006e-05, -6.9060e-04,  ..., -5.6823e-04,
          6.2614e-06, -1.9272e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(197.5052, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7877, device='cuda:0')



h[100].sum tensor(-6.5699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.4983, device='cuda:0')



h[200].sum tensor(-22.4969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0436, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.0228e-02, 2.5737e-02,  ..., 8.8385e-03, 8.0261e-02,
         6.1222e-02],
        [0.0000e+00, 1.8024e-01, 6.1315e-02,  ..., 2.2709e-02, 1.8043e-01,
         1.4727e-01],
        [0.0000e+00, 1.1445e-01, 3.7907e-02,  ..., 1.3581e-02, 1.1454e-01,
         9.0656e-02],
        ...,
        [0.0000e+00, 1.2139e-04, 0.0000e+00,  ..., 0.0000e+00, 2.5331e-05,
         0.0000e+00],
        [0.0000e+00, 1.2140e-04, 0.0000e+00,  ..., 0.0000e+00, 2.5334e-05,
         0.0000e+00],
        [0.0000e+00, 1.2142e-04, 0.0000e+00,  ..., 0.0000e+00, 2.5336e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58496.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0000, 0.0000,  ..., 0.3159, 0.2381, 0.1752],
        [0.0266, 0.0000, 0.0000,  ..., 0.4148, 0.3244, 0.2233],
        [0.0278, 0.0000, 0.0000,  ..., 0.3830, 0.2972, 0.2079],
        ...,
        [0.0470, 0.0171, 0.0451,  ..., 0.0275, 0.0110, 0.0159],
        [0.0470, 0.0171, 0.0451,  ..., 0.0275, 0.0110, 0.0159],
        [0.0470, 0.0171, 0.0451,  ..., 0.0275, 0.0110, 0.0159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433401.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8048.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.7806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3929.3916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(800.9394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0934],
        [ 0.0869],
        [ 0.0810],
        ...,
        [-0.6047],
        [-0.5980],
        [-0.5925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105488.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0014],
        [1.0034],
        ...,
        [1.0019],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366945.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0014],
        [1.0035],
        ...,
        [1.0019],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366958.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0268e-05, -6.9018e-04,  ..., -5.6398e-04,
         -2.6329e-06, -1.9124e-03],
        [-7.8745e-03,  2.5835e-02,  8.4969e-03,  ...,  3.0180e-03,
          2.5862e-02,  2.0311e-02],
        [ 0.0000e+00,  1.0268e-05, -6.9018e-04,  ..., -5.6398e-04,
         -2.6329e-06, -1.9124e-03],
        ...,
        [ 0.0000e+00,  1.0268e-05, -6.9018e-04,  ..., -5.6398e-04,
         -2.6329e-06, -1.9124e-03],
        [ 0.0000e+00,  1.0268e-05, -6.9018e-04,  ..., -5.6398e-04,
         -2.6329e-06, -1.9124e-03],
        [ 0.0000e+00,  1.0268e-05, -6.9018e-04,  ..., -5.6398e-04,
         -2.6329e-06, -1.9124e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-179.7950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6093, device='cuda:0')



h[100].sum tensor(-3.6996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5729, device='cuda:0')



h[200].sum tensor(-22.6031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.5868e-02, 8.4977e-03,  ..., 3.0183e-03, 2.5864e-02,
         2.0313e-02],
        [0.0000e+00, 2.1146e-02, 6.8172e-03,  ..., 2.3628e-03, 2.1135e-02,
         1.6247e-02],
        [0.0000e+00, 9.4029e-02, 3.0671e-02,  ..., 1.0777e-02, 9.4123e-02,
         7.3218e-02],
        ...,
        [0.0000e+00, 4.1547e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.1552e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.1556e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39523.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0436, 0.0112, 0.0121,  ..., 0.0858, 0.0571, 0.0480],
        [0.0426, 0.0095, 0.0030,  ..., 0.1024, 0.0699, 0.0576],
        [0.0397, 0.0043, 0.0000,  ..., 0.1562, 0.1122, 0.0875],
        ...,
        [0.0477, 0.0172, 0.0456,  ..., 0.0276, 0.0111, 0.0156],
        [0.0477, 0.0172, 0.0456,  ..., 0.0276, 0.0111, 0.0156],
        [0.0477, 0.0172, 0.0456,  ..., 0.0276, 0.0111, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(346161.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8411.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(93.7000, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4385.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(633.1601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3168],
        [-0.2179],
        [-0.1334],
        ...,
        [-0.6241],
        [-0.6224],
        [-0.6220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134874.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0014],
        [1.0035],
        ...,
        [1.0019],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366958.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0035],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366971.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        ...,
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-162.7945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9125, device='cuda:0')



h[100].sum tensor(-3.8028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6172, device='cuda:0')



h[200].sum tensor(-22.5986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39354.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0477, 0.0170, 0.0455,  ..., 0.0272, 0.0110, 0.0152],
        [0.0477, 0.0169, 0.0451,  ..., 0.0279, 0.0112, 0.0159],
        [0.0475, 0.0167, 0.0441,  ..., 0.0297, 0.0116, 0.0175],
        ...,
        [0.0484, 0.0172, 0.0460,  ..., 0.0276, 0.0111, 0.0155],
        [0.0484, 0.0172, 0.0461,  ..., 0.0276, 0.0111, 0.0155],
        [0.0484, 0.0172, 0.0461,  ..., 0.0276, 0.0111, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344993.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8551.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.6585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4427.7295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(634.7386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5304],
        [-0.5621],
        [-0.5621],
        ...,
        [-0.6179],
        [-0.6303],
        [-0.6327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134933.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0035],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366971.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0035],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366971.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [-2.0951e-03,  6.8764e-03,  1.7609e-03,  ...,  3.9575e-04,
          6.8884e-03,  4.0299e-03],
        ...,
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03],
        [ 0.0000e+00, -1.0490e-05, -6.8911e-04,  ..., -5.5953e-04,
         -8.8487e-06, -1.8971e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-210.7840, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5411, device='cuda:0')



h[100].sum tensor(-3.4470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4168, device='cuda:0')



h[200].sum tensor(-22.6120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0031,  ..., 0.0006, 0.0125, 0.0070],
        [0.0000, 0.0178, 0.0043,  ..., 0.0008, 0.0178, 0.0096],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36872.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0146, 0.0331,  ..., 0.0429, 0.0205, 0.0260],
        [0.0430, 0.0102, 0.0146,  ..., 0.0729, 0.0391, 0.0463],
        [0.0409, 0.0072, 0.0048,  ..., 0.0933, 0.0510, 0.0605],
        ...,
        [0.0484, 0.0172, 0.0460,  ..., 0.0276, 0.0111, 0.0155],
        [0.0484, 0.0172, 0.0461,  ..., 0.0276, 0.0111, 0.0155],
        [0.0484, 0.0172, 0.0461,  ..., 0.0276, 0.0111, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(333331.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8665.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.4640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(92.0319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4615.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(606.4359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2563],
        [-0.0851],
        [ 0.0391],
        ...,
        [-0.6354],
        [-0.6337],
        [-0.6332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158704.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0035],
        ...,
        [1.0019],
        [1.0014],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366971.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0036],
        ...,
        [1.0019],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366985.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.5896e-05, -6.9248e-04,  ..., -5.5657e-04,
         -6.5038e-06, -1.8873e-03],
        [ 0.0000e+00, -1.5896e-05, -6.9248e-04,  ..., -5.5657e-04,
         -6.5038e-06, -1.8873e-03],
        [-2.5175e-03,  8.2795e-03,  2.2583e-03,  ...,  5.9397e-04,
          8.3010e-03,  5.2520e-03],
        ...,
        [ 0.0000e+00, -1.5896e-05, -6.9248e-04,  ..., -5.5657e-04,
         -6.5038e-06, -1.8873e-03],
        [-6.7432e-03,  2.2204e-02,  7.2113e-03,  ...,  2.5252e-03,
          2.2245e-02,  1.7235e-02],
        [ 0.0000e+00, -1.5896e-05, -6.9248e-04,  ..., -5.5657e-04,
         -6.5038e-06, -1.8873e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-25.6657, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5168, device='cuda:0')



h[100].sum tensor(-4.7035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2899, device='cuda:0')



h[200].sum tensor(-22.5639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0023,  ..., 0.0006, 0.0083, 0.0053],
        [0.0000, 0.0276, 0.0085,  ..., 0.0027, 0.0277, 0.0200],
        ...,
        [0.0000, 0.0225, 0.0073,  ..., 0.0026, 0.0225, 0.0174],
        [0.0000, 0.0183, 0.0058,  ..., 0.0020, 0.0184, 0.0139],
        [0.0000, 0.0816, 0.0263,  ..., 0.0091, 0.0818, 0.0627]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46691.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0455, 0.0129, 0.0200,  ..., 0.0585, 0.0330, 0.0345],
        [0.0434, 0.0097, 0.0122,  ..., 0.0808, 0.0472, 0.0493],
        [0.0407, 0.0052, 0.0030,  ..., 0.1185, 0.0738, 0.0723],
        ...,
        [0.0455, 0.0117, 0.0154,  ..., 0.0797, 0.0513, 0.0450],
        [0.0445, 0.0100, 0.0057,  ..., 0.0946, 0.0625, 0.0537],
        [0.0417, 0.0051, 0.0000,  ..., 0.1423, 0.0994, 0.0808]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382180.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8483.7363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.0753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4187.7329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(702.8437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0823],
        [ 0.0927],
        [ 0.1062],
        ...,
        [-0.2999],
        [-0.1848],
        [-0.1274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115283.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0014],
        [1.0036],
        ...,
        [1.0019],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366985.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(502.0964, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0015],
        [1.0037],
        ...,
        [1.0020],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366998.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.7658e-05, -6.9203e-04,  ..., -5.5169e-04,
         -2.4059e-06, -1.8754e-03],
        [ 0.0000e+00, -1.7658e-05, -6.9203e-04,  ..., -5.5169e-04,
         -2.4059e-06, -1.8754e-03],
        [ 0.0000e+00, -1.7658e-05, -6.9203e-04,  ..., -5.5169e-04,
         -2.4059e-06, -1.8754e-03],
        ...,
        [ 0.0000e+00, -1.7658e-05, -6.9203e-04,  ..., -5.5169e-04,
         -2.4059e-06, -1.8754e-03],
        [ 0.0000e+00, -1.7658e-05, -6.9203e-04,  ..., -5.5169e-04,
         -2.4059e-06, -1.8754e-03],
        [ 0.0000e+00, -1.7658e-05, -6.9203e-04,  ..., -5.5169e-04,
         -2.4059e-06, -1.8754e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-117.1006, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5393, device='cuda:0')



h[100].sum tensor(-3.8622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7088, device='cuda:0')



h[200].sum tensor(-22.5950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42824.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0477, 0.0162, 0.0423,  ..., 0.0316, 0.0139, 0.0181],
        [0.0480, 0.0167, 0.0455,  ..., 0.0277, 0.0113, 0.0157],
        [0.0483, 0.0168, 0.0458,  ..., 0.0276, 0.0113, 0.0155],
        ...,
        [0.0487, 0.0170, 0.0462,  ..., 0.0279, 0.0114, 0.0156],
        [0.0487, 0.0170, 0.0463,  ..., 0.0279, 0.0114, 0.0156],
        [0.0487, 0.0170, 0.0463,  ..., 0.0279, 0.0114, 0.0156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368364.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8603.2803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.2669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4325.2451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(662.1819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4957],
        [-0.6401],
        [-0.7320],
        ...,
        [-0.6449],
        [-0.6431],
        [-0.6426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141828.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0015],
        [1.0037],
        ...,
        [1.0020],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366998.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0015],
        [1.0038],
        ...,
        [1.0020],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367010.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.8472e-05, -6.9019e-04,  ..., -5.4600e-04,
          2.8811e-06, -1.8645e-03],
        [ 0.0000e+00, -1.8472e-05, -6.9019e-04,  ..., -5.4600e-04,
          2.8811e-06, -1.8645e-03],
        [ 0.0000e+00, -1.8472e-05, -6.9019e-04,  ..., -5.4600e-04,
          2.8811e-06, -1.8645e-03],
        ...,
        [ 0.0000e+00, -1.8472e-05, -6.9019e-04,  ..., -5.4600e-04,
          2.8811e-06, -1.8645e-03],
        [ 0.0000e+00, -1.8472e-05, -6.9019e-04,  ..., -5.4600e-04,
          2.8811e-06, -1.8645e-03],
        [ 0.0000e+00, -1.8472e-05, -6.9019e-04,  ..., -5.4600e-04,
          2.8811e-06, -1.8645e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-172.6039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7639, device='cuda:0')



h[100].sum tensor(-3.2915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3032, device='cuda:0')



h[200].sum tensor(-22.6161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.3243e-03, 2.2775e-03,  ..., 6.1153e-04, 8.3663e-03,
         5.3159e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1533e-05,
         0.0000e+00],
        [0.0000e+00, 1.6679e-02, 5.2476e-03,  ..., 1.7693e-03, 1.6733e-02,
         1.2502e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1664e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1666e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1667e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38850.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0457, 0.0133, 0.0271,  ..., 0.0506, 0.0256, 0.0308],
        [0.0466, 0.0143, 0.0306,  ..., 0.0460, 0.0239, 0.0269],
        [0.0454, 0.0122, 0.0190,  ..., 0.0683, 0.0411, 0.0395],
        ...,
        [0.0487, 0.0169, 0.0463,  ..., 0.0280, 0.0114, 0.0157],
        [0.0487, 0.0169, 0.0463,  ..., 0.0280, 0.0114, 0.0157],
        [0.0487, 0.0169, 0.0463,  ..., 0.0280, 0.0114, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(348175.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8599.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.6247, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4318.7480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(628.0414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5428],
        [-0.5508],
        [-0.5034],
        ...,
        [-0.6478],
        [-0.6458],
        [-0.6452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135395.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0015],
        [1.0038],
        ...,
        [1.0020],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367010.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0039],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367023.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.0885e-05, -6.9167e-04,  ..., -5.4219e-04,
          1.3222e-05, -1.8543e-03],
        [ 0.0000e+00, -1.0885e-05, -6.9167e-04,  ..., -5.4219e-04,
          1.3222e-05, -1.8543e-03],
        [ 0.0000e+00, -1.0885e-05, -6.9167e-04,  ..., -5.4219e-04,
          1.3222e-05, -1.8543e-03],
        ...,
        [ 0.0000e+00, -1.0885e-05, -6.9167e-04,  ..., -5.4219e-04,
          1.3222e-05, -1.8543e-03],
        [ 0.0000e+00, -1.0885e-05, -6.9167e-04,  ..., -5.4219e-04,
          1.3222e-05, -1.8543e-03],
        [ 0.0000e+00, -1.0885e-05, -6.9167e-04,  ..., -5.4219e-04,
          1.3222e-05, -1.8543e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(17.0147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5061, device='cuda:0')



h[100].sum tensor(-4.4715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1422, device='cuda:0')



h[200].sum tensor(-22.5703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.2892e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.2931e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.2994e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.3539e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.3546e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.3551e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44733.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0478, 0.0163, 0.0443,  ..., 0.0296, 0.0122, 0.0170],
        [0.0476, 0.0161, 0.0435,  ..., 0.0308, 0.0125, 0.0182],
        [0.0468, 0.0150, 0.0376,  ..., 0.0387, 0.0166, 0.0241],
        ...,
        [0.0487, 0.0168, 0.0462,  ..., 0.0283, 0.0115, 0.0160],
        [0.0487, 0.0168, 0.0462,  ..., 0.0283, 0.0115, 0.0160],
        [0.0487, 0.0168, 0.0462,  ..., 0.0283, 0.0115, 0.0160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367076.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8420.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.9455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4005.8130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(686.5234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4863],
        [-0.3710],
        [-0.2117],
        ...,
        [-0.6494],
        [-0.6477],
        [-0.6474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104950.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0039],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367023.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0040],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367036.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        ...,
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-90.0777, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4406, device='cuda:0')



h[100].sum tensor(-3.5852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5482, device='cuda:0')



h[200].sum tensor(-22.6036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0723, 0.0237,  ..., 0.0084, 0.0724, 0.0567],
        [0.0000, 0.0117, 0.0035,  ..., 0.0011, 0.0117, 0.0083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39796.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0371, 0.0017, 0.0000,  ..., 0.2174, 0.1584, 0.1219],
        [0.0409, 0.0057, 0.0012,  ..., 0.1445, 0.1003, 0.0821],
        [0.0432, 0.0091, 0.0074,  ..., 0.1119, 0.0756, 0.0634],
        ...,
        [0.0489, 0.0175, 0.0467,  ..., 0.0281, 0.0115, 0.0155],
        [0.0489, 0.0175, 0.0467,  ..., 0.0281, 0.0115, 0.0155],
        [0.0489, 0.0175, 0.0467,  ..., 0.0281, 0.0115, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347799.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8552.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.4804, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4179.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(636.2812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0857],
        [ 0.0849],
        [ 0.0820],
        ...,
        [-0.6603],
        [-0.6585],
        [-0.6581]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128657.1328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0040],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367036.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0040],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367036.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        ...,
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03],
        [ 0.0000e+00, -1.4652e-05, -6.8390e-04,  ..., -5.3273e-04,
         -1.0399e-05, -1.8335e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-84.6787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6562, device='cuda:0')



h[100].sum tensor(-3.6233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5797, device='cuda:0')



h[200].sum tensor(-22.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41237.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0473, 0.0161, 0.0385,  ..., 0.0369, 0.0171, 0.0215],
        [0.0480, 0.0170, 0.0443,  ..., 0.0299, 0.0127, 0.0168],
        [0.0484, 0.0174, 0.0462,  ..., 0.0278, 0.0113, 0.0154],
        ...,
        [0.0489, 0.0175, 0.0467,  ..., 0.0281, 0.0115, 0.0155],
        [0.0489, 0.0175, 0.0467,  ..., 0.0281, 0.0115, 0.0155],
        [0.0489, 0.0175, 0.0467,  ..., 0.0281, 0.0115, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358684.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8527.1699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.8682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4070.4563, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(649.3800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4509],
        [-0.6118],
        [-0.7209],
        ...,
        [-0.6602],
        [-0.6585],
        [-0.6581]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122665.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0015],
        [1.0040],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367036.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0016],
        [1.0040],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367049.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.7535e-05, -6.7797e-04,  ..., -5.2570e-04,
         -3.2286e-05, -1.8073e-03],
        [ 0.0000e+00, -1.7535e-05, -6.7797e-04,  ..., -5.2570e-04,
         -3.2286e-05, -1.8073e-03],
        [ 0.0000e+00, -1.7535e-05, -6.7797e-04,  ..., -5.2570e-04,
         -3.2286e-05, -1.8073e-03],
        ...,
        [ 0.0000e+00, -1.7535e-05, -6.7797e-04,  ..., -5.2570e-04,
         -3.2286e-05, -1.8073e-03],
        [ 0.0000e+00, -1.7535e-05, -6.7797e-04,  ..., -5.2570e-04,
         -3.2286e-05, -1.8073e-03],
        [ 0.0000e+00, -1.7535e-05, -6.7797e-04,  ..., -5.2570e-04,
         -3.2286e-05, -1.8073e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-61.3012, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2300, device='cuda:0')



h[100].sum tensor(-3.7401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6636, device='cuda:0')



h[200].sum tensor(-22.5970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40676.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0462, 0.0143, 0.0233,  ..., 0.0561, 0.0302, 0.0330],
        [0.0469, 0.0153, 0.0299,  ..., 0.0482, 0.0248, 0.0282],
        [0.0477, 0.0162, 0.0367,  ..., 0.0404, 0.0191, 0.0235],
        ...,
        [0.0494, 0.0181, 0.0473,  ..., 0.0281, 0.0114, 0.0152],
        [0.0494, 0.0181, 0.0474,  ..., 0.0281, 0.0114, 0.0152],
        [0.0495, 0.0181, 0.0474,  ..., 0.0281, 0.0114, 0.0152]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(352391.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8624.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8026, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.4023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4103.0806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(645.5004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0211],
        [ 0.0071],
        [-0.0157],
        ...,
        [-0.6709],
        [-0.6696],
        [-0.6697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126476.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0016],
        [1.0040],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367049.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0016],
        [1.0041],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367062.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.0677e-06, -6.7617e-04,  ..., -5.2018e-04,
         -4.4446e-05, -1.7820e-03],
        [ 0.0000e+00, -7.0677e-06, -6.7617e-04,  ..., -5.2018e-04,
         -4.4446e-05, -1.7820e-03],
        [ 0.0000e+00, -7.0677e-06, -6.7617e-04,  ..., -5.2018e-04,
         -4.4446e-05, -1.7820e-03],
        ...,
        [ 0.0000e+00, -7.0677e-06, -6.7617e-04,  ..., -5.2018e-04,
         -4.4446e-05, -1.7820e-03],
        [ 0.0000e+00, -7.0677e-06, -6.7617e-04,  ..., -5.2018e-04,
         -4.4446e-05, -1.7820e-03],
        [ 0.0000e+00, -7.0677e-06, -6.7617e-04,  ..., -5.2018e-04,
         -4.4446e-05, -1.7820e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-64.3679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7669, device='cuda:0')



h[100].sum tensor(-3.6332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5959, device='cuda:0')



h[200].sum tensor(-22.6004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0252, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40022.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0490, 0.0182, 0.0471,  ..., 0.0278, 0.0113, 0.0149],
        [0.0490, 0.0182, 0.0471,  ..., 0.0278, 0.0113, 0.0149],
        [0.0492, 0.0183, 0.0472,  ..., 0.0279, 0.0114, 0.0150],
        ...,
        [0.0497, 0.0185, 0.0477,  ..., 0.0282, 0.0115, 0.0151],
        [0.0497, 0.0185, 0.0477,  ..., 0.0282, 0.0115, 0.0151],
        [0.0497, 0.0185, 0.0477,  ..., 0.0282, 0.0115, 0.0151]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(352512., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8776.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(95.3643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4294.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(632.0594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6597],
        [-0.6851],
        [-0.6994],
        ...,
        [-0.6819],
        [-0.6800],
        [-0.6795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155631.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0016],
        [1.0041],
        ...,
        [1.0020],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367062.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0017],
        [1.0042],
        ...,
        [1.0021],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367076.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.2903e-05, -6.8045e-04,  ..., -5.1599e-04,
         -4.7676e-05, -1.7645e-03],
        [ 0.0000e+00,  1.2903e-05, -6.8045e-04,  ..., -5.1599e-04,
         -4.7676e-05, -1.7645e-03],
        [ 0.0000e+00,  1.2903e-05, -6.8045e-04,  ..., -5.1599e-04,
         -4.7676e-05, -1.7645e-03],
        ...,
        [ 0.0000e+00,  1.2903e-05, -6.8045e-04,  ..., -5.1599e-04,
         -4.7676e-05, -1.7645e-03],
        [ 0.0000e+00,  1.2903e-05, -6.8045e-04,  ..., -5.1599e-04,
         -4.7676e-05, -1.7645e-03],
        [ 0.0000e+00,  1.2903e-05, -6.8045e-04,  ..., -5.1599e-04,
         -4.7676e-05, -1.7645e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-56.9365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7331, device='cuda:0')



h[100].sum tensor(-3.6126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5910, device='cuda:0')



h[200].sum tensor(-22.6006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.1621e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1660e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1726e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.2278e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.2285e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.2290e-05, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40875.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0488, 0.0178, 0.0435,  ..., 0.0323, 0.0145, 0.0177],
        [0.0478, 0.0164, 0.0314,  ..., 0.0467, 0.0252, 0.0261],
        [0.0467, 0.0147, 0.0172,  ..., 0.0640, 0.0378, 0.0364],
        ...,
        [0.0499, 0.0186, 0.0478,  ..., 0.0285, 0.0116, 0.0153],
        [0.0499, 0.0186, 0.0479,  ..., 0.0285, 0.0116, 0.0153],
        [0.0499, 0.0186, 0.0479,  ..., 0.0285, 0.0116, 0.0153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358883.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8699.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8198, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.4794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4066.7686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(643.1710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0680],
        [-0.0067],
        [ 0.0368],
        ...,
        [-0.6886],
        [-0.6866],
        [-0.6861]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135393.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0017],
        [1.0042],
        ...,
        [1.0021],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367076.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0017],
        [1.0043],
        ...,
        [1.0021],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367090.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  4.9859e-05, -6.8719e-04,  ..., -5.1151e-04,
         -4.2105e-05, -1.7537e-03],
        [ 0.0000e+00,  4.9859e-05, -6.8719e-04,  ..., -5.1151e-04,
         -4.2105e-05, -1.7537e-03],
        [ 0.0000e+00,  4.9859e-05, -6.8719e-04,  ..., -5.1151e-04,
         -4.2105e-05, -1.7537e-03],
        ...,
        [ 0.0000e+00,  4.9859e-05, -6.8719e-04,  ..., -5.1151e-04,
         -4.2105e-05, -1.7537e-03],
        [ 0.0000e+00,  4.9859e-05, -6.8719e-04,  ..., -5.1151e-04,
         -4.2105e-05, -1.7537e-03],
        [ 0.0000e+00,  4.9859e-05, -6.8719e-04,  ..., -5.1151e-04,
         -4.2105e-05, -1.7537e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-46.5873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9141, device='cuda:0')



h[100].sum tensor(-3.5840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6174, device='cuda:0')



h[200].sum tensor(-22.6010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40531.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0483, 0.0178, 0.0448,  ..., 0.0315, 0.0125, 0.0184],
        [0.0484, 0.0179, 0.0450,  ..., 0.0313, 0.0124, 0.0181],
        [0.0485, 0.0178, 0.0433,  ..., 0.0335, 0.0144, 0.0192],
        ...,
        [0.0492, 0.0179, 0.0423,  ..., 0.0351, 0.0164, 0.0194],
        [0.0482, 0.0165, 0.0314,  ..., 0.0479, 0.0256, 0.0272],
        [0.0477, 0.0158, 0.0260,  ..., 0.0543, 0.0303, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(354236.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8699.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.6015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4093.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(635.3707, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4956],
        [-0.4366],
        [-0.3360],
        ...,
        [-0.5068],
        [-0.3890],
        [-0.3138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144878.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0017],
        [1.0043],
        ...,
        [1.0021],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367090.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0044],
        ...,
        [1.0021],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367103.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.2849e-05, -6.9516e-04,  ..., -5.0879e-04,
         -3.3994e-05, -1.7414e-03],
        [ 0.0000e+00,  8.2849e-05, -6.9516e-04,  ..., -5.0879e-04,
         -3.3994e-05, -1.7414e-03],
        [-6.2023e-03,  2.0968e-02,  6.7280e-03,  ...,  2.3877e-03,
          2.0869e-02,  1.6240e-02],
        ...,
        [ 0.0000e+00,  8.2849e-05, -6.9516e-04,  ..., -5.0879e-04,
         -3.3994e-05, -1.7414e-03],
        [ 0.0000e+00,  8.2849e-05, -6.9516e-04,  ..., -5.0879e-04,
         -3.3994e-05, -1.7414e-03],
        [ 0.0000e+00,  8.2849e-05, -6.9516e-04,  ..., -5.0879e-04,
         -3.3994e-05, -1.7414e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(358.8562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1767, device='cuda:0')



h[100].sum tensor(-6.2480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5552, device='cuda:0')



h[200].sum tensor(-22.4950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0383, 0.0121,  ..., 0.0042, 0.0379, 0.0292],
        [0.0000, 0.0384, 0.0121,  ..., 0.0043, 0.0380, 0.0293],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57500.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.4636e-02, 1.2263e-02, 1.9845e-03,  ..., 8.1703e-02, 4.9928e-02,
         4.8328e-02],
        [4.2145e-02, 8.0738e-03, 1.1605e-04,  ..., 1.3023e-01, 8.9424e-02,
         7.4648e-02],
        [4.0927e-02, 5.8734e-03, 0.0000e+00,  ..., 1.5702e-01, 1.1111e-01,
         8.9259e-02],
        ...,
        [4.9733e-02, 1.8482e-02, 4.7746e-02,  ..., 2.8993e-02, 1.1995e-02,
         1.5840e-02],
        [4.9742e-02, 1.8486e-02, 4.7754e-02,  ..., 2.8999e-02, 1.1998e-02,
         1.5843e-02],
        [4.9748e-02, 1.8488e-02, 4.7759e-02,  ..., 2.9003e-02, 1.1999e-02,
         1.5846e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433722.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8524.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.5097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3749.7097, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(784.0526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1197],
        [ 0.1171],
        [ 0.1159],
        ...,
        [-0.6937],
        [-0.6918],
        [-0.6912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121063.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0044],
        ...,
        [1.0021],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367103.7812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(505.6121, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0045],
        ...,
        [1.0021],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367117.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0645e-03,  7.0718e-03,  1.7749e-03,  ...,  4.5986e-04,
          6.9485e-03,  4.2760e-03],
        [-9.0614e-03,  3.0691e-02,  1.0168e-02,  ...,  3.7349e-03,
          3.0587e-02,  2.4612e-02],
        [-1.4062e-02,  4.7569e-02,  1.6167e-02,  ...,  6.0753e-03,
          4.7479e-02,  3.9145e-02],
        ...,
        [ 0.0000e+00,  1.0300e-04, -7.0159e-04,  ..., -5.0643e-04,
         -2.5880e-05, -1.7242e-03],
        [ 0.0000e+00,  1.0300e-04, -7.0159e-04,  ..., -5.0643e-04,
         -2.5880e-05, -1.7242e-03],
        [ 0.0000e+00,  1.0300e-04, -7.0159e-04,  ..., -5.0643e-04,
         -2.5880e-05, -1.7242e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(143.4190, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9528, device='cuda:0')



h[100].sum tensor(-4.7662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4997, device='cuda:0')



h[200].sum tensor(-22.5526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1006, 0.0328,  ..., 0.0119, 0.1002, 0.0794],
        [0.0000, 0.1127, 0.0371,  ..., 0.0135, 0.1122, 0.0897],
        [0.0000, 0.1495, 0.0502,  ..., 0.0186, 0.1491, 0.1215],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50586.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0312, 0.0000, 0.0000,  ..., 0.3511, 0.2686, 0.1951],
        [0.0304, 0.0000, 0.0000,  ..., 0.3779, 0.2913, 0.2088],
        [0.0291, 0.0000, 0.0000,  ..., 0.4213, 0.3288, 0.2306],
        ...,
        [0.0500, 0.0182, 0.0480,  ..., 0.0294, 0.0122, 0.0162],
        [0.0500, 0.0183, 0.0480,  ..., 0.0294, 0.0122, 0.0162],
        [0.0501, 0.0183, 0.0480,  ..., 0.0294, 0.0122, 0.0162]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407873.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8624.1475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.7905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3829.1714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(724.0035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1200],
        [ 0.1180],
        [ 0.1128],
        ...,
        [-0.7010],
        [-0.6991],
        [-0.6987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124845.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0018],
        [1.0045],
        ...,
        [1.0021],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367117.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0019],
        [1.0046],
        ...,
        [1.0021],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367130.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1830e-03,  1.7639e-02,  5.5282e-03,  ...,  1.9279e-03,
          1.7533e-02,  1.3399e-02],
        [ 0.0000e+00,  1.0134e-04, -7.0417e-04,  ..., -5.0392e-04,
         -1.8659e-05, -1.7021e-03],
        [-1.4198e-02,  4.8143e-02,  1.6369e-02,  ...,  6.1578e-03,
          4.8062e-02,  3.9667e-02],
        ...,
        [ 0.0000e+00,  1.0134e-04, -7.0417e-04,  ..., -5.0392e-04,
         -1.8659e-05, -1.7021e-03],
        [ 0.0000e+00,  1.0134e-04, -7.0417e-04,  ..., -5.0392e-04,
         -1.8659e-05, -1.7021e-03],
        [ 0.0000e+00,  1.0134e-04, -7.0417e-04,  ..., -5.0392e-04,
         -1.8659e-05, -1.7021e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(322.9276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8832, device='cuda:0')



h[100].sum tensor(-5.9684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.3662, device='cuda:0')



h[200].sum tensor(-22.5037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0423, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0147, 0.0044,  ..., 0.0015, 0.0143, 0.0106],
        [0.0000, 0.1060, 0.0347,  ..., 0.0126, 0.1056, 0.0841],
        [0.0000, 0.0989, 0.0329,  ..., 0.0121, 0.0985, 0.0797],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54564.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0436, 0.0082, 0.0077,  ..., 0.1245, 0.0843, 0.0718],
        [0.0375, 0.0020, 0.0000,  ..., 0.2453, 0.1819, 0.1379],
        [0.0352, 0.0005, 0.0000,  ..., 0.3021, 0.2290, 0.1682],
        ...,
        [0.0506, 0.0180, 0.0484,  ..., 0.0298, 0.0121, 0.0165],
        [0.0499, 0.0170, 0.0425,  ..., 0.0369, 0.0166, 0.0213],
        [0.0489, 0.0155, 0.0338,  ..., 0.0473, 0.0232, 0.0282]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411248.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8782.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.6751, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3984.2754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(756.8923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0448],
        [ 0.1032],
        [ 0.1112],
        ...,
        [-0.6482],
        [-0.5543],
        [-0.4193]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136736.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0019],
        [1.0046],
        ...,
        [1.0021],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367130.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0046],
        ...,
        [1.0022],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367144.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.4406e-05, -7.0445e-04,  ..., -4.9868e-04,
         -1.3449e-05, -1.6812e-03],
        [ 0.0000e+00,  8.4406e-05, -7.0445e-04,  ..., -4.9868e-04,
         -1.3449e-05, -1.6812e-03],
        [-5.0222e-03,  1.7118e-02,  5.3493e-03,  ...,  1.8638e-03,
          1.7035e-02,  1.2989e-02],
        ...,
        [ 0.0000e+00,  8.4406e-05, -7.0445e-04,  ..., -4.9868e-04,
         -1.3449e-05, -1.6812e-03],
        [ 0.0000e+00,  8.4406e-05, -7.0445e-04,  ..., -4.9868e-04,
         -1.3449e-05, -1.6812e-03],
        [ 0.0000e+00,  8.4406e-05, -7.0445e-04,  ..., -4.9868e-04,
         -1.3449e-05, -1.6812e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-170.8409, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9988, device='cuda:0')



h[100].sum tensor(-2.7714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0453, device='cuda:0')



h[200].sum tensor(-22.6308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0236, 0.0069,  ..., 0.0022, 0.0233, 0.0167],
        [0.0000, 0.0219, 0.0056,  ..., 0.0015, 0.0216, 0.0136],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36148.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0486, 0.0151, 0.0320,  ..., 0.0489, 0.0240, 0.0291],
        [0.0450, 0.0099, 0.0122,  ..., 0.0893, 0.0509, 0.0555],
        [0.0431, 0.0071, 0.0037,  ..., 0.1064, 0.0602, 0.0680],
        ...,
        [0.0513, 0.0181, 0.0490,  ..., 0.0301, 0.0118, 0.0166],
        [0.0513, 0.0181, 0.0490,  ..., 0.0302, 0.0118, 0.0166],
        [0.0513, 0.0181, 0.0490,  ..., 0.0302, 0.0118, 0.0166]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341876.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9062.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.6982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4304.3174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(596.6686, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2686],
        [-0.0719],
        [ 0.0679],
        ...,
        [-0.7277],
        [-0.7257],
        [-0.7252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158542.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0046],
        ...,
        [1.0022],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367144.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0047],
        ...,
        [1.0022],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367157.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.0406e-03,  3.0816e-02,  1.0218e-02,  ...,  3.7697e-03,
          3.0759e-02,  2.4809e-02],
        [-9.1058e-03,  3.1037e-02,  1.0297e-02,  ...,  3.8004e-03,
          3.0981e-02,  2.5000e-02],
        [ 0.0000e+00,  7.9273e-05, -7.0544e-04,  ..., -4.9377e-04,
         -3.2256e-06, -1.6641e-03],
        ...,
        [ 0.0000e+00,  7.9273e-05, -7.0544e-04,  ..., -4.9377e-04,
         -3.2256e-06, -1.6641e-03],
        [ 0.0000e+00,  7.9273e-05, -7.0544e-04,  ..., -4.9377e-04,
         -3.2256e-06, -1.6641e-03],
        [ 0.0000e+00,  7.9273e-05, -7.0544e-04,  ..., -4.9377e-04,
         -3.2256e-06, -1.6641e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93.6805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9818, device='cuda:0')



h[100].sum tensor(-4.5076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3578, device='cuda:0')



h[200].sum tensor(-22.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1264, 0.0420,  ..., 0.0155, 0.1262, 0.1020],
        [0.0000, 0.0911, 0.0302,  ..., 0.0111, 0.0909, 0.0732],
        [0.0000, 0.0839, 0.0269,  ..., 0.0096, 0.0837, 0.0654],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48187.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0360, 0.0000, 0.0000,  ..., 0.3070, 0.2309, 0.1721],
        [0.0355, 0.0000, 0.0000,  ..., 0.3089, 0.2309, 0.1742],
        [0.0354, 0.0000, 0.0000,  ..., 0.2968, 0.2180, 0.1698],
        ...,
        [0.0518, 0.0182, 0.0495,  ..., 0.0304, 0.0117, 0.0167],
        [0.0518, 0.0182, 0.0495,  ..., 0.0304, 0.0117, 0.0167],
        [0.0518, 0.0182, 0.0495,  ..., 0.0304, 0.0117, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(399548.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9059.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.1407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4141.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(705.5352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1007],
        [ 0.1023],
        [ 0.1040],
        ...,
        [-0.7391],
        [-0.7371],
        [-0.7366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144659.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0019],
        [1.0047],
        ...,
        [1.0022],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367157.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0020],
        [1.0048],
        ...,
        [1.0022],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367170.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9463e-03,  6.7186e-03,  1.6550e-03,  ...,  4.3312e-04,
          6.6501e-03,  4.0602e-03],
        [-2.6600e-03,  9.1509e-03,  2.5195e-03,  ...,  7.7057e-04,
          9.0844e-03,  6.1552e-03],
        [ 0.0000e+00,  8.5629e-05, -7.0241e-04,  ..., -4.8714e-04,
          1.1587e-05, -1.6530e-03],
        ...,
        [ 0.0000e+00,  8.5629e-05, -7.0241e-04,  ..., -4.8714e-04,
          1.1587e-05, -1.6530e-03],
        [ 0.0000e+00,  8.5629e-05, -7.0241e-04,  ..., -4.8714e-04,
          1.1587e-05, -1.6530e-03],
        [ 0.0000e+00,  8.5629e-05, -7.0241e-04,  ..., -4.8714e-04,
          1.1587e-05, -1.6530e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-111.8448, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7613, device='cuda:0')



h[100].sum tensor(-3.1170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3029, device='cuda:0')



h[200].sum tensor(-22.6157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.9756e-02, 1.8306e-02,  ..., 6.2937e-03, 5.9510e-02,
         4.4560e-02],
        [0.0000e+00, 1.9811e-02, 5.5130e-03,  ..., 1.7258e-03, 1.9530e-02,
         1.3459e-02],
        [0.0000e+00, 9.4264e-03, 2.5244e-03,  ..., 7.7208e-04, 9.1371e-03,
         6.1673e-03],
        ...,
        [0.0000e+00, 3.4731e-04, 0.0000e+00,  ..., 0.0000e+00, 4.6996e-05,
         0.0000e+00],
        [0.0000e+00, 3.4735e-04, 0.0000e+00,  ..., 0.0000e+00, 4.7002e-05,
         0.0000e+00],
        [0.0000e+00, 3.4738e-04, 0.0000e+00,  ..., 0.0000e+00, 4.7006e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38673.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0400, 0.0024, 0.0000,  ..., 0.1850, 0.1246, 0.1106],
        [0.0444, 0.0079, 0.0077,  ..., 0.1155, 0.0712, 0.0702],
        [0.0481, 0.0132, 0.0211,  ..., 0.0698, 0.0382, 0.0422],
        ...,
        [0.0521, 0.0185, 0.0498,  ..., 0.0305, 0.0116, 0.0168],
        [0.0521, 0.0185, 0.0498,  ..., 0.0305, 0.0116, 0.0168],
        [0.0521, 0.0185, 0.0498,  ..., 0.0305, 0.0116, 0.0168]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355742.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9180.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.8571, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4282.6255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(621.7205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1075],
        [ 0.0592],
        [-0.0465],
        ...,
        [-0.7471],
        [-0.7451],
        [-0.7446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153409.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0020],
        [1.0048],
        ...,
        [1.0022],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367170.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0020],
        [1.0049],
        ...,
        [1.0022],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367183.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0299e-04, -6.9115e-04,  ..., -4.7456e-04,
          2.4933e-05, -1.6512e-03],
        [ 0.0000e+00,  1.0299e-04, -6.9115e-04,  ..., -4.7456e-04,
          2.4933e-05, -1.6512e-03],
        [ 0.0000e+00,  1.0299e-04, -6.9115e-04,  ..., -4.7456e-04,
          2.4933e-05, -1.6512e-03],
        ...,
        [ 0.0000e+00,  1.0299e-04, -6.9115e-04,  ..., -4.7456e-04,
          2.4933e-05, -1.6512e-03],
        [ 0.0000e+00,  1.0299e-04, -6.9115e-04,  ..., -4.7456e-04,
          2.4933e-05, -1.6512e-03],
        [ 0.0000e+00,  1.0299e-04, -6.9115e-04,  ..., -4.7456e-04,
          2.4933e-05, -1.6512e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-49.9183, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3731, device='cuda:0')



h[100].sum tensor(-3.3492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5384, device='cuda:0')



h[200].sum tensor(-22.6056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.1211e-04, 0.0000e+00,  ..., 0.0000e+00, 9.9773e-05,
         0.0000e+00],
        [0.0000e+00, 4.1242e-04, 0.0000e+00,  ..., 0.0000e+00, 9.9849e-05,
         0.0000e+00],
        [0.0000e+00, 1.1759e-02, 3.3399e-03,  ..., 1.0990e-03, 1.1455e-02,
         8.1171e-03],
        ...,
        [0.0000e+00, 4.1777e-04, 0.0000e+00,  ..., 0.0000e+00, 1.0114e-04,
         0.0000e+00],
        [0.0000e+00, 4.1782e-04, 0.0000e+00,  ..., 0.0000e+00, 1.0115e-04,
         0.0000e+00],
        [0.0000e+00, 4.1786e-04, 0.0000e+00,  ..., 0.0000e+00, 1.0116e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41126.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0493, 0.0162, 0.0306,  ..., 0.0513, 0.0259, 0.0298],
        [0.0482, 0.0147, 0.0189,  ..., 0.0655, 0.0360, 0.0384],
        [0.0462, 0.0117, 0.0078,  ..., 0.0929, 0.0552, 0.0555],
        ...,
        [0.0519, 0.0192, 0.0497,  ..., 0.0307, 0.0115, 0.0167],
        [0.0519, 0.0192, 0.0497,  ..., 0.0307, 0.0115, 0.0167],
        [0.0520, 0.0192, 0.0497,  ..., 0.0307, 0.0115, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372650.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9091.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.5755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4174.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(639.9856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0438],
        [ 0.0661],
        [ 0.0917],
        ...,
        [-0.7520],
        [-0.7500],
        [-0.7495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152375.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0020],
        [1.0049],
        ...,
        [1.0022],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367183.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0049],
        ...,
        [1.0023],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367197.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.1879e-04, -6.7989e-04,  ..., -4.6277e-04,
          4.1550e-05, -1.6522e-03],
        [ 0.0000e+00,  1.1879e-04, -6.7989e-04,  ..., -4.6277e-04,
          4.1550e-05, -1.6522e-03],
        [ 0.0000e+00,  1.1879e-04, -6.7989e-04,  ..., -4.6277e-04,
          4.1550e-05, -1.6522e-03],
        ...,
        [ 0.0000e+00,  1.1879e-04, -6.7989e-04,  ..., -4.6277e-04,
          4.1550e-05, -1.6522e-03],
        [ 0.0000e+00,  1.1879e-04, -6.7989e-04,  ..., -4.6277e-04,
          4.1550e-05, -1.6522e-03],
        [ 0.0000e+00,  1.1879e-04, -6.7989e-04,  ..., -4.6277e-04,
          4.1550e-05, -1.6522e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-31.1580, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8724, device='cuda:0')



h[100].sum tensor(-3.2991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4652, device='cuda:0')



h[200].sum tensor(-22.6069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40357.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0501, 0.0186, 0.0443,  ..., 0.0360, 0.0140, 0.0207],
        [0.0506, 0.0191, 0.0474,  ..., 0.0323, 0.0120, 0.0180],
        [0.0511, 0.0195, 0.0489,  ..., 0.0308, 0.0114, 0.0167],
        ...,
        [0.0518, 0.0198, 0.0497,  ..., 0.0309, 0.0115, 0.0165],
        [0.0518, 0.0198, 0.0497,  ..., 0.0309, 0.0115, 0.0165],
        [0.0518, 0.0198, 0.0497,  ..., 0.0309, 0.0115, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363701.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9022.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.7764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4132.7798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(631.7211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1739],
        [-0.3276],
        [-0.4900],
        ...,
        [-0.7493],
        [-0.7503],
        [-0.7512]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151667.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0049],
        ...,
        [1.0023],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367197.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0050],
        ...,
        [1.0023],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367210.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.4222e-03,  1.5311e-02,  4.7243e-03,  ...,  1.6546e-03,
          1.5248e-02,  1.1424e-02],
        [-4.9979e-03,  1.7287e-02,  5.4269e-03,  ...,  1.9291e-03,
          1.7226e-02,  1.3126e-02],
        [-5.1116e-03,  1.7678e-02,  5.5657e-03,  ...,  1.9833e-03,
          1.7617e-02,  1.3462e-02],
        ...,
        [ 0.0000e+00,  1.3113e-04, -6.7241e-04,  ..., -4.5407e-04,
          5.5912e-05, -1.6514e-03],
        [ 0.0000e+00,  1.3113e-04, -6.7241e-04,  ..., -4.5407e-04,
          5.5912e-05, -1.6514e-03],
        [ 0.0000e+00,  1.3113e-04, -6.7241e-04,  ..., -4.5407e-04,
          5.5912e-05, -1.6514e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6160, device='cuda:0')



h[100].sum tensor(-3.3957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5739, device='cuda:0')



h[200].sum tensor(-22.6023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0561, 0.0171,  ..., 0.0059, 0.0558, 0.0412],
        [0.0000, 0.0652, 0.0203,  ..., 0.0072, 0.0650, 0.0491],
        [0.0000, 0.0823, 0.0264,  ..., 0.0095, 0.0821, 0.0638],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41598.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.3199e-02, 0.0000e+00, 0.0000e+00,  ..., 2.7614e-01, 1.9716e-01,
         1.5970e-01],
        [3.3984e-02, 9.5997e-06, 0.0000e+00,  ..., 2.5521e-01, 1.7864e-01,
         1.4931e-01],
        [3.5640e-02, 1.3505e-03, 0.0000e+00,  ..., 2.3059e-01, 1.5902e-01,
         1.3565e-01],
        ...,
        [5.1754e-02, 2.0253e-02, 4.9704e-02,  ..., 3.1109e-02, 1.1365e-02,
         1.6459e-02],
        [5.1762e-02, 2.0257e-02, 4.9711e-02,  ..., 3.1114e-02, 1.1367e-02,
         1.6462e-02],
        [5.1768e-02, 2.0260e-02, 4.9717e-02,  ..., 3.1119e-02, 1.1368e-02,
         1.6465e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372020.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8950.3496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.5212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4063.7666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(642.6824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0789],
        [ 0.0841],
        [ 0.0848],
        ...,
        [-0.7614],
        [-0.7594],
        [-0.7588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143886.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0050],
        ...,
        [1.0023],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367210.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367223.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.6773e-03,  1.6236e-02,  5.0552e-03,  ...,  1.7910e-03,
          1.6177e-02,  1.2211e-02],
        [-1.9723e-03,  6.9279e-03,  1.7460e-03,  ...,  4.9778e-04,
          6.8615e-03,  4.1940e-03],
        [-2.7050e-03,  9.4493e-03,  2.6425e-03,  ...,  8.4810e-04,
          9.3849e-03,  6.3658e-03],
        ...,
        [ 0.0000e+00,  1.4125e-04, -6.6675e-04,  ..., -4.4514e-04,
          6.9599e-05, -1.6517e-03],
        [ 0.0000e+00,  1.4125e-04, -6.6675e-04,  ..., -4.4514e-04,
          6.9599e-05, -1.6517e-03],
        [ 0.0000e+00,  1.4125e-04, -6.6675e-04,  ..., -4.4514e-04,
          6.9599e-05, -1.6517e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-28.7535, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8579, device='cuda:0')



h[100].sum tensor(-3.0575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3170, device='cuda:0')



h[200].sum tensor(-22.6155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0272, 0.0075,  ..., 0.0024, 0.0269, 0.0180],
        [0.0000, 0.0602, 0.0185,  ..., 0.0065, 0.0599, 0.0447],
        [0.0000, 0.0272, 0.0081,  ..., 0.0028, 0.0270, 0.0197],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38787.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0413, 0.0094, 0.0018,  ..., 0.1259, 0.0745, 0.0773],
        [0.0394, 0.0071, 0.0000,  ..., 0.1541, 0.0967, 0.0933],
        [0.0428, 0.0109, 0.0036,  ..., 0.1175, 0.0702, 0.0709],
        ...,
        [0.0517, 0.0207, 0.0498,  ..., 0.0315, 0.0112, 0.0164],
        [0.0517, 0.0207, 0.0498,  ..., 0.0315, 0.0112, 0.0164],
        [0.0517, 0.0207, 0.0498,  ..., 0.0315, 0.0112, 0.0164]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358700.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9024.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(98.9589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4293.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(613.0662, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0831],
        [ 0.0899],
        [ 0.0154],
        ...,
        [-0.7680],
        [-0.7661],
        [-0.7657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170284.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367223.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367223.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1551e-03,  7.5569e-03,  1.9697e-03,  ...,  5.8518e-04,
          7.4911e-03,  4.7358e-03],
        [-4.8714e-03,  1.6904e-02,  5.2927e-03,  ...,  1.8838e-03,
          1.6845e-02,  1.2787e-02],
        [ 0.0000e+00,  1.4125e-04, -6.6675e-04,  ..., -4.4514e-04,
          6.9599e-05, -1.6517e-03],
        ...,
        [ 0.0000e+00,  1.4125e-04, -6.6675e-04,  ..., -4.4514e-04,
          6.9599e-05, -1.6517e-03],
        [ 0.0000e+00,  1.4125e-04, -6.6675e-04,  ..., -4.4514e-04,
          6.9599e-05, -1.6517e-03],
        [ 0.0000e+00,  1.4125e-04, -6.6675e-04,  ..., -4.4514e-04,
          6.9599e-05, -1.6517e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(296.4709, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4925, device='cuda:0')



h[100].sum tensor(-5.0645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8708, device='cuda:0')



h[200].sum tensor(-22.5325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0375, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0588, 0.0180,  ..., 0.0063, 0.0585, 0.0435],
        [0.0000, 0.0271, 0.0074,  ..., 0.0023, 0.0268, 0.0179],
        [0.0000, 0.0234, 0.0068,  ..., 0.0023, 0.0232, 0.0164],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57437.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0394, 0.0070, 0.0000,  ..., 0.1542, 0.0966, 0.0934],
        [0.0415, 0.0096, 0.0022,  ..., 0.1245, 0.0737, 0.0763],
        [0.0447, 0.0131, 0.0118,  ..., 0.0971, 0.0559, 0.0583],
        ...,
        [0.0517, 0.0207, 0.0498,  ..., 0.0315, 0.0112, 0.0164],
        [0.0517, 0.0207, 0.0498,  ..., 0.0315, 0.0112, 0.0164],
        [0.0517, 0.0207, 0.0498,  ..., 0.0315, 0.0112, 0.0164]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(459793.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8772.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.2346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3792.6924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(781.0519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1021],
        [ 0.0626],
        [-0.0379],
        ...,
        [-0.7677],
        [-0.7657],
        [-0.7650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135002.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0020],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367223.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(468.8118, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367236.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.9184e-03,  3.4369e-02,  1.1497e-02,  ...,  4.3137e-03,
          3.4330e-02,  2.7818e-02],
        [-1.9824e-02,  6.8536e-02,  2.3644e-02,  ...,  9.0606e-03,
          6.8523e-02,  5.7248e-02],
        [-9.9774e-03,  3.4572e-02,  1.1569e-02,  ...,  4.3419e-03,
          3.4533e-02,  2.7993e-02],
        ...,
        [ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03],
        [ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03],
        [ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-31.0825, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4601, device='cuda:0')



h[100].sum tensor(-2.9500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2589, device='cuda:0')



h[200].sum tensor(-22.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1677, 0.0567,  ..., 0.0214, 0.1675, 0.1373],
        [0.0000, 0.1561, 0.0526,  ..., 0.0198, 0.1560, 0.1273],
        [0.0000, 0.1700, 0.0575,  ..., 0.0218, 0.1699, 0.1393],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38385.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.9616e-02, 1.0069e-04, 0.0000e+00,  ..., 3.8340e-01, 2.9410e-01,
         2.0963e-01],
        [2.8425e-02, 0.0000e+00, 0.0000e+00,  ..., 4.0562e-01, 3.1235e-01,
         2.2165e-01],
        [2.8412e-02, 0.0000e+00, 0.0000e+00,  ..., 4.0579e-01, 3.1180e-01,
         2.2216e-01],
        ...,
        [5.1794e-02, 2.0817e-02, 4.9832e-02,  ..., 3.1928e-02, 1.1167e-02,
         1.6575e-02],
        [5.1802e-02, 2.0820e-02, 4.9838e-02,  ..., 3.1933e-02, 1.1169e-02,
         1.6578e-02],
        [5.1808e-02, 2.0824e-02, 4.9844e-02,  ..., 3.1938e-02, 1.1171e-02,
         1.6581e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(357715.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8981.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.6544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4222.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(612.9284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0646],
        [ 0.0620],
        [ 0.0643],
        ...,
        [-0.7739],
        [-0.7720],
        [-0.7716]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161542.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367236.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367236.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03],
        [-2.4391e-03,  8.5692e-03,  2.3253e-03,  ...,  7.2932e-04,
          8.5108e-03,  5.5959e-03],
        [ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03],
        ...,
        [ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03],
        [ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03],
        [ 0.0000e+00,  1.5552e-04, -6.6574e-04,  ..., -4.3959e-04,
          9.0847e-05, -1.6511e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(9.9072, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7546, device='cuda:0')



h[100].sum tensor(-3.2005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4480, device='cuda:0')



h[200].sum tensor(-22.6089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0444, 0.0129,  ..., 0.0043, 0.0442, 0.0311],
        [0.0000, 0.0075, 0.0018,  ..., 0.0005, 0.0072, 0.0043],
        [0.0000, 0.0091, 0.0023,  ..., 0.0007, 0.0088, 0.0056],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40054.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0105, 0.0000,  ..., 0.1157, 0.0665, 0.0712],
        [0.0463, 0.0153, 0.0153,  ..., 0.0730, 0.0376, 0.0439],
        [0.0474, 0.0163, 0.0176,  ..., 0.0680, 0.0353, 0.0399],
        ...,
        [0.0518, 0.0208, 0.0498,  ..., 0.0319, 0.0112, 0.0166],
        [0.0518, 0.0208, 0.0498,  ..., 0.0319, 0.0112, 0.0166],
        [0.0518, 0.0208, 0.0498,  ..., 0.0319, 0.0112, 0.0166]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363841.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9001.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(99.9935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4234.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(625.0851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2954],
        [-0.4054],
        [-0.4562],
        ...,
        [-0.7739],
        [-0.7720],
        [-0.7716]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164793.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0050],
        ...,
        [1.0024],
        [1.0018],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367236.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0051],
        ...,
        [1.0024],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367249.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0001, -0.0017],
        [-0.0063,  0.0218,  0.0070,  ...,  0.0026,  0.0218,  0.0170],
        [-0.0094,  0.0326,  0.0108,  ...,  0.0041,  0.0325,  0.0262],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0001, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0001, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0001, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(118.0989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8992, device='cuda:0')



h[100].sum tensor(-3.7815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9074, device='cuda:0')



h[200].sum tensor(-22.5840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0417, 0.0126,  ..., 0.0044, 0.0415, 0.0304],
        [0.0000, 0.0594, 0.0188,  ..., 0.0068, 0.0592, 0.0455],
        [0.0000, 0.0904, 0.0292,  ..., 0.0107, 0.0902, 0.0706],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44401.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.4759e-02, 2.3013e-03, 0.0000e+00,  ..., 2.4080e-01, 1.7053e-01,
         1.3821e-01],
        [3.2666e-02, 1.3896e-03, 0.0000e+00,  ..., 2.7890e-01, 2.0244e-01,
         1.5863e-01],
        [2.9835e-02, 1.5026e-04, 0.0000e+00,  ..., 3.2622e-01, 2.4112e-01,
         1.8468e-01],
        ...,
        [5.1883e-02, 2.0622e-02, 4.9802e-02,  ..., 3.2453e-02, 1.1160e-02,
         1.7040e-02],
        [5.1890e-02, 2.0625e-02, 4.9808e-02,  ..., 3.2458e-02, 1.1162e-02,
         1.7043e-02],
        [5.1897e-02, 2.0628e-02, 4.9814e-02,  ..., 3.2463e-02, 1.1164e-02,
         1.7046e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382919.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8880.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.7945, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4051.7568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(668.7759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0049],
        [ 0.0107],
        [ 0.0157],
        ...,
        [-0.7772],
        [-0.7752],
        [-0.7743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143451.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0051],
        ...,
        [1.0024],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367249.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0051],
        ...,
        [1.0025],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367263.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [-0.0026,  0.0092,  0.0025,  ...,  0.0008,  0.0092,  0.0061],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33.6443, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0079, device='cuda:0')



h[100].sum tensor(-3.2027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4850, device='cuda:0')



h[200].sum tensor(-22.6075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0098, 0.0025,  ..., 0.0008, 0.0097, 0.0061],
        [0.0000, 0.0502, 0.0162,  ..., 0.0060, 0.0500, 0.0392],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41122.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0481, 0.0165, 0.0236,  ..., 0.0640, 0.0336, 0.0368],
        [0.0443, 0.0124, 0.0114,  ..., 0.1062, 0.0644, 0.0624],
        [0.0388, 0.0067, 0.0000,  ..., 0.1844, 0.1258, 0.1068],
        ...,
        [0.0520, 0.0202, 0.0498,  ..., 0.0330, 0.0112, 0.0176],
        [0.0520, 0.0202, 0.0498,  ..., 0.0330, 0.0112, 0.0176],
        [0.0520, 0.0202, 0.0498,  ..., 0.0330, 0.0112, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(371326.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8929.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8232, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.7217, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4131.3721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(638.6058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0299],
        [ 0.0443],
        [ 0.0799],
        ...,
        [-0.7822],
        [-0.7803],
        [-0.7798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147903.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0021],
        [1.0051],
        ...,
        [1.0025],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367263.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0021],
        [1.0052],
        ...,
        [1.0025],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367276.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-126.8371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0059, device='cuda:0')



h[100].sum tensor(-2.2709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7542, device='cuda:0')



h[200].sum tensor(-22.6462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0170, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0418, 0.0132,  ..., 0.0048, 0.0417, 0.0320],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35371.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0441, 0.0113, 0.0036,  ..., 0.1243, 0.0811, 0.0710],
        [0.0491, 0.0169, 0.0230,  ..., 0.0634, 0.0344, 0.0353],
        [0.0509, 0.0188, 0.0393,  ..., 0.0444, 0.0197, 0.0242],
        ...,
        [0.0525, 0.0201, 0.0503,  ..., 0.0336, 0.0113, 0.0179],
        [0.0525, 0.0201, 0.0503,  ..., 0.0336, 0.0113, 0.0179],
        [0.0525, 0.0201, 0.0503,  ..., 0.0336, 0.0113, 0.0179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349256.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9071.7432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.2577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.4954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4259.0811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(591.8314, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0710],
        [-0.3031],
        [-0.5485],
        ...,
        [-0.7950],
        [-0.7931],
        [-0.7926]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152613.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0021],
        [1.0052],
        ...,
        [1.0025],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367276.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0021],
        [1.0052],
        ...,
        [1.0025],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367288.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0051,  0.0179,  0.0056,  ...,  0.0020,  0.0178,  0.0136],
        [-0.0102,  0.0359,  0.0120,  ...,  0.0045,  0.0359,  0.0291],
        [-0.0148,  0.0519,  0.0177,  ...,  0.0067,  0.0519,  0.0429],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-98.1471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4390, device='cuda:0')



h[100].sum tensor(-2.5045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9636, device='cuda:0')



h[200].sum tensor(-22.6357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1164, 0.0384,  ..., 0.0143, 0.1164, 0.0932],
        [0.0000, 0.1652, 0.0557,  ..., 0.0211, 0.1652, 0.1352],
        [0.0000, 0.2439, 0.0836,  ..., 0.0320, 0.2440, 0.2030],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35909.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0271, 0.0000, 0.0000,  ..., 0.4156, 0.3178, 0.2300],
        [0.0210, 0.0000, 0.0000,  ..., 0.5715, 0.4546, 0.3076],
        [0.0150, 0.0000, 0.0000,  ..., 0.7354, 0.5985, 0.3887],
        ...,
        [0.0532, 0.0199, 0.0510,  ..., 0.0343, 0.0113, 0.0178],
        [0.0532, 0.0200, 0.0510,  ..., 0.0343, 0.0113, 0.0178],
        [0.0532, 0.0200, 0.0510,  ..., 0.0343, 0.0113, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(351151.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9298.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.3218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.7117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4474.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.4180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0641],
        [ 0.0349],
        [ 0.0060],
        ...,
        [-0.8135],
        [-0.8115],
        [-0.8110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179702.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0021],
        [1.0052],
        ...,
        [1.0025],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367288.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0053],
        ...,
        [1.0026],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367301.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(13.9218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4258, device='cuda:0')



h[100].sum tensor(-3.2054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5461, device='cuda:0')



h[200].sum tensor(-22.6053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0107, 0.0029,  ..., 0.0010, 0.0106, 0.0071],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0131, 0.0037,  ..., 0.0013, 0.0130, 0.0091],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41965.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0501, 0.0159, 0.0277,  ..., 0.0604, 0.0281, 0.0345],
        [0.0510, 0.0171, 0.0345,  ..., 0.0527, 0.0233, 0.0293],
        [0.0494, 0.0150, 0.0222,  ..., 0.0743, 0.0393, 0.0422],
        ...,
        [0.0539, 0.0197, 0.0517,  ..., 0.0349, 0.0113, 0.0176],
        [0.0539, 0.0197, 0.0517,  ..., 0.0349, 0.0113, 0.0176],
        [0.0539, 0.0198, 0.0518,  ..., 0.0350, 0.0113, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385101.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9343.1982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.5159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4375.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(659.1291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5404],
        [-0.4549],
        [-0.2885],
        ...,
        [-0.8304],
        [-0.8282],
        [-0.8276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173527.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0022],
        [1.0053],
        ...,
        [1.0026],
        [1.0019],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367301.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0053],
        ...,
        [1.0026],
        [1.0020],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367314.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0067,  0.0017,  ...,  0.0005,  0.0067,  0.0041],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(198.8313, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0812, device='cuda:0')



h[100].sum tensor(-4.3254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3724, device='cuda:0')



h[200].sum tensor(-22.5565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0236, 0.0068,  ..., 0.0024, 0.0236, 0.0167],
        [0.0000, 0.0426, 0.0129,  ..., 0.0046, 0.0426, 0.0315],
        [0.0000, 0.0707, 0.0229,  ..., 0.0085, 0.0708, 0.0558],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49730.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0410, 0.0035, 0.0000,  ..., 0.1888, 0.1253, 0.1098],
        [0.0383, 0.0009, 0.0000,  ..., 0.2387, 0.1666, 0.1369],
        [0.0360, 0.0008, 0.0000,  ..., 0.2940, 0.2142, 0.1657],
        ...,
        [0.0547, 0.0194, 0.0526,  ..., 0.0355, 0.0115, 0.0176],
        [0.0547, 0.0194, 0.0526,  ..., 0.0355, 0.0115, 0.0176],
        [0.0547, 0.0194, 0.0527,  ..., 0.0355, 0.0115, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419800.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9449.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6703, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.7168, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4407.5405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(733.3716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0959],
        [ 0.0952],
        [ 0.0941],
        ...,
        [-0.8443],
        [-0.8420],
        [-0.8414]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173478.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0053],
        ...,
        [1.0026],
        [1.0020],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367314.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0054],
        ...,
        [1.0027],
        [1.0020],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367326.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0001, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-90.5711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1191, device='cuda:0')



h[100].sum tensor(-2.6038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0629, device='cuda:0')



h[200].sum tensor(-22.6298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0024,  ..., 0.0006, 0.0111, 0.0059],
        [0.0000, 0.0058, 0.0012,  ..., 0.0003, 0.0058, 0.0030],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38125.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0490, 0.0136, 0.0274,  ..., 0.0653, 0.0253, 0.0407],
        [0.0505, 0.0152, 0.0346,  ..., 0.0565, 0.0213, 0.0338],
        [0.0524, 0.0171, 0.0432,  ..., 0.0464, 0.0166, 0.0258],
        ...,
        [0.0548, 0.0192, 0.0529,  ..., 0.0361, 0.0115, 0.0178],
        [0.0548, 0.0192, 0.0529,  ..., 0.0361, 0.0115, 0.0178],
        [0.0549, 0.0192, 0.0529,  ..., 0.0361, 0.0115, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370375.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9525.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5292, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.5798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4521.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(635.3430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7000],
        [-0.7425],
        [-0.8014],
        ...,
        [-0.8543],
        [-0.8520],
        [-0.8514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171333.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0022],
        [1.0054],
        ...,
        [1.0027],
        [1.0020],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367326.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0022],
        [1.0055],
        ...,
        [1.0027],
        [1.0021],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367339.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024,  0.0085,  0.0023,  ...,  0.0008,  0.0085,  0.0056],
        [-0.0023,  0.0083,  0.0022,  ...,  0.0007,  0.0083,  0.0054],
        [-0.0047,  0.0166,  0.0051,  ...,  0.0019,  0.0166,  0.0126],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0004,  0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78.4747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6664, device='cuda:0')



h[100].sum tensor(-3.4244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7273, device='cuda:0')



h[200].sum tensor(-22.5936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0277, 0.0083,  ..., 0.0030, 0.0278, 0.0202],
        [0.0000, 0.0569, 0.0172,  ..., 0.0062, 0.0569, 0.0420],
        [0.0000, 0.0278, 0.0076,  ..., 0.0026, 0.0279, 0.0186],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43059.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0437, 0.0073, 0.0042,  ..., 0.1341, 0.0798, 0.0796],
        [0.0399, 0.0027, 0.0000,  ..., 0.1770, 0.1108, 0.1059],
        [0.0411, 0.0040, 0.0000,  ..., 0.1630, 0.0994, 0.0981],
        ...,
        [0.0544, 0.0192, 0.0527,  ..., 0.0365, 0.0116, 0.0181],
        [0.0545, 0.0192, 0.0527,  ..., 0.0365, 0.0116, 0.0181],
        [0.0545, 0.0192, 0.0527,  ..., 0.0365, 0.0116, 0.0181]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386478.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9346.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.9072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4421.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(678.7726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1236],
        [ 0.1349],
        [ 0.1403],
        ...,
        [-0.8555],
        [-0.8533],
        [-0.8529]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162051.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0022],
        [1.0055],
        ...,
        [1.0027],
        [1.0021],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367339.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(451.3231, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0055],
        ...,
        [1.0027],
        [1.0021],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367352.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60.5460, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6807, device='cuda:0')



h[100].sum tensor(-3.1761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5833, device='cuda:0')



h[200].sum tensor(-22.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42395.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0190, 0.0519,  ..., 0.0361, 0.0114, 0.0178],
        [0.0530, 0.0189, 0.0514,  ..., 0.0368, 0.0116, 0.0185],
        [0.0528, 0.0188, 0.0503,  ..., 0.0386, 0.0121, 0.0202],
        ...,
        [0.0541, 0.0194, 0.0528,  ..., 0.0369, 0.0117, 0.0182],
        [0.0541, 0.0194, 0.0528,  ..., 0.0369, 0.0117, 0.0182],
        [0.0541, 0.0195, 0.0528,  ..., 0.0369, 0.0117, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389292.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9338.0615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.5209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4612.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(665.4900, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7055],
        [-0.7184],
        [-0.7150],
        ...,
        [-0.8571],
        [-0.8556],
        [-0.8557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184998.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0023],
        [1.0055],
        ...,
        [1.0027],
        [1.0021],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367352.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0023],
        [1.0056],
        ...,
        [1.0028],
        [1.0021],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367366., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0016]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-31.4310, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9535, device='cuda:0')



h[100].sum tensor(-2.5236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0387, device='cuda:0')



h[200].sum tensor(-22.6315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0012,  ..., 0.0004, 0.0061, 0.0029],
        [0.0000, 0.0113, 0.0024,  ..., 0.0007, 0.0114, 0.0058],
        [0.0000, 0.0060, 0.0012,  ..., 0.0004, 0.0061, 0.0029],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39004.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0142, 0.0269,  ..., 0.0658, 0.0265, 0.0402],
        [0.0469, 0.0128, 0.0201,  ..., 0.0739, 0.0306, 0.0462],
        [0.0486, 0.0145, 0.0288,  ..., 0.0642, 0.0254, 0.0391],
        ...,
        [0.0538, 0.0194, 0.0528,  ..., 0.0373, 0.0117, 0.0186],
        [0.0538, 0.0194, 0.0528,  ..., 0.0373, 0.0117, 0.0186],
        [0.0538, 0.0194, 0.0528,  ..., 0.0373, 0.0117, 0.0186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(376850.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9162.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.3519, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4401.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(640.7428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2634],
        [-0.1656],
        [-0.1223],
        ...,
        [-0.8603],
        [-0.8581],
        [-0.8576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159047.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0023],
        [1.0056],
        ...,
        [1.0028],
        [1.0021],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367366., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0023],
        [1.0057],
        ...,
        [1.0028],
        [1.0021],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367379.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        [-0.0049,  0.0176,  0.0055,  ...,  0.0020,  0.0176,  0.0133],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(254.5363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1906, device='cuda:0')



h[100].sum tensor(-4.0160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2422, device='cuda:0')



h[200].sum tensor(-22.5652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0027,  ..., 0.0009, 0.0103, 0.0065],
        [0.0000, 0.0522, 0.0162,  ..., 0.0060, 0.0523, 0.0393],
        [0.0000, 0.0784, 0.0255,  ..., 0.0096, 0.0786, 0.0619],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48368.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0443, 0.0093, 0.0112,  ..., 0.1242, 0.0752, 0.0722],
        [0.0366, 0.0034, 0.0000,  ..., 0.2276, 0.1580, 0.1306],
        [0.0310, 0.0008, 0.0000,  ..., 0.3159, 0.2314, 0.1787],
        ...,
        [0.0539, 0.0193, 0.0530,  ..., 0.0376, 0.0118, 0.0188],
        [0.0539, 0.0193, 0.0530,  ..., 0.0376, 0.0118, 0.0188],
        [0.0539, 0.0193, 0.0531,  ..., 0.0376, 0.0118, 0.0188]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409914.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9079.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.2231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4291.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(722.5180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0805],
        [ 0.1147],
        [ 0.1195],
        ...,
        [-0.8651],
        [-0.8630],
        [-0.8626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155080.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0023],
        [1.0057],
        ...,
        [1.0028],
        [1.0021],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367379.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0024],
        [1.0058],
        ...,
        [1.0028],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367392.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0033,  0.0118,  0.0034,  ...,  0.0012,  0.0118,  0.0083],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0004,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2.3220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1810, device='cuda:0')



h[100].sum tensor(-2.5296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0720, device='cuda:0')



h[200].sum tensor(-22.6301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0376, 0.0118,  ..., 0.0044, 0.0378, 0.0285],
        [0.0000, 0.0123, 0.0034,  ..., 0.0013, 0.0125, 0.0083],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38198.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0405, 0.0060, 0.0032,  ..., 0.1750, 0.1169, 0.1000],
        [0.0465, 0.0122, 0.0167,  ..., 0.1027, 0.0599, 0.0582],
        [0.0506, 0.0167, 0.0325,  ..., 0.0591, 0.0266, 0.0324],
        ...,
        [0.0539, 0.0197, 0.0535,  ..., 0.0377, 0.0120, 0.0184],
        [0.0539, 0.0197, 0.0535,  ..., 0.0377, 0.0120, 0.0184],
        [0.0539, 0.0197, 0.0535,  ..., 0.0377, 0.0120, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368632.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9173.9600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5381, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.3025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4459.6108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(632.9333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0965],
        [ 0.0426],
        [-0.0669],
        ...,
        [-0.8774],
        [-0.8752],
        [-0.8747]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169887.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0024],
        [1.0058],
        ...,
        [1.0028],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367392.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0024],
        [1.0059],
        ...,
        [1.0028],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367404.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [-0.0087,  0.0311,  0.0103,  ...,  0.0040,  0.0312,  0.0250],
        [-0.0102,  0.0364,  0.0122,  ...,  0.0047,  0.0365,  0.0296],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38.8263, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6837, device='cuda:0')



h[100].sum tensor(-2.6264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1454, device='cuda:0')



h[200].sum tensor(-22.6252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0316, 0.0103,  ..., 0.0040, 0.0318, 0.0250],
        [0.0000, 0.0690, 0.0229,  ..., 0.0088, 0.0691, 0.0555],
        [0.0000, 0.1628, 0.0549,  ..., 0.0211, 0.1629, 0.1330],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39863.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0443, 0.0104, 0.0079,  ..., 0.1473, 0.0997, 0.0804],
        [0.0376, 0.0053, 0.0000,  ..., 0.2406, 0.1752, 0.1326],
        [0.0300, 0.0000, 0.0000,  ..., 0.3534, 0.2674, 0.1950],
        ...,
        [0.0539, 0.0202, 0.0538,  ..., 0.0379, 0.0122, 0.0180],
        [0.0539, 0.0202, 0.0539,  ..., 0.0379, 0.0122, 0.0180],
        [0.0539, 0.0202, 0.0539,  ..., 0.0379, 0.0122, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380850.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9185.5322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.6477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4530.9175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(645.9689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0787],
        [ 0.1155],
        [ 0.1250],
        ...,
        [-0.8881],
        [-0.8858],
        [-0.8853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190023.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0024],
        [1.0059],
        ...,
        [1.0028],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367404.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0025],
        [1.0060],
        ...,
        [1.0029],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367417.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(303.3572, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3252, device='cuda:0')



h[100].sum tensor(-3.9440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2619, device='cuda:0')



h[200].sum tensor(-22.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47136.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0167, 0.0286,  ..., 0.0639, 0.0292, 0.0356],
        [0.0515, 0.0189, 0.0456,  ..., 0.0459, 0.0167, 0.0239],
        [0.0528, 0.0200, 0.0523,  ..., 0.0388, 0.0124, 0.0188],
        ...,
        [0.0540, 0.0205, 0.0541,  ..., 0.0380, 0.0123, 0.0178],
        [0.0540, 0.0205, 0.0541,  ..., 0.0380, 0.0123, 0.0178],
        [0.0540, 0.0205, 0.0541,  ..., 0.0380, 0.0123, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403584.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9015.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.6580, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4258.8560, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(716.6849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0912],
        [-0.2907],
        [-0.4900],
        ...,
        [-0.8916],
        [-0.8897],
        [-0.8897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160776.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0025],
        [1.0060],
        ...,
        [1.0029],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367417.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0025],
        [1.0061],
        ...,
        [1.0029],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367430.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103.8941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0176, device='cuda:0')



h[100].sum tensor(-2.8169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3403, device='cuda:0')



h[200].sum tensor(-22.6154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40734.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0202, 0.0536,  ..., 0.0373, 0.0121, 0.0172],
        [0.0529, 0.0199, 0.0497,  ..., 0.0414, 0.0154, 0.0196],
        [0.0526, 0.0195, 0.0448,  ..., 0.0469, 0.0196, 0.0227],
        ...,
        [0.0542, 0.0207, 0.0545,  ..., 0.0382, 0.0124, 0.0176],
        [0.0542, 0.0207, 0.0545,  ..., 0.0382, 0.0124, 0.0176],
        [0.0542, 0.0207, 0.0546,  ..., 0.0382, 0.0124, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382178.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9111.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.7297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4356.7275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(661.9077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8976],
        [-0.8373],
        [-0.6940],
        ...,
        [-0.9065],
        [-0.9043],
        [-0.9037]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169176.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0025],
        [1.0061],
        ...,
        [1.0029],
        [1.0022],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367430.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0026],
        [1.0062],
        ...,
        [1.0029],
        [1.0022],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367443.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0097,  0.0027,  ...,  0.0010,  0.0098,  0.0066],
        [-0.0063,  0.0228,  0.0074,  ...,  0.0028,  0.0228,  0.0178],
        [-0.0080,  0.0287,  0.0095,  ...,  0.0037,  0.0288,  0.0229],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(696.9889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6702, device='cuda:0')



h[100].sum tensor(-5.8914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7734, device='cuda:0')



h[200].sum tensor(-22.4756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0463, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0434, 0.0139,  ..., 0.0053, 0.0436, 0.0335],
        [0.0000, 0.0963, 0.0313,  ..., 0.0121, 0.0965, 0.0758],
        [0.0000, 0.1163, 0.0384,  ..., 0.0148, 0.1165, 0.0930],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65130.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0381, 0.0048, 0.0000,  ..., 0.2022, 0.1385, 0.1155],
        [0.0305, 0.0008, 0.0000,  ..., 0.3044, 0.2225, 0.1725],
        [0.0259, 0.0000, 0.0000,  ..., 0.3768, 0.2838, 0.2120],
        ...,
        [0.0546, 0.0205, 0.0549,  ..., 0.0383, 0.0126, 0.0177],
        [0.0546, 0.0205, 0.0549,  ..., 0.0384, 0.0126, 0.0177],
        [0.0546, 0.0205, 0.0549,  ..., 0.0384, 0.0126, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(499281.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9045.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.6667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4305.6260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(867.4196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1169],
        [ 0.1126],
        [ 0.1075],
        ...,
        [-0.9066],
        [-0.9092],
        [-0.9103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180503.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0026],
        [1.0062],
        ...,
        [1.0029],
        [1.0022],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367443.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0027],
        [1.0063],
        ...,
        [1.0029],
        [1.0022],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367455.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0075,  0.0272,  0.0089,  ...,  0.0034,  0.0272,  0.0216],
        [-0.0132,  0.0475,  0.0161,  ...,  0.0063,  0.0475,  0.0391],
        [-0.0071,  0.0258,  0.0084,  ...,  0.0033,  0.0258,  0.0204],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(104.9007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5429, device='cuda:0')



h[100].sum tensor(-2.6989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2709, device='cuda:0')



h[200].sum tensor(-22.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1331, 0.0443,  ..., 0.0172, 0.1332, 0.1073],
        [0.0000, 0.1375, 0.0459,  ..., 0.0178, 0.1377, 0.1111],
        [0.0000, 0.1456, 0.0487,  ..., 0.0189, 0.1457, 0.1180],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40089.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0305, 0.0004, 0.0000,  ..., 0.3134, 0.2346, 0.1758],
        [0.0263, 0.0000, 0.0000,  ..., 0.3601, 0.2711, 0.2032],
        [0.0256, 0.0000, 0.0000,  ..., 0.3688, 0.2769, 0.2089],
        ...,
        [0.0547, 0.0203, 0.0549,  ..., 0.0384, 0.0128, 0.0180],
        [0.0547, 0.0203, 0.0549,  ..., 0.0384, 0.0128, 0.0180],
        [0.0547, 0.0203, 0.0549,  ..., 0.0384, 0.0128, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(379766.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9238.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.1910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4388.6704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(650.3713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0705],
        [ 0.1109],
        [ 0.1250],
        ...,
        [-0.9196],
        [-0.9174],
        [-0.9169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176653.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0027],
        [1.0063],
        ...,
        [1.0029],
        [1.0022],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367455.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0028],
        [1.0064],
        ...,
        [1.0030],
        [1.0023],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367468.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0095,  0.0345,  0.0115,  ...,  0.0045,  0.0345,  0.0279],
        [-0.0185,  0.0669,  0.0230,  ...,  0.0090,  0.0670,  0.0558],
        [-0.0181,  0.0655,  0.0225,  ...,  0.0088,  0.0655,  0.0546],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(128.4759, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9603, device='cuda:0')



h[100].sum tensor(-2.7731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3319, device='cuda:0')



h[200].sum tensor(-22.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1698, 0.0572,  ..., 0.0223, 0.1698, 0.1388],
        [0.0000, 0.2245, 0.0766,  ..., 0.0299, 0.2246, 0.1859],
        [0.0000, 0.2352, 0.0804,  ..., 0.0313, 0.2352, 0.1951],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41817.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0234, 0.0000, 0.0000,  ..., 0.4354, 0.3433, 0.2395],
        [0.0153, 0.0000, 0.0000,  ..., 0.5529, 0.4428, 0.3033],
        [0.0137, 0.0000, 0.0000,  ..., 0.5725, 0.4580, 0.3150],
        ...,
        [0.0550, 0.0200, 0.0552,  ..., 0.0384, 0.0131, 0.0183],
        [0.0550, 0.0200, 0.0552,  ..., 0.0385, 0.0131, 0.0183],
        [0.0550, 0.0200, 0.0552,  ..., 0.0385, 0.0131, 0.0183]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391568.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9242.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.0299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4287.5132, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(665.2024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0690],
        [ 0.0985],
        [ 0.1108],
        ...,
        [-0.9243],
        [-0.9219],
        [-0.9206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167115.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0028],
        [1.0064],
        ...,
        [1.0030],
        [1.0023],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367468.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(525.8604, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0028],
        [1.0065],
        ...,
        [1.0030],
        [1.0023],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367481.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0039,  0.0143,  0.0043,  ...,  0.0017,  0.0144,  0.0105],
        [-0.0094,  0.0343,  0.0114,  ...,  0.0044,  0.0343,  0.0277],
        [-0.0081,  0.0294,  0.0097,  ...,  0.0038,  0.0294,  0.0235],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(364.2855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0314, device='cuda:0')



h[100].sum tensor(-3.9711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3651, device='cuda:0')



h[200].sum tensor(-22.5596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0959, 0.0310,  ..., 0.0121, 0.0960, 0.0752],
        [0.0000, 0.0952, 0.0308,  ..., 0.0120, 0.0953, 0.0746],
        [0.0000, 0.0993, 0.0322,  ..., 0.0125, 0.0994, 0.0781],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49865.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.5731e-02, 1.6915e-03, 0.0000e+00,  ..., 2.3394e-01, 1.6724e-01,
         1.3453e-01],
        [3.3611e-02, 8.4626e-05, 0.0000e+00,  ..., 2.5742e-01, 1.8572e-01,
         1.4847e-01],
        [3.3871e-02, 1.6912e-04, 0.0000e+00,  ..., 2.5630e-01, 1.8438e-01,
         1.4799e-01],
        ...,
        [5.5506e-02, 1.9862e-02, 5.5679e-02,  ..., 3.8579e-02, 1.3275e-02,
         1.8365e-02],
        [5.5514e-02, 1.9866e-02, 5.5686e-02,  ..., 3.8585e-02, 1.3277e-02,
         1.8368e-02],
        [5.5522e-02, 1.9869e-02, 5.5694e-02,  ..., 3.8591e-02, 1.3280e-02,
         1.8371e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429235.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9243.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.4822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4190.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(735.5131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1503],
        [ 0.1549],
        [ 0.1567],
        ...,
        [-0.9346],
        [-0.9335],
        [-0.9335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163738.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0028],
        [1.0065],
        ...,
        [1.0030],
        [1.0023],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367481.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0065],
        ...,
        [1.0031],
        [1.0023],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367493.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0053,  0.0192,  0.0061,  ...,  0.0024,  0.0192,  0.0147],
        [-0.0047,  0.0173,  0.0054,  ...,  0.0021,  0.0174,  0.0131],
        [-0.0052,  0.0189,  0.0060,  ...,  0.0023,  0.0189,  0.0144],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [-0.0059,  0.0216,  0.0069,  ...,  0.0027,  0.0216,  0.0168],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(280.7647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9021, device='cuda:0')



h[100].sum tensor(-3.5570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0540, device='cuda:0')



h[200].sum tensor(-22.5777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0664, 0.0206,  ..., 0.0080, 0.0666, 0.0500],
        [0.0000, 0.0682, 0.0212,  ..., 0.0083, 0.0684, 0.0515],
        [0.0000, 0.0661, 0.0205,  ..., 0.0080, 0.0663, 0.0497],
        ...,
        [0.0000, 0.0224, 0.0070,  ..., 0.0027, 0.0226, 0.0171],
        [0.0000, 0.0184, 0.0056,  ..., 0.0022, 0.0186, 0.0136],
        [0.0000, 0.0798, 0.0253,  ..., 0.0099, 0.0800, 0.0614]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46458.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0365, 0.0003, 0.0000,  ..., 0.2064, 0.1378, 0.1234],
        [0.0369, 0.0009, 0.0000,  ..., 0.2055, 0.1381, 0.1221],
        [0.0374, 0.0010, 0.0000,  ..., 0.2034, 0.1363, 0.1210],
        ...,
        [0.0511, 0.0143, 0.0203,  ..., 0.0893, 0.0524, 0.0488],
        [0.0495, 0.0126, 0.0084,  ..., 0.1038, 0.0633, 0.0578],
        [0.0449, 0.0076, 0.0000,  ..., 0.1502, 0.0993, 0.0859]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411698.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9357.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3155, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.3162, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4193.8916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(709.6135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1806],
        [ 0.1798],
        [ 0.1789],
        ...,
        [-0.4255],
        [-0.3415],
        [-0.2990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153747.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0065],
        ...,
        [1.0031],
        [1.0023],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367493.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0030],
        [1.0066],
        ...,
        [1.0031],
        [1.0024],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367506.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0003,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(394.0994, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8026, device='cuda:0')



h[100].sum tensor(-4.0868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4778, device='cuda:0')



h[200].sum tensor(-22.5522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50620.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0528, 0.0168, 0.0394,  ..., 0.0556, 0.0240, 0.0302],
        [0.0542, 0.0182, 0.0488,  ..., 0.0458, 0.0173, 0.0237],
        [0.0549, 0.0188, 0.0523,  ..., 0.0424, 0.0151, 0.0214],
        ...,
        [0.0565, 0.0199, 0.0568,  ..., 0.0389, 0.0132, 0.0184],
        [0.0565, 0.0199, 0.0568,  ..., 0.0390, 0.0132, 0.0184],
        [0.0565, 0.0199, 0.0568,  ..., 0.0390, 0.0132, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(430393.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9380.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4168.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(746.6747, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1436],
        [-0.3259],
        [-0.5114],
        ...,
        [-0.9653],
        [-0.9629],
        [-0.9622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157970.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0030],
        [1.0066],
        ...,
        [1.0031],
        [1.0024],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367506.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0031],
        [1.0067],
        ...,
        [1.0032],
        [1.0024],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367518.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0002,  0.0002, -0.0017],
        [-0.0046,  0.0170,  0.0053,  ...,  0.0021,  0.0171,  0.0129],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0007,  ..., -0.0002,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(359.8577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4879, device='cuda:0')



h[100].sum tensor(-3.8230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2857, device='cuda:0')



h[200].sum tensor(-22.5635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0313, 0.0096,  ..., 0.0038, 0.0315, 0.0232],
        [0.0000, 0.0431, 0.0131,  ..., 0.0052, 0.0433, 0.0317],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0231, 0.0067,  ..., 0.0027, 0.0233, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47685.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0520, 0.0162, 0.0256,  ..., 0.0719, 0.0380, 0.0385],
        [0.0458, 0.0096, 0.0077,  ..., 0.1331, 0.0843, 0.0760],
        [0.0404, 0.0045, 0.0000,  ..., 0.1852, 0.1225, 0.1085],
        ...,
        [0.0555, 0.0191, 0.0480,  ..., 0.0491, 0.0199, 0.0248],
        [0.0529, 0.0165, 0.0290,  ..., 0.0693, 0.0335, 0.0382],
        [0.0473, 0.0107, 0.0092,  ..., 0.1163, 0.0666, 0.0684]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(418182.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9499.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.8864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4402.1631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(719.2627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1685],
        [-0.0089],
        [ 0.0956],
        ...,
        [-0.6431],
        [-0.3747],
        [-0.1266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173005.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0031],
        [1.0067],
        ...,
        [1.0032],
        [1.0024],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367518.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0031],
        [1.0069],
        ...,
        [1.0032],
        [1.0025],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367531.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2294e-03,  1.5551e-02,  4.8250e-03,  ...,  1.9384e-03,
          1.5609e-02,  1.1607e-02],
        [-1.1253e-02,  4.1210e-02,  1.3916e-02,  ...,  5.4999e-03,
          4.1251e-02,  3.3695e-02],
        [-1.9018e-03,  7.0479e-03,  1.8123e-03,  ...,  7.5824e-04,
          7.1111e-03,  4.2875e-03],
        ...,
        [ 0.0000e+00,  9.9992e-05, -6.4946e-04,  ..., -2.0610e-04,
          1.6786e-04, -1.6933e-03],
        [ 0.0000e+00,  9.9992e-05, -6.4946e-04,  ..., -2.0610e-04,
          1.6786e-04, -1.6933e-03],
        [ 0.0000e+00,  9.9992e-05, -6.4946e-04,  ..., -2.0610e-04,
          1.6786e-04, -1.6933e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(145.7406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7372, device='cuda:0')



h[100].sum tensor(-2.6499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2993, device='cuda:0')



h[200].sum tensor(-22.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1464, 0.0491,  ..., 0.0194, 0.1466, 0.1189],
        [0.0000, 0.0741, 0.0235,  ..., 0.0094, 0.0743, 0.0567],
        [0.0000, 0.0826, 0.0265,  ..., 0.0106, 0.0828, 0.0639],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39486.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0282, 0.0000, 0.0000,  ..., 0.3415, 0.2540, 0.1954],
        [0.0324, 0.0005, 0.0000,  ..., 0.2765, 0.1967, 0.1609],
        [0.0359, 0.0027, 0.0000,  ..., 0.2379, 0.1652, 0.1385],
        ...,
        [0.0567, 0.0207, 0.0576,  ..., 0.0392, 0.0133, 0.0182],
        [0.0567, 0.0208, 0.0576,  ..., 0.0393, 0.0133, 0.0182],
        [0.0568, 0.0208, 0.0576,  ..., 0.0393, 0.0133, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(384240.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9654.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.1886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4704.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(642.5496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1200],
        [ 0.1209],
        [ 0.1149],
        ...,
        [-0.9862],
        [-0.9837],
        [-0.9831]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208179.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0031],
        [1.0069],
        ...,
        [1.0032],
        [1.0025],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367531.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0032],
        [1.0070],
        ...,
        [1.0033],
        [1.0025],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367544.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0037,  0.0137,  0.0042,  ...,  0.0017,  0.0137,  0.0100],
        [-0.0074,  0.0273,  0.0090,  ...,  0.0036,  0.0274,  0.0217],
        [-0.0037,  0.0138,  0.0042,  ...,  0.0017,  0.0138,  0.0100],
        ...,
        [ 0.0000,  0.0001, -0.0006,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0006,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0001, -0.0006,  ..., -0.0002,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(461.6919, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0974, device='cuda:0')



h[100].sum tensor(-4.0753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5208, device='cuda:0')



h[200].sum tensor(-22.5497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0903, 0.0299,  ..., 0.0119, 0.0905, 0.0721],
        [0.0000, 0.0903, 0.0292,  ..., 0.0117, 0.0905, 0.0704],
        [0.0000, 0.0895, 0.0296,  ..., 0.0118, 0.0897, 0.0714],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53290.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0257, 0.0006, 0.0000,  ..., 0.3796, 0.2906, 0.2144],
        [0.0228, 0.0000, 0.0000,  ..., 0.4125, 0.3168, 0.2337],
        [0.0245, 0.0000, 0.0000,  ..., 0.3974, 0.3051, 0.2247],
        ...,
        [0.0565, 0.0209, 0.0575,  ..., 0.0394, 0.0133, 0.0186],
        [0.0565, 0.0209, 0.0575,  ..., 0.0394, 0.0133, 0.0186],
        [0.0565, 0.0209, 0.0575,  ..., 0.0394, 0.0133, 0.0186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450473.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9348.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.2876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4338.4634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(763.8685, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0779],
        [ 0.0762],
        [ 0.0774],
        ...,
        [-0.9872],
        [-0.9849],
        [-0.9843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176996.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0032],
        [1.0070],
        ...,
        [1.0033],
        [1.0025],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367544.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0033],
        [1.0071],
        ...,
        [1.0033],
        [1.0026],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367557.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0006,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0006,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0006,  ..., -0.0002,  0.0002, -0.0017],
        ...,
        [ 0.0000,  0.0002, -0.0006,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0006,  ..., -0.0002,  0.0002, -0.0017],
        [ 0.0000,  0.0002, -0.0006,  ..., -0.0002,  0.0002, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(225.9755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6688, device='cuda:0')



h[100].sum tensor(-2.7829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4355, device='cuda:0')



h[200].sum tensor(-22.6100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42918.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0540, 0.0199, 0.0532,  ..., 0.0417, 0.0142, 0.0214],
        [0.0547, 0.0204, 0.0557,  ..., 0.0392, 0.0130, 0.0192],
        [0.0551, 0.0207, 0.0563,  ..., 0.0388, 0.0130, 0.0187],
        ...,
        [0.0560, 0.0210, 0.0572,  ..., 0.0395, 0.0132, 0.0190],
        [0.0560, 0.0210, 0.0572,  ..., 0.0395, 0.0132, 0.0190],
        [0.0560, 0.0210, 0.0572,  ..., 0.0395, 0.0132, 0.0190]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403966.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9262.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9795, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.1246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4278.1592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(672.0139, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5573],
        [-0.7603],
        [-0.9186],
        ...,
        [-0.9828],
        [-0.9805],
        [-0.9800]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165253.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0033],
        [1.0071],
        ...,
        [1.0033],
        [1.0026],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367557.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0034],
        [1.0073],
        ...,
        [1.0033],
        [1.0026],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367570.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ..., -0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0002,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0002,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(196.5015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0616, device='cuda:0')



h[100].sum tensor(-2.5264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2006, device='cuda:0')



h[200].sum tensor(-22.6215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40623.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0544, 0.0204, 0.0558,  ..., 0.0387, 0.0128, 0.0191],
        [0.0545, 0.0204, 0.0559,  ..., 0.0388, 0.0128, 0.0191],
        [0.0547, 0.0205, 0.0560,  ..., 0.0390, 0.0129, 0.0192],
        ...,
        [0.0556, 0.0209, 0.0569,  ..., 0.0397, 0.0131, 0.0196],
        [0.0556, 0.0209, 0.0569,  ..., 0.0397, 0.0131, 0.0196],
        [0.0556, 0.0209, 0.0569,  ..., 0.0397, 0.0131, 0.0196]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391277.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9207.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.9685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4266.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(648.5176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0375],
        [-1.0880],
        [-1.1275],
        ...,
        [-0.9793],
        [-0.9772],
        [-0.9766]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165175.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0034],
        [1.0073],
        ...,
        [1.0033],
        [1.0026],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367570.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0036],
        [1.0074],
        ...,
        [1.0034],
        [1.0027],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367583.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0052,  0.0195,  0.0062,  ...,  0.0025,  0.0195,  0.0148],
        [-0.0088,  0.0326,  0.0108,  ...,  0.0044,  0.0326,  0.0261],
        [-0.0060,  0.0223,  0.0072,  ...,  0.0029,  0.0223,  0.0172],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0001,  0.0003, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0001,  0.0003, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0001,  0.0003, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(972.3849, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2860, device='cuda:0')



h[100].sum tensor(-6.1798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4478, device='cuda:0')



h[200].sum tensor(-22.4455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0541, 0.0169,  ..., 0.0070, 0.0543, 0.0405],
        [0.0000, 0.0789, 0.0250,  ..., 0.0103, 0.0791, 0.0600],
        [0.0000, 0.1505, 0.0504,  ..., 0.0202, 0.1507, 0.1217],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76428.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0016, 0.0000,  ..., 0.2411, 0.1663, 0.1442],
        [0.0252, 0.0000, 0.0000,  ..., 0.3214, 0.2338, 0.1898],
        [0.0171, 0.0000, 0.0000,  ..., 0.4350, 0.3329, 0.2515],
        ...,
        [0.0558, 0.0208, 0.0570,  ..., 0.0400, 0.0131, 0.0199],
        [0.0558, 0.0208, 0.0570,  ..., 0.0400, 0.0131, 0.0200],
        [0.0558, 0.0208, 0.0570,  ..., 0.0400, 0.0131, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594876., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8727.3135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.2457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.0053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3791.5562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(963.9318, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1335],
        [ 0.1246],
        [ 0.1176],
        ...,
        [-0.9856],
        [-0.9834],
        [-0.9829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132624.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0036],
        [1.0074],
        ...,
        [1.0034],
        [1.0027],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367583.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0037],
        [1.0075],
        ...,
        [1.0034],
        [1.0027],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367596.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0046,  0.0171,  0.0053,  ...,  0.0022,  0.0171,  0.0127],
        [-0.0021,  0.0078,  0.0021,  ...,  0.0010,  0.0079,  0.0048],
        [-0.0079,  0.0296,  0.0097,  ...,  0.0040,  0.0296,  0.0235],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0001,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0001,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ..., -0.0001,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(451.3339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2969, device='cuda:0')



h[100].sum tensor(-3.7036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2578, device='cuda:0')



h[200].sum tensor(-22.5634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0386, 0.0108,  ..., 0.0048, 0.0388, 0.0254],
        [0.0000, 0.0685, 0.0214,  ..., 0.0090, 0.0687, 0.0512],
        [0.0000, 0.0388, 0.0115,  ..., 0.0050, 0.0390, 0.0274],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53090.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0350, 0.0024, 0.0000,  ..., 0.2029, 0.1297, 0.1247],
        [0.0355, 0.0024, 0.0000,  ..., 0.2015, 0.1296, 0.1232],
        [0.0388, 0.0052, 0.0000,  ..., 0.1745, 0.1093, 0.1065],
        ...,
        [0.0563, 0.0210, 0.0575,  ..., 0.0403, 0.0133, 0.0198],
        [0.0563, 0.0210, 0.0575,  ..., 0.0403, 0.0133, 0.0198],
        [0.0563, 0.0211, 0.0576,  ..., 0.0403, 0.0133, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463789.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9072.5195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.4197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3994.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(765.9541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1487],
        [ 0.1536],
        [ 0.1545],
        ...,
        [-1.0026],
        [-1.0003],
        [-0.9998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141263.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0037],
        [1.0075],
        ...,
        [1.0034],
        [1.0027],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367596.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(559.8986, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0038],
        [1.0077],
        ...,
        [1.0034],
        [1.0027],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367608.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.3768e-03,  1.2670e-02,  3.7834e-03,  ...,  1.6639e-03,
          1.2723e-02,  8.9846e-03],
        [-3.4483e-03,  1.2935e-02,  3.8773e-03,  ...,  1.7008e-03,
          1.2989e-02,  9.2128e-03],
        [ 0.0000e+00,  1.4660e-04, -6.5124e-04,  ..., -7.9446e-05,
          2.0845e-04, -1.7877e-03],
        ...,
        [ 0.0000e+00,  1.4660e-04, -6.5124e-04,  ..., -7.9446e-05,
          2.0845e-04, -1.7877e-03],
        [ 0.0000e+00,  1.4660e-04, -6.5124e-04,  ..., -7.9446e-05,
          2.0845e-04, -1.7877e-03],
        [ 0.0000e+00,  1.4660e-04, -6.5124e-04,  ..., -7.9446e-05,
          2.0845e-04, -1.7877e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(425.6735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9632, device='cuda:0')



h[100].sum tensor(-3.5873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2090, device='cuda:0')



h[200].sum tensor(-22.5680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0699, 0.0219,  ..., 0.0093, 0.0701, 0.0524],
        [0.0000, 0.0236, 0.0068,  ..., 0.0030, 0.0238, 0.0162],
        [0.0000, 0.0134, 0.0039,  ..., 0.0017, 0.0137, 0.0092],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51446.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0408, 0.0070, 0.0000,  ..., 0.1787, 0.1174, 0.1048],
        [0.0471, 0.0126, 0.0105,  ..., 0.1178, 0.0700, 0.0681],
        [0.0514, 0.0165, 0.0259,  ..., 0.0810, 0.0427, 0.0452],
        ...,
        [0.0571, 0.0213, 0.0583,  ..., 0.0406, 0.0134, 0.0195],
        [0.0571, 0.0213, 0.0583,  ..., 0.0406, 0.0134, 0.0195],
        [0.0571, 0.0213, 0.0583,  ..., 0.0406, 0.0134, 0.0195]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458607.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9335.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8000, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.4883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4279.0020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(752.8968, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1334],
        [-0.3772],
        [-0.6560],
        ...,
        [-1.0219],
        [-1.0195],
        [-1.0189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166278.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0038],
        [1.0077],
        ...,
        [1.0034],
        [1.0027],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367608.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0039],
        [1.0078],
        ...,
        [1.0033],
        [1.0026],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367621.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.1105e-04, -6.5215e-04,  ..., -6.4297e-05,
          1.8290e-04, -1.7841e-03],
        [-1.0313e-02,  3.8455e-02,  1.2924e-02,  ...,  5.2744e-03,
          3.8498e-02,  3.1198e-02],
        [ 0.0000e+00,  1.1105e-04, -6.5215e-04,  ..., -6.4297e-05,
          1.8290e-04, -1.7841e-03],
        ...,
        [ 0.0000e+00,  1.1105e-04, -6.5215e-04,  ..., -6.4297e-05,
          1.8290e-04, -1.7841e-03],
        [ 0.0000e+00,  1.1105e-04, -6.5215e-04,  ..., -6.4297e-05,
          1.8290e-04, -1.7841e-03],
        [ 0.0000e+00,  1.1105e-04, -6.5215e-04,  ..., -6.4297e-05,
          1.8290e-04, -1.7841e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(395.6917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3524, device='cuda:0')



h[100].sum tensor(-3.4882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1198, device='cuda:0')



h[200].sum tensor(-22.5719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0876, 0.0283,  ..., 0.0119, 0.0878, 0.0678],
        [0.0000, 0.0318, 0.0098,  ..., 0.0042, 0.0321, 0.0234],
        [0.0000, 0.0915, 0.0296,  ..., 0.0124, 0.0917, 0.0711],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50022.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0080, 0.0000,  ..., 0.1806, 0.1211, 0.1039],
        [0.0455, 0.0103, 0.0000,  ..., 0.1527, 0.0989, 0.0875],
        [0.0430, 0.0079, 0.0000,  ..., 0.1851, 0.1248, 0.1064],
        ...,
        [0.0580, 0.0212, 0.0591,  ..., 0.0408, 0.0136, 0.0193],
        [0.0580, 0.0212, 0.0591,  ..., 0.0408, 0.0136, 0.0193],
        [0.0580, 0.0212, 0.0591,  ..., 0.0408, 0.0136, 0.0193]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445209.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9626.2764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.3309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4545.5674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(742.6166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2638],
        [-0.2727],
        [-0.3137],
        ...,
        [-1.0319],
        [-1.0298],
        [-1.0303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191995.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0039],
        [1.0078],
        ...,
        [1.0033],
        [1.0026],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367621.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0079],
        ...,
        [1.0033],
        [1.0026],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367634.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6505e-03,  9.9764e-03,  2.8392e-03,  ...,  1.3180e-03,
          1.0052e-02,  6.7097e-03],
        [-7.9139e-03,  2.9594e-02,  9.7841e-03,  ...,  4.0495e-03,
          2.9655e-02,  2.3584e-02],
        [-9.7583e-03,  3.6469e-02,  1.2218e-02,  ...,  5.0067e-03,
          3.6525e-02,  2.9497e-02],
        ...,
        [ 0.0000e+00,  9.7346e-05, -6.5808e-04,  ..., -5.7448e-05,
          1.8059e-04, -1.7875e-03],
        [ 0.0000e+00,  9.7346e-05, -6.5808e-04,  ..., -5.7448e-05,
          1.8059e-04, -1.7875e-03],
        [ 0.0000e+00,  9.7346e-05, -6.5808e-04,  ..., -5.7448e-05,
          1.8059e-04, -1.7875e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(271.7630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1519, device='cuda:0')



h[100].sum tensor(-2.9454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6522, device='cuda:0')



h[200].sum tensor(-22.5976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0433, 0.0139,  ..., 0.0059, 0.0436, 0.0334],
        [0.0000, 0.1016, 0.0332,  ..., 0.0139, 0.1019, 0.0799],
        [0.0000, 0.1682, 0.0568,  ..., 0.0231, 0.1684, 0.1371],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44999.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0413, 0.0065, 0.0000,  ..., 0.2112, 0.1448, 0.1219],
        [0.0325, 0.0021, 0.0000,  ..., 0.3266, 0.2401, 0.1875],
        [0.0240, 0.0000, 0.0000,  ..., 0.4525, 0.3473, 0.2572],
        ...,
        [0.0587, 0.0211, 0.0595,  ..., 0.0410, 0.0137, 0.0194],
        [0.0587, 0.0211, 0.0595,  ..., 0.0410, 0.0137, 0.0194],
        [0.0587, 0.0211, 0.0596,  ..., 0.0410, 0.0137, 0.0194]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422817.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9767.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.5419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4568.0103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(703.1979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1083],
        [ 0.0991],
        [ 0.0923],
        ...,
        [-1.0541],
        [-1.0515],
        [-1.0509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177107.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0040],
        [1.0079],
        ...,
        [1.0033],
        [1.0026],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367634.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0041],
        [1.0081],
        ...,
        [1.0033],
        [1.0026],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367647.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.0328e-03,  1.5181e-02,  4.6643e-03,  ...,  2.0460e-03,
          1.5259e-02,  1.1165e-02],
        [ 0.0000e+00,  1.1180e-04, -6.6926e-04,  ..., -5.1882e-05,
          2.0030e-04, -1.7947e-03],
        [ 0.0000e+00,  1.1180e-04, -6.6926e-04,  ..., -5.1882e-05,
          2.0030e-04, -1.7947e-03],
        ...,
        [ 0.0000e+00,  1.1180e-04, -6.6926e-04,  ..., -5.1882e-05,
          2.0030e-04, -1.7947e-03],
        [ 0.0000e+00,  1.1180e-04, -6.6926e-04,  ..., -5.1882e-05,
          2.0030e-04, -1.7947e-03],
        [ 0.0000e+00,  1.1180e-04, -6.6926e-04,  ..., -5.1882e-05,
          2.0030e-04, -1.7947e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(248.1400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6714, device='cuda:0')



h[100].sum tensor(-2.8162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5819, device='cuda:0')



h[200].sum tensor(-22.6032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0309, 0.0094,  ..., 0.0041, 0.0312, 0.0226],
        [0.0000, 0.0156, 0.0047,  ..., 0.0021, 0.0159, 0.0112],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43414.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0475, 0.0109, 0.0047,  ..., 0.1400, 0.0881, 0.0806],
        [0.0514, 0.0145, 0.0175,  ..., 0.0967, 0.0536, 0.0552],
        [0.0547, 0.0176, 0.0370,  ..., 0.0634, 0.0272, 0.0354],
        ...,
        [0.0589, 0.0207, 0.0595,  ..., 0.0413, 0.0138, 0.0197],
        [0.0589, 0.0207, 0.0595,  ..., 0.0413, 0.0138, 0.0197],
        [0.0589, 0.0207, 0.0595,  ..., 0.0413, 0.0138, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414368.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9936.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.0600, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4734.3706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(682.7256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1188],
        [-0.2740],
        [-0.4330],
        ...,
        [-1.0612],
        [-1.0586],
        [-1.0580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201156.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0041],
        [1.0081],
        ...,
        [1.0033],
        [1.0026],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367647.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0032],
        [1.0025],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367660.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.3718e-03,  1.6511e-02,  5.1151e-03,  ...,  2.2344e-03,
          1.6591e-02,  1.2281e-02],
        [-1.9051e-03,  7.2699e-03,  1.8453e-03,  ...,  9.4813e-04,
          7.3571e-03,  4.3348e-03],
        [-2.4667e-03,  9.3735e-03,  2.5896e-03,  ...,  1.2409e-03,
          9.4592e-03,  6.1436e-03],
        ...,
        [ 0.0000e+00,  1.3258e-04, -6.8014e-04,  ..., -4.5336e-05,
          2.2486e-04, -1.8025e-03],
        [ 0.0000e+00,  1.3258e-04, -6.8014e-04,  ..., -4.5336e-05,
          2.2486e-04, -1.8025e-03],
        [ 0.0000e+00,  1.3258e-04, -6.8014e-04,  ..., -4.5336e-05,
          2.2486e-04, -1.8025e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(203.3683, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1479, device='cuda:0')



h[100].sum tensor(-2.5769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3593, device='cuda:0')



h[200].sum tensor(-22.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0276, 0.0076,  ..., 0.0036, 0.0280, 0.0179],
        [0.0000, 0.0604, 0.0184,  ..., 0.0081, 0.0607, 0.0442],
        [0.0000, 0.0277, 0.0083,  ..., 0.0037, 0.0281, 0.0198],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41733.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0456, 0.0088, 0.0024,  ..., 0.1375, 0.0782, 0.0840],
        [0.0427, 0.0061, 0.0000,  ..., 0.1670, 0.1007, 0.1021],
        [0.0468, 0.0100, 0.0047,  ..., 0.1312, 0.0745, 0.0797],
        ...,
        [0.0591, 0.0204, 0.0594,  ..., 0.0416, 0.0139, 0.0201],
        [0.0591, 0.0204, 0.0594,  ..., 0.0416, 0.0139, 0.0201],
        [0.0591, 0.0204, 0.0594,  ..., 0.0416, 0.0139, 0.0201]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(408834.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9987.0752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.4447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4776.3535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(665.7188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0531],
        [ 0.1209],
        [ 0.0912],
        ...,
        [-1.0653],
        [-1.0628],
        [-1.0622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205286.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0042],
        [1.0082],
        ...,
        [1.0032],
        [1.0025],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367660.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0042],
        [1.0083],
        ...,
        [1.0032],
        [1.0025],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367673.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1800e-03,  8.3463e-03,  2.2053e-03,  ...,  1.1018e-03,
          8.4316e-03,  5.2315e-03],
        [ 0.0000e+00,  1.5781e-04, -6.9107e-04,  ..., -3.7683e-05,
          2.4936e-04, -1.8085e-03],
        [ 0.0000e+00,  1.5781e-04, -6.9107e-04,  ..., -3.7683e-05,
          2.4936e-04, -1.8085e-03],
        ...,
        [ 0.0000e+00,  1.5781e-04, -6.9107e-04,  ..., -3.7683e-05,
          2.4936e-04, -1.8085e-03],
        [ 0.0000e+00,  1.5781e-04, -6.9107e-04,  ..., -3.7683e-05,
          2.4936e-04, -1.8085e-03],
        [ 0.0000e+00,  1.5781e-04, -6.9107e-04,  ..., -3.7683e-05,
          2.4936e-04, -1.8085e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(256.5767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4442, device='cuda:0')



h[100].sum tensor(-2.7638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5487, device='cuda:0')



h[200].sum tensor(-22.6043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0284, 0.0084,  ..., 0.0038, 0.0287, 0.0202],
        [0.0000, 0.0088, 0.0022,  ..., 0.0011, 0.0092, 0.0052],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44113.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0462, 0.0089, 0.0042,  ..., 0.1338, 0.0769, 0.0816],
        [0.0511, 0.0135, 0.0198,  ..., 0.0914, 0.0467, 0.0545],
        [0.0551, 0.0173, 0.0390,  ..., 0.0610, 0.0261, 0.0345],
        ...,
        [0.0591, 0.0201, 0.0592,  ..., 0.0420, 0.0141, 0.0205],
        [0.0591, 0.0201, 0.0592,  ..., 0.0420, 0.0141, 0.0205],
        [0.0591, 0.0201, 0.0592,  ..., 0.0420, 0.0141, 0.0205]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(418113.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9908.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0680, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.6472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4555.1851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(688.9561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0922],
        [-0.0563],
        [-0.2570],
        ...,
        [-1.0455],
        [-1.0558],
        [-1.0589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184464.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0042],
        [1.0083],
        ...,
        [1.0032],
        [1.0025],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367673.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0043],
        [1.0084],
        ...,
        [1.0032],
        [1.0025],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367686.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0718e-03,  7.9701e-03,  2.0627e-03,  ...,  1.0669e-03,
          8.0531e-03,  4.8972e-03],
        [ 0.0000e+00,  1.6795e-04, -6.9635e-04,  ..., -1.8891e-05,
          2.5744e-04, -1.8101e-03],
        [ 0.0000e+00,  1.6795e-04, -6.9635e-04,  ..., -1.8891e-05,
          2.5744e-04, -1.8101e-03],
        ...,
        [ 0.0000e+00,  1.6795e-04, -6.9635e-04,  ..., -1.8891e-05,
          2.5744e-04, -1.8101e-03],
        [ 0.0000e+00,  1.6795e-04, -6.9635e-04,  ..., -1.8891e-05,
          2.5744e-04, -1.8101e-03],
        [ 0.0000e+00,  1.6795e-04, -6.9635e-04,  ..., -1.8891e-05,
          2.5744e-04, -1.8101e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(282.3816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6253, device='cuda:0')



h[100].sum tensor(-2.8199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5752, device='cuda:0')



h[200].sum tensor(-22.6007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0202, 0.0048,  ..., 0.0027, 0.0205, 0.0113],
        [0.0000, 0.0149, 0.0036,  ..., 0.0019, 0.0152, 0.0086],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46403.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0474, 0.0097, 0.0060,  ..., 0.1136, 0.0570, 0.0717],
        [0.0503, 0.0125, 0.0167,  ..., 0.0934, 0.0447, 0.0576],
        [0.0548, 0.0167, 0.0386,  ..., 0.0624, 0.0252, 0.0362],
        ...,
        [0.0592, 0.0200, 0.0592,  ..., 0.0423, 0.0141, 0.0207],
        [0.0592, 0.0200, 0.0592,  ..., 0.0423, 0.0141, 0.0207],
        [0.0592, 0.0200, 0.0592,  ..., 0.0424, 0.0141, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434942.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9869.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.0690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4538.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(709.2632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0573],
        [-0.0739],
        [-0.2868],
        ...,
        [-1.0739],
        [-1.0714],
        [-1.0709]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180238.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0043],
        [1.0084],
        ...,
        [1.0032],
        [1.0025],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367686.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0044],
        [1.0085],
        ...,
        [1.0031],
        [1.0024],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367698.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.1409e-03,  3.4661e-02,  1.1509e-02,  ...,  4.8215e-03,
          3.4716e-02,  2.7858e-02],
        [-1.4695e-02,  5.5631e-02,  1.8924e-02,  ...,  7.7419e-03,
          5.5667e-02,  4.5885e-02],
        [-1.0435e-02,  3.9546e-02,  1.3237e-02,  ...,  5.5019e-03,
          3.9597e-02,  3.2058e-02],
        ...,
        [ 0.0000e+00,  1.5078e-04, -6.9358e-04,  ...,  1.5549e-05,
          2.3746e-04, -1.8083e-03],
        [ 0.0000e+00,  1.5078e-04, -6.9358e-04,  ...,  1.5549e-05,
          2.3746e-04, -1.8083e-03],
        [ 0.0000e+00,  1.5078e-04, -6.9358e-04,  ...,  1.5549e-05,
          2.3746e-04, -1.8083e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(130.0624, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3852, device='cuda:0')



h[100].sum tensor(-2.1160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9557, device='cuda:0')



h[200].sum tensor(-22.6354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4988e-01, 5.0003e-02,  ..., 2.0851e-02, 1.5009e-01,
         1.2108e-01],
        [0.0000e+00, 1.9203e-01, 6.4904e-02,  ..., 2.6721e-02, 1.9221e-01,
         1.5730e-01],
        [0.0000e+00, 2.0349e-01, 6.8950e-02,  ..., 2.8317e-02, 2.0365e-01,
         1.6714e-01],
        ...,
        [0.0000e+00, 6.1543e-04, 0.0000e+00,  ..., 6.3464e-05, 9.6921e-04,
         0.0000e+00],
        [0.0000e+00, 6.1548e-04, 0.0000e+00,  ..., 6.3469e-05, 9.6929e-04,
         0.0000e+00],
        [0.0000e+00, 6.1550e-04, 0.0000e+00,  ..., 6.3471e-05, 9.6932e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38877.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8014e-02, 1.0179e-04, 0.0000e+00,  ..., 4.0158e-01, 3.0567e-01,
         2.3110e-01],
        [2.1196e-02, 0.0000e+00, 0.0000e+00,  ..., 4.9414e-01, 3.8264e-01,
         2.8372e-01],
        [1.9800e-02, 0.0000e+00, 0.0000e+00,  ..., 5.1028e-01, 3.9407e-01,
         2.9386e-01],
        ...,
        [5.9617e-02, 2.0124e-02, 5.9679e-02,  ..., 4.2542e-02, 1.4172e-02,
         2.0365e-02],
        [5.9624e-02, 2.0127e-02, 5.9685e-02,  ..., 4.2548e-02, 1.4174e-02,
         2.0367e-02],
        [5.9628e-02, 2.0129e-02, 5.9688e-02,  ..., 4.2551e-02, 1.4175e-02,
         2.0369e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(397145.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10097.4121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.5581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4839.1504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(643.6963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1327],
        [ 0.1367],
        [ 0.1432],
        ...,
        [-1.0893],
        [-1.0867],
        [-1.0861]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207870.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0044],
        [1.0085],
        ...,
        [1.0031],
        [1.0024],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367698.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0045],
        [1.0086],
        ...,
        [1.0031],
        [1.0024],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367710.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.3451e-03,  1.6592e-02,  5.1259e-03,  ...,  2.3670e-03,
          1.6654e-02,  1.2324e-02],
        [-2.0987e-03,  8.0895e-03,  2.1196e-03,  ...,  1.1818e-03,
          8.1595e-03,  5.0151e-03],
        [-2.2465e-03,  8.6489e-03,  2.3174e-03,  ...,  1.2597e-03,
          8.7183e-03,  5.4959e-03],
        ...,
        [ 0.0000e+00,  1.4632e-04, -6.8883e-04,  ...,  7.4489e-05,
          2.2422e-04, -1.8129e-03],
        [ 0.0000e+00,  1.4632e-04, -6.8883e-04,  ...,  7.4489e-05,
          2.2422e-04, -1.8129e-03],
        [ 0.0000e+00,  1.4632e-04, -6.8883e-04,  ...,  7.4489e-05,
          2.2422e-04, -1.8129e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(367.1451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4434, device='cuda:0')



h[100].sum tensor(-3.0570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8409, device='cuda:0')



h[200].sum tensor(-22.5871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0277, 0.0075,  ..., 0.0041, 0.0280, 0.0179],
        [0.0000, 0.0576, 0.0174,  ..., 0.0082, 0.0578, 0.0417],
        [0.0000, 0.0278, 0.0082,  ..., 0.0041, 0.0281, 0.0198],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0003, 0.0009, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0003, 0.0009, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0003, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47741.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0467, 0.0086, 0.0025,  ..., 0.1359, 0.0762, 0.0829],
        [0.0443, 0.0061, 0.0000,  ..., 0.1623, 0.0961, 0.0990],
        [0.0486, 0.0101, 0.0056,  ..., 0.1274, 0.0718, 0.0763],
        ...,
        [0.0597, 0.0204, 0.0600,  ..., 0.0428, 0.0143, 0.0201],
        [0.0597, 0.0204, 0.0600,  ..., 0.0428, 0.0143, 0.0202],
        [0.0597, 0.0204, 0.0600,  ..., 0.0428, 0.0143, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(437423.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9886.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.2189, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4480.5459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(731.8517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0368],
        [ 0.0282],
        [-0.1336],
        ...,
        [-1.0983],
        [-1.0957],
        [-1.0951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175045.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0045],
        [1.0086],
        ...,
        [1.0031],
        [1.0024],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367710.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0046],
        [1.0087],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367722.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0267,  0.0087,  ...,  0.0039,  0.0268,  0.0210],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(441.1745, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7379, device='cuda:0')



h[100].sum tensor(-3.2367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0300, device='cuda:0')



h[200].sum tensor(-22.5770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0986, 0.0326,  ..., 0.0143, 0.0988, 0.0788],
        [0.0000, 0.0672, 0.0215,  ..., 0.0100, 0.0674, 0.0518],
        [0.0000, 0.0404, 0.0120,  ..., 0.0062, 0.0406, 0.0288],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0007, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0007, 0.0008, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0007, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49132.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.1605e-02, 1.7531e-04, 0.0000e+00,  ..., 3.4625e-01, 2.5334e-01,
         2.0107e-01],
        [3.4813e-02, 6.1515e-04, 0.0000e+00,  ..., 2.9900e-01, 2.1249e-01,
         1.7491e-01],
        [3.7856e-02, 7.9321e-04, 0.0000e+00,  ..., 2.5360e-01, 1.7206e-01,
         1.5021e-01],
        ...,
        [5.9599e-02, 2.1001e-02, 6.0235e-02,  ..., 4.3011e-02, 1.4385e-02,
         1.9883e-02],
        [5.9605e-02, 2.1003e-02, 6.0241e-02,  ..., 4.3016e-02, 1.4387e-02,
         1.9886e-02],
        [5.9609e-02, 2.1005e-02, 6.0243e-02,  ..., 4.3019e-02, 1.4388e-02,
         1.9887e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442359.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9817.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5379, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.3439, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4342.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(747.6894, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1293],
        [ 0.1371],
        [ 0.1447],
        ...,
        [-1.1045],
        [-1.1019],
        [-1.1013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170690.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0046],
        [1.0087],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367722.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(549.6252, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0047],
        [1.0089],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367734.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0002,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(409.8954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4213, device='cuda:0')



h[100].sum tensor(-2.9699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8376, device='cuda:0')



h[200].sum tensor(-22.5898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0010, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0010, 0.0008, 0.0000],
        [0.0000, 0.0092, 0.0024,  ..., 0.0022, 0.0094, 0.0056],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0010, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0010, 0.0008, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0010, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47501.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0574, 0.0204, 0.0557,  ..., 0.0459, 0.0157, 0.0225],
        [0.0562, 0.0194, 0.0464,  ..., 0.0555, 0.0217, 0.0291],
        [0.0532, 0.0165, 0.0238,  ..., 0.0830, 0.0398, 0.0472],
        ...,
        [0.0596, 0.0216, 0.0605,  ..., 0.0431, 0.0144, 0.0197],
        [0.0596, 0.0216, 0.0606,  ..., 0.0431, 0.0144, 0.0197],
        [0.0596, 0.0216, 0.0606,  ..., 0.0431, 0.0144, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(439419., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9885.0010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.8164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4486.9365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(731.3488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6123],
        [-0.3730],
        [-0.1393],
        ...,
        [-1.1110],
        [-1.1084],
        [-1.1078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201269.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0047],
        [1.0089],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367734.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0048],
        [1.0090],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367745.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0026,  0.0102,  0.0029,  ...,  0.0017,  0.0103,  0.0068],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [-0.0026,  0.0102,  0.0029,  ...,  0.0017,  0.0103,  0.0068],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1073.7306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.1699, device='cuda:0')



h[100].sum tensor(-5.6691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2847, device='cuda:0')



h[200].sum tensor(-22.4497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0214, 0.0061,  ..., 0.0041, 0.0216, 0.0143],
        [0.0000, 0.0436, 0.0126,  ..., 0.0072, 0.0438, 0.0297],
        [0.0000, 0.0087, 0.0023,  ..., 0.0023, 0.0090, 0.0053],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0012, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0012, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0012, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76666.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0120, 0.0042,  ..., 0.1253, 0.0676, 0.0743],
        [0.0470, 0.0107, 0.0000,  ..., 0.1372, 0.0755, 0.0820],
        [0.0515, 0.0148, 0.0128,  ..., 0.1004, 0.0509, 0.0582],
        ...,
        [0.0597, 0.0219, 0.0609,  ..., 0.0433, 0.0145, 0.0196],
        [0.0597, 0.0219, 0.0609,  ..., 0.0433, 0.0145, 0.0196],
        [0.0597, 0.0219, 0.0609,  ..., 0.0433, 0.0145, 0.0196]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601279.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9728.0967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.2470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.8370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4476.4873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(979.5416, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1043],
        [ 0.1029],
        [ 0.0381],
        ...,
        [-1.1198],
        [-1.1172],
        [-1.1166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208191.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0048],
        [1.0090],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367745.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0049],
        [1.0092],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367757.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0021,  0.0080,  0.0021,  ...,  0.0014,  0.0080,  0.0049],
        [-0.0021,  0.0080,  0.0021,  ...,  0.0014,  0.0080,  0.0049],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(473.3259, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9339, device='cuda:0')



h[100].sum tensor(-3.0413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9125, device='cuda:0')



h[200].sum tensor(-22.5844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0256, 0.0069,  ..., 0.0048, 0.0257, 0.0160],
        [0.0000, 0.0202, 0.0050,  ..., 0.0041, 0.0204, 0.0114],
        [0.0000, 0.0148, 0.0037,  ..., 0.0033, 0.0150, 0.0086],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0013, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0013, 0.0007, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0013, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47789.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0465, 0.0103, 0.0000,  ..., 0.1332, 0.0685, 0.0812],
        [0.0480, 0.0117, 0.0043,  ..., 0.1196, 0.0588, 0.0726],
        [0.0507, 0.0142, 0.0116,  ..., 0.1003, 0.0469, 0.0596],
        ...,
        [0.0597, 0.0219, 0.0611,  ..., 0.0435, 0.0145, 0.0197],
        [0.0597, 0.0219, 0.0611,  ..., 0.0435, 0.0145, 0.0197],
        [0.0597, 0.0219, 0.0611,  ..., 0.0435, 0.0145, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438329.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9802.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.9025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4339.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(741.2575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1199],
        [ 0.0630],
        [-0.0568],
        ...,
        [-1.1240],
        [-1.1212],
        [-1.1203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186217.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0049],
        [1.0092],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367757.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0050],
        [1.0093],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367769.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [-0.0053,  0.0205,  0.0065,  ...,  0.0032,  0.0206,  0.0157],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0001, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(368.7459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4436, device='cuda:0')



h[100].sum tensor(-2.5079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4026, device='cuda:0')



h[200].sum tensor(-22.6113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0435, 0.0125,  ..., 0.0074, 0.0436, 0.0295],
        [0.0000, 0.0173, 0.0046,  ..., 0.0037, 0.0174, 0.0107],
        [0.0000, 0.0526, 0.0157,  ..., 0.0086, 0.0527, 0.0373],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0014, 0.0007, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0014, 0.0007, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0014, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43958.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0456, 0.0089, 0.0000,  ..., 0.1488, 0.0830, 0.0894],
        [0.0469, 0.0100, 0.0008,  ..., 0.1383, 0.0757, 0.0827],
        [0.0444, 0.0074, 0.0000,  ..., 0.1681, 0.0978, 0.1009],
        ...,
        [0.0594, 0.0216, 0.0613,  ..., 0.0438, 0.0146, 0.0197],
        [0.0594, 0.0216, 0.0613,  ..., 0.0438, 0.0146, 0.0197],
        [0.0594, 0.0216, 0.0613,  ..., 0.0438, 0.0146, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421350.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9765.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.8707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4380.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(706.0259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1561],
        [ 0.1570],
        [ 0.1585],
        ...,
        [-1.1287],
        [-1.1253],
        [-1.1232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190474.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0050],
        [1.0093],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367769.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0051],
        [1.0094],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367781.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0188,  0.0059,  ...,  0.0029,  0.0188,  0.0142],
        [-0.0043,  0.0166,  0.0051,  ...,  0.0026,  0.0166,  0.0123],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(503.8372, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9027, device='cuda:0')



h[100].sum tensor(-2.9840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9080, device='cuda:0')



h[200].sum tensor(-22.5856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0759, 0.0238,  ..., 0.0118, 0.0759, 0.0572],
        [0.0000, 0.0329, 0.0100,  ..., 0.0058, 0.0329, 0.0239],
        [0.0000, 0.0172, 0.0051,  ..., 0.0036, 0.0173, 0.0123],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0013, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0013, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0013, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46973.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0448, 0.0074, 0.0000,  ..., 0.1820, 0.1182, 0.1051],
        [0.0490, 0.0116, 0.0047,  ..., 0.1361, 0.0828, 0.0773],
        [0.0527, 0.0154, 0.0200,  ..., 0.0975, 0.0536, 0.0539],
        ...,
        [0.0592, 0.0212, 0.0614,  ..., 0.0440, 0.0147, 0.0198],
        [0.0592, 0.0212, 0.0614,  ..., 0.0440, 0.0147, 0.0198],
        [0.0592, 0.0212, 0.0614,  ..., 0.0440, 0.0147, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(428122.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9761.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.8437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4494.5913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(725.1970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2165],
        [-0.2870],
        [-0.3875],
        ...,
        [-1.1368],
        [-1.1343],
        [-1.1337]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206806.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0051],
        [1.0094],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367781.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0052],
        [1.0095],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367793.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0076,  0.0019,  ...,  0.0013,  0.0077,  0.0045],
        [-0.0047,  0.0184,  0.0057,  ...,  0.0028,  0.0184,  0.0137],
        [-0.0025,  0.0100,  0.0028,  ...,  0.0017,  0.0100,  0.0066],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(357.2679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2969, device='cuda:0')



h[100].sum tensor(-2.3103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2350, device='cuda:0')



h[200].sum tensor(-22.6202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0534, 0.0158,  ..., 0.0086, 0.0534, 0.0378],
        [0.0000, 0.0330, 0.0086,  ..., 0.0057, 0.0330, 0.0202],
        [0.0000, 0.0544, 0.0161,  ..., 0.0087, 0.0544, 0.0386],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0012, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0012, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0012, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42868.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0452, 0.0076, 0.0000,  ..., 0.1505, 0.0850, 0.0907],
        [0.0442, 0.0066, 0.0000,  ..., 0.1554, 0.0858, 0.0952],
        [0.0422, 0.0045, 0.0000,  ..., 0.1816, 0.1062, 0.1109],
        ...,
        [0.0588, 0.0206, 0.0595,  ..., 0.0461, 0.0160, 0.0212],
        [0.0590, 0.0208, 0.0614,  ..., 0.0442, 0.0146, 0.0200],
        [0.0590, 0.0208, 0.0614,  ..., 0.0442, 0.0146, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415513.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9627.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.4215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4424.4478, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(690.2438, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3412],
        [-0.1844],
        [-0.0922],
        ...,
        [-0.8915],
        [-1.0055],
        [-1.0854]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183282.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0052],
        [1.0095],
        ...,
        [1.0031],
        [1.0024],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367793.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0053],
        [1.0096],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367805.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(291.0543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3547, device='cuda:0')



h[100].sum tensor(-1.9999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9512, device='cuda:0')



h[200].sum tensor(-22.6360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0012, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0012, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0012, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0012, 0.0009, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0012, 0.0009, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0012, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40788.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0576, 0.0199, 0.0603,  ..., 0.0432, 0.0142, 0.0195],
        [0.0575, 0.0198, 0.0589,  ..., 0.0448, 0.0152, 0.0206],
        [0.0574, 0.0196, 0.0568,  ..., 0.0474, 0.0165, 0.0225],
        ...,
        [0.0590, 0.0205, 0.0617,  ..., 0.0444, 0.0146, 0.0201],
        [0.0590, 0.0206, 0.0617,  ..., 0.0444, 0.0146, 0.0201],
        [0.0590, 0.0206, 0.0617,  ..., 0.0444, 0.0146, 0.0201]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407319.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9645.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.4806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4520.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(669.2194, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0594],
        [-0.9030],
        [-0.6913],
        ...,
        [-1.1404],
        [-1.1322],
        [-1.1258]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189419., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0053],
        [1.0096],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367805.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0053],
        [1.0097],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367816.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(347.4390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9513, device='cuda:0')



h[100].sum tensor(-2.2354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1845, device='cuda:0')



h[200].sum tensor(-22.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0011, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0011, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0011, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0011, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0011, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0011, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42462.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0579, 0.0196, 0.0610,  ..., 0.0434, 0.0142, 0.0194],
        [0.0581, 0.0197, 0.0611,  ..., 0.0435, 0.0142, 0.0194],
        [0.0583, 0.0198, 0.0613,  ..., 0.0438, 0.0143, 0.0196],
        ...,
        [0.0593, 0.0202, 0.0623,  ..., 0.0446, 0.0146, 0.0200],
        [0.0593, 0.0202, 0.0623,  ..., 0.0446, 0.0146, 0.0200],
        [0.0593, 0.0202, 0.0623,  ..., 0.0446, 0.0146, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415559.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9732.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.6934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4738.0088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(680.8705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0646],
        [-1.1747],
        [-1.2619],
        ...,
        [-1.1654],
        [-1.1630],
        [-1.1626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200280.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0053],
        [1.0097],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367816.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0054],
        [1.0098],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367827.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [-0.0045,  0.0175,  0.0054,  ...,  0.0027,  0.0175,  0.0130],
        [-0.0067,  0.0260,  0.0084,  ...,  0.0038,  0.0260,  0.0204],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(273.5285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3103, device='cuda:0')



h[100].sum tensor(-1.9732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9447, device='cuda:0')



h[200].sum tensor(-22.6362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0495, 0.0143,  ..., 0.0078, 0.0495, 0.0345],
        [0.0000, 0.0409, 0.0127,  ..., 0.0066, 0.0409, 0.0308],
        [0.0000, 0.0394, 0.0122,  ..., 0.0064, 0.0394, 0.0295],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40155.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0474, 0.0076, 0.0000,  ..., 0.1486, 0.0854, 0.0884],
        [0.0466, 0.0066, 0.0000,  ..., 0.1659, 0.1008, 0.0978],
        [0.0454, 0.0052, 0.0000,  ..., 0.1817, 0.1122, 0.1076],
        ...,
        [0.0600, 0.0200, 0.0630,  ..., 0.0448, 0.0145, 0.0198],
        [0.0600, 0.0200, 0.0630,  ..., 0.0448, 0.0145, 0.0198],
        [0.0600, 0.0200, 0.0630,  ..., 0.0448, 0.0145, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409233.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9924.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.6074, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4917.7705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(661.2749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1349],
        [ 0.1983],
        [ 0.2208],
        ...,
        [-1.1889],
        [-1.1861],
        [-1.1855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219406.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0054],
        [1.0098],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367827.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0055],
        [1.0099],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367838.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(379.7124, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1061, device='cuda:0')



h[100].sum tensor(-2.3911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3532, device='cuda:0')



h[200].sum tensor(-22.6131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0010, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45968.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0586, 0.0189, 0.0581,  ..., 0.0476, 0.0164, 0.0220],
        [0.0577, 0.0178, 0.0499,  ..., 0.0562, 0.0213, 0.0280],
        [0.0574, 0.0173, 0.0457,  ..., 0.0609, 0.0238, 0.0313],
        ...,
        [0.0606, 0.0200, 0.0634,  ..., 0.0449, 0.0145, 0.0198],
        [0.0606, 0.0200, 0.0634,  ..., 0.0449, 0.0145, 0.0198],
        [0.0606, 0.0200, 0.0634,  ..., 0.0449, 0.0145, 0.0198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(443886.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9960.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.4498, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4902.4517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(713.8959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4532],
        [-0.4315],
        [-0.3610],
        ...,
        [-1.2046],
        [-1.2018],
        [-1.2012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217295.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0055],
        [1.0099],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367838.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(512.2514, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0056],
        [1.0100],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367849.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0243,  0.0078,  ...,  0.0036,  0.0242,  0.0189],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(399.8355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3840, device='cuda:0')



h[100].sum tensor(-2.4239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3938, device='cuda:0')



h[200].sum tensor(-22.6106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0759, 0.0251,  ..., 0.0116, 0.0758, 0.0610],
        [0.0000, 0.0446, 0.0140,  ..., 0.0072, 0.0446, 0.0341],
        [0.0000, 0.0007, 0.0000,  ..., 0.0011, 0.0007, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0011, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0011, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0011, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45459.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0449, 0.0061, 0.0000,  ..., 0.2345, 0.1655, 0.1329],
        [0.0487, 0.0083, 0.0000,  ..., 0.1777, 0.1180, 0.1005],
        [0.0539, 0.0138, 0.0022,  ..., 0.1124, 0.0662, 0.0615],
        ...,
        [0.0608, 0.0203, 0.0634,  ..., 0.0451, 0.0143, 0.0202],
        [0.0608, 0.0203, 0.0634,  ..., 0.0451, 0.0143, 0.0202],
        [0.0608, 0.0203, 0.0634,  ..., 0.0451, 0.0143, 0.0202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440248.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9968.2402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.8263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4839.9170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(713.3853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0234],
        [ 0.0133],
        [ 0.0108],
        ...,
        [-1.2149],
        [-1.2121],
        [-1.2115]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208953.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0056],
        [1.0100],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367849.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0057],
        [1.0102],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367860.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0276,  0.0089,  ...,  0.0041,  0.0275,  0.0217],
        [-0.0031,  0.0125,  0.0036,  ...,  0.0020,  0.0125,  0.0087],
        [-0.0040,  0.0160,  0.0049,  ...,  0.0025,  0.0160,  0.0117],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(588.9783, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2160, device='cuda:0')



h[100].sum tensor(-3.0913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0998, device='cuda:0')



h[200].sum tensor(-22.5734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0798, 0.0250,  ..., 0.0122, 0.0797, 0.0606],
        [0.0000, 0.0801, 0.0251,  ..., 0.0123, 0.0800, 0.0609],
        [0.0000, 0.0261, 0.0075,  ..., 0.0047, 0.0261, 0.0181],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0012, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0012, 0.0007, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0012, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51419.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.4875e-02, 2.1766e-04, 0.0000e+00,  ..., 3.3746e-01, 2.3806e-01,
         1.9908e-01],
        [3.8698e-02, 1.4525e-03, 0.0000e+00,  ..., 2.8211e-01, 1.9197e-01,
         1.6722e-01],
        [4.5333e-02, 5.6938e-03, 0.0000e+00,  ..., 1.9991e-01, 1.2665e-01,
         1.1808e-01],
        ...,
        [6.0413e-02, 2.0370e-02, 6.0290e-02,  ..., 4.8266e-02, 1.5597e-02,
         2.2894e-02],
        [5.9493e-02, 1.9484e-02, 5.4395e-02,  ..., 5.4452e-02, 1.8541e-02,
         2.7532e-02],
        [5.9031e-02, 1.9041e-02, 5.1444e-02,  ..., 5.7546e-02, 2.0014e-02,
         2.9852e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463912.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9873.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.5827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4684.4204, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(770.1409, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1905],
        [ 0.1872],
        [ 0.1836],
        ...,
        [-1.0924],
        [-1.0069],
        [-0.9530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196246.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0057],
        [1.0102],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367860.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0058],
        [1.0103],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367872.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(447.2998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0425, device='cuda:0')



h[100].sum tensor(-2.4629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4900, device='cuda:0')



h[200].sum tensor(-22.6069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0014, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0014, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0014, 0.0008, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0014, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0014, 0.0008, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0014, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44881.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0549, 0.0170, 0.0262,  ..., 0.0864, 0.0430, 0.0477],
        [0.0578, 0.0195, 0.0485,  ..., 0.0571, 0.0214, 0.0295],
        [0.0593, 0.0207, 0.0589,  ..., 0.0473, 0.0153, 0.0228],
        ...,
        [0.0608, 0.0216, 0.0629,  ..., 0.0451, 0.0140, 0.0210],
        [0.0608, 0.0216, 0.0630,  ..., 0.0451, 0.0140, 0.0210],
        [0.0608, 0.0216, 0.0630,  ..., 0.0451, 0.0140, 0.0210]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433505., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9996.5283, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.3617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4960.5967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(706.6021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0879],
        [-0.3232],
        [-0.5220],
        ...,
        [-1.2224],
        [-1.2197],
        [-1.2191]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221656.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0058],
        [1.0103],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367872.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0059],
        [1.0103],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367883.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0037,  0.0146,  0.0044,  ...,  0.0024,  0.0146,  0.0105],
        [-0.0027,  0.0108,  0.0030,  ...,  0.0018,  0.0108,  0.0072],
        [-0.0019,  0.0076,  0.0019,  ...,  0.0014,  0.0076,  0.0044],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(384.8581, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9283, device='cuda:0')



h[100].sum tensor(-2.1356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1811, device='cuda:0')



h[200].sum tensor(-22.6242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0511, 0.0149,  ..., 0.0085, 0.0511, 0.0356],
        [0.0000, 0.0374, 0.0101,  ..., 0.0066, 0.0374, 0.0239],
        [0.0000, 0.0302, 0.0075,  ..., 0.0056, 0.0302, 0.0177],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0015, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0015, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0015, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42141.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0455, 0.0107, 0.0000,  ..., 0.1622, 0.0916, 0.0985],
        [0.0446, 0.0099, 0.0000,  ..., 0.1637, 0.0888, 0.1012],
        [0.0453, 0.0103, 0.0000,  ..., 0.1551, 0.0806, 0.0967],
        ...,
        [0.0605, 0.0222, 0.0626,  ..., 0.0450, 0.0137, 0.0215],
        [0.0605, 0.0222, 0.0626,  ..., 0.0450, 0.0137, 0.0215],
        [0.0605, 0.0222, 0.0626,  ..., 0.0450, 0.0137, 0.0215]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423378., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9873.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.8465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.7703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4841.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(684.4117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1179],
        [ 0.1599],
        [ 0.1643],
        ...,
        [-1.2204],
        [-1.2177],
        [-1.2171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203162.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0059],
        [1.0103],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367883.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0060],
        [1.0104],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367895.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [-0.0053,  0.0210,  0.0066,  ...,  0.0033,  0.0210,  0.0160],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(722.6616, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1201, device='cuda:0')



h[100].sum tensor(-3.3419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3781, device='cuda:0')



h[200].sum tensor(-22.5565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0015, 0.0009, 0.0000],
        [0.0000, 0.0217, 0.0067,  ..., 0.0045, 0.0218, 0.0160],
        [0.0000, 0.0701, 0.0230,  ..., 0.0112, 0.0702, 0.0557],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0016, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0016, 0.0009, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0016, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55276.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0560, 0.0200, 0.0339,  ..., 0.0740, 0.0356, 0.0403],
        [0.0501, 0.0163, 0.0171,  ..., 0.1536, 0.1000, 0.0866],
        [0.0403, 0.0102, 0.0000,  ..., 0.2880, 0.2088, 0.1650],
        ...,
        [0.0605, 0.0227, 0.0625,  ..., 0.0450, 0.0135, 0.0219],
        [0.0605, 0.0227, 0.0625,  ..., 0.0450, 0.0135, 0.0219],
        [0.0605, 0.0227, 0.0625,  ..., 0.0450, 0.0135, 0.0219]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(490252.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9704.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.2290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4675.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(799.0617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0954],
        [ 0.0147],
        [ 0.0844],
        ...,
        [-1.2238],
        [-1.2211],
        [-1.2205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194087.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0060],
        [1.0104],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367895.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0061],
        [1.0106],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367906.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0004,  0.0002, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(486.2153, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2333, device='cuda:0')



h[100].sum tensor(-2.4113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5179, device='cuda:0')



h[200].sum tensor(-22.6074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.0015, 0.0010, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0015, 0.0010, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0015, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0016, 0.0010, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0016, 0.0010, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0016, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45593.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0590, 0.0222, 0.0613,  ..., 0.0437, 0.0130, 0.0215],
        [0.0590, 0.0222, 0.0609,  ..., 0.0444, 0.0132, 0.0222],
        [0.0588, 0.0221, 0.0597,  ..., 0.0462, 0.0137, 0.0239],
        ...,
        [0.0605, 0.0229, 0.0627,  ..., 0.0450, 0.0134, 0.0222],
        [0.0605, 0.0230, 0.0627,  ..., 0.0450, 0.0134, 0.0222],
        [0.0605, 0.0230, 0.0627,  ..., 0.0450, 0.0134, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442948.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9697.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.3200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4593.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(718.4768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2149],
        [-1.0878],
        [-0.8956],
        ...,
        [-1.2279],
        [-1.2242],
        [-1.2227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175111.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0061],
        [1.0106],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367906.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0061],
        [1.0107],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367917.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0249,  0.0080,  ...,  0.0038,  0.0249,  0.0193],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0002, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(291.0908, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0658, device='cuda:0')



h[100].sum tensor(-1.7094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7629, device='cuda:0')



h[200].sum tensor(-22.6461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0703, 0.0224,  ..., 0.0111, 0.0705, 0.0539],
        [0.0000, 0.0458, 0.0145,  ..., 0.0077, 0.0459, 0.0348],
        [0.0000, 0.0008, 0.0000,  ..., 0.0014, 0.0010, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0014, 0.0010, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0014, 0.0010, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0014, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39318.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0415, 0.0115, 0.0000,  ..., 0.2364, 0.1577, 0.1400],
        [0.0480, 0.0153, 0.0040,  ..., 0.1689, 0.1072, 0.0985],
        [0.0560, 0.0200, 0.0286,  ..., 0.0842, 0.0427, 0.0468],
        ...,
        [0.0610, 0.0228, 0.0630,  ..., 0.0452, 0.0134, 0.0225],
        [0.0610, 0.0228, 0.0631,  ..., 0.0452, 0.0134, 0.0225],
        [0.0610, 0.0228, 0.0631,  ..., 0.0452, 0.0134, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415648.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9992.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.5643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.8801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5075.3115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(657.3466, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0836],
        [-0.0457],
        [-0.3180],
        ...,
        [-1.2460],
        [-1.2433],
        [-1.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214613.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0061],
        [1.0107],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367917.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0062],
        [1.0108],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367929.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [-0.0021,  0.0085,  0.0022,  ...,  0.0015,  0.0086,  0.0053],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1188.6709, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.9330, device='cuda:0')



h[100].sum tensor(-5.0221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2501, device='cuda:0')



h[200].sum tensor(-22.4584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0553, 0.0164,  ..., 0.0088, 0.0555, 0.0392],
        [0.0000, 0.0077, 0.0017,  ..., 0.0021, 0.0079, 0.0040],
        [0.0000, 0.0536, 0.0165,  ..., 0.0086, 0.0537, 0.0395],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0012, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0012, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0012, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70520.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0409, 0.0097, 0.0000,  ..., 0.2294, 0.1454, 0.1397],
        [0.0435, 0.0114, 0.0000,  ..., 0.2088, 0.1317, 0.1261],
        [0.0385, 0.0088, 0.0000,  ..., 0.2769, 0.1864, 0.1662],
        ...,
        [0.0617, 0.0224, 0.0634,  ..., 0.0456, 0.0134, 0.0230],
        [0.0617, 0.0224, 0.0634,  ..., 0.0456, 0.0134, 0.0230],
        [0.0617, 0.0224, 0.0634,  ..., 0.0456, 0.0134, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557823.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9731.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.5047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4617.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(935.1826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0631],
        [ 0.0573],
        [ 0.0516],
        ...,
        [-1.2597],
        [-1.2573],
        [-1.2571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185030.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0062],
        [1.0108],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367929.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0063],
        [1.0109],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367940.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0080,  0.0320,  0.0105,  ...,  0.0047,  0.0321,  0.0254],
        [-0.0086,  0.0347,  0.0114,  ...,  0.0051,  0.0347,  0.0277],
        [-0.0081,  0.0323,  0.0106,  ...,  0.0047,  0.0324,  0.0257],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(598.7993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8519, device='cuda:0')



h[100].sum tensor(-2.9037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0466, device='cuda:0')



h[200].sum tensor(-22.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1216, 0.0398,  ..., 0.0178, 0.1218, 0.0961],
        [0.0000, 0.1402, 0.0463,  ..., 0.0204, 0.1404, 0.1120],
        [0.0000, 0.1461, 0.0484,  ..., 0.0213, 0.1463, 0.1170],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0010, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51607.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0261, 0.0014, 0.0000,  ..., 0.4484, 0.3282, 0.2673],
        [0.0258, 0.0013, 0.0000,  ..., 0.4580, 0.3363, 0.2726],
        [0.0273, 0.0020, 0.0000,  ..., 0.4392, 0.3201, 0.2620],
        ...,
        [0.0623, 0.0218, 0.0637,  ..., 0.0460, 0.0134, 0.0235],
        [0.0623, 0.0218, 0.0637,  ..., 0.0460, 0.0134, 0.0235],
        [0.0623, 0.0218, 0.0637,  ..., 0.0460, 0.0134, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(475156.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10026.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.2863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4836.7358, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(773.4940, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0745],
        [ 0.0757],
        [ 0.0799],
        ...,
        [-1.2775],
        [-1.2747],
        [-1.2741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-181379.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0063],
        [1.0109],
        ...,
        [1.0031],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367940.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0064],
        [1.0111],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367951.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0091,  0.0366,  0.0121,  ...,  0.0053,  0.0366,  0.0293],
        [-0.0064,  0.0256,  0.0082,  ...,  0.0037,  0.0257,  0.0199],
        [-0.0021,  0.0087,  0.0023,  ...,  0.0014,  0.0088,  0.0054],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(498.0177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7636, device='cuda:0')



h[100].sum tensor(-2.5920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7415, device='cuda:0')



h[200].sum tensor(-22.5940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1330, 0.0438,  ..., 0.0192, 0.1333, 0.1060],
        [0.0000, 0.0748, 0.0232,  ..., 0.0111, 0.0751, 0.0559],
        [0.0000, 0.0643, 0.0195,  ..., 0.0096, 0.0646, 0.0469],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0007, 0.0011, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0007, 0.0011, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0007, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47491.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0027, 0.0000,  ..., 0.3752, 0.2646, 0.2271],
        [0.0366, 0.0045, 0.0000,  ..., 0.2991, 0.1983, 0.1843],
        [0.0397, 0.0057, 0.0000,  ..., 0.2523, 0.1575, 0.1579],
        ...,
        [0.0631, 0.0212, 0.0642,  ..., 0.0463, 0.0135, 0.0238],
        [0.0631, 0.0212, 0.0642,  ..., 0.0463, 0.0135, 0.0238],
        [0.0631, 0.0212, 0.0642,  ..., 0.0463, 0.0135, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455904.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10269.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.6692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5058.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(739.5660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1421],
        [ 0.1559],
        [ 0.1697],
        ...,
        [-1.2967],
        [-1.2937],
        [-1.2931]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195397.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0064],
        [1.0111],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367951.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(523.4445, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0064],
        [1.0112],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367962.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [-0.0025,  0.0100,  0.0027,  ...,  0.0015,  0.0101,  0.0066],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(405.1980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6887, device='cuda:0')



h[100].sum tensor(-2.2951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4384, device='cuda:0')



h[200].sum tensor(-22.6102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0005, 0.0011, 0.0000],
        [0.0000, 0.0106, 0.0028,  ..., 0.0019, 0.0110, 0.0066],
        [0.0000, 0.0208, 0.0056,  ..., 0.0033, 0.0211, 0.0134],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0011, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0011, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46400.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0591, 0.0177, 0.0419,  ..., 0.0667, 0.0249, 0.0389],
        [0.0569, 0.0161, 0.0254,  ..., 0.0863, 0.0375, 0.0522],
        [0.0534, 0.0134, 0.0120,  ..., 0.1161, 0.0555, 0.0731],
        ...,
        [0.0639, 0.0208, 0.0648,  ..., 0.0466, 0.0136, 0.0240],
        [0.0639, 0.0208, 0.0648,  ..., 0.0466, 0.0136, 0.0240],
        [0.0639, 0.0208, 0.0648,  ..., 0.0466, 0.0136, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458289.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10490.6182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.1174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5318.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(730.4953, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3758],
        [-0.1367],
        [ 0.0437],
        ...,
        [-1.3153],
        [-1.3121],
        [-1.3114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215685.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0064],
        [1.0112],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367962.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0065],
        [1.0113],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367973.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0276,  0.0090,  ...,  0.0039,  0.0277,  0.0217],
        [-0.0063,  0.0256,  0.0082,  ...,  0.0037,  0.0257,  0.0200],
        [-0.0016,  0.0066,  0.0015,  ...,  0.0010,  0.0067,  0.0037],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(555.1757, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3555, device='cuda:0')



h[100].sum tensor(-2.7990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9741, device='cuda:0')



h[200].sum tensor(-22.5803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1249, 0.0409,  ..., 0.0178, 0.1253, 0.0991],
        [0.0000, 0.0921, 0.0293,  ..., 0.0132, 0.0924, 0.0709],
        [0.0000, 0.1168, 0.0380,  ..., 0.0167, 0.1171, 0.0921],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0004, 0.0011, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0004, 0.0011, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0004, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49035.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0324, 0.0009, 0.0000,  ..., 0.3969, 0.2818, 0.2406],
        [0.0338, 0.0012, 0.0000,  ..., 0.3740, 0.2619, 0.2279],
        [0.0338, 0.0011, 0.0000,  ..., 0.3765, 0.2636, 0.2295],
        ...,
        [0.0643, 0.0206, 0.0652,  ..., 0.0469, 0.0137, 0.0242],
        [0.0643, 0.0206, 0.0652,  ..., 0.0469, 0.0137, 0.0242],
        [0.0643, 0.0206, 0.0652,  ..., 0.0469, 0.0137, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463800.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10554.9395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.6261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5331.8857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(756.3264, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1008],
        [ 0.0995],
        [ 0.1001],
        ...,
        [-1.3227],
        [-1.3194],
        [-1.3186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219917.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0065],
        [1.0113],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367973.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0066],
        [1.0114],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367984.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [-0.0026,  0.0105,  0.0029,  ...,  0.0015,  0.0106,  0.0070],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(492.6471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5898, device='cuda:0')



h[100].sum tensor(-2.5212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7161, device='cuda:0')



h[200].sum tensor(-22.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0111, 0.0029,  ..., 0.0019, 0.0115, 0.0070],
        [0.0000, 0.0093, 0.0023,  ..., 0.0016, 0.0096, 0.0054],
        [0.0000, 0.0443, 0.0125,  ..., 0.0065, 0.0447, 0.0299],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0004, 0.0012, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0004, 0.0012, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0004, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48907.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0579, 0.0162, 0.0304,  ..., 0.0805, 0.0340, 0.0487],
        [0.0556, 0.0143, 0.0167,  ..., 0.0992, 0.0446, 0.0619],
        [0.0513, 0.0110, 0.0000,  ..., 0.1364, 0.0675, 0.0873],
        ...,
        [0.0644, 0.0204, 0.0654,  ..., 0.0471, 0.0138, 0.0244],
        [0.0644, 0.0204, 0.0654,  ..., 0.0471, 0.0138, 0.0244],
        [0.0644, 0.0204, 0.0654,  ..., 0.0471, 0.0138, 0.0244]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471814.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10594.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.8065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5424.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(754.2667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2977],
        [-0.1050],
        [ 0.0088],
        ...,
        [-1.3348],
        [-1.3318],
        [-1.3311]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225635.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0066],
        [1.0114],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367984.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0067],
        [1.0115],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367995.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0001,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(585.0762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1663, device='cuda:0')



h[100].sum tensor(-2.7335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9465, device='cuda:0')



h[200].sum tensor(-22.5822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0012, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0012, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0012, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0012, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0005, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47795.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0622, 0.0195, 0.0624,  ..., 0.0474, 0.0143, 0.0248],
        [0.0624, 0.0196, 0.0634,  ..., 0.0467, 0.0136, 0.0245],
        [0.0622, 0.0195, 0.0620,  ..., 0.0488, 0.0143, 0.0265],
        ...,
        [0.0641, 0.0203, 0.0654,  ..., 0.0472, 0.0138, 0.0245],
        [0.0641, 0.0203, 0.0654,  ..., 0.0472, 0.0138, 0.0245],
        [0.0641, 0.0203, 0.0654,  ..., 0.0472, 0.0138, 0.0245]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(454256.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10528.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.3517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5425.8193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(747.2408, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9953],
        [-1.1037],
        [-1.1303],
        ...,
        [-1.3345],
        [-1.3321],
        [-1.3323]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225353.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0067],
        [1.0115],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367995.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0067],
        [1.0117],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368006.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [-0.0020,  0.0083,  0.0022,  ...,  0.0013,  0.0084,  0.0051],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(456.2100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2202, device='cuda:0')



h[100].sum tensor(-2.1928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3699, device='cuda:0')



h[200].sum tensor(-22.6131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0007, 0.0012, 0.0000],
        [0.0000, 0.0090, 0.0022,  ..., 0.0018, 0.0093, 0.0051],
        [0.0000, 0.0282, 0.0082,  ..., 0.0045, 0.0285, 0.0197],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0007, 0.0012, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0007, 0.0012, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0007, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45562.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0542, 0.0144, 0.0097,  ..., 0.1242, 0.0684, 0.0741],
        [0.0521, 0.0126, 0.0071,  ..., 0.1400, 0.0769, 0.0856],
        [0.0492, 0.0104, 0.0011,  ..., 0.1667, 0.0940, 0.1037],
        ...,
        [0.0637, 0.0206, 0.0657,  ..., 0.0473, 0.0138, 0.0242],
        [0.0625, 0.0197, 0.0556,  ..., 0.0571, 0.0199, 0.0310],
        [0.0600, 0.0179, 0.0330,  ..., 0.0788, 0.0340, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451878.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10368.1006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.2171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5306.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(732.6671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0648],
        [ 0.0764],
        [ 0.0903],
        ...,
        [-1.1263],
        [-0.8619],
        [-0.5314]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205801.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0067],
        [1.0117],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368006.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0068],
        [1.0118],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368017.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0021,  0.0088,  0.0023,  ...,  0.0014,  0.0089,  0.0055],
        [-0.0020,  0.0083,  0.0022,  ...,  0.0014,  0.0084,  0.0051],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0002,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(523.3583, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3312, device='cuda:0')



h[100].sum tensor(-2.3417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5322, device='cuda:0')



h[200].sum tensor(-22.6035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0461, 0.0131,  ..., 0.0072, 0.0463, 0.0313],
        [0.0000, 0.0232, 0.0065,  ..., 0.0040, 0.0235, 0.0154],
        [0.0000, 0.0090, 0.0022,  ..., 0.0020, 0.0093, 0.0051],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0009, 0.0012, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0009, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46817.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0497, 0.0112, 0.0000,  ..., 0.1449, 0.0746, 0.0911],
        [0.0531, 0.0136, 0.0117,  ..., 0.1174, 0.0571, 0.0726],
        [0.0571, 0.0167, 0.0287,  ..., 0.0841, 0.0357, 0.0503],
        ...,
        [0.0635, 0.0210, 0.0662,  ..., 0.0474, 0.0140, 0.0237],
        [0.0635, 0.0210, 0.0662,  ..., 0.0474, 0.0140, 0.0237],
        [0.0635, 0.0210, 0.0662,  ..., 0.0474, 0.0140, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(459163.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10342.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.5219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5333.4189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(742.9190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0131],
        [-0.0806],
        [-0.2809],
        ...,
        [-1.3503],
        [-1.3477],
        [-1.3477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231388.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0068],
        [1.0118],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368017.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0069],
        [1.0119],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368028.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(495.7893, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2684, device='cuda:0')



h[100].sum tensor(-2.1750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3769, device='cuda:0')



h[200].sum tensor(-22.6126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0010, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0010, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45320.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0589, 0.0186, 0.0518,  ..., 0.0602, 0.0192, 0.0342],
        [0.0594, 0.0189, 0.0537,  ..., 0.0585, 0.0185, 0.0327],
        [0.0597, 0.0191, 0.0524,  ..., 0.0600, 0.0200, 0.0335],
        ...,
        [0.0633, 0.0212, 0.0666,  ..., 0.0475, 0.0141, 0.0233],
        [0.0633, 0.0212, 0.0666,  ..., 0.0475, 0.0141, 0.0233],
        [0.0633, 0.0212, 0.0666,  ..., 0.0475, 0.0141, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451173.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10263.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.9790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5258.2314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(734.4253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3141],
        [-0.3566],
        [-0.3449],
        ...,
        [-1.3651],
        [-1.3619],
        [-1.3613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225433.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0069],
        [1.0119],
        ...,
        [1.0032],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368028.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0070],
        [1.0120],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368040.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0002, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(929.0564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1303, device='cuda:0')



h[100].sum tensor(-3.5279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9640, device='cuda:0')



h[200].sum tensor(-22.5308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0384, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.0011, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0011, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0011, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0011, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0011, 0.0011, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0011, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60163.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0204, 0.0651,  ..., 0.0463, 0.0137, 0.0225],
        [0.0615, 0.0205, 0.0653,  ..., 0.0465, 0.0138, 0.0226],
        [0.0618, 0.0207, 0.0655,  ..., 0.0468, 0.0139, 0.0228],
        ...,
        [0.0630, 0.0211, 0.0667,  ..., 0.0477, 0.0142, 0.0233],
        [0.0630, 0.0211, 0.0667,  ..., 0.0478, 0.0142, 0.0233],
        [0.0630, 0.0211, 0.0667,  ..., 0.0478, 0.0142, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524718.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10059.9971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5864, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.7906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5140.3135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(863.4907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3356],
        [-1.4487],
        [-1.5298],
        ...,
        [-1.3670],
        [-1.3650],
        [-1.3652]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222703.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0070],
        [1.0120],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368040.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0071],
        [1.0121],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368051.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024,  0.0099,  0.0027,  ...,  0.0017,  0.0099,  0.0064],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(510.9859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4854, device='cuda:0')



h[100].sum tensor(-2.0403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2626, device='cuda:0')



h[200].sum tensor(-22.6192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0213, 0.0057,  ..., 0.0040, 0.0215, 0.0136],
        [0.0000, 0.0108, 0.0027,  ..., 0.0026, 0.0109, 0.0064],
        [0.0000, 0.0010, 0.0000,  ..., 0.0012, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0012, 0.0012, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0012, 0.0012, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0012, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44069.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0528, 0.0135, 0.0123,  ..., 0.1204, 0.0610, 0.0722],
        [0.0569, 0.0167, 0.0301,  ..., 0.0855, 0.0386, 0.0487],
        [0.0603, 0.0194, 0.0538,  ..., 0.0581, 0.0209, 0.0303],
        ...,
        [0.0627, 0.0208, 0.0664,  ..., 0.0481, 0.0142, 0.0234],
        [0.0627, 0.0208, 0.0664,  ..., 0.0481, 0.0142, 0.0234],
        [0.0627, 0.0208, 0.0664,  ..., 0.0481, 0.0142, 0.0234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(443179.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10092.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.2116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5220.8003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(724.8392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2467],
        [-0.5888],
        [-0.9435],
        ...,
        [-1.3709],
        [-1.3679],
        [-1.3672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218857.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0071],
        [1.0121],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368051.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0072],
        [1.0123],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368062.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [-0.0043,  0.0179,  0.0055,  ...,  0.0028,  0.0179,  0.0133],
        [-0.0037,  0.0156,  0.0047,  ...,  0.0025,  0.0156,  0.0112],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(613.0339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5189, device='cuda:0')



h[100].sum tensor(-2.2928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5597, device='cuda:0')



h[200].sum tensor(-22.6031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0188, 0.0055,  ..., 0.0038, 0.0189, 0.0133],
        [0.0000, 0.0310, 0.0091,  ..., 0.0055, 0.0311, 0.0219],
        [0.0000, 0.0950, 0.0303,  ..., 0.0145, 0.0951, 0.0730],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0013, 0.0012, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0013, 0.0012, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0013, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45846.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0557, 0.0159, 0.0216,  ..., 0.1042, 0.0553, 0.0590],
        [0.0519, 0.0129, 0.0055,  ..., 0.1524, 0.0914, 0.0889],
        [0.0458, 0.0084, 0.0000,  ..., 0.2429, 0.1625, 0.1434],
        ...,
        [0.0597, 0.0183, 0.0363,  ..., 0.0768, 0.0338, 0.0418],
        [0.0604, 0.0189, 0.0438,  ..., 0.0698, 0.0290, 0.0372],
        [0.0618, 0.0201, 0.0589,  ..., 0.0555, 0.0193, 0.0280]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447967.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10035.3721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.6835, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5119.7622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(741.5217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0909],
        [ 0.0271],
        [ 0.1006],
        ...,
        [-0.7149],
        [-0.8453],
        [-1.0418]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216320.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0072],
        [1.0123],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368062.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(505.5288, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0073],
        [1.0123],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368072.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(401.4403, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.7775, device='cuda:0')



h[100].sum tensor(-1.5304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7208, device='cuda:0')



h[200].sum tensor(-22.6488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40478.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0601, 0.0191, 0.0603,  ..., 0.0519, 0.0166, 0.0260],
        [0.0604, 0.0194, 0.0598,  ..., 0.0524, 0.0176, 0.0259],
        [0.0603, 0.0192, 0.0536,  ..., 0.0586, 0.0222, 0.0297],
        ...,
        [0.0624, 0.0204, 0.0666,  ..., 0.0487, 0.0145, 0.0233],
        [0.0624, 0.0204, 0.0666,  ..., 0.0487, 0.0145, 0.0233],
        [0.0624, 0.0204, 0.0666,  ..., 0.0487, 0.0145, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(430540.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10009.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.6591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.9849, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5115.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(697.8662, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4918],
        [-0.5550],
        [-0.5179],
        ...,
        [-1.3828],
        [-1.3799],
        [-1.3792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212011.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0073],
        [1.0123],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368072.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0073],
        [1.0124],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368083.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [-0.0020,  0.0085,  0.0022,  ...,  0.0015,  0.0085,  0.0052],
        [-0.0021,  0.0087,  0.0023,  ...,  0.0015,  0.0087,  0.0054],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(668.9789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5002, device='cuda:0')



h[100].sum tensor(-2.3865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7030, device='cuda:0')



h[200].sum tensor(-22.5957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0022,  ..., 0.0025, 0.0094, 0.0052],
        [0.0000, 0.0288, 0.0083,  ..., 0.0053, 0.0288, 0.0199],
        [0.0000, 0.0672, 0.0205,  ..., 0.0107, 0.0672, 0.0492],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49148.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0556, 0.0144, 0.0240,  ..., 0.0967, 0.0449, 0.0564],
        [0.0515, 0.0104, 0.0060,  ..., 0.1450, 0.0788, 0.0871],
        [0.0473, 0.0063, 0.0000,  ..., 0.1971, 0.1156, 0.1203],
        ...,
        [0.0629, 0.0202, 0.0671,  ..., 0.0491, 0.0148, 0.0231],
        [0.0629, 0.0202, 0.0672,  ..., 0.0491, 0.0148, 0.0231],
        [0.0629, 0.0202, 0.0672,  ..., 0.0491, 0.0148, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(468209.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10060.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.8704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5006.5791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(776.0054, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0261],
        [ 0.0963],
        [ 0.1616],
        ...,
        [-1.4030],
        [-1.4000],
        [-1.3993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217186.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0073],
        [1.0124],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368083.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0074],
        [1.0125],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368094., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(697.9525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3961, device='cuda:0')



h[100].sum tensor(-2.4675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8339, device='cuda:0')



h[200].sum tensor(-22.5899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0014, 0.0011, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0014, 0.0011, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0014, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0011, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0011, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0014, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50724.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0615, 0.0190, 0.0644,  ..., 0.0496, 0.0156, 0.0232],
        [0.0618, 0.0193, 0.0662,  ..., 0.0481, 0.0146, 0.0222],
        [0.0622, 0.0195, 0.0665,  ..., 0.0484, 0.0147, 0.0224],
        ...,
        [0.0634, 0.0199, 0.0677,  ..., 0.0494, 0.0150, 0.0229],
        [0.0634, 0.0199, 0.0677,  ..., 0.0494, 0.0150, 0.0229],
        [0.0634, 0.0199, 0.0677,  ..., 0.0494, 0.0150, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(484885.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10164.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6603, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5043.1768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(790.8828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1203],
        [-1.3414],
        [-1.4879],
        ...,
        [-1.4208],
        [-1.4177],
        [-1.4169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230674.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0074],
        [1.0125],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368094., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0075],
        [1.0126],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368104.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0018],
        [-0.0039,  0.0166,  0.0050,  ...,  0.0026,  0.0166,  0.0122],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0018],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0018],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0018],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0003,  0.0003, -0.0018]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(616.7998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2468, device='cuda:0')



h[100].sum tensor(-2.2009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5199, device='cuda:0')



h[200].sum tensor(-22.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0175, 0.0051,  ..., 0.0037, 0.0174, 0.0122],
        [0.0000, 0.0274, 0.0086,  ..., 0.0051, 0.0273, 0.0207],
        [0.0000, 0.1266, 0.0413,  ..., 0.0190, 0.1263, 0.1002],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0014, 0.0011, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0014, 0.0011, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0014, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46464.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0523, 0.0085, 0.0043,  ..., 0.1808, 0.1131, 0.1047],
        [0.0486, 0.0046, 0.0000,  ..., 0.2488, 0.1682, 0.1450],
        [0.0416, 0.0004, 0.0000,  ..., 0.3720, 0.2659, 0.2188],
        ...,
        [0.0638, 0.0199, 0.0683,  ..., 0.0497, 0.0152, 0.0225],
        [0.0638, 0.0199, 0.0683,  ..., 0.0498, 0.0152, 0.0225],
        [0.0638, 0.0199, 0.0683,  ..., 0.0498, 0.0152, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458130.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10249.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2381, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.8720, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5081.2021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(759.4944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1462],
        [ 0.1435],
        [ 0.1431],
        ...,
        [-1.4392],
        [-1.4359],
        [-1.4351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232088.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0075],
        [1.0126],
        ...,
        [1.0032],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368104.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0076],
        [1.0127],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368115.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0188,  0.0784,  0.0268,  ...,  0.0114,  0.0782,  0.0652],
        [-0.0075,  0.0313,  0.0102,  ...,  0.0047,  0.0312,  0.0247],
        [-0.0039,  0.0165,  0.0050,  ...,  0.0027,  0.0165,  0.0121],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(693.6931, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6107, device='cuda:0')



h[100].sum tensor(-2.3813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7192, device='cuda:0')



h[200].sum tensor(-22.5934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1332, 0.0436,  ..., 0.0201, 0.1329, 0.1059],
        [0.0000, 0.1697, 0.0565,  ..., 0.0252, 0.1694, 0.1372],
        [0.0000, 0.0992, 0.0317,  ..., 0.0153, 0.0990, 0.0767],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0015, 0.0010, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0015, 0.0010, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0015, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49957.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0365, 0.0000, 0.0000,  ..., 0.4733, 0.3500, 0.2776],
        [0.0360, 0.0000, 0.0000,  ..., 0.4822, 0.3559, 0.2832],
        [0.0401, 0.0000, 0.0000,  ..., 0.3796, 0.2666, 0.2257],
        ...,
        [0.0637, 0.0199, 0.0685,  ..., 0.0500, 0.0154, 0.0224],
        [0.0637, 0.0199, 0.0685,  ..., 0.0500, 0.0154, 0.0224],
        [0.0637, 0.0199, 0.0685,  ..., 0.0500, 0.0154, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477644.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10165.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5768, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.1084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4877.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(793.4042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1406],
        [ 0.1447],
        [ 0.1543],
        ...,
        [-1.4475],
        [-1.4443],
        [-1.4435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221061.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0076],
        [1.0127],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368115.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0077],
        [1.0128],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368126.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0106,  0.0442,  0.0148,  ...,  0.0066,  0.0441,  0.0358],
        [-0.0076,  0.0319,  0.0104,  ...,  0.0048,  0.0318,  0.0252],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [-0.0039,  0.0167,  0.0051,  ...,  0.0027,  0.0166,  0.0122]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1075.2029, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7258, device='cuda:0')



h[100].sum tensor(-3.4847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0510, device='cuda:0')



h[200].sum tensor(-22.5232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1682, 0.0559,  ..., 0.0251, 0.1677, 0.1358],
        [0.0000, 0.0822, 0.0263,  ..., 0.0130, 0.0818, 0.0638],
        [0.0000, 0.0453, 0.0141,  ..., 0.0078, 0.0450, 0.0340],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0180, 0.0052,  ..., 0.0040, 0.0178, 0.0125],
        [0.0000, 0.0316, 0.0092,  ..., 0.0059, 0.0314, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63250.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0373, 0.0000, 0.0000,  ..., 0.4790, 0.3584, 0.2789],
        [0.0457, 0.0039, 0.0000,  ..., 0.3146, 0.2230, 0.1826],
        [0.0530, 0.0092, 0.0064,  ..., 0.1900, 0.1229, 0.1085],
        ...,
        [0.0617, 0.0175, 0.0452,  ..., 0.0720, 0.0305, 0.0366],
        [0.0591, 0.0146, 0.0239,  ..., 0.1077, 0.0572, 0.0590],
        [0.0556, 0.0106, 0.0072,  ..., 0.1548, 0.0920, 0.0885]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546922.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9958.9756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.1951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4592.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(912.4133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1035],
        [ 0.0182],
        [-0.2454],
        ...,
        [-0.6999],
        [-0.4408],
        [-0.1621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203770.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0077],
        [1.0128],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368126.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0078],
        [1.0130],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368136.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(935.3637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7230, device='cuda:0')



h[100].sum tensor(-2.9702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4661, device='cuda:0')



h[200].sum tensor(-22.5544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54174.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0591, 0.0160, 0.0435,  ..., 0.0713, 0.0281, 0.0376],
        [0.0593, 0.0161, 0.0435,  ..., 0.0716, 0.0283, 0.0378],
        [0.0597, 0.0163, 0.0440,  ..., 0.0717, 0.0284, 0.0377],
        ...,
        [0.0634, 0.0195, 0.0682,  ..., 0.0505, 0.0155, 0.0227],
        [0.0634, 0.0195, 0.0682,  ..., 0.0505, 0.0155, 0.0227],
        [0.0634, 0.0195, 0.0682,  ..., 0.0505, 0.0155, 0.0227]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487712.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10069.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.6294, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4882.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(827.7864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2660],
        [-0.1963],
        [-0.1264],
        ...,
        [-1.4537],
        [-1.4506],
        [-1.4498]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223961.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0078],
        [1.0130],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368136.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0079],
        [1.0131],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368147.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [-0.0052,  0.0221,  0.0069,  ...,  0.0035,  0.0220,  0.0168],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1054.2262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4951, device='cuda:0')



h[100].sum tensor(-3.2691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7250, device='cuda:0')



h[200].sum tensor(-22.5343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0806, 0.0250,  ..., 0.0129, 0.0803, 0.0605],
        [0.0000, 0.0192, 0.0056,  ..., 0.0043, 0.0190, 0.0134],
        [0.0000, 0.0233, 0.0070,  ..., 0.0049, 0.0230, 0.0169],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61395.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0530, 0.0087, 0.0000,  ..., 0.1616, 0.0980, 0.0931],
        [0.0567, 0.0128, 0.0109,  ..., 0.1151, 0.0635, 0.0639],
        [0.0582, 0.0144, 0.0251,  ..., 0.1011, 0.0534, 0.0548],
        ...,
        [0.0633, 0.0193, 0.0681,  ..., 0.0506, 0.0156, 0.0230],
        [0.0633, 0.0193, 0.0681,  ..., 0.0507, 0.0156, 0.0230],
        [0.0633, 0.0193, 0.0681,  ..., 0.0506, 0.0156, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534700.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9880.8896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.0450, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4629.5439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(895.6762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5852],
        [-0.6979],
        [-0.8590],
        ...,
        [-1.4563],
        [-1.4530],
        [-1.4520]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187524.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0079],
        [1.0131],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368147.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0080],
        [1.0132],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368158.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0097,  0.0026,  ...,  0.0018,  0.0096,  0.0062],
        [-0.0044,  0.0188,  0.0058,  ...,  0.0030,  0.0187,  0.0140],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(552.1439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0097, device='cuda:0')



h[100].sum tensor(-1.7374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0469, device='cuda:0')



h[200].sum tensor(-22.6310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0487, 0.0145,  ..., 0.0084, 0.0484, 0.0350],
        [0.0000, 0.0260, 0.0072,  ..., 0.0052, 0.0257, 0.0173],
        [0.0000, 0.0691, 0.0210,  ..., 0.0113, 0.0688, 0.0506],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43389.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0479, 0.0028, 0.0000,  ..., 0.2262, 0.1421, 0.1349],
        [0.0522, 0.0069, 0.0000,  ..., 0.1685, 0.0987, 0.0989],
        [0.0530, 0.0076, 0.0000,  ..., 0.1643, 0.0962, 0.0960],
        ...,
        [0.0637, 0.0191, 0.0684,  ..., 0.0508, 0.0157, 0.0229],
        [0.0637, 0.0191, 0.0684,  ..., 0.0508, 0.0157, 0.0229],
        [0.0637, 0.0191, 0.0684,  ..., 0.0508, 0.0157, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450287.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10212.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.6179, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5052.6963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(732.8799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1541],
        [ 0.1321],
        [ 0.0728],
        ...,
        [-1.4748],
        [-1.4717],
        [-1.4709]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238317.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0080],
        [1.0132],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368158.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0081],
        [1.0133],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368169.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(822.0361, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3624, device='cuda:0')



h[100].sum tensor(-2.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9751, device='cuda:0')



h[200].sum tensor(-22.5780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0017, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53636.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0625, 0.0182, 0.0671,  ..., 0.0494, 0.0153, 0.0220],
        [0.0626, 0.0183, 0.0673,  ..., 0.0496, 0.0153, 0.0221],
        [0.0629, 0.0185, 0.0676,  ..., 0.0499, 0.0154, 0.0223],
        ...,
        [0.0640, 0.0187, 0.0673,  ..., 0.0525, 0.0166, 0.0239],
        [0.0642, 0.0189, 0.0688,  ..., 0.0510, 0.0158, 0.0228],
        [0.0642, 0.0189, 0.0688,  ..., 0.0510, 0.0158, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(499618.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10044.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.5000, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4532.4780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(835.8323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6829],
        [-1.6763],
        [-1.6500],
        ...,
        [-1.4278],
        [-1.4679],
        [-1.4840]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184846.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0081],
        [1.0133],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368169.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(518.9751, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0082],
        [1.0134],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368180.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0042,  0.0180,  0.0055,  ...,  0.0029,  0.0179,  0.0133],
        [-0.0043,  0.0184,  0.0057,  ...,  0.0030,  0.0183,  0.0136],
        [-0.0057,  0.0243,  0.0077,  ...,  0.0038,  0.0242,  0.0187],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(726.7302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6172, device='cuda:0')



h[100].sum tensor(-2.2724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7201, device='cuda:0')



h[200].sum tensor(-22.5950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0817, 0.0255,  ..., 0.0130, 0.0814, 0.0615],
        [0.0000, 0.0777, 0.0241,  ..., 0.0125, 0.0775, 0.0581],
        [0.0000, 0.0833, 0.0268,  ..., 0.0133, 0.0830, 0.0648],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48653.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0026, 0.0000,  ..., 0.2362, 0.1528, 0.1385],
        [0.0474, 0.0008, 0.0000,  ..., 0.2701, 0.1789, 0.1593],
        [0.0476, 0.0013, 0.0000,  ..., 0.2859, 0.1938, 0.1675],
        ...,
        [0.0649, 0.0191, 0.0695,  ..., 0.0512, 0.0159, 0.0224],
        [0.0649, 0.0191, 0.0695,  ..., 0.0512, 0.0159, 0.0224],
        [0.0649, 0.0191, 0.0695,  ..., 0.0512, 0.0159, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476500.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10458.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5156.0059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(777.8736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1208],
        [ 0.1431],
        [ 0.1434],
        ...,
        [-1.5161],
        [-1.5128],
        [-1.5119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257357.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0082],
        [1.0134],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368180.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0082],
        [1.0135],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368191.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0294,  0.0095,  ...,  0.0045,  0.0293,  0.0231],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        ...,
        [-0.0029,  0.0126,  0.0036,  ...,  0.0022,  0.0126,  0.0087],
        [-0.0065,  0.0276,  0.0089,  ...,  0.0043,  0.0275,  0.0216],
        [-0.0046,  0.0197,  0.0061,  ...,  0.0031,  0.0196,  0.0148]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(806.8583, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4860, device='cuda:0')



h[100].sum tensor(-2.5101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9932, device='cuda:0')



h[200].sum tensor(-22.5786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1092, 0.0359,  ..., 0.0169, 0.1090, 0.0871],
        [0.0000, 0.0711, 0.0225,  ..., 0.0115, 0.0709, 0.0544],
        [0.0000, 0.0329, 0.0097,  ..., 0.0061, 0.0327, 0.0235],
        ...,
        [0.0000, 0.0526, 0.0166,  ..., 0.0089, 0.0524, 0.0403],
        [0.0000, 0.0916, 0.0289,  ..., 0.0144, 0.0913, 0.0699],
        [0.0000, 0.1074, 0.0345,  ..., 0.0167, 0.1072, 0.0835]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50976.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0416, 0.0000, 0.0000,  ..., 0.4318, 0.3154, 0.2514],
        [0.0454, 0.0007, 0.0000,  ..., 0.3437, 0.2412, 0.2009],
        [0.0484, 0.0009, 0.0000,  ..., 0.2903, 0.1975, 0.1694],
        ...,
        [0.0544, 0.0066, 0.0007,  ..., 0.2023, 0.1260, 0.1172],
        [0.0500, 0.0015, 0.0000,  ..., 0.2635, 0.1711, 0.1555],
        [0.0502, 0.0019, 0.0000,  ..., 0.2682, 0.1767, 0.1574]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488471.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10556.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.0735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5257.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(799.5348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0333],
        [ 0.0395],
        [ 0.0441],
        ...,
        [-0.0994],
        [ 0.1015],
        [ 0.0806]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270846.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0082],
        [1.0135],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368191.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0083],
        [1.0136],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368202.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0092,  0.0392,  0.0130,  ...,  0.0059,  0.0391,  0.0315],
        [-0.0099,  0.0423,  0.0141,  ...,  0.0064,  0.0422,  0.0341],
        [-0.0091,  0.0389,  0.0129,  ...,  0.0059,  0.0388,  0.0312],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(675.9702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1044, device='cuda:0')



h[100].sum tensor(-2.0695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4991, device='cuda:0')



h[200].sum tensor(-22.6065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1391, 0.0457,  ..., 0.0212, 0.1389, 0.1108],
        [0.0000, 0.1431, 0.0471,  ..., 0.0217, 0.1428, 0.1142],
        [0.0000, 0.1323, 0.0433,  ..., 0.0202, 0.1321, 0.1049],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0017, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47013.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.2660e-02, 0.0000e+00, 0.0000e+00,  ..., 3.8875e-01, 2.8102e-01,
         2.2726e-01],
        [4.3821e-02, 0.0000e+00, 0.0000e+00,  ..., 3.7642e-01, 2.7184e-01,
         2.1930e-01],
        [4.6101e-02, 3.1877e-04, 0.0000e+00,  ..., 3.3924e-01, 2.4136e-01,
         1.9729e-01],
        ...,
        [6.5382e-02, 1.8919e-02, 6.9890e-02,  ..., 5.1330e-02, 1.5964e-02,
         2.2754e-02],
        [6.5384e-02, 1.8920e-02, 6.9892e-02,  ..., 5.1332e-02, 1.5965e-02,
         2.2755e-02],
        [6.5380e-02, 1.8918e-02, 6.9887e-02,  ..., 5.1329e-02, 1.5964e-02,
         2.2754e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471464.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10465.2842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.5600, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5095.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(767.5337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1124],
        [ 0.1038],
        [ 0.0500],
        ...,
        [-1.5383],
        [-1.5349],
        [-1.5340]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238888.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0083],
        [1.0136],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368202.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0083],
        [1.0137],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368213.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024,  0.0104,  0.0028,  ...,  0.0018,  0.0103,  0.0067],
        [-0.0037,  0.0160,  0.0048,  ...,  0.0026,  0.0160,  0.0116],
        [-0.0078,  0.0336,  0.0110,  ...,  0.0051,  0.0335,  0.0267],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(766.3849, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9895, device='cuda:0')



h[100].sum tensor(-2.2843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7745, device='cuda:0')



h[200].sum tensor(-22.5915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0376, 0.0107,  ..., 0.0068, 0.0374, 0.0255],
        [0.0000, 0.0846, 0.0265,  ..., 0.0135, 0.0844, 0.0639],
        [0.0000, 0.1070, 0.0344,  ..., 0.0167, 0.1068, 0.0831],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0017, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0017, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0017, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50102.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0501, 0.0043, 0.0013,  ..., 0.1931, 0.1117, 0.1169],
        [0.0450, 0.0008, 0.0000,  ..., 0.2817, 0.1826, 0.1699],
        [0.0421, 0.0000, 0.0000,  ..., 0.3438, 0.2337, 0.2064],
        ...,
        [0.0654, 0.0187, 0.0697,  ..., 0.0514, 0.0160, 0.0233],
        [0.0654, 0.0187, 0.0697,  ..., 0.0514, 0.0160, 0.0233],
        [0.0654, 0.0187, 0.0697,  ..., 0.0514, 0.0160, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485882.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10419.4814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.3459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5061.5776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(793.5179, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1322],
        [ 0.1498],
        [ 0.1412],
        ...,
        [-1.5407],
        [-1.5372],
        [-1.5360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235195.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0083],
        [1.0137],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368213.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0084],
        [1.0138],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368224.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035,  0.0155,  0.0046,  ...,  0.0026,  0.0154,  0.0111],
        [-0.0066,  0.0285,  0.0092,  ...,  0.0044,  0.0285,  0.0223],
        [-0.0016,  0.0074,  0.0018,  ...,  0.0014,  0.0073,  0.0041],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(704.7119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0921, device='cuda:0')



h[100].sum tensor(-2.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4973, device='cuda:0')



h[200].sum tensor(-22.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1169, 0.0378,  ..., 0.0181, 0.1167, 0.0915],
        [0.0000, 0.0644, 0.0193,  ..., 0.0106, 0.0642, 0.0465],
        [0.0000, 0.0607, 0.0180,  ..., 0.0101, 0.0605, 0.0433],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0013, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0013, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0018, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47949.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0436, 0.0000, 0.0000,  ..., 0.2955, 0.1948, 0.1788],
        [0.0450, 0.0000, 0.0000,  ..., 0.2583, 0.1604, 0.1583],
        [0.0467, 0.0011, 0.0000,  ..., 0.2278, 0.1338, 0.1409],
        ...,
        [0.0653, 0.0186, 0.0696,  ..., 0.0515, 0.0159, 0.0237],
        [0.0653, 0.0186, 0.0696,  ..., 0.0515, 0.0159, 0.0237],
        [0.0653, 0.0186, 0.0696,  ..., 0.0515, 0.0159, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478331.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10434.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.3241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5135.1479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(770.0367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1636],
        [ 0.1664],
        [ 0.1703],
        ...,
        [-1.5429],
        [-1.5396],
        [-1.5388]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239153.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0084],
        [1.0138],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368224.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0085],
        [1.0139],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368235.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0087,  0.0376,  0.0124,  ...,  0.0057,  0.0375,  0.0300],
        [-0.0090,  0.0389,  0.0129,  ...,  0.0059,  0.0389,  0.0312],
        [-0.0041,  0.0177,  0.0054,  ...,  0.0029,  0.0177,  0.0130],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0004,  0.0003, -0.0019],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0004,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(777.6011, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6731, device='cuda:0')



h[100].sum tensor(-2.2081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7283, device='cuda:0')



h[200].sum tensor(-22.5947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1160, 0.0375,  ..., 0.0180, 0.1158, 0.0907],
        [0.0000, 0.1166, 0.0377,  ..., 0.0181, 0.1164, 0.0912],
        [0.0000, 0.1116, 0.0359,  ..., 0.0174, 0.1114, 0.0869],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0018, 0.0013, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0018, 0.0013, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0018, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50019.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5012e-02, 1.0688e-04, 0.0000e+00,  ..., 3.0068e-01, 2.0700e-01,
         1.7913e-01],
        [4.4899e-02, 0.0000e+00, 0.0000e+00,  ..., 3.0289e-01, 2.0845e-01,
         1.8084e-01],
        [4.6894e-02, 1.8805e-03, 0.0000e+00,  ..., 2.7204e-01, 1.8301e-01,
         1.6277e-01],
        ...,
        [6.5228e-02, 1.8529e-02, 6.9544e-02,  ..., 5.1577e-02, 1.5924e-02,
         2.4085e-02],
        [6.5230e-02, 1.8530e-02, 6.9545e-02,  ..., 5.1579e-02, 1.5924e-02,
         2.4086e-02],
        [6.5226e-02, 1.8528e-02, 6.9541e-02,  ..., 5.1575e-02, 1.5923e-02,
         2.4084e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485827.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10299.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5759, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.2158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4901.3623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(791.5590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1469],
        [ 0.1583],
        [ 0.1673],
        ...,
        [-1.5451],
        [-1.5369],
        [-1.5161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203293.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0085],
        [1.0139],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368235.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0085],
        [1.0140],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368246.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0020,  0.0089,  0.0023,  ...,  0.0017,  0.0088,  0.0054],
        [-0.0019,  0.0083,  0.0021,  ...,  0.0016,  0.0083,  0.0049],
        [-0.0038,  0.0168,  0.0051,  ...,  0.0028,  0.0168,  0.0123],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(562.1959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4325, device='cuda:0')



h[100].sum tensor(-1.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9626, device='cuda:0')



h[200].sum tensor(-22.6362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0287, 0.0082,  ..., 0.0058, 0.0286, 0.0197],
        [0.0000, 0.0588, 0.0174,  ..., 0.0101, 0.0587, 0.0417],
        [0.0000, 0.0289, 0.0076,  ..., 0.0058, 0.0287, 0.0179],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0020, 0.0013, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0020, 0.0013, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0020, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43298.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0084, 0.0064,  ..., 0.1413, 0.0743, 0.0845],
        [0.0508, 0.0048, 0.0000,  ..., 0.1779, 0.0993, 0.1082],
        [0.0534, 0.0074, 0.0035,  ..., 0.1510, 0.0795, 0.0909],
        ...,
        [0.0654, 0.0190, 0.0701,  ..., 0.0517, 0.0160, 0.0236],
        [0.0654, 0.0190, 0.0701,  ..., 0.0517, 0.0160, 0.0236],
        [0.0654, 0.0190, 0.0701,  ..., 0.0517, 0.0160, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460876.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10510.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.9376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.3971, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5310.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(726.6718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0729],
        [ 0.1114],
        [ 0.0173],
        ...,
        [-1.5646],
        [-1.5612],
        [-1.5601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262924.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0085],
        [1.0140],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368246.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0086],
        [1.0142],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368256.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(888.4424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0278, device='cuda:0')



h[100].sum tensor(-2.4698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0723, device='cuda:0')



h[200].sum tensor(-22.5752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0020, 0.0012, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0021, 0.0012, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0021, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0021, 0.0012, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0021, 0.0012, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0021, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53143.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0629, 0.0179, 0.0630,  ..., 0.0559, 0.0186, 0.0264],
        [0.0629, 0.0177, 0.0615,  ..., 0.0577, 0.0195, 0.0277],
        [0.0630, 0.0177, 0.0603,  ..., 0.0595, 0.0203, 0.0290],
        ...,
        [0.0645, 0.0182, 0.0580,  ..., 0.0635, 0.0240, 0.0308],
        [0.0653, 0.0191, 0.0677,  ..., 0.0546, 0.0180, 0.0249],
        [0.0656, 0.0194, 0.0706,  ..., 0.0519, 0.0161, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503713.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10343.3965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.6118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5001.0713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(821.4595, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1050],
        [-0.9901],
        [-0.8790],
        ...,
        [-1.3117],
        [-1.4565],
        [-1.5359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231032.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0086],
        [1.0142],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368256.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0087],
        [1.0143],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368267.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040,  0.0175,  0.0054,  ...,  0.0030,  0.0174,  0.0128],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(642.1880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3254, device='cuda:0')



h[100].sum tensor(-1.7666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2392, device='cuda:0')



h[200].sum tensor(-22.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0280, 0.0080,  ..., 0.0059, 0.0278, 0.0191],
        [0.0000, 0.0185, 0.0054,  ..., 0.0046, 0.0184, 0.0129],
        [0.0000, 0.0013, 0.0000,  ..., 0.0021, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46575.0195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0534, 0.0076, 0.0070,  ..., 0.1874, 0.1173, 0.1081],
        [0.0570, 0.0113, 0.0137,  ..., 0.1477, 0.0890, 0.0826],
        [0.0600, 0.0143, 0.0211,  ..., 0.1151, 0.0651, 0.0619],
        ...,
        [0.0657, 0.0195, 0.0710,  ..., 0.0521, 0.0162, 0.0231],
        [0.0657, 0.0195, 0.0710,  ..., 0.0521, 0.0162, 0.0231],
        [0.0657, 0.0195, 0.0710,  ..., 0.0521, 0.0162, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480318.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10513.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.1557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5327.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(758.5019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0528],
        [ 0.0504],
        [ 0.0511],
        ...,
        [-1.5951],
        [-1.5916],
        [-1.5907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275675.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0087],
        [1.0143],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368267.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0088],
        [1.0144],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368277.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023,  0.0102,  0.0028,  ...,  0.0020,  0.0102,  0.0066],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(794.3646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7617, device='cuda:0')



h[100].sum tensor(-2.1674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7412, device='cuda:0')



h[200].sum tensor(-22.5938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0172, 0.0042,  ..., 0.0044, 0.0171, 0.0098],
        [0.0000, 0.0113, 0.0028,  ..., 0.0036, 0.0111, 0.0066],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0012, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0012, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52888.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0580, 0.0125, 0.0197,  ..., 0.1016, 0.0461, 0.0576],
        [0.0604, 0.0149, 0.0350,  ..., 0.0838, 0.0361, 0.0450],
        [0.0624, 0.0169, 0.0521,  ..., 0.0678, 0.0256, 0.0345],
        ...,
        [0.0658, 0.0194, 0.0711,  ..., 0.0523, 0.0163, 0.0232],
        [0.0658, 0.0194, 0.0711,  ..., 0.0523, 0.0163, 0.0232],
        [0.0658, 0.0194, 0.0711,  ..., 0.0523, 0.0163, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518630.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10340.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.8151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5007.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(821.5296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9775],
        [-1.0018],
        [-0.9414],
        ...,
        [-1.6036],
        [-1.6001],
        [-1.5991]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237702.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0088],
        [1.0144],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368277.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(491.3009, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0089],
        [1.0146],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368288.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0038,  0.0166,  0.0050,  ...,  0.0029,  0.0166,  0.0121],
        [-0.0065,  0.0287,  0.0093,  ...,  0.0046,  0.0286,  0.0224],
        [-0.0021,  0.0095,  0.0025,  ...,  0.0018,  0.0094,  0.0059],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(794.9158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5971, device='cuda:0')



h[100].sum tensor(-2.1544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7172, device='cuda:0')



h[200].sum tensor(-22.5938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1307, 0.0427,  ..., 0.0205, 0.1305, 0.1033],
        [0.0000, 0.0911, 0.0288,  ..., 0.0149, 0.0909, 0.0692],
        [0.0000, 0.0500, 0.0157,  ..., 0.0091, 0.0499, 0.0379],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0022, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0022, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0022, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51787.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0000, 0.0000,  ..., 0.3483, 0.2414, 0.2089],
        [0.0446, 0.0005, 0.0000,  ..., 0.3004, 0.2017, 0.1804],
        [0.0509, 0.0054, 0.0023,  ..., 0.2180, 0.1388, 0.1293],
        ...,
        [0.0659, 0.0191, 0.0711,  ..., 0.0527, 0.0164, 0.0236],
        [0.0659, 0.0191, 0.0711,  ..., 0.0527, 0.0164, 0.0236],
        [0.0659, 0.0191, 0.0711,  ..., 0.0527, 0.0164, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504481.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10281.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.6745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4808.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(817.5446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1273],
        [ 0.1273],
        [ 0.0987],
        ...,
        [-1.6102],
        [-1.6067],
        [-1.6057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212116.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0089],
        [1.0146],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368288.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0090],
        [1.0147],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368299.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [-0.0036,  0.0158,  0.0047,  ...,  0.0027,  0.0157,  0.0113],
        [-0.0069,  0.0305,  0.0099,  ...,  0.0048,  0.0304,  0.0239],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(908.4813, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3844, device='cuda:0')



h[100].sum tensor(-2.4481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1244, device='cuda:0')



h[200].sum tensor(-22.5726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0296, 0.0085,  ..., 0.0062, 0.0294, 0.0203],
        [0.0000, 0.0565, 0.0173,  ..., 0.0100, 0.0563, 0.0415],
        [0.0000, 0.0843, 0.0263,  ..., 0.0140, 0.0841, 0.0633],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0022, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0022, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0022, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54955.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.5758e-02, 8.6559e-03, 1.2862e-02,  ..., 1.4709e-01, 8.3745e-02,
         8.5134e-02],
        [5.0255e-02, 4.0573e-03, 0.0000e+00,  ..., 2.2594e-01, 1.4401e-01,
         1.3435e-01],
        [4.5416e-02, 1.7168e-04, 0.0000e+00,  ..., 2.9958e-01, 2.0081e-01,
         1.8009e-01],
        ...,
        [6.5041e-02, 1.7740e-02, 6.0833e-02,  ..., 6.2894e-02, 2.2665e-02,
         3.0443e-02],
        [6.6136e-02, 1.8972e-02, 7.1498e-02,  ..., 5.2989e-02, 1.6542e-02,
         2.3632e-02],
        [6.6132e-02, 1.8970e-02, 7.1495e-02,  ..., 5.2985e-02, 1.6541e-02,
         2.3630e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517742.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10351.3525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0636, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.5323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4860.5981, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(844.5634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1193],
        [ 0.1328],
        [ 0.1353],
        ...,
        [-1.2494],
        [-1.4492],
        [-1.5598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221306.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0090],
        [1.0147],
        ...,
        [1.0033],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368299.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0091],
        [1.0148],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368310., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0020,  0.0091,  0.0024,  ...,  0.0018,  0.0091,  0.0056],
        [-0.0052,  0.0230,  0.0073,  ...,  0.0038,  0.0230,  0.0175],
        [-0.0064,  0.0281,  0.0090,  ...,  0.0045,  0.0280,  0.0219],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(970.2979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8800, device='cuda:0')



h[100].sum tensor(-2.6116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3430, device='cuda:0')



h[200].sum tensor(-22.5601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0476, 0.0141,  ..., 0.0088, 0.0474, 0.0339],
        [0.0000, 0.0869, 0.0272,  ..., 0.0144, 0.0867, 0.0656],
        [0.0000, 0.1266, 0.0412,  ..., 0.0200, 0.1264, 0.0996],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56390.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.9494e-02, 9.7136e-04, 0.0000e+00,  ..., 2.4164e-01, 1.5519e-01,
         1.4455e-01],
        [4.6563e-02, 5.5415e-05, 0.0000e+00,  ..., 2.9015e-01, 1.9300e-01,
         1.7442e-01],
        [4.4919e-02, 0.0000e+00, 0.0000e+00,  ..., 3.2603e-01, 2.2236e-01,
         1.9590e-01],
        ...,
        [6.6648e-02, 1.8934e-02, 7.2208e-02,  ..., 5.3298e-02, 1.6741e-02,
         2.3502e-02],
        [6.6646e-02, 1.8933e-02, 7.2206e-02,  ..., 5.3296e-02, 1.6740e-02,
         2.3501e-02],
        [6.6643e-02, 1.8932e-02, 7.2202e-02,  ..., 5.3293e-02, 1.6739e-02,
         2.3500e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522290.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10511.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2072, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.8330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4996.8066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(857.4251, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0425],
        [ 0.0513],
        [ 0.0608],
        ...,
        [-1.6430],
        [-1.6393],
        [-1.6383]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253551.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0091],
        [1.0148],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368310., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0091],
        [1.0148],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368310., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0005,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(622.8576, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9651, device='cuda:0')



h[100].sum tensor(-1.6878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1865, device='cuda:0')



h[200].sum tensor(-22.6245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45329.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0647, 0.0182, 0.0704,  ..., 0.0515, 0.0162, 0.0226],
        [0.0649, 0.0182, 0.0706,  ..., 0.0517, 0.0162, 0.0227],
        [0.0653, 0.0184, 0.0708,  ..., 0.0521, 0.0164, 0.0229],
        ...,
        [0.0666, 0.0189, 0.0722,  ..., 0.0533, 0.0167, 0.0235],
        [0.0666, 0.0189, 0.0722,  ..., 0.0533, 0.0167, 0.0235],
        [0.0666, 0.0189, 0.0722,  ..., 0.0533, 0.0167, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474124.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10576.2627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.5051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5188.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(761.0753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8316],
        [-1.8450],
        [-1.8497],
        ...,
        [-1.6342],
        [-1.6332],
        [-1.6326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248684.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0091],
        [1.0148],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368310., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0092],
        [1.0149],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368321.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0019],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0019],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0019]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(615.0083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3499, device='cuda:0')



h[100].sum tensor(-1.6304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0966, device='cuda:0')



h[200].sum tensor(-22.6278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0022, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0023, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0023, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0023, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0023, 0.0011, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0023, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44683.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0648, 0.0179, 0.0705,  ..., 0.0518, 0.0163, 0.0226],
        [0.0650, 0.0180, 0.0706,  ..., 0.0520, 0.0164, 0.0227],
        [0.0653, 0.0182, 0.0709,  ..., 0.0524, 0.0165, 0.0229],
        ...,
        [0.0667, 0.0187, 0.0723,  ..., 0.0536, 0.0169, 0.0235],
        [0.0667, 0.0187, 0.0723,  ..., 0.0536, 0.0169, 0.0235],
        [0.0667, 0.0187, 0.0723,  ..., 0.0536, 0.0169, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470562.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10641.4795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0709, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.6311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5262.4502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(752.3888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6509],
        [-1.7450],
        [-1.8196],
        ...,
        [-1.6509],
        [-1.6471],
        [-1.6461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267393.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0092],
        [1.0149],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368321.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0093],
        [1.0150],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368332.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [-0.0021,  0.0097,  0.0026,  ...,  0.0019,  0.0096,  0.0061],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0006,  0.0003, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(745.8157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0398, device='cuda:0')



h[100].sum tensor(-1.9207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4897, device='cuda:0')



h[200].sum tensor(-22.6066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0153, 0.0034,  ..., 0.0043, 0.0151, 0.0080],
        [0.0000, 0.0108, 0.0026,  ..., 0.0037, 0.0106, 0.0061],
        [0.0000, 0.0091, 0.0020,  ..., 0.0034, 0.0089, 0.0046],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0024, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0024, 0.0012, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0024, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48293.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0574, 0.0092, 0.0149,  ..., 0.1094, 0.0489, 0.0644],
        [0.0598, 0.0117, 0.0271,  ..., 0.0931, 0.0398, 0.0523],
        [0.0609, 0.0128, 0.0340,  ..., 0.0872, 0.0363, 0.0480],
        ...,
        [0.0666, 0.0184, 0.0722,  ..., 0.0539, 0.0169, 0.0236],
        [0.0666, 0.0184, 0.0722,  ..., 0.0539, 0.0169, 0.0236],
        [0.0666, 0.0184, 0.0722,  ..., 0.0539, 0.0169, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485656.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10539.0869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.9449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5111.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(786.1935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1743],
        [-0.4365],
        [-0.7163],
        ...,
        [-1.6538],
        [-1.6502],
        [-1.6492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256807.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0093],
        [1.0150],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368332.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0093],
        [1.0151],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368343.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016,  0.0075,  0.0018,  ...,  0.0016,  0.0075,  0.0042],
        [-0.0016,  0.0075,  0.0018,  ...,  0.0016,  0.0075,  0.0042],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1002.0956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6241, device='cuda:0')



h[100].sum tensor(-2.4970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3056, device='cuda:0')



h[200].sum tensor(-22.5648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0276, 0.0070,  ..., 0.0062, 0.0273, 0.0164],
        [0.0000, 0.0211, 0.0047,  ..., 0.0052, 0.0208, 0.0109],
        [0.0000, 0.0146, 0.0031,  ..., 0.0043, 0.0143, 0.0073],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0025, 0.0012, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0025, 0.0012, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0025, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58159.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0541, 0.0054, 0.0045,  ..., 0.1367, 0.0663, 0.0831],
        [0.0549, 0.0065, 0.0077,  ..., 0.1272, 0.0591, 0.0772],
        [0.0578, 0.0098, 0.0212,  ..., 0.1067, 0.0472, 0.0626],
        ...,
        [0.0662, 0.0182, 0.0718,  ..., 0.0541, 0.0169, 0.0237],
        [0.0662, 0.0182, 0.0718,  ..., 0.0541, 0.0169, 0.0237],
        [0.0662, 0.0182, 0.0718,  ..., 0.0541, 0.0169, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542268.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10332.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.0894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4844.2231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(869.1255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0393],
        [-0.1024],
        [-0.3339],
        ...,
        [-1.6491],
        [-1.6456],
        [-1.6447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240744.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0093],
        [1.0151],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368343.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0094],
        [1.0152],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368354.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0006,  0.0003, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(677.1204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9399, device='cuda:0')



h[100].sum tensor(-1.5587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0367, device='cuda:0')



h[200].sum tensor(-22.6307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0026, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0026, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0026, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0027, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0027, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0027, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46274., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0632, 0.0171, 0.0680,  ..., 0.0541, 0.0167, 0.0248],
        [0.0637, 0.0174, 0.0693,  ..., 0.0532, 0.0165, 0.0236],
        [0.0642, 0.0177, 0.0701,  ..., 0.0530, 0.0165, 0.0232],
        ...,
        [0.0656, 0.0182, 0.0714,  ..., 0.0543, 0.0169, 0.0238],
        [0.0656, 0.0182, 0.0714,  ..., 0.0543, 0.0169, 0.0238],
        [0.0656, 0.0182, 0.0714,  ..., 0.0543, 0.0169, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477527.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10223.1475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2283, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.8026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4741.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(768.8102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1936],
        [-1.4130],
        [-1.5846],
        ...,
        [-1.6344],
        [-1.6308],
        [-1.6297]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224583.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0094],
        [1.0152],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368354.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0095],
        [1.0153],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368365.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(802.4941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2880, device='cuda:0')



h[100].sum tensor(-1.7974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3798, device='cuda:0')



h[200].sum tensor(-22.6129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0231, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0028, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0028, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0028, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0013, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0013, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48796.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0629, 0.0175, 0.0683,  ..., 0.0538, 0.0166, 0.0240],
        [0.0625, 0.0171, 0.0656,  ..., 0.0568, 0.0180, 0.0266],
        [0.0617, 0.0160, 0.0573,  ..., 0.0652, 0.0225, 0.0329],
        ...,
        [0.0652, 0.0185, 0.0713,  ..., 0.0544, 0.0168, 0.0236],
        [0.0652, 0.0185, 0.0713,  ..., 0.0544, 0.0168, 0.0236],
        [0.0652, 0.0185, 0.0713,  ..., 0.0544, 0.0168, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486471.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10034.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4747, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.4545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4529.9043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(793.8624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2344],
        [-0.9796],
        [-0.6639],
        ...,
        [-1.6419],
        [-1.6387],
        [-1.6378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205594.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0095],
        [1.0153],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368365.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0096],
        [1.0153],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368375.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1529.7162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0778, device='cuda:0')



h[100].sum tensor(-3.5835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6868, device='cuda:0')



h[200].sum tensor(-22.4828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0029, 0.0012, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0030, 0.0012, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0030, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0030, 0.0012, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0030, 0.0012, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0030, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74515.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0623, 0.0172, 0.0602,  ..., 0.0616, 0.0223, 0.0281],
        [0.0631, 0.0181, 0.0682,  ..., 0.0547, 0.0173, 0.0236],
        [0.0630, 0.0177, 0.0638,  ..., 0.0594, 0.0202, 0.0268],
        ...,
        [0.0650, 0.0190, 0.0717,  ..., 0.0547, 0.0168, 0.0232],
        [0.0650, 0.0190, 0.0717,  ..., 0.0547, 0.0168, 0.0232],
        [0.0650, 0.0190, 0.0717,  ..., 0.0547, 0.0168, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622351.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9750.2959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.9863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.8621, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4296.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1021.3439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6390],
        [-0.9995],
        [-1.1792],
        ...,
        [-1.6541],
        [-1.6508],
        [-1.6500]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198539.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0096],
        [1.0153],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368375.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(483.0880, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0096],
        [1.0154],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368385.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0123,  0.0035,  ...,  0.0024,  0.0121,  0.0082],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1054.8521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8284, device='cuda:0')



h[100].sum tensor(-2.4030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1893, device='cuda:0')



h[200].sum tensor(-22.5672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0317, 0.0092,  ..., 0.0073, 0.0313, 0.0218],
        [0.0000, 0.0134, 0.0035,  ..., 0.0047, 0.0130, 0.0082],
        [0.0000, 0.0015, 0.0000,  ..., 0.0030, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0030, 0.0011, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0030, 0.0011, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0030, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58021.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0535, 0.0069, 0.0067,  ..., 0.1726, 0.1024, 0.0994],
        [0.0591, 0.0129, 0.0275,  ..., 0.1069, 0.0541, 0.0571],
        [0.0629, 0.0173, 0.0550,  ..., 0.0686, 0.0270, 0.0319],
        ...,
        [0.0656, 0.0196, 0.0729,  ..., 0.0550, 0.0170, 0.0226],
        [0.0656, 0.0196, 0.0729,  ..., 0.0550, 0.0170, 0.0226],
        [0.0656, 0.0196, 0.0729,  ..., 0.0550, 0.0170, 0.0226]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540955.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10148.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3818, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.8723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4759.9619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(873.8131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1000],
        [-0.4293],
        [-0.7722],
        ...,
        [-1.6843],
        [-1.6808],
        [-1.6799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259223.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0096],
        [1.0154],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368385.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0154],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368395.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [-0.0024,  0.0109,  0.0030,  ...,  0.0023,  0.0108,  0.0071],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1243.7617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7274, device='cuda:0')



h[100].sum tensor(-2.8842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9051, device='cuda:0')



h[200].sum tensor(-22.5308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0009, 0.0000],
        [0.0000, 0.0121, 0.0030,  ..., 0.0045, 0.0116, 0.0071],
        [0.0000, 0.0101, 0.0023,  ..., 0.0042, 0.0097, 0.0055],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0010, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0010, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62417.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0634, 0.0180, 0.0632,  ..., 0.0617, 0.0218, 0.0271],
        [0.0615, 0.0156, 0.0430,  ..., 0.0809, 0.0334, 0.0405],
        [0.0597, 0.0137, 0.0292,  ..., 0.0957, 0.0409, 0.0516],
        ...,
        [0.0663, 0.0199, 0.0740,  ..., 0.0553, 0.0173, 0.0223],
        [0.0662, 0.0199, 0.0740,  ..., 0.0553, 0.0173, 0.0223],
        [0.0662, 0.0199, 0.0740,  ..., 0.0553, 0.0173, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559829.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10146.4932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.2828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4512.5454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(922.7978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3464],
        [-0.9952],
        [-0.6062],
        ...,
        [-1.7039],
        [-1.7001],
        [-1.6992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234334.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0154],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368395.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0155],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368404.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0034,  0.0153,  0.0045,  ...,  0.0029,  0.0152,  0.0108],
        [-0.0059,  0.0269,  0.0086,  ...,  0.0045,  0.0267,  0.0207],
        [-0.0107,  0.0482,  0.0161,  ...,  0.0076,  0.0481,  0.0390],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(872.7789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2405, device='cuda:0')



h[100].sum tensor(-1.9580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6651, device='cuda:0')



h[200].sum tensor(-22.5977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0504, 0.0158,  ..., 0.0100, 0.0500, 0.0380],
        [0.0000, 0.1169, 0.0377,  ..., 0.0195, 0.1163, 0.0908],
        [0.0000, 0.1438, 0.0471,  ..., 0.0234, 0.1432, 0.1138],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0009, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0009, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50966.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.0720e-02, 4.8974e-03, 2.4817e-04,  ..., 2.2714e-01, 1.4356e-01,
         1.3350e-01],
        [4.3886e-02, 1.4977e-04, 0.0000e+00,  ..., 3.4686e-01, 2.3861e-01,
         2.0675e-01],
        [3.9700e-02, 0.0000e+00, 0.0000e+00,  ..., 4.3605e-01, 3.1181e-01,
         2.6018e-01],
        ...,
        [6.5096e-02, 1.8314e-02, 5.7440e-02,  ..., 7.1102e-02, 2.7984e-02,
         3.2467e-02],
        [6.6524e-02, 2.0076e-02, 7.4542e-02,  ..., 5.5503e-02, 1.7431e-02,
         2.2102e-02],
        [6.6522e-02, 2.0075e-02, 7.4540e-02,  ..., 5.5501e-02, 1.7430e-02,
         2.2101e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504358.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10339.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.3043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4875.1460, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(821.5261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1546],
        [ 0.1672],
        [ 0.1617],
        ...,
        [-0.9704],
        [-1.3522],
        [-1.5787]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268119.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0155],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368404.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0156],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368414.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030,  0.0136,  0.0039,  ...,  0.0026,  0.0135,  0.0093],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [-0.0030,  0.0136,  0.0039,  ...,  0.0026,  0.0135,  0.0093],
        ...,
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020],
        [ 0.0000,  0.0003, -0.0007,  ...,  0.0007,  0.0002, -0.0020]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(992.1512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0506, device='cuda:0')



h[100].sum tensor(-2.2321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0757, device='cuda:0')



h[200].sum tensor(-22.5765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0122, 0.0031,  ..., 0.0046, 0.0118, 0.0073],
        [0.0000, 0.0499, 0.0141,  ..., 0.0100, 0.0494, 0.0334],
        [0.0000, 0.0123, 0.0031,  ..., 0.0046, 0.0119, 0.0073],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0009, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0009, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55684.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0605, 0.0141, 0.0282,  ..., 0.0961, 0.0443, 0.0500],
        [0.0566, 0.0095, 0.0000,  ..., 0.1348, 0.0686, 0.0766],
        [0.0576, 0.0109, 0.0063,  ..., 0.1233, 0.0594, 0.0696],
        ...,
        [0.0666, 0.0200, 0.0747,  ..., 0.0557, 0.0175, 0.0222],
        [0.0666, 0.0200, 0.0747,  ..., 0.0557, 0.0175, 0.0222],
        [0.0666, 0.0200, 0.0747,  ..., 0.0557, 0.0175, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532994.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10234.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.4751, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4696.0498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(865.7398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6165],
        [-0.2341],
        [ 0.0235],
        ...,
        [-1.7371],
        [-1.7333],
        [-1.7323]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237494.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0156],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368414.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0100],
        [1.0156],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368424.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [-0.0018,  0.0085,  0.0021,  ...,  0.0019,  0.0084,  0.0049],
        [-0.0017,  0.0081,  0.0020,  ...,  0.0019,  0.0080,  0.0046],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(790.7977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8847, device='cuda:0')



h[100].sum tensor(-1.6912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3209, device='cuda:0')



h[200].sum tensor(-22.6158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0021,  ..., 0.0042, 0.0092, 0.0050],
        [0.0000, 0.0284, 0.0079,  ..., 0.0069, 0.0279, 0.0188],
        [0.0000, 0.0576, 0.0167,  ..., 0.0111, 0.0570, 0.0397],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0031, 0.0011, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0031, 0.0011, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0031, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48379.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0586, 0.0126, 0.0276,  ..., 0.1017, 0.0460, 0.0558],
        [0.0548, 0.0078, 0.0080,  ..., 0.1442, 0.0747, 0.0842],
        [0.0516, 0.0037, 0.0000,  ..., 0.1811, 0.0997, 0.1088],
        ...,
        [0.0663, 0.0196, 0.0740,  ..., 0.0559, 0.0174, 0.0226],
        [0.0663, 0.0196, 0.0740,  ..., 0.0559, 0.0174, 0.0225],
        [0.0663, 0.0196, 0.0740,  ..., 0.0559, 0.0174, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493180.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10225.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4168, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.6690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4716.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(799.6811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7015e-01],
        [-1.2301e-03],
        [ 1.1861e-01],
        ...,
        [-1.7316e+00],
        [-1.7280e+00],
        [-1.7270e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244932.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0100],
        [1.0156],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368424.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0101],
        [1.0156],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368435.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        ...,
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1056.3269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6522, device='cuda:0')



h[100].sum tensor(-2.2778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1636, device='cuda:0')



h[200].sum tensor(-22.5709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0012, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0012, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0012, 0.0000],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0012, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0012, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54402.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0640, 0.0185, 0.0714,  ..., 0.0541, 0.0166, 0.0221],
        [0.0642, 0.0185, 0.0716,  ..., 0.0543, 0.0166, 0.0222],
        [0.0645, 0.0187, 0.0718,  ..., 0.0547, 0.0167, 0.0224],
        ...,
        [0.0659, 0.0193, 0.0733,  ..., 0.0560, 0.0171, 0.0230],
        [0.0659, 0.0193, 0.0733,  ..., 0.0560, 0.0171, 0.0230],
        [0.0659, 0.0193, 0.0733,  ..., 0.0560, 0.0171, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513580.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10069.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.9623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4557.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(850.0117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5109],
        [-1.6922],
        [-1.8201],
        ...,
        [-1.7257],
        [-1.7222],
        [-1.7213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223114.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0101],
        [1.0156],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368435.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0102],
        [1.0157],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368445.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        ...,
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1273.5190, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0958, device='cuda:0')



h[100].sum tensor(-2.7566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8128, device='cuda:0')



h[200].sum tensor(-22.5336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63806.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0155, 0.0480,  ..., 0.0754, 0.0299, 0.0373],
        [0.0621, 0.0162, 0.0525,  ..., 0.0715, 0.0277, 0.0345],
        [0.0596, 0.0129, 0.0307,  ..., 0.1115, 0.0593, 0.0596],
        ...,
        [0.0658, 0.0190, 0.0729,  ..., 0.0561, 0.0169, 0.0234],
        [0.0658, 0.0190, 0.0729,  ..., 0.0561, 0.0169, 0.0234],
        [0.0658, 0.0190, 0.0729,  ..., 0.0561, 0.0169, 0.0234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573616.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9847.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.8322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4260.4980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(934.2827, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0188],
        [ 0.0249],
        [ 0.0985],
        ...,
        [-1.7262],
        [-1.7228],
        [-1.7220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196733.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0102],
        [1.0157],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368445.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0103],
        [1.0158],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368454.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0044,  0.0202,  0.0061,  ...,  0.0036,  0.0200,  0.0148],
        [-0.0064,  0.0293,  0.0094,  ...,  0.0049,  0.0292,  0.0226],
        [-0.0044,  0.0202,  0.0061,  ...,  0.0036,  0.0200,  0.0148],
        ...,
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0008,  ...,  0.0008,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1231.8843, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1185, device='cuda:0')



h[100].sum tensor(-2.6181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6700, device='cuda:0')



h[200].sum tensor(-22.5428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1081, 0.0342,  ..., 0.0183, 0.1074, 0.0825],
        [0.0000, 0.1157, 0.0369,  ..., 0.0194, 0.1150, 0.0890],
        [0.0000, 0.1972, 0.0654,  ..., 0.0311, 0.1963, 0.1587],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0032, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60728.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0375, 0.0000, 0.0000,  ..., 0.3699, 0.2589, 0.2251],
        [0.0344, 0.0000, 0.0000,  ..., 0.4240, 0.3046, 0.2581],
        [0.0285, 0.0000, 0.0000,  ..., 0.5339, 0.3974, 0.3245],
        ...,
        [0.0658, 0.0194, 0.0731,  ..., 0.0562, 0.0169, 0.0232],
        [0.0657, 0.0194, 0.0731,  ..., 0.0562, 0.0169, 0.0232],
        [0.0651, 0.0188, 0.0677,  ..., 0.0612, 0.0200, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548000.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9897.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.0851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4416.1196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(906.1180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2070],
        [ 0.2015],
        [ 0.1956],
        ...,
        [-1.6860],
        [-1.5944],
        [-1.4459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219773.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0103],
        [1.0158],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368454.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0103],
        [1.0158],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368464.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1059.0371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7382, device='cuda:0')



h[100].sum tensor(-2.1690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0300, device='cuda:0')



h[200].sum tensor(-22.5760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0032, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0032, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0032, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0033, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0033, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0033, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55653.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0628, 0.0182, 0.0627,  ..., 0.0624, 0.0213, 0.0277],
        [0.0638, 0.0190, 0.0702,  ..., 0.0559, 0.0171, 0.0232],
        [0.0643, 0.0194, 0.0719,  ..., 0.0548, 0.0164, 0.0223],
        ...,
        [0.0658, 0.0200, 0.0734,  ..., 0.0561, 0.0168, 0.0229],
        [0.0658, 0.0200, 0.0734,  ..., 0.0561, 0.0168, 0.0229],
        [0.0658, 0.0200, 0.0734,  ..., 0.0561, 0.0168, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528706.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9881.5137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.9099, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4412.5361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(862.9803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8926],
        [-1.2437],
        [-1.4998],
        ...,
        [-1.7393],
        [-1.7357],
        [-1.7346]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213057.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0103],
        [1.0158],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368464.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0104],
        [1.0159],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368473.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0095,  0.0435,  0.0144,  ...,  0.0070,  0.0433,  0.0347],
        [-0.0104,  0.0478,  0.0159,  ...,  0.0076,  0.0476,  0.0384],
        [-0.0066,  0.0306,  0.0098,  ...,  0.0051,  0.0304,  0.0237],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(760.6346, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2328, device='cuda:0')



h[100].sum tensor(-1.4585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0795, device='cuda:0')



h[200].sum tensor(-22.6297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.2028, 0.0675,  ..., 0.0320, 0.2020, 0.1634],
        [0.0000, 0.1701, 0.0560,  ..., 0.0273, 0.1693, 0.1354],
        [0.0000, 0.1300, 0.0420,  ..., 0.0216, 0.1293, 0.1011],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0033, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0033, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0033, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45956.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0275, 0.0000, 0.0000,  ..., 0.5358, 0.3992, 0.3222],
        [0.0298, 0.0000, 0.0000,  ..., 0.4944, 0.3632, 0.2976],
        [0.0342, 0.0000, 0.0000,  ..., 0.4235, 0.3025, 0.2551],
        ...,
        [0.0660, 0.0204, 0.0737,  ..., 0.0562, 0.0167, 0.0226],
        [0.0660, 0.0204, 0.0737,  ..., 0.0562, 0.0167, 0.0226],
        [0.0660, 0.0204, 0.0737,  ..., 0.0561, 0.0167, 0.0226]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487310.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10106.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.1555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4759.3613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(771.0969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1472],
        [ 0.1456],
        [ 0.1478],
        ...,
        [-1.7625],
        [-1.7590],
        [-1.7580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245774.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0104],
        [1.0159],
        ...,
        [1.0032],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368473.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(510.7544, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0105],
        [1.0159],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368481.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0069,  0.0015,  ...,  0.0017,  0.0068,  0.0034],
        [-0.0014,  0.0069,  0.0015,  ...,  0.0017,  0.0068,  0.0034],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0008,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(862.0032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7576, device='cuda:0')



h[100].sum tensor(-1.6995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4484, device='cuda:0')



h[200].sum tensor(-22.6102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0135, 0.0027,  ..., 0.0048, 0.0131, 0.0058],
        [0.0000, 0.0135, 0.0027,  ..., 0.0048, 0.0131, 0.0058],
        [0.0000, 0.0135, 0.0027,  ..., 0.0049, 0.0131, 0.0058],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0032, 0.0013, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0032, 0.0013, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0032, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48254.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0569, 0.0148, 0.0302,  ..., 0.0961, 0.0354, 0.0539],
        [0.0563, 0.0144, 0.0264,  ..., 0.1004, 0.0371, 0.0572],
        [0.0572, 0.0150, 0.0301,  ..., 0.0975, 0.0355, 0.0548],
        ...,
        [0.0667, 0.0208, 0.0743,  ..., 0.0563, 0.0167, 0.0225],
        [0.0667, 0.0208, 0.0743,  ..., 0.0563, 0.0167, 0.0225],
        [0.0667, 0.0207, 0.0743,  ..., 0.0563, 0.0167, 0.0225]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(497433.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10125.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.5608, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4686.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(794.2443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4843],
        [-0.3782],
        [-0.3919],
        ...,
        [-1.7859],
        [-1.7822],
        [-1.7812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221085.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0105],
        [1.0159],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368481.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0106],
        [1.0160],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368490.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021],
        [-0.0033,  0.0156,  0.0046,  ...,  0.0029,  0.0155,  0.0109],
        [-0.0064,  0.0296,  0.0095,  ...,  0.0049,  0.0294,  0.0228],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(703.4329, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.6368, device='cuda:0')



h[100].sum tensor(-1.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9925, device='cuda:0')



h[200].sum tensor(-22.6351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0169, 0.0046,  ..., 0.0052, 0.0166, 0.0110],
        [0.0000, 0.0552, 0.0173,  ..., 0.0107, 0.0548, 0.0416],
        [0.0000, 0.1343, 0.0436,  ..., 0.0220, 0.1337, 0.1048],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0031, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44765.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0586, 0.0146, 0.0261,  ..., 0.1312, 0.0738, 0.0704],
        [0.0506, 0.0088, 0.0013,  ..., 0.2333, 0.1539, 0.1342],
        [0.0401, 0.0028, 0.0000,  ..., 0.3781, 0.2692, 0.2239],
        ...,
        [0.0676, 0.0209, 0.0751,  ..., 0.0565, 0.0168, 0.0225],
        [0.0676, 0.0209, 0.0751,  ..., 0.0565, 0.0168, 0.0224],
        [0.0675, 0.0209, 0.0751,  ..., 0.0565, 0.0168, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489445.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10472.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.0449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.1694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5034.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(756.2817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5180],
        [-0.1383],
        [ 0.0733],
        ...,
        [-1.8149],
        [-1.8110],
        [-1.8100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272777.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0106],
        [1.0160],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368490.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0107],
        [1.0161],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368499.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023,  0.0108,  0.0029,  ...,  0.0022,  0.0107,  0.0067],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021],
        [-0.0037,  0.0173,  0.0052,  ...,  0.0031,  0.0172,  0.0124],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0021]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(940.1676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2123, device='cuda:0')



h[100].sum tensor(-1.9274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8071, device='cuda:0')



h[200].sum tensor(-22.5906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0553, 0.0166,  ..., 0.0105, 0.0550, 0.0395],
        [0.0000, 0.0516, 0.0146,  ..., 0.0100, 0.0513, 0.0342],
        [0.0000, 0.0365, 0.0108,  ..., 0.0079, 0.0362, 0.0256],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0029, 0.0013, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0029, 0.0013, 0.0000],
        [0.0000, 0.0240, 0.0071,  ..., 0.0061, 0.0238, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50953.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0459, 0.0050, 0.0000,  ..., 0.2569, 0.1621, 0.1529],
        [0.0487, 0.0072, 0.0000,  ..., 0.2156, 0.1270, 0.1280],
        [0.0510, 0.0089, 0.0000,  ..., 0.2018, 0.1190, 0.1183],
        ...,
        [0.0679, 0.0205, 0.0711,  ..., 0.0606, 0.0199, 0.0252],
        [0.0662, 0.0193, 0.0518,  ..., 0.0778, 0.0325, 0.0362],
        [0.0616, 0.0159, 0.0243,  ..., 0.1284, 0.0707, 0.0685]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516747.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10514.0146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.5719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4939.8716, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(813.5312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1590],
        [ 0.1637],
        [ 0.1643],
        ...,
        [-1.6051],
        [-1.3063],
        [-0.8922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270113.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0107],
        [1.0161],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368499.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0107],
        [1.0162],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368509.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040,  0.0187,  0.0057,  ...,  0.0033,  0.0186,  0.0135],
        [-0.0035,  0.0163,  0.0048,  ...,  0.0030,  0.0162,  0.0115],
        [-0.0034,  0.0160,  0.0047,  ...,  0.0029,  0.0159,  0.0112],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(932.6401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6155, device='cuda:0')



h[100].sum tensor(-1.8849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7199, device='cuda:0')



h[200].sum tensor(-22.5930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0680, 0.0203,  ..., 0.0123, 0.0677, 0.0481],
        [0.0000, 0.0893, 0.0277,  ..., 0.0153, 0.0890, 0.0663],
        [0.0000, 0.0855, 0.0264,  ..., 0.0148, 0.0852, 0.0630],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0028, 0.0014, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0028, 0.0014, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0028, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51696.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.2968e-02, 3.6349e-03, 0.0000e+00,  ..., 2.7208e-01, 1.7102e-01,
         1.6480e-01],
        [3.8792e-02, 7.0780e-04, 0.0000e+00,  ..., 3.2596e-01, 2.1409e-01,
         1.9844e-01],
        [3.8530e-02, 2.1315e-04, 0.0000e+00,  ..., 3.3452e-01, 2.2086e-01,
         2.0366e-01],
        ...,
        [6.8146e-02, 2.0566e-02, 7.5041e-02,  ..., 5.6870e-02, 1.6856e-02,
         2.3194e-02],
        [6.8135e-02, 2.0562e-02, 7.5030e-02,  ..., 5.6860e-02, 1.6854e-02,
         2.3190e-02],
        [6.8133e-02, 2.0560e-02, 7.5028e-02,  ..., 5.6858e-02, 1.6853e-02,
         2.3188e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522111.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10436.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.7396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4867.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(820.4338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1359],
        [ 0.1603],
        [ 0.1647],
        ...,
        [-1.7507],
        [-1.6839],
        [-1.6407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253715.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0107],
        [1.0162],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368509.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0108],
        [1.0162],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368519.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0084,  0.0020,  ...,  0.0018,  0.0083,  0.0046],
        [-0.0018,  0.0085,  0.0021,  ...,  0.0018,  0.0085,  0.0048],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(808.5219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7023, device='cuda:0')



h[100].sum tensor(-1.5601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2942, device='cuda:0')



h[200].sum tensor(-22.6179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0573, 0.0165,  ..., 0.0107, 0.0571, 0.0389],
        [0.0000, 0.0287, 0.0080,  ..., 0.0067, 0.0284, 0.0187],
        [0.0000, 0.0099, 0.0021,  ..., 0.0040, 0.0097, 0.0048],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47008.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0085, 0.0000,  ..., 0.1824, 0.0986, 0.1097],
        [0.0536, 0.0114, 0.0061,  ..., 0.1480, 0.0752, 0.0869],
        [0.0584, 0.0147, 0.0215,  ..., 0.1099, 0.0490, 0.0617],
        ...,
        [0.0678, 0.0204, 0.0744,  ..., 0.0571, 0.0167, 0.0238],
        [0.0678, 0.0204, 0.0744,  ..., 0.0570, 0.0167, 0.0237],
        [0.0678, 0.0204, 0.0744,  ..., 0.0570, 0.0167, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498277.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10487.6279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.5359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4995.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(772.8182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1497],
        [ 0.1363],
        [ 0.1046],
        ...,
        [-1.8362],
        [-1.8320],
        [-1.8301]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273207.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0108],
        [1.0162],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368519.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0109],
        [1.0163],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368529.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1080.9431, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1989, device='cuda:0')



h[100].sum tensor(-2.1064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0973, device='cuda:0')



h[200].sum tensor(-22.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0127, 0.0023,  ..., 0.0044, 0.0125, 0.0049],
        [0.0000, 0.0072, 0.0012,  ..., 0.0036, 0.0071, 0.0025],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0017, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0017, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55256.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0589, 0.0157, 0.0395,  ..., 0.0879, 0.0304, 0.0486],
        [0.0611, 0.0169, 0.0492,  ..., 0.0786, 0.0263, 0.0412],
        [0.0637, 0.0183, 0.0606,  ..., 0.0677, 0.0215, 0.0327],
        ...,
        [0.0675, 0.0202, 0.0739,  ..., 0.0573, 0.0165, 0.0242],
        [0.0675, 0.0202, 0.0739,  ..., 0.0573, 0.0165, 0.0242],
        [0.0675, 0.0202, 0.0739,  ..., 0.0573, 0.0165, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538560., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10216.6416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.4821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4746.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(847.1064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2273],
        [-1.3842],
        [-1.5610],
        ...,
        [-1.8338],
        [-1.8300],
        [-1.8289]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239865.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0109],
        [1.0163],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368529.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0110],
        [1.0164],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368539.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(857.4067, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8825, device='cuda:0')



h[100].sum tensor(-1.5748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3206, device='cuda:0')



h[200].sum tensor(-22.6151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0016, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0016, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0017, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0017, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49726.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0652, 0.0196, 0.0721,  ..., 0.0553, 0.0158, 0.0232],
        [0.0654, 0.0197, 0.0723,  ..., 0.0555, 0.0158, 0.0233],
        [0.0658, 0.0199, 0.0726,  ..., 0.0559, 0.0159, 0.0235],
        ...,
        [0.0673, 0.0205, 0.0741,  ..., 0.0574, 0.0163, 0.0242],
        [0.0673, 0.0205, 0.0741,  ..., 0.0574, 0.0163, 0.0242],
        [0.0673, 0.0205, 0.0741,  ..., 0.0574, 0.0163, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515614.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10150.4248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.8238, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4796.0698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(798.7508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9092],
        [-1.9727],
        [-2.0243],
        ...,
        [-1.8281],
        [-1.8265],
        [-1.8267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237354.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0110],
        [1.0164],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368539.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0111],
        [1.0165],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368548.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(818.8939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8267, device='cuda:0')



h[100].sum tensor(-1.4473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1663, device='cuda:0')



h[200].sum tensor(-22.6246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0210, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0101, 0.0022,  ..., 0.0042, 0.0099, 0.0049],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0016, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0016, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0016, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47173.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0581, 0.0162, 0.0291,  ..., 0.1083, 0.0505, 0.0593],
        [0.0623, 0.0185, 0.0512,  ..., 0.0754, 0.0279, 0.0374],
        [0.0633, 0.0191, 0.0578,  ..., 0.0701, 0.0242, 0.0338],
        ...,
        [0.0672, 0.0209, 0.0745,  ..., 0.0575, 0.0162, 0.0239],
        [0.0672, 0.0209, 0.0745,  ..., 0.0575, 0.0162, 0.0239],
        [0.0672, 0.0209, 0.0745,  ..., 0.0575, 0.0162, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503036.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10255.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.2950, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5049.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(767.7486, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2727],
        [-0.5861],
        [-0.7703],
        ...,
        [-1.8514],
        [-1.8475],
        [-1.8464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269001.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0111],
        [1.0165],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368548.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0112],
        [1.0166],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368557.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1009.9637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0067, device='cuda:0')



h[100].sum tensor(-1.8295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7770, device='cuda:0')



h[200].sum tensor(-22.5926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0015, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0015, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0031, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50854.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0655, 0.0203, 0.0733,  ..., 0.0556, 0.0157, 0.0227],
        [0.0657, 0.0204, 0.0735,  ..., 0.0558, 0.0157, 0.0228],
        [0.0660, 0.0206, 0.0737,  ..., 0.0562, 0.0158, 0.0230],
        ...,
        [0.0676, 0.0213, 0.0753,  ..., 0.0576, 0.0162, 0.0237],
        [0.0676, 0.0212, 0.0753,  ..., 0.0576, 0.0162, 0.0237],
        [0.0676, 0.0212, 0.0753,  ..., 0.0576, 0.0162, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513300.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10183.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6243, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.6344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4969.1924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(804.9209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0141],
        [-2.0495],
        [-2.0732],
        ...,
        [-1.8676],
        [-1.8629],
        [-1.8614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243923.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0112],
        [1.0166],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368557.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0113],
        [1.0167],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368566.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [-0.0055,  0.0260,  0.0083,  ...,  0.0044,  0.0259,  0.0197],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(941.2083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1876, device='cuda:0')



h[100].sum tensor(-1.6597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5113, device='cuda:0')



h[200].sum tensor(-22.6056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0014, 0.0000],
        [0.0000, 0.0487, 0.0150,  ..., 0.0097, 0.0483, 0.0357],
        [0.0000, 0.0960, 0.0309,  ..., 0.0165, 0.0956, 0.0738],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0014, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0014, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0031, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49942.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0591, 0.0169, 0.0269,  ..., 0.1186, 0.0615, 0.0633],
        [0.0501, 0.0124, 0.0056,  ..., 0.2129, 0.1328, 0.1235],
        [0.0415, 0.0082, 0.0000,  ..., 0.3107, 0.2084, 0.1853],
        ...,
        [0.0679, 0.0214, 0.0759,  ..., 0.0578, 0.0162, 0.0235],
        [0.0679, 0.0214, 0.0758,  ..., 0.0578, 0.0162, 0.0235],
        [0.0679, 0.0214, 0.0758,  ..., 0.0578, 0.0162, 0.0235]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515326.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10299.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.6840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5148.6489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(794.4357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0208],
        [ 0.0706],
        [ 0.1090],
        ...,
        [-1.8898],
        [-1.8857],
        [-1.8845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269107.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0113],
        [1.0167],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368566.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(492.2755, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0114],
        [1.0168],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368575.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0077,  0.0018,  ...,  0.0018,  0.0076,  0.0040],
        [-0.0019,  0.0093,  0.0024,  ...,  0.0020,  0.0092,  0.0053],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1193.4551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6118, device='cuda:0')



h[100].sum tensor(-2.1585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3038, device='cuda:0')



h[200].sum tensor(-22.5634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0602, 0.0176,  ..., 0.0114, 0.0598, 0.0410],
        [0.0000, 0.0287, 0.0080,  ..., 0.0069, 0.0284, 0.0186],
        [0.0000, 0.0194, 0.0047,  ..., 0.0055, 0.0191, 0.0106],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0014, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0014, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59895.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0110, 0.0000,  ..., 0.1902, 0.1031, 0.1133],
        [0.0523, 0.0131, 0.0033,  ..., 0.1568, 0.0796, 0.0914],
        [0.0557, 0.0149, 0.0063,  ..., 0.1332, 0.0633, 0.0756],
        ...,
        [0.0681, 0.0213, 0.0761,  ..., 0.0579, 0.0162, 0.0236],
        [0.0681, 0.0213, 0.0761,  ..., 0.0579, 0.0162, 0.0236],
        [0.0681, 0.0213, 0.0761,  ..., 0.0579, 0.0162, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579627.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10211.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.5797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5049.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(879.4393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1206],
        [ 0.0911],
        [ 0.0435],
        ...,
        [-1.9025],
        [-1.8983],
        [-1.8971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273180.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0114],
        [1.0168],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368575.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0115],
        [1.0169],
        ...,
        [1.0034],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368584.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1269.6212, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7009, device='cuda:0')



h[100].sum tensor(-2.2845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4629, device='cuda:0')



h[200].sum tensor(-22.5517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0014, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0029, 0.0014, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0030, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60606.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0201, 0.0740,  ..., 0.0560, 0.0156, 0.0229],
        [0.0660, 0.0201, 0.0730,  ..., 0.0575, 0.0162, 0.0240],
        [0.0657, 0.0200, 0.0698,  ..., 0.0612, 0.0179, 0.0268],
        ...,
        [0.0682, 0.0211, 0.0761,  ..., 0.0581, 0.0162, 0.0239],
        [0.0678, 0.0209, 0.0720,  ..., 0.0617, 0.0188, 0.0263],
        [0.0664, 0.0201, 0.0583,  ..., 0.0737, 0.0274, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571015.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10227.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.9877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5141.3208, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(885.1398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6210],
        [-1.5249],
        [-1.3723],
        ...,
        [-1.8481],
        [-1.7207],
        [-1.4772]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281148.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0115],
        [1.0169],
        ...,
        [1.0034],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368584.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0117],
        [1.0170],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368593.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [-0.0020,  0.0098,  0.0025,  ...,  0.0021,  0.0097,  0.0057],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(930.0715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4998, device='cuda:0')



h[100].sum tensor(-1.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4108, device='cuda:0')



h[200].sum tensor(-22.6113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0112, 0.0026,  ..., 0.0043, 0.0109, 0.0058],
        [0.0000, 0.0095, 0.0020,  ..., 0.0040, 0.0092, 0.0043],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50923.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0647, 0.0189, 0.0643,  ..., 0.0652, 0.0208, 0.0297],
        [0.0626, 0.0177, 0.0483,  ..., 0.0805, 0.0297, 0.0404],
        [0.0617, 0.0172, 0.0414,  ..., 0.0878, 0.0334, 0.0456],
        ...,
        [0.0683, 0.0207, 0.0760,  ..., 0.0583, 0.0162, 0.0243],
        [0.0683, 0.0207, 0.0760,  ..., 0.0583, 0.0162, 0.0243],
        [0.0683, 0.0206, 0.0760,  ..., 0.0583, 0.0162, 0.0243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526214.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10258.6934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.6887, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5070.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(805.4350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1784],
        [-1.3788],
        [-1.4891],
        ...,
        [-1.9177],
        [-1.9136],
        [-1.9124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253529.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0117],
        [1.0170],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368593.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0118],
        [1.0171],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368603.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1318.1129, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6572, device='cuda:0')



h[100].sum tensor(-2.3155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6026, device='cuda:0')



h[200].sum tensor(-22.5466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0349, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0029, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60628.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0662, 0.0194, 0.0739,  ..., 0.0564, 0.0156, 0.0236],
        [0.0665, 0.0195, 0.0741,  ..., 0.0566, 0.0157, 0.0237],
        [0.0668, 0.0197, 0.0744,  ..., 0.0570, 0.0158, 0.0239],
        ...,
        [0.0643, 0.0179, 0.0399,  ..., 0.0906, 0.0383, 0.0463],
        [0.0645, 0.0181, 0.0417,  ..., 0.0890, 0.0372, 0.0452],
        [0.0655, 0.0186, 0.0502,  ..., 0.0813, 0.0319, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571289.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10144.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5769, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.2440, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4924.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(890.8429, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9754],
        [-2.0336],
        [-2.0638],
        ...,
        [-0.6856],
        [-0.7014],
        [-0.7603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238353.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0118],
        [1.0171],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368603.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0119],
        [1.0172],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368612.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [-0.0025,  0.0121,  0.0033,  ...,  0.0024,  0.0120,  0.0078]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1030.9150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3898, device='cuda:0')



h[100].sum tensor(-1.7163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6869, device='cuda:0')



h[200].sum tensor(-22.5962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0029, 0.0015, 0.0000],
        [0.0000, 0.0139, 0.0035,  ..., 0.0046, 0.0136, 0.0080],
        [0.0000, 0.0117, 0.0027,  ..., 0.0043, 0.0114, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52547.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0648, 0.0180, 0.0610,  ..., 0.0686, 0.0233, 0.0324],
        [0.0658, 0.0185, 0.0679,  ..., 0.0628, 0.0191, 0.0284],
        [0.0651, 0.0181, 0.0617,  ..., 0.0694, 0.0226, 0.0330],
        ...,
        [0.0673, 0.0191, 0.0656,  ..., 0.0684, 0.0224, 0.0317],
        [0.0643, 0.0172, 0.0422,  ..., 0.0896, 0.0358, 0.0464],
        [0.0629, 0.0163, 0.0325,  ..., 0.0986, 0.0411, 0.0528]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528892.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10350.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.1388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5128.5161, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(822.1460, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5020],
        [-0.6792],
        [-0.7567],
        ...,
        [-1.6877],
        [-1.4944],
        [-1.3380]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266392.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0119],
        [1.0172],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368612.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0120],
        [1.0173],
        ...,
        [1.0035],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368621.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0007,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(951.4077, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4833, device='cuda:0')



h[100].sum tensor(-1.5340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4084, device='cuda:0')



h[200].sum tensor(-22.6109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0028, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0028, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49846.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0639, 0.0170, 0.0554,  ..., 0.0746, 0.0259, 0.0370],
        [0.0656, 0.0180, 0.0664,  ..., 0.0649, 0.0198, 0.0301],
        [0.0658, 0.0181, 0.0661,  ..., 0.0659, 0.0202, 0.0308],
        ...,
        [0.0690, 0.0197, 0.0764,  ..., 0.0590, 0.0163, 0.0252],
        [0.0690, 0.0197, 0.0764,  ..., 0.0590, 0.0163, 0.0252],
        [0.0690, 0.0197, 0.0764,  ..., 0.0590, 0.0163, 0.0252]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516814., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10350.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.3614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5063.9497, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(805.8808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3182],
        [-0.5102],
        [-0.6155],
        ...,
        [-1.9515],
        [-1.9473],
        [-1.9460]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248040.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0120],
        [1.0173],
        ...,
        [1.0035],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368621.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0121],
        [1.0174],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368630.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(821.7163, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.6648, device='cuda:0')



h[100].sum tensor(-1.2653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9965, device='cuda:0')



h[200].sum tensor(-22.6331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0194, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0014, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0028, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46239.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0672, 0.0188, 0.0750,  ..., 0.0570, 0.0158, 0.0239],
        [0.0675, 0.0189, 0.0752,  ..., 0.0572, 0.0159, 0.0240],
        [0.0678, 0.0191, 0.0755,  ..., 0.0576, 0.0160, 0.0243],
        ...,
        [0.0694, 0.0197, 0.0771,  ..., 0.0591, 0.0164, 0.0250],
        [0.0694, 0.0197, 0.0771,  ..., 0.0591, 0.0164, 0.0250],
        [0.0694, 0.0197, 0.0771,  ..., 0.0591, 0.0164, 0.0250]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504920.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10515.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.1654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.9090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5257.9927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(773.6684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9736],
        [-1.8586],
        [-1.6729],
        ...,
        [-1.9739],
        [-1.9695],
        [-1.9682]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273546.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0121],
        [1.0174],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368630.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0122],
        [1.0174],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368639.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0099,  0.0477,  0.0158,  ...,  0.0075,  0.0475,  0.0382],
        [-0.0078,  0.0376,  0.0123,  ...,  0.0060,  0.0374,  0.0296],
        [-0.0066,  0.0321,  0.0103,  ...,  0.0052,  0.0319,  0.0248],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1171.0071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1725, device='cuda:0')



h[100].sum tensor(-1.9239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0935, device='cuda:0')



h[200].sum tensor(-22.5753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1420, 0.0461,  ..., 0.0229, 0.1413, 0.1109],
        [0.0000, 0.1778, 0.0586,  ..., 0.0280, 0.1770, 0.1414],
        [0.0000, 0.1878, 0.0621,  ..., 0.0295, 0.1870, 0.1500],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0029, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53571.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0279, 0.0000, 0.0000,  ..., 0.4933, 0.3585, 0.3052],
        [0.0226, 0.0000, 0.0000,  ..., 0.5764, 0.4291, 0.3568],
        [0.0206, 0.0000, 0.0000,  ..., 0.6127, 0.4600, 0.3791],
        ...,
        [0.0697, 0.0198, 0.0778,  ..., 0.0592, 0.0164, 0.0247],
        [0.0697, 0.0198, 0.0777,  ..., 0.0592, 0.0164, 0.0247],
        [0.0697, 0.0198, 0.0777,  ..., 0.0592, 0.0164, 0.0247]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530801.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10603.4072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.6107, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5362.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(834.8450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1185],
        [ 0.1059],
        [ 0.0952],
        ...,
        [-1.9927],
        [-1.9882],
        [-1.9868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311164.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0122],
        [1.0174],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368639.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0123],
        [1.0175],
        ...,
        [1.0036],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368648.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0080,  0.0386,  0.0126,  ...,  0.0062,  0.0384,  0.0304],
        [-0.0054,  0.0261,  0.0083,  ...,  0.0044,  0.0260,  0.0197],
        [-0.0109,  0.0525,  0.0175,  ...,  0.0082,  0.0522,  0.0422],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1346.6748, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4448, device='cuda:0')



h[100].sum tensor(-2.2072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5716, device='cuda:0')



h[200].sum tensor(-22.5495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1124, 0.0357,  ..., 0.0188, 0.1117, 0.0855],
        [0.0000, 0.1661, 0.0545,  ..., 0.0265, 0.1652, 0.1314],
        [0.0000, 0.0918, 0.0285,  ..., 0.0158, 0.0911, 0.0679],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59531.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0373, 0.0012, 0.0000,  ..., 0.3639, 0.2488, 0.2240],
        [0.0329, 0.0000, 0.0000,  ..., 0.4257, 0.2995, 0.2630],
        [0.0386, 0.0016, 0.0000,  ..., 0.3526, 0.2386, 0.2171],
        ...,
        [0.0702, 0.0203, 0.0782,  ..., 0.0593, 0.0162, 0.0243],
        [0.0701, 0.0203, 0.0782,  ..., 0.0593, 0.0162, 0.0243],
        [0.0701, 0.0203, 0.0782,  ..., 0.0592, 0.0162, 0.0243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560683.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10434.9502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4540, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.0061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4932.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(898.9649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1734],
        [ 0.1732],
        [ 0.1611],
        ...,
        [-1.9924],
        [-1.9874],
        [-1.9853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254016.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0123],
        [1.0175],
        ...,
        [1.0036],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368648.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0124],
        [1.0176],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368657.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0054,  0.0266,  0.0084,  ...,  0.0045,  0.0264,  0.0201],
        [-0.0085,  0.0414,  0.0136,  ...,  0.0066,  0.0412,  0.0328],
        [-0.0097,  0.0470,  0.0156,  ...,  0.0074,  0.0468,  0.0376],
        ...,
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022],
        [ 0.0000,  0.0004, -0.0007,  ...,  0.0007,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1050.7151, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6941, device='cuda:0')



h[100].sum tensor(-1.5856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5853, device='cuda:0')



h[200].sum tensor(-22.6028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1274, 0.0410,  ..., 0.0210, 0.1266, 0.0983],
        [0.0000, 0.1320, 0.0426,  ..., 0.0217, 0.1312, 0.1022],
        [0.0000, 0.1493, 0.0486,  ..., 0.0242, 0.1484, 0.1169],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0013, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0030, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49892.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0313, 0.0000, 0.0000,  ..., 0.4452, 0.3191, 0.2752],
        [0.0320, 0.0000, 0.0000,  ..., 0.4371, 0.3117, 0.2702],
        [0.0344, 0.0000, 0.0000,  ..., 0.4193, 0.2985, 0.2582],
        ...,
        [0.0697, 0.0200, 0.0783,  ..., 0.0594, 0.0165, 0.0247],
        [0.0697, 0.0200, 0.0783,  ..., 0.0594, 0.0165, 0.0247],
        [0.0697, 0.0200, 0.0783,  ..., 0.0593, 0.0165, 0.0247]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516454.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10517.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.4909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5221.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(811.4953, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1391],
        [ 0.1396],
        [ 0.1313],
        ...,
        [-2.0001],
        [-1.9958],
        [-1.9951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288162.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0124],
        [1.0176],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368657.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(518.2619, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0125],
        [1.0177],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368667.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0022],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0022],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1024.6798, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3618, device='cuda:0')



h[100].sum tensor(-1.4711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3906, device='cuda:0')



h[200].sum tensor(-22.6120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0031, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0013, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0013, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49792.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0664, 0.0185, 0.0741,  ..., 0.0589, 0.0166, 0.0256],
        [0.0644, 0.0171, 0.0559,  ..., 0.0754, 0.0277, 0.0369],
        [0.0620, 0.0154, 0.0299,  ..., 0.0988, 0.0440, 0.0526],
        ...,
        [0.0690, 0.0196, 0.0779,  ..., 0.0594, 0.0166, 0.0252],
        [0.0690, 0.0196, 0.0779,  ..., 0.0594, 0.0166, 0.0252],
        [0.0690, 0.0196, 0.0779,  ..., 0.0594, 0.0166, 0.0252]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516656.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10364.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.2535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5131.7568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(811.0273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1353],
        [-0.7102],
        [-0.2831],
        ...,
        [-1.9924],
        [-1.9877],
        [-1.9828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288397.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0125],
        [1.0177],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368667.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0126],
        [1.0178],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368677.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(940.3985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0063, device='cuda:0')



h[100].sum tensor(-1.2560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0464, device='cuda:0')



h[200].sum tensor(-22.6303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0104, 0.0022,  ..., 0.0044, 0.0098, 0.0049],
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0110, 0.0024,  ..., 0.0045, 0.0103, 0.0054],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0014, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0014, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47311.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0625, 0.0158, 0.0501,  ..., 0.0806, 0.0293, 0.0414],
        [0.0642, 0.0169, 0.0604,  ..., 0.0715, 0.0241, 0.0347],
        [0.0629, 0.0160, 0.0487,  ..., 0.0828, 0.0308, 0.0427],
        ...,
        [0.0685, 0.0193, 0.0775,  ..., 0.0595, 0.0166, 0.0255],
        [0.0685, 0.0193, 0.0775,  ..., 0.0595, 0.0166, 0.0255],
        [0.0685, 0.0193, 0.0775,  ..., 0.0595, 0.0166, 0.0255]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508790.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10262.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.0819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5049.8013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(788.8509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5570],
        [-1.6914],
        [-1.7174],
        ...,
        [-1.9943],
        [-1.9901],
        [-1.9887]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280906.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0126],
        [1.0178],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368677.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0127],
        [1.0179],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368686.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1266.1230, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0407, device='cuda:0')



h[100].sum tensor(-1.8426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0742, device='cuda:0')



h[200].sum tensor(-22.5770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55213.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0179, 0.0741,  ..., 0.0588, 0.0166, 0.0259],
        [0.0657, 0.0178, 0.0731,  ..., 0.0602, 0.0171, 0.0273],
        [0.0649, 0.0173, 0.0672,  ..., 0.0663, 0.0202, 0.0320],
        ...,
        [0.0685, 0.0189, 0.0776,  ..., 0.0597, 0.0168, 0.0257],
        [0.0685, 0.0189, 0.0776,  ..., 0.0596, 0.0168, 0.0257],
        [0.0685, 0.0189, 0.0775,  ..., 0.0596, 0.0168, 0.0257]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537053.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10071.5449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.5904, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4737.9224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(865.0706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5807],
        [-1.3898],
        [-1.1532],
        ...,
        [-1.9979],
        [-1.9935],
        [-1.9921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246567.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0127],
        [1.0179],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368686.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0128],
        [1.0181],
        ...,
        [1.0036],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368695.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0098,  0.0025,  ...,  0.0021,  0.0096,  0.0057],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [-0.0019,  0.0098,  0.0025,  ...,  0.0021,  0.0096,  0.0057],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1578.5879, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5505, device='cuda:0')



h[100].sum tensor(-2.4020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0254, device='cuda:0')



h[200].sum tensor(-22.5254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0390, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0019,  ..., 0.0043, 0.0090, 0.0043],
        [0.0000, 0.0362, 0.0089,  ..., 0.0081, 0.0354, 0.0200],
        [0.0000, 0.0098, 0.0019,  ..., 0.0043, 0.0091, 0.0043],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65204.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0595, 0.0128, 0.0266,  ..., 0.1020, 0.0433, 0.0567],
        [0.0567, 0.0108, 0.0091,  ..., 0.1187, 0.0520, 0.0693],
        [0.0589, 0.0126, 0.0261,  ..., 0.1043, 0.0422, 0.0596],
        ...,
        [0.0687, 0.0187, 0.0778,  ..., 0.0598, 0.0171, 0.0257],
        [0.0687, 0.0187, 0.0778,  ..., 0.0598, 0.0171, 0.0257],
        [0.0687, 0.0187, 0.0778,  ..., 0.0598, 0.0171, 0.0257]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587450., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9929.7051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.5854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4469.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(956.8151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2939],
        [-0.3143],
        [-0.2501],
        ...,
        [-2.0142],
        [-2.0099],
        [-2.0085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235132.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0128],
        [1.0181],
        ...,
        [1.0036],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368695.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0129],
        [1.0182],
        ...,
        [1.0036],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368704., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [-0.0020,  0.0103,  0.0027,  ...,  0.0022,  0.0101,  0.0061],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1058.0166, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4036, device='cuda:0')



h[100].sum tensor(-1.4322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3967, device='cuda:0')



h[200].sum tensor(-22.6120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0313, 0.0080,  ..., 0.0074, 0.0305, 0.0182],
        [0.0000, 0.0278, 0.0075,  ..., 0.0069, 0.0270, 0.0175],
        [0.0000, 0.0434, 0.0114,  ..., 0.0091, 0.0425, 0.0262],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0013, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0013, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50207.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0533, 0.0081, 0.0064,  ..., 0.1447, 0.0692, 0.0871],
        [0.0532, 0.0079, 0.0013,  ..., 0.1487, 0.0727, 0.0896],
        [0.0530, 0.0078, 0.0007,  ..., 0.1512, 0.0736, 0.0917],
        ...,
        [0.0691, 0.0188, 0.0784,  ..., 0.0599, 0.0173, 0.0254],
        [0.0691, 0.0188, 0.0784,  ..., 0.0599, 0.0173, 0.0254],
        [0.0691, 0.0188, 0.0784,  ..., 0.0599, 0.0173, 0.0254]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(519780.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10272.4043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.4338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4838.6372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(824.1701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2382],
        [-0.1181],
        [-0.1768],
        ...,
        [-2.0325],
        [-2.0282],
        [-2.0268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262695.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0129],
        [1.0182],
        ...,
        [1.0036],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368704., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0130],
        [1.0183],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368712.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0306,  0.0098,  ...,  0.0051,  0.0304,  0.0235],
        [-0.0041,  0.0204,  0.0062,  ...,  0.0037,  0.0201,  0.0147],
        [-0.0048,  0.0237,  0.0074,  ...,  0.0041,  0.0235,  0.0176],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1084.2166, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1868, device='cuda:0')



h[100].sum tensor(-1.4839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5111, device='cuda:0')



h[200].sum tensor(-22.6064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0865, 0.0266,  ..., 0.0154, 0.0855, 0.0632],
        [0.0000, 0.0992, 0.0310,  ..., 0.0172, 0.0982, 0.0740],
        [0.0000, 0.0903, 0.0279,  ..., 0.0159, 0.0893, 0.0664],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0012, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0012, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52840.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0399, 0.0000, 0.0000,  ..., 0.3132, 0.2074, 0.1954],
        [0.0403, 0.0000, 0.0000,  ..., 0.3131, 0.2079, 0.1951],
        [0.0422, 0.0000, 0.0000,  ..., 0.2943, 0.1920, 0.1829],
        ...,
        [0.0698, 0.0190, 0.0793,  ..., 0.0601, 0.0176, 0.0249],
        [0.0698, 0.0190, 0.0793,  ..., 0.0601, 0.0176, 0.0249],
        [0.0698, 0.0190, 0.0793,  ..., 0.0601, 0.0176, 0.0249]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541201.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10398.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.6572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4877.8521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(850.0045, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2283],
        [ 0.2311],
        [ 0.2346],
        ...,
        [-2.0599],
        [-2.0553],
        [-2.0539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275393.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0130],
        [1.0183],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368712.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0131],
        [1.0183],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368721.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(995.6472, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1432, device='cuda:0')



h[100].sum tensor(-1.3084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2125, device='cuda:0')



h[200].sum tensor(-22.6216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0016,  ..., 0.0042, 0.0079, 0.0035],
        [0.0000, 0.0019, 0.0000,  ..., 0.0033, 0.0011, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0033, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0033, 0.0012, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0033, 0.0012, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0033, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48525.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0605, 0.0126, 0.0234,  ..., 0.1096, 0.0490, 0.0602],
        [0.0642, 0.0154, 0.0481,  ..., 0.0852, 0.0338, 0.0429],
        [0.0672, 0.0176, 0.0677,  ..., 0.0681, 0.0232, 0.0306],
        ...,
        [0.0702, 0.0192, 0.0799,  ..., 0.0603, 0.0177, 0.0247],
        [0.0702, 0.0192, 0.0799,  ..., 0.0603, 0.0177, 0.0247],
        [0.0702, 0.0192, 0.0799,  ..., 0.0603, 0.0177, 0.0247]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516944.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10555.4590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.8309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5111.0967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(813.5304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1625],
        [-0.5582],
        [-0.9711],
        ...,
        [-2.0776],
        [-2.0730],
        [-2.0715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296871.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0131],
        [1.0183],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368721.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0131],
        [1.0184],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368730.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1754.6844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2757, device='cuda:0')



h[100].sum tensor(-2.6259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5696, device='cuda:0')



h[200].sum tensor(-22.4987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0443, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0446, 0.0134,  ..., 0.0095, 0.0437, 0.0319],
        [0.0000, 0.0019, 0.0000,  ..., 0.0033, 0.0011, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0034, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0034, 0.0011, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0034, 0.0011, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0034, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68247.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0521, 0.0066, 0.0086,  ..., 0.2289, 0.1502, 0.1357],
        [0.0630, 0.0140, 0.0375,  ..., 0.1111, 0.0576, 0.0586],
        [0.0669, 0.0175, 0.0619,  ..., 0.0731, 0.0278, 0.0335],
        ...,
        [0.0701, 0.0194, 0.0801,  ..., 0.0604, 0.0178, 0.0246],
        [0.0701, 0.0194, 0.0801,  ..., 0.0604, 0.0178, 0.0246],
        [0.0701, 0.0194, 0.0800,  ..., 0.0604, 0.0178, 0.0246]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600417.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10305.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4821.4600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(988.0681, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1042],
        [-0.0479],
        [-0.2361],
        ...,
        [-2.0849],
        [-2.0803],
        [-2.0788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275346.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0131],
        [1.0184],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368730.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0132],
        [1.0185],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368740.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0100,  0.0026,  ...,  0.0022,  0.0098,  0.0059],
        [-0.0045,  0.0224,  0.0070,  ...,  0.0040,  0.0222,  0.0165],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1191.6082, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7753, device='cuda:0')



h[100].sum tensor(-1.5746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7432, device='cuda:0')



h[200].sum tensor(-22.5952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1064, 0.0336,  ..., 0.0185, 0.1053, 0.0800],
        [0.0000, 0.0560, 0.0167,  ..., 0.0112, 0.0551, 0.0393],
        [0.0000, 0.0584, 0.0176,  ..., 0.0116, 0.0575, 0.0413],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0035, 0.0012, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0035, 0.0012, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0035, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52024.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0389, 0.0000, 0.0000,  ..., 0.3329, 0.2231, 0.2069],
        [0.0452, 0.0017, 0.0000,  ..., 0.2599, 0.1634, 0.1596],
        [0.0498, 0.0049, 0.0000,  ..., 0.2179, 0.1312, 0.1317],
        ...,
        [0.0697, 0.0194, 0.0798,  ..., 0.0606, 0.0177, 0.0248],
        [0.0697, 0.0194, 0.0797,  ..., 0.0605, 0.0177, 0.0248],
        [0.0697, 0.0194, 0.0797,  ..., 0.0605, 0.0177, 0.0248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526974.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10347.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.9306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4939.7705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(847.0851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2266],
        [ 0.2057],
        [ 0.0569],
        ...,
        [-2.0848],
        [-2.0803],
        [-2.0788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271447.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0132],
        [1.0185],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368740.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0133],
        [1.0186],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368749.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(997.9215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3453, device='cuda:0')



h[100].sum tensor(-1.2079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0960, device='cuda:0')



h[200].sum tensor(-22.6286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0020, 0.0000,  ..., 0.0034, 0.0013, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0035, 0.0013, 0.0000],
        [0.0000, 0.0272, 0.0074,  ..., 0.0071, 0.0264, 0.0169],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0035, 0.0013, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0035, 0.0013, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0035, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48494.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0664, 0.0176, 0.0700,  ..., 0.0649, 0.0212, 0.0286],
        [0.0630, 0.0148, 0.0399,  ..., 0.0999, 0.0476, 0.0519],
        [0.0551, 0.0088, 0.0149,  ..., 0.1740, 0.1015, 0.1019],
        ...,
        [0.0695, 0.0192, 0.0795,  ..., 0.0607, 0.0177, 0.0251],
        [0.0695, 0.0192, 0.0794,  ..., 0.0607, 0.0177, 0.0251],
        [0.0695, 0.0192, 0.0794,  ..., 0.0607, 0.0177, 0.0251]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(519066.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10315.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.5435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4953.9160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(814.9248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3785],
        [-0.1527],
        [ 0.0400],
        ...,
        [-2.0893],
        [-2.0848],
        [-2.0834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275791.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0133],
        [1.0186],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368749.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(500.4866, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0133],
        [1.0187],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368758.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1138.8944, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7627, device='cuda:0')



h[100].sum tensor(-1.4100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4492, device='cuda:0')



h[200].sum tensor(-22.6087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0014, 0.0000],
        [0.0000, 0.0270, 0.0073,  ..., 0.0071, 0.0262, 0.0166],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0036, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0036, 0.0014, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0036, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52039.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0649, 0.0168, 0.0589,  ..., 0.0741, 0.0278, 0.0351],
        [0.0607, 0.0134, 0.0305,  ..., 0.1135, 0.0563, 0.0616],
        [0.0534, 0.0077, 0.0090,  ..., 0.1773, 0.1010, 0.1050],
        ...,
        [0.0691, 0.0192, 0.0790,  ..., 0.0608, 0.0177, 0.0255],
        [0.0691, 0.0192, 0.0790,  ..., 0.0608, 0.0177, 0.0255],
        [0.0690, 0.0191, 0.0790,  ..., 0.0608, 0.0177, 0.0255]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533890.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10235.1621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4945.2881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(840.0633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2958],
        [-0.0265],
        [ 0.1289],
        ...,
        [-2.0337],
        [-2.0489],
        [-2.0598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282293.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0133],
        [1.0187],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368758.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0134],
        [1.0188],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368767.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1216.2717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0042, device='cuda:0')



h[100].sum tensor(-1.5045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6306, device='cuda:0')



h[200].sum tensor(-22.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0015, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0015, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0036, 0.0015, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0036, 0.0015, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0036, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53401.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0617, 0.0144, 0.0301,  ..., 0.0988, 0.0452, 0.0521],
        [0.0611, 0.0139, 0.0234,  ..., 0.1051, 0.0493, 0.0565],
        [0.0614, 0.0141, 0.0238,  ..., 0.1055, 0.0494, 0.0566],
        ...,
        [0.0687, 0.0190, 0.0786,  ..., 0.0610, 0.0176, 0.0259],
        [0.0687, 0.0190, 0.0786,  ..., 0.0609, 0.0176, 0.0259],
        [0.0687, 0.0190, 0.0786,  ..., 0.0609, 0.0176, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537194.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10118.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.9724, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4879.7236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(850.6744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0326],
        [ 0.0715],
        [ 0.1305],
        ...,
        [-2.0851],
        [-2.0807],
        [-2.0794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272563.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0134],
        [1.0188],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368767.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0135],
        [1.0189],
        ...,
        [1.0034],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368776.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0099,  0.0026,  ...,  0.0022,  0.0097,  0.0057],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [-0.0019,  0.0099,  0.0026,  ...,  0.0022,  0.0097,  0.0057],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1198.5442, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5231, device='cuda:0')



h[100].sum tensor(-1.4506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5603, device='cuda:0')



h[200].sum tensor(-22.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0166, 0.0043,  ..., 0.0056, 0.0159, 0.0100],
        [0.0000, 0.0500, 0.0139,  ..., 0.0104, 0.0493, 0.0316],
        [0.0000, 0.0167, 0.0044,  ..., 0.0056, 0.0160, 0.0101],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0036, 0.0016, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0036, 0.0016, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0036, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52779.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0571, 0.0116, 0.0179,  ..., 0.1213, 0.0563, 0.0696],
        [0.0525, 0.0082, 0.0000,  ..., 0.1558, 0.0783, 0.0936],
        [0.0571, 0.0116, 0.0154,  ..., 0.1252, 0.0582, 0.0723],
        ...,
        [0.0686, 0.0189, 0.0785,  ..., 0.0611, 0.0176, 0.0262],
        [0.0686, 0.0189, 0.0785,  ..., 0.0611, 0.0176, 0.0262],
        [0.0686, 0.0189, 0.0785,  ..., 0.0611, 0.0176, 0.0262]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534516.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9987.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4758.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(849.6756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4003],
        [-0.1752],
        [-0.1973],
        ...,
        [-2.0854],
        [-2.0808],
        [-2.0758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239574.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0135],
        [1.0189],
        ...,
        [1.0034],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368776.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0136],
        [1.0190],
        ...,
        [1.0034],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368785.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0042,  0.0213,  0.0066,  ...,  0.0039,  0.0211,  0.0154],
        [-0.0019,  0.0101,  0.0026,  ...,  0.0022,  0.0099,  0.0059],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0009,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1053.9944, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7065, device='cuda:0')



h[100].sum tensor(-1.2107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1488, device='cuda:0')



h[200].sum tensor(-22.6252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0288, 0.0079,  ..., 0.0073, 0.0282, 0.0182],
        [0.0000, 0.0310, 0.0087,  ..., 0.0076, 0.0303, 0.0200],
        [0.0000, 0.0497, 0.0138,  ..., 0.0104, 0.0490, 0.0313],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0016, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0016, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48452.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0521, 0.0076, 0.0000,  ..., 0.1772, 0.0999, 0.1061],
        [0.0519, 0.0077, 0.0000,  ..., 0.1712, 0.0919, 0.1029],
        [0.0493, 0.0057, 0.0000,  ..., 0.1884, 0.0999, 0.1154],
        ...,
        [0.0690, 0.0190, 0.0791,  ..., 0.0614, 0.0176, 0.0261],
        [0.0690, 0.0190, 0.0791,  ..., 0.0613, 0.0176, 0.0261],
        [0.0690, 0.0190, 0.0790,  ..., 0.0613, 0.0176, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518337.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10160.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.6637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4994.3442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(811.6322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2309],
        [ 0.2364],
        [ 0.2393],
        ...,
        [-2.1186],
        [-2.1141],
        [-2.1128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268584.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0136],
        [1.0190],
        ...,
        [1.0034],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368785.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0137],
        [1.0191],
        ...,
        [1.0034],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368793.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0139,  0.0040,  ...,  0.0028,  0.0138,  0.0092],
        [-0.0036,  0.0187,  0.0057,  ...,  0.0035,  0.0185,  0.0132],
        [-0.0036,  0.0183,  0.0055,  ...,  0.0034,  0.0182,  0.0130],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1629.5026, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9089, device='cuda:0')



h[100].sum tensor(-2.1918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9316, device='cuda:0')



h[200].sum tensor(-22.5290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0314, 0.0089,  ..., 0.0076, 0.0308, 0.0205],
        [0.0000, 0.0487, 0.0142,  ..., 0.0101, 0.0481, 0.0329],
        [0.0000, 0.1043, 0.0330,  ..., 0.0182, 0.1036, 0.0781],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0034, 0.0015, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0034, 0.0015, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0034, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65537.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0502, 0.0057, 0.0000,  ..., 0.1944, 0.1085, 0.1175],
        [0.0461, 0.0031, 0.0000,  ..., 0.2466, 0.1511, 0.1511],
        [0.0408, 0.0007, 0.0000,  ..., 0.3277, 0.2200, 0.2023],
        ...,
        [0.0697, 0.0190, 0.0798,  ..., 0.0616, 0.0178, 0.0259],
        [0.0697, 0.0190, 0.0798,  ..., 0.0616, 0.0178, 0.0259],
        [0.0696, 0.0190, 0.0798,  ..., 0.0616, 0.0178, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601148.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10181.9912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0657, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.3094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5046.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(959.1065, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1845],
        [ 0.1925],
        [ 0.1787],
        ...,
        [-2.1511],
        [-2.1465],
        [-2.1451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293701.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0137],
        [1.0191],
        ...,
        [1.0034],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368793.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0138],
        [1.0193],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368801.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [-0.0032,  0.0163,  0.0049,  ...,  0.0031,  0.0162,  0.0113],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1046.7916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2566, device='cuda:0')



h[100].sum tensor(-1.2259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2291, device='cuda:0')



h[200].sum tensor(-22.6221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0014, 0.0000],
        [0.0000, 0.0180, 0.0049,  ..., 0.0056, 0.0175, 0.0114],
        [0.0000, 0.0151, 0.0039,  ..., 0.0052, 0.0146, 0.0089],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0015, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0015, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0033, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48708.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0167, 0.0603,  ..., 0.0750, 0.0278, 0.0353],
        [0.0628, 0.0142, 0.0370,  ..., 0.1071, 0.0512, 0.0567],
        [0.0589, 0.0112, 0.0194,  ..., 0.1467, 0.0802, 0.0830],
        ...,
        [0.0702, 0.0190, 0.0804,  ..., 0.0619, 0.0179, 0.0259],
        [0.0702, 0.0190, 0.0803,  ..., 0.0619, 0.0179, 0.0259],
        [0.0702, 0.0190, 0.0803,  ..., 0.0619, 0.0179, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526471.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10494.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.2289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5339.8667, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(812.6796, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2343],
        [-1.0864],
        [-0.7255],
        ...,
        [-2.1732],
        [-2.1686],
        [-2.1612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316131.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0138],
        [1.0193],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368801.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0139],
        [1.0193],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368809.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0047,  0.0244,  0.0077,  ...,  0.0043,  0.0242,  0.0182],
        [-0.0110,  0.0558,  0.0187,  ...,  0.0088,  0.0556,  0.0450],
        [-0.0045,  0.0231,  0.0072,  ...,  0.0041,  0.0229,  0.0170],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1224.7201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7816, device='cuda:0')



h[100].sum tensor(-1.5181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7442, device='cuda:0')



h[200].sum tensor(-22.5925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1194, 0.0383,  ..., 0.0202, 0.1188, 0.0912],
        [0.0000, 0.0946, 0.0297,  ..., 0.0166, 0.0941, 0.0700],
        [0.0000, 0.0965, 0.0310,  ..., 0.0169, 0.0960, 0.0739],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53382.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0410, 0.0006, 0.0000,  ..., 0.3324, 0.2234, 0.2048],
        [0.0415, 0.0000, 0.0000,  ..., 0.3275, 0.2194, 0.2018],
        [0.0455, 0.0032, 0.0000,  ..., 0.2990, 0.1998, 0.1822],
        ...,
        [0.0706, 0.0191, 0.0807,  ..., 0.0622, 0.0180, 0.0259],
        [0.0706, 0.0190, 0.0807,  ..., 0.0622, 0.0180, 0.0259],
        [0.0706, 0.0190, 0.0807,  ..., 0.0622, 0.0180, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544793.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10439.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.8993, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5206.4761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(859.3914, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1567],
        [ 0.1245],
        [-0.0458],
        ...,
        [-2.1975],
        [-2.1926],
        [-2.1911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298030.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0139],
        [1.0193],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368809.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0139],
        [1.0194],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368817.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035,  0.0179,  0.0054,  ...,  0.0033,  0.0178,  0.0126],
        [-0.0020,  0.0105,  0.0028,  ...,  0.0022,  0.0104,  0.0063],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1094.5251, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2863, device='cuda:0')



h[100].sum tensor(-1.2963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3796, device='cuda:0')



h[200].sum tensor(-22.6135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0231, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0606, 0.0178,  ..., 0.0116, 0.0601, 0.0410],
        [0.0000, 0.0339, 0.0098,  ..., 0.0078, 0.0335, 0.0228],
        [0.0000, 0.0121, 0.0029,  ..., 0.0046, 0.0117, 0.0064],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0032, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50165.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0503, 0.0058, 0.0000,  ..., 0.1977, 0.1081, 0.1201],
        [0.0559, 0.0095, 0.0092,  ..., 0.1579, 0.0819, 0.0925],
        [0.0622, 0.0139, 0.0298,  ..., 0.1111, 0.0498, 0.0603],
        ...,
        [0.0708, 0.0192, 0.0810,  ..., 0.0625, 0.0180, 0.0260],
        [0.0708, 0.0192, 0.0810,  ..., 0.0624, 0.0180, 0.0260],
        [0.0708, 0.0192, 0.0810,  ..., 0.0624, 0.0180, 0.0260]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534826.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10572.8809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5522, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.4195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5423.6504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(828.4274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1046],
        [-0.0576],
        [-0.3416],
        ...,
        [-2.2153],
        [-2.2102],
        [-2.2087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313236.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0139],
        [1.0194],
        ...,
        [1.0035],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368817.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0140],
        [1.0194],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368826.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1043.7565, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8861, device='cuda:0')



h[100].sum tensor(-1.1874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1750, device='cuda:0')



h[200].sum tensor(-22.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0031, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0031, 0.0015, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0031, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48128.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0684, 0.0183, 0.0787,  ..., 0.0604, 0.0173, 0.0252],
        [0.0686, 0.0184, 0.0789,  ..., 0.0606, 0.0174, 0.0253],
        [0.0690, 0.0186, 0.0792,  ..., 0.0610, 0.0175, 0.0256],
        ...,
        [0.0706, 0.0193, 0.0809,  ..., 0.0627, 0.0180, 0.0264],
        [0.0706, 0.0193, 0.0809,  ..., 0.0627, 0.0180, 0.0263],
        [0.0706, 0.0192, 0.0809,  ..., 0.0627, 0.0180, 0.0263]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524710.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10532.9551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.6203, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5425.0947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(811.0554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2780],
        [-1.6822],
        [-1.9716],
        ...,
        [-2.2210],
        [-2.2160],
        [-2.2145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308792., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0140],
        [1.0194],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368826.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0140],
        [1.0195],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368835.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1217.4607, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2478, device='cuda:0')



h[100].sum tensor(-1.4214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6662, device='cuda:0')



h[200].sum tensor(-22.5991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53522.0430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0679, 0.0183, 0.0782,  ..., 0.0606, 0.0173, 0.0257],
        [0.0678, 0.0183, 0.0777,  ..., 0.0615, 0.0176, 0.0265],
        [0.0677, 0.0183, 0.0761,  ..., 0.0638, 0.0182, 0.0284],
        ...,
        [0.0694, 0.0188, 0.0765,  ..., 0.0666, 0.0198, 0.0296],
        [0.0681, 0.0180, 0.0686,  ..., 0.0739, 0.0235, 0.0351],
        [0.0674, 0.0176, 0.0646,  ..., 0.0776, 0.0253, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549911., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10233.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8763, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5156.2490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(862.2322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1151],
        [-1.9404],
        [-1.6853],
        ...,
        [-1.9865],
        [-1.8347],
        [-1.7385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265960.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0140],
        [1.0195],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368835.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(491.6905, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0140],
        [1.0195],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368844.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [-0.0024,  0.0130,  0.0037,  ...,  0.0026,  0.0129,  0.0084],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(955.6577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.3093, device='cuda:0')



h[100].sum tensor(-0.9663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7985, device='cuda:0')



h[200].sum tensor(-22.6442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0017, 0.0000],
        [0.0000, 0.0147, 0.0037,  ..., 0.0050, 0.0143, 0.0085],
        [0.0000, 0.0443, 0.0134,  ..., 0.0094, 0.0439, 0.0314],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0018, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0018, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46499.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0627, 0.0153, 0.0418,  ..., 0.0924, 0.0370, 0.0490],
        [0.0592, 0.0129, 0.0265,  ..., 0.1281, 0.0638, 0.0726],
        [0.0513, 0.0081, 0.0055,  ..., 0.2145, 0.1308, 0.1292],
        ...,
        [0.0697, 0.0192, 0.0801,  ..., 0.0632, 0.0179, 0.0274],
        [0.0697, 0.0192, 0.0801,  ..., 0.0631, 0.0179, 0.0274],
        [0.0697, 0.0192, 0.0801,  ..., 0.0631, 0.0179, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522894.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10318.8262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.4181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5337.0151, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(795.7814, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2749],
        [-0.1265],
        [ 0.0250],
        ...,
        [-2.2152],
        [-2.2105],
        [-2.2091]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299745.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0140],
        [1.0195],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368844.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0141],
        [1.0196],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368853.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0074,  0.0383,  0.0125,  ...,  0.0063,  0.0382,  0.0300],
        [-0.0068,  0.0351,  0.0114,  ...,  0.0058,  0.0350,  0.0272],
        [-0.0063,  0.0329,  0.0107,  ...,  0.0055,  0.0328,  0.0254],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1328.2803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5524, device='cuda:0')



h[100].sum tensor(-1.5180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8568, device='cuda:0')



h[200].sum tensor(-22.5872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0816, 0.0258,  ..., 0.0148, 0.0812, 0.0609],
        [0.0000, 0.1149, 0.0367,  ..., 0.0197, 0.1145, 0.0870],
        [0.0000, 0.1246, 0.0401,  ..., 0.0211, 0.1241, 0.0952],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0018, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0018, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55223.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0363, 0.0000, 0.0000,  ..., 0.3564, 0.2394, 0.2231],
        [0.0327, 0.0000, 0.0000,  ..., 0.3928, 0.2669, 0.2472],
        [0.0315, 0.0000, 0.0000,  ..., 0.4060, 0.2760, 0.2561],
        ...,
        [0.0696, 0.0192, 0.0802,  ..., 0.0633, 0.0179, 0.0276],
        [0.0695, 0.0192, 0.0802,  ..., 0.0633, 0.0179, 0.0275],
        [0.0695, 0.0192, 0.0802,  ..., 0.0633, 0.0179, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558636.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10097.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.1331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5084.8662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(876.5895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1165],
        [ 0.1170],
        [ 0.1188],
        ...,
        [-2.2214],
        [-2.2168],
        [-2.2154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275416.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0141],
        [1.0196],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368853.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0142],
        [1.0196],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368861.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0037,  0.0195,  0.0060,  ...,  0.0036,  0.0194,  0.0139],
        [-0.0016,  0.0088,  0.0022,  ...,  0.0020,  0.0087,  0.0048],
        [-0.0016,  0.0085,  0.0021,  ...,  0.0020,  0.0084,  0.0045],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1139.9390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6314, device='cuda:0')



h[100].sum tensor(-1.2062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2839, device='cuda:0')



h[200].sum tensor(-22.6182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0396, 0.0104,  ..., 0.0087, 0.0392, 0.0227],
        [0.0000, 0.0539, 0.0154,  ..., 0.0108, 0.0534, 0.0349],
        [0.0000, 0.0448, 0.0122,  ..., 0.0095, 0.0444, 0.0271],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0018, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0018, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50395.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.0037, 0.0000,  ..., 0.2134, 0.1124, 0.1342],
        [0.0448, 0.0033, 0.0000,  ..., 0.2219, 0.1192, 0.1394],
        [0.0472, 0.0049, 0.0000,  ..., 0.2055, 0.1073, 0.1283],
        ...,
        [0.0697, 0.0192, 0.0806,  ..., 0.0635, 0.0181, 0.0276],
        [0.0697, 0.0192, 0.0806,  ..., 0.0635, 0.0181, 0.0276],
        [0.0697, 0.0192, 0.0806,  ..., 0.0634, 0.0181, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537522., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10204.8076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.4628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5240.3467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(833.8644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2096],
        [ 0.2100],
        [ 0.2105],
        ...,
        [-2.2361],
        [-2.2314],
        [-2.2300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287351.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0142],
        [1.0196],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368861.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0142],
        [1.0197],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368869.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0052,  0.0273,  0.0087,  ...,  0.0047,  0.0272,  0.0206],
        [-0.0024,  0.0126,  0.0036,  ...,  0.0026,  0.0125,  0.0080],
        [-0.0026,  0.0139,  0.0040,  ...,  0.0027,  0.0137,  0.0091],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1186.9587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5278, device='cuda:0')



h[100].sum tensor(-1.2671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4149, device='cuda:0')



h[200].sum tensor(-22.6110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0685, 0.0205,  ..., 0.0129, 0.0681, 0.0474],
        [0.0000, 0.0747, 0.0227,  ..., 0.0138, 0.0743, 0.0527],
        [0.0000, 0.0451, 0.0130,  ..., 0.0095, 0.0447, 0.0297],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51230.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0485, 0.0052, 0.0000,  ..., 0.2182, 0.1267, 0.1335],
        [0.0490, 0.0054, 0.0000,  ..., 0.2191, 0.1284, 0.1338],
        [0.0521, 0.0076, 0.0000,  ..., 0.1931, 0.1088, 0.1163],
        ...,
        [0.0700, 0.0193, 0.0812,  ..., 0.0636, 0.0182, 0.0275],
        [0.0700, 0.0193, 0.0812,  ..., 0.0636, 0.0182, 0.0275],
        [0.0700, 0.0193, 0.0811,  ..., 0.0636, 0.0182, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540445.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10186.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.4039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5228.6880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(846.4551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1775],
        [ 0.0744],
        [-0.1257],
        ...,
        [-2.2548],
        [-2.2501],
        [-2.2486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276164.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0142],
        [1.0197],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368869.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0143],
        [1.0197],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368877.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0146,  0.0043,  ...,  0.0028,  0.0145,  0.0097],
        [-0.0018,  0.0096,  0.0025,  ...,  0.0021,  0.0095,  0.0055],
        [-0.0014,  0.0076,  0.0018,  ...,  0.0018,  0.0075,  0.0037],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1473.1951, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6352, device='cuda:0')



h[100].sum tensor(-1.7025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3072, device='cuda:0')



h[200].sum tensor(-22.5647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0507, 0.0150,  ..., 0.0103, 0.0503, 0.0345],
        [0.0000, 0.0436, 0.0125,  ..., 0.0093, 0.0432, 0.0284],
        [0.0000, 0.0755, 0.0229,  ..., 0.0139, 0.0750, 0.0533],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58700.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0420, 0.0018, 0.0000,  ..., 0.2945, 0.1872, 0.1836],
        [0.0450, 0.0025, 0.0000,  ..., 0.2533, 0.1514, 0.1575],
        [0.0431, 0.0018, 0.0000,  ..., 0.2726, 0.1649, 0.1705],
        ...,
        [0.0704, 0.0193, 0.0818,  ..., 0.0638, 0.0184, 0.0275],
        [0.0704, 0.0193, 0.0818,  ..., 0.0638, 0.0184, 0.0275],
        [0.0704, 0.0193, 0.0818,  ..., 0.0637, 0.0184, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574283.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10297.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.7230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5299.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(908.5768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0771],
        [ 0.1007],
        [ 0.1143],
        ...,
        [-2.2774],
        [-2.2725],
        [-2.2710]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310199.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0143],
        [1.0197],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368877.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0144],
        [1.0198],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368885.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1030.2405, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4189, device='cuda:0')



h[100].sum tensor(-1.0174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9606, device='cuda:0')



h[200].sum tensor(-22.6354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0524, 0.0162,  ..., 0.0105, 0.0519, 0.0383],
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0032, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47514.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0526, 0.0075, 0.0063,  ..., 0.2022, 0.1208, 0.1213],
        [0.0623, 0.0140, 0.0331,  ..., 0.1129, 0.0536, 0.0616],
        [0.0661, 0.0167, 0.0563,  ..., 0.0832, 0.0318, 0.0415],
        ...,
        [0.0707, 0.0192, 0.0822,  ..., 0.0640, 0.0185, 0.0276],
        [0.0706, 0.0192, 0.0822,  ..., 0.0639, 0.0185, 0.0276],
        [0.0706, 0.0192, 0.0822,  ..., 0.0639, 0.0185, 0.0276]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528201., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10463.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5470.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(811.7686, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0091],
        [-0.3046],
        [-0.7836],
        ...,
        [-2.2887],
        [-2.2825],
        [-2.2789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321139.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0144],
        [1.0198],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368885.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0145],
        [1.0198],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368894.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1464.2983, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8927, device='cuda:0')



h[100].sum tensor(-1.6416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1987, device='cuda:0')



h[200].sum tensor(-22.5687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0032, 0.0017, 0.0000],
        [0.0000, 0.0276, 0.0075,  ..., 0.0069, 0.0272, 0.0171],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0033, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59379.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0166, 0.0618,  ..., 0.0774, 0.0282, 0.0377],
        [0.0643, 0.0153, 0.0448,  ..., 0.0926, 0.0386, 0.0483],
        [0.0569, 0.0102, 0.0190,  ..., 0.1512, 0.0780, 0.0892],
        ...,
        [0.0705, 0.0190, 0.0822,  ..., 0.0641, 0.0186, 0.0279],
        [0.0705, 0.0190, 0.0822,  ..., 0.0641, 0.0186, 0.0279],
        [0.0705, 0.0190, 0.0822,  ..., 0.0641, 0.0186, 0.0279]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580486.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10110.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.5530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4962.0186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(923.8207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2463],
        [-1.0197],
        [-0.5692],
        ...,
        [-2.2971],
        [-2.2923],
        [-2.2908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249982.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0145],
        [1.0198],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368894.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0146],
        [1.0199],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368902.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016,  0.0086,  0.0021,  ...,  0.0020,  0.0085,  0.0046],
        [-0.0016,  0.0090,  0.0023,  ...,  0.0020,  0.0089,  0.0049],
        [-0.0032,  0.0170,  0.0051,  ...,  0.0032,  0.0169,  0.0118],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1597.6963, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1583, device='cuda:0')



h[100].sum tensor(-1.8017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5297, device='cuda:0')



h[200].sum tensor(-22.5504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0298, 0.0083,  ..., 0.0073, 0.0293, 0.0189],
        [0.0000, 0.0585, 0.0169,  ..., 0.0115, 0.0580, 0.0387],
        [0.0000, 0.0477, 0.0132,  ..., 0.0099, 0.0473, 0.0295],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0017, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0033, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63135.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0052, 0.0000,  ..., 0.1998, 0.1114, 0.1228],
        [0.0476, 0.0035, 0.0000,  ..., 0.2110, 0.1155, 0.1315],
        [0.0473, 0.0033, 0.0000,  ..., 0.2184, 0.1210, 0.1365],
        ...,
        [0.0704, 0.0189, 0.0823,  ..., 0.0643, 0.0186, 0.0281],
        [0.0703, 0.0189, 0.0823,  ..., 0.0643, 0.0186, 0.0281],
        [0.0703, 0.0189, 0.0823,  ..., 0.0643, 0.0186, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604041.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10096.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.5668, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5063.4072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(953.4398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2085],
        [ 0.2243],
        [ 0.2280],
        ...,
        [-2.2978],
        [-2.2918],
        [-2.2892]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282653.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0146],
        [1.0199],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368902.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0146],
        [1.0200],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368910.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [-0.0030,  0.0164,  0.0049,  ...,  0.0031,  0.0163,  0.0112],
        [-0.0066,  0.0348,  0.0113,  ...,  0.0058,  0.0347,  0.0269],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1327.5012, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2960, device='cuda:0')



h[100].sum tensor(-1.3482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6732, device='cuda:0')



h[200].sum tensor(-22.5977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0426, 0.0127,  ..., 0.0092, 0.0420, 0.0297],
        [0.0000, 0.0727, 0.0226,  ..., 0.0136, 0.0722, 0.0531],
        [0.0000, 0.0581, 0.0175,  ..., 0.0115, 0.0576, 0.0406],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53937.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0483, 0.0039, 0.0000,  ..., 0.2228, 0.1337, 0.1370],
        [0.0440, 0.0016, 0.0000,  ..., 0.2674, 0.1681, 0.1666],
        [0.0452, 0.0014, 0.0000,  ..., 0.2556, 0.1575, 0.1592],
        ...,
        [0.0698, 0.0188, 0.0821,  ..., 0.0644, 0.0186, 0.0285],
        [0.0698, 0.0188, 0.0821,  ..., 0.0644, 0.0186, 0.0285],
        [0.0698, 0.0188, 0.0821,  ..., 0.0644, 0.0186, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552391.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10065.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.8601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5066.3345, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(875.4687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1767],
        [ 0.1779],
        [ 0.1775],
        ...,
        [-2.2952],
        [-2.2905],
        [-2.2890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264164.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0146],
        [1.0200],
        ...,
        [1.0035],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368910.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0147],
        [1.0201],
        ...,
        [1.0035],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368918.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [-0.0030,  0.0161,  0.0048,  ...,  0.0031,  0.0160,  0.0109],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1608.2379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5806, device='cuda:0')



h[100].sum tensor(-1.7236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4453, device='cuda:0')



h[200].sum tensor(-22.5562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0593, 0.0172,  ..., 0.0117, 0.0588, 0.0393],
        [0.0000, 0.0151, 0.0038,  ..., 0.0053, 0.0146, 0.0086],
        [0.0000, 0.0181, 0.0048,  ..., 0.0057, 0.0175, 0.0111],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60416.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0527, 0.0069, 0.0000,  ..., 0.1697, 0.0899, 0.1028],
        [0.0583, 0.0110, 0.0130,  ..., 0.1287, 0.0616, 0.0743],
        [0.0566, 0.0097, 0.0040,  ..., 0.1486, 0.0768, 0.0876],
        ...,
        [0.0697, 0.0187, 0.0821,  ..., 0.0645, 0.0186, 0.0287],
        [0.0697, 0.0187, 0.0821,  ..., 0.0645, 0.0186, 0.0287],
        [0.0697, 0.0187, 0.0821,  ..., 0.0645, 0.0186, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579784.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9959.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.8015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5005.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(933.1590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1457],
        [ 0.1518],
        [ 0.1589],
        ...,
        [-2.2986],
        [-2.2939],
        [-2.2925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270107.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0147],
        [1.0201],
        ...,
        [1.0035],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368918.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(500.5624, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0147],
        [1.0201],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368926.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1415.9270, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4593, device='cuda:0')



h[100].sum tensor(-1.4161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8432, device='cuda:0')



h[200].sum tensor(-22.5882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0035, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0035, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0035, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56327.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0177, 0.0799,  ..., 0.0623, 0.0180, 0.0277],
        [0.0676, 0.0178, 0.0802,  ..., 0.0625, 0.0181, 0.0278],
        [0.0680, 0.0180, 0.0805,  ..., 0.0630, 0.0182, 0.0280],
        ...,
        [0.0697, 0.0186, 0.0823,  ..., 0.0647, 0.0187, 0.0289],
        [0.0694, 0.0185, 0.0806,  ..., 0.0663, 0.0196, 0.0301],
        [0.0685, 0.0178, 0.0748,  ..., 0.0715, 0.0224, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564560.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9994.9307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.8129, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5039.0605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(899.0842, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0685],
        [-2.2337],
        [-2.3655],
        ...,
        [-2.2769],
        [-2.2118],
        [-2.0950]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265466.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0147],
        [1.0201],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368926.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0148],
        [1.0202],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368934.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0053,  0.0286,  0.0091,  ...,  0.0049,  0.0284,  0.0216],
        [-0.0032,  0.0173,  0.0052,  ...,  0.0033,  0.0172,  0.0120],
        [-0.0016,  0.0088,  0.0022,  ...,  0.0021,  0.0087,  0.0047],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1168.4188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4335, device='cuda:0')



h[100].sum tensor(-1.0465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1089, device='cuda:0')



h[200].sum tensor(-22.6276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0838, 0.0257,  ..., 0.0153, 0.0832, 0.0601],
        [0.0000, 0.0816, 0.0249,  ..., 0.0150, 0.0810, 0.0582],
        [0.0000, 0.0724, 0.0217,  ..., 0.0137, 0.0718, 0.0504],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49152.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0345, 0.0000, 0.0000,  ..., 0.3540, 0.2332, 0.2261],
        [0.0365, 0.0000, 0.0000,  ..., 0.3298, 0.2120, 0.2106],
        [0.0381, 0.0000, 0.0000,  ..., 0.3114, 0.1952, 0.1988],
        ...,
        [0.0700, 0.0187, 0.0827,  ..., 0.0650, 0.0188, 0.0289],
        [0.0700, 0.0187, 0.0827,  ..., 0.0650, 0.0188, 0.0289],
        [0.0700, 0.0187, 0.0827,  ..., 0.0650, 0.0188, 0.0289]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533559.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10178.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.1186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5298.4111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(836.4416, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1594],
        [ 0.1744],
        [ 0.1841],
        ...,
        [-2.3250],
        [-2.3203],
        [-2.3188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284494.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0148],
        [1.0202],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368934.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0148],
        [1.0203],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368942.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0043,  0.0231,  0.0072,  ...,  0.0041,  0.0230,  0.0169],
        [-0.0045,  0.0243,  0.0076,  ...,  0.0043,  0.0241,  0.0179],
        [-0.0061,  0.0329,  0.0106,  ...,  0.0056,  0.0327,  0.0252],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1576.4485, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4040, device='cuda:0')



h[100].sum tensor(-1.6189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2734, device='cuda:0')



h[200].sum tensor(-22.5637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1281, 0.0412,  ..., 0.0218, 0.1274, 0.0979],
        [0.0000, 0.0962, 0.0307,  ..., 0.0171, 0.0955, 0.0730],
        [0.0000, 0.0532, 0.0164,  ..., 0.0109, 0.0526, 0.0387],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0128, 0.0029,  ..., 0.0050, 0.0122, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59978.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0000, 0.0000,  ..., 0.4539, 0.3240, 0.2886],
        [0.0383, 0.0005, 0.0000,  ..., 0.3625, 0.2492, 0.2286],
        [0.0472, 0.0026, 0.0000,  ..., 0.2649, 0.1703, 0.1642],
        ...,
        [0.0701, 0.0185, 0.0810,  ..., 0.0671, 0.0202, 0.0302],
        [0.0687, 0.0174, 0.0692,  ..., 0.0775, 0.0269, 0.0376],
        [0.0641, 0.0138, 0.0367,  ..., 0.1122, 0.0500, 0.0620]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581221.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10023.0176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.2620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4989.5122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(939.8028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1511],
        [ 0.1720],
        [ 0.1930],
        ...,
        [-2.0713],
        [-1.6790],
        [-1.1003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249402.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0148],
        [1.0203],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368942.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0148],
        [1.0203],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368942.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045,  0.0244,  0.0076,  ...,  0.0043,  0.0242,  0.0180],
        [-0.0026,  0.0144,  0.0041,  ...,  0.0029,  0.0143,  0.0095],
        [-0.0016,  0.0088,  0.0022,  ...,  0.0020,  0.0086,  0.0047],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1268.7961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4149, device='cuda:0')



h[100].sum tensor(-1.1830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3984, device='cuda:0')



h[200].sum tensor(-22.6117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0728, 0.0219,  ..., 0.0137, 0.0721, 0.0507],
        [0.0000, 0.0598, 0.0173,  ..., 0.0118, 0.0592, 0.0397],
        [0.0000, 0.0357, 0.0088,  ..., 0.0083, 0.0351, 0.0190],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51873.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0474, 0.0018, 0.0000,  ..., 0.2256, 0.1314, 0.1413],
        [0.0483, 0.0033, 0.0000,  ..., 0.2118, 0.1180, 0.1329],
        [0.0499, 0.0044, 0.0000,  ..., 0.1895, 0.0968, 0.1190],
        ...,
        [0.0704, 0.0188, 0.0833,  ..., 0.0652, 0.0190, 0.0289],
        [0.0704, 0.0188, 0.0833,  ..., 0.0652, 0.0190, 0.0289],
        [0.0704, 0.0188, 0.0833,  ..., 0.0652, 0.0190, 0.0288]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546189.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10247.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7145, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.9137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5275.6377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(862.5719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2380],
        [ 0.2521],
        [ 0.2346],
        ...,
        [-2.3457],
        [-2.3411],
        [-2.3397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291205.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0148],
        [1.0203],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368942.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0148],
        [1.0203],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368950.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [-0.0043,  0.0231,  0.0072,  ...,  0.0041,  0.0229,  0.0169],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1341.8960, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1191, device='cuda:0')



h[100].sum tensor(-1.2865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6474, device='cuda:0')



h[200].sum tensor(-22.5993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0022, 0.0000,  ..., 0.0034, 0.0016, 0.0000],
        [0.0000, 0.0340, 0.0097,  ..., 0.0080, 0.0334, 0.0224],
        [0.0000, 0.0644, 0.0196,  ..., 0.0125, 0.0638, 0.0460],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0035, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53151.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0647, 0.0147, 0.0460,  ..., 0.0955, 0.0413, 0.0500],
        [0.0567, 0.0088, 0.0184,  ..., 0.1715, 0.0971, 0.1017],
        [0.0484, 0.0037, 0.0000,  ..., 0.2590, 0.1641, 0.1604],
        ...,
        [0.0710, 0.0188, 0.0840,  ..., 0.0654, 0.0192, 0.0287],
        [0.0710, 0.0188, 0.0840,  ..., 0.0654, 0.0192, 0.0287],
        [0.0710, 0.0188, 0.0840,  ..., 0.0654, 0.0192, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553037.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10301.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8289, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5230.5791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(878.8196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3315],
        [-0.8036],
        [-0.3208],
        ...,
        [-2.3714],
        [-2.3663],
        [-2.3645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286449.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0148],
        [1.0203],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368950.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0148],
        [1.0204],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368958., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036,  0.0196,  0.0060,  ...,  0.0036,  0.0194,  0.0139],
        [-0.0032,  0.0176,  0.0052,  ...,  0.0033,  0.0174,  0.0122],
        [-0.0032,  0.0178,  0.0053,  ...,  0.0033,  0.0176,  0.0124],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1500.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2259, device='cuda:0')



h[100].sum tensor(-1.5005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1013, device='cuda:0')



h[200].sum tensor(-22.5744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0777, 0.0236,  ..., 0.0143, 0.0771, 0.0550],
        [0.0000, 0.0530, 0.0149,  ..., 0.0107, 0.0524, 0.0339],
        [0.0000, 0.0548, 0.0156,  ..., 0.0110, 0.0542, 0.0355],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0034, 0.0017, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0034, 0.0017, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0034, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59489.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0012, 0.0000,  ..., 0.2767, 0.1771, 0.1730],
        [0.0499, 0.0030, 0.0000,  ..., 0.2241, 0.1314, 0.1392],
        [0.0523, 0.0042, 0.0000,  ..., 0.2027, 0.1141, 0.1249],
        ...,
        [0.0714, 0.0188, 0.0844,  ..., 0.0656, 0.0194, 0.0288],
        [0.0714, 0.0188, 0.0844,  ..., 0.0656, 0.0194, 0.0288],
        [0.0714, 0.0188, 0.0844,  ..., 0.0656, 0.0194, 0.0288]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589435.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10357.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.9890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5268.1030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(932.0031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0515],
        [-0.1937],
        [-0.3414],
        ...,
        [-2.3918],
        [-2.3868],
        [-2.3852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296613.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0148],
        [1.0204],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368958., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0149],
        [1.0204],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368965.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0105,  0.0028,  ...,  0.0023,  0.0103,  0.0062],
        [-0.0012,  0.0071,  0.0016,  ...,  0.0018,  0.0070,  0.0033],
        [-0.0031,  0.0171,  0.0051,  ...,  0.0032,  0.0169,  0.0118],
        ...,
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0005, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1165.4485, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4416, device='cuda:0')



h[100].sum tensor(-1.0159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1100, device='cuda:0')



h[200].sum tensor(-22.6277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0298, 0.0082,  ..., 0.0073, 0.0292, 0.0189],
        [0.0000, 0.0725, 0.0217,  ..., 0.0136, 0.0719, 0.0506],
        [0.0000, 0.0436, 0.0123,  ..., 0.0094, 0.0430, 0.0282],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0034, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49228.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0553, 0.0062, 0.0081,  ..., 0.1734, 0.0937, 0.1050],
        [0.0491, 0.0012, 0.0000,  ..., 0.2295, 0.1337, 0.1436],
        [0.0515, 0.0029, 0.0000,  ..., 0.2095, 0.1180, 0.1300],
        ...,
        [0.0716, 0.0187, 0.0845,  ..., 0.0658, 0.0195, 0.0290],
        [0.0716, 0.0187, 0.0845,  ..., 0.0658, 0.0195, 0.0290],
        [0.0716, 0.0187, 0.0845,  ..., 0.0658, 0.0195, 0.0290]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542201.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10588.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.3022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5518.0811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(839.7357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0634],
        [ 0.1659],
        [ 0.1939],
        ...,
        [-2.4024],
        [-2.3975],
        [-2.3959]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338601.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0149],
        [1.0204],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368965.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0149],
        [1.0205],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368974.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0037,  0.0203,  0.0062,  ...,  0.0037,  0.0201,  0.0145],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0008,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1670.0939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6546, device='cuda:0')



h[100].sum tensor(-1.6645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4561, device='cuda:0')



h[200].sum tensor(-22.5534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0185, 0.0050,  ..., 0.0058, 0.0179, 0.0115],
        [0.0000, 0.0222, 0.0063,  ..., 0.0063, 0.0216, 0.0146],
        [0.0000, 0.0022, 0.0000,  ..., 0.0034, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62316.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0574, 0.0080, 0.0130,  ..., 0.1646, 0.0917, 0.0974],
        [0.0612, 0.0112, 0.0186,  ..., 0.1314, 0.0667, 0.0747],
        [0.0640, 0.0135, 0.0316,  ..., 0.1082, 0.0488, 0.0590],
        ...,
        [0.0713, 0.0189, 0.0844,  ..., 0.0660, 0.0195, 0.0291],
        [0.0713, 0.0189, 0.0844,  ..., 0.0660, 0.0195, 0.0291],
        [0.0713, 0.0189, 0.0844,  ..., 0.0660, 0.0195, 0.0291]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601860., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10297.4873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.3274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5154.0093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(956.8414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1737],
        [ 0.1380],
        [ 0.1095],
        ...,
        [-2.4046],
        [-2.3997],
        [-2.3981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300608.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0149],
        [1.0205],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368974.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0149],
        [1.0205],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368982.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1420.8589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1923, device='cuda:0')



h[100].sum tensor(-1.2739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6581, device='cuda:0')



h[200].sum tensor(-22.5966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0035, 0.0017, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0036, 0.0017, 0.0000],
        [0.0000, 0.0104, 0.0021,  ..., 0.0048, 0.0098, 0.0045],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0036, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0036, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0036, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55254.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0683, 0.0179, 0.0800,  ..., 0.0651, 0.0196, 0.0291],
        [0.0672, 0.0169, 0.0700,  ..., 0.0744, 0.0254, 0.0357],
        [0.0641, 0.0145, 0.0448,  ..., 0.0984, 0.0396, 0.0529],
        ...,
        [0.0709, 0.0190, 0.0841,  ..., 0.0661, 0.0195, 0.0292],
        [0.0709, 0.0190, 0.0841,  ..., 0.0661, 0.0195, 0.0292],
        [0.0709, 0.0190, 0.0841,  ..., 0.0661, 0.0195, 0.0292]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564289.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10252.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.9583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5087.3584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(898.2349, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1864],
        [-1.8272],
        [-1.3208],
        ...,
        [-2.4011],
        [-2.3964],
        [-2.3949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284055.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0149],
        [1.0205],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368982.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0150],
        [1.0206],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368991.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [-0.0031,  0.0172,  0.0051,  ...,  0.0033,  0.0170,  0.0118],
        [-0.0018,  0.0103,  0.0027,  ...,  0.0023,  0.0101,  0.0059],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1596.7048, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3596, device='cuda:0')



h[100].sum tensor(-1.4587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1208, device='cuda:0')



h[200].sum tensor(-22.5743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0417, 0.0110,  ..., 0.0095, 0.0411, 0.0240],
        [0.0000, 0.0259, 0.0062,  ..., 0.0071, 0.0253, 0.0129],
        [0.0000, 0.0643, 0.0189,  ..., 0.0128, 0.0636, 0.0432],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59966.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0538, 0.0079, 0.0000,  ..., 0.1601, 0.0780, 0.0975],
        [0.0529, 0.0071, 0.0043,  ..., 0.1704, 0.0856, 0.1045],
        [0.0462, 0.0026, 0.0000,  ..., 0.2412, 0.1402, 0.1519],
        ...,
        [0.0704, 0.0193, 0.0838,  ..., 0.0662, 0.0194, 0.0293],
        [0.0704, 0.0193, 0.0838,  ..., 0.0662, 0.0194, 0.0293],
        [0.0704, 0.0193, 0.0838,  ..., 0.0662, 0.0194, 0.0293]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587886.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10043.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.6184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4910.1167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(941.4869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2345],
        [ 0.2350],
        [ 0.2387],
        ...,
        [-2.3990],
        [-2.3944],
        [-2.3929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265644.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0150],
        [1.0206],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368991.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(452.0438, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0150],
        [1.0207],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368999.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1266.7759, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2951, device='cuda:0')



h[100].sum tensor(-0.9793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0886, device='cuda:0')



h[200].sum tensor(-22.6287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49749.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0670, 0.0182, 0.0782,  ..., 0.0667, 0.0197, 0.0305],
        [0.0679, 0.0186, 0.0811,  ..., 0.0643, 0.0188, 0.0285],
        [0.0683, 0.0188, 0.0815,  ..., 0.0648, 0.0190, 0.0287],
        ...,
        [0.0701, 0.0195, 0.0836,  ..., 0.0663, 0.0194, 0.0293],
        [0.0701, 0.0195, 0.0836,  ..., 0.0664, 0.0194, 0.0293],
        [0.0701, 0.0195, 0.0836,  ..., 0.0663, 0.0194, 0.0293]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540935.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10160.7588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.5120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5157.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(849.5792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4288],
        [-1.3939],
        [-1.2584],
        ...,
        [-2.4002],
        [-2.3956],
        [-2.3941]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293254.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0150],
        [1.0207],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368999.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0207],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369007.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2180.7720, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3556, device='cuda:0')



h[100].sum tensor(-2.1616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7274, device='cuda:0')



h[200].sum tensor(-22.4899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72472.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.0187, 0.0814,  ..., 0.0639, 0.0188, 0.0280],
        [0.0681, 0.0188, 0.0816,  ..., 0.0641, 0.0188, 0.0281],
        [0.0684, 0.0190, 0.0820,  ..., 0.0646, 0.0190, 0.0284],
        ...,
        [0.0702, 0.0197, 0.0838,  ..., 0.0665, 0.0195, 0.0293],
        [0.0702, 0.0197, 0.0838,  ..., 0.0665, 0.0195, 0.0293],
        [0.0702, 0.0197, 0.0838,  ..., 0.0665, 0.0195, 0.0293]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644319.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9983.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.3419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4985.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1043.9628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3129],
        [-2.3691],
        [-2.3626],
        ...,
        [-2.4134],
        [-2.4087],
        [-2.4072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288401.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0207],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369007.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0208],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369014.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [-0.0016,  0.0090,  0.0023,  ...,  0.0022,  0.0089,  0.0048],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1172.2417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.4480, device='cuda:0')



h[100].sum tensor(-0.8317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8188, device='cuda:0')



h[200].sum tensor(-22.6444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0109, 0.0023,  ..., 0.0050, 0.0103, 0.0049],
        [0.0000, 0.0289, 0.0080,  ..., 0.0077, 0.0284, 0.0178],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47430.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0656, 0.0170, 0.0598,  ..., 0.0832, 0.0317, 0.0412],
        [0.0607, 0.0132, 0.0295,  ..., 0.1227, 0.0575, 0.0687],
        [0.0545, 0.0083, 0.0078,  ..., 0.1757, 0.0934, 0.1053],
        ...,
        [0.0705, 0.0199, 0.0843,  ..., 0.0666, 0.0197, 0.0291],
        [0.0705, 0.0199, 0.0843,  ..., 0.0667, 0.0197, 0.0291],
        [0.0705, 0.0199, 0.0843,  ..., 0.0666, 0.0197, 0.0291]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535841.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10311.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.8721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5338.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(828.8663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3700],
        [-0.0849],
        [ 0.0901],
        ...,
        [-2.4354],
        [-2.4307],
        [-2.4292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315693.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0208],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369014.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0152],
        [1.0209],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369022.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0067,  0.0368,  0.0120,  ...,  0.0062,  0.0366,  0.0285],
        [-0.0057,  0.0313,  0.0101,  ...,  0.0054,  0.0312,  0.0238],
        [-0.0054,  0.0300,  0.0096,  ...,  0.0052,  0.0299,  0.0227],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1394.2659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5950, device='cuda:0')



h[100].sum tensor(-1.1118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4247, device='cuda:0')



h[200].sum tensor(-22.6105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0989, 0.0311,  ..., 0.0179, 0.0983, 0.0727],
        [0.0000, 0.1364, 0.0442,  ..., 0.0234, 0.1356, 0.1046],
        [0.0000, 0.0960, 0.0301,  ..., 0.0175, 0.0953, 0.0702],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52880.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0415, 0.0009, 0.0000,  ..., 0.3098, 0.2004, 0.1949],
        [0.0386, 0.0000, 0.0000,  ..., 0.3507, 0.2347, 0.2212],
        [0.0425, 0.0011, 0.0000,  ..., 0.3105, 0.2012, 0.1945],
        ...,
        [0.0707, 0.0200, 0.0846,  ..., 0.0668, 0.0199, 0.0291],
        [0.0707, 0.0200, 0.0846,  ..., 0.0668, 0.0199, 0.0291],
        [0.0707, 0.0200, 0.0846,  ..., 0.0668, 0.0199, 0.0291]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557804.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10255.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.5516, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5169.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(880.0242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2168],
        [ 0.1999],
        [ 0.1834],
        ...,
        [-2.4518],
        [-2.4470],
        [-2.4454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305378.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0152],
        [1.0209],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369022.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0153],
        [1.0210],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369029.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1684.1941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1120, device='cuda:0')



h[100].sum tensor(-1.4808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2307, device='cuda:0')



h[200].sum tensor(-22.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0018, 0.0000],
        [0.0000, 0.0227, 0.0058,  ..., 0.0067, 0.0222, 0.0125],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0038, 0.0018, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0038, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61494.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0172, 0.0647,  ..., 0.0807, 0.0284, 0.0398],
        [0.0631, 0.0148, 0.0363,  ..., 0.1059, 0.0452, 0.0571],
        [0.0574, 0.0104, 0.0146,  ..., 0.1540, 0.0771, 0.0905],
        ...,
        [0.0710, 0.0200, 0.0850,  ..., 0.0670, 0.0202, 0.0291],
        [0.0710, 0.0200, 0.0850,  ..., 0.0670, 0.0202, 0.0291],
        [0.0710, 0.0200, 0.0850,  ..., 0.0670, 0.0202, 0.0291]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599142.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10092.0439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.6137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4805.3037, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(963.2038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7587],
        [-0.3174],
        [-0.0039],
        ...,
        [-2.4699],
        [-2.4648],
        [-2.4627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252532.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0153],
        [1.0210],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369029.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0153],
        [1.0211],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369037.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0049,  0.0272,  0.0087,  ...,  0.0048,  0.0271,  0.0203],
        [-0.0028,  0.0158,  0.0047,  ...,  0.0031,  0.0156,  0.0106],
        [-0.0040,  0.0223,  0.0069,  ...,  0.0041,  0.0221,  0.0161],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0004, -0.0024],
        [-0.0024,  0.0138,  0.0039,  ...,  0.0028,  0.0136,  0.0089],
        [-0.0024,  0.0138,  0.0039,  ...,  0.0028,  0.0136,  0.0089]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1414.4905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9687, device='cuda:0')



h[100].sum tensor(-1.1259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4793, device='cuda:0')



h[200].sum tensor(-22.6069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0397, 0.0111,  ..., 0.0092, 0.0391, 0.0246],
        [0.0000, 0.0829, 0.0255,  ..., 0.0155, 0.0823, 0.0590],
        [0.0000, 0.0484, 0.0141,  ..., 0.0105, 0.0479, 0.0321],
        ...,
        [0.0000, 0.0272, 0.0073,  ..., 0.0074, 0.0267, 0.0163],
        [0.0000, 0.0272, 0.0073,  ..., 0.0074, 0.0267, 0.0163],
        [0.0000, 0.0272, 0.0073,  ..., 0.0074, 0.0267, 0.0163]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52719.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0055, 0.0000,  ..., 0.2053, 0.1165, 0.1252],
        [0.0492, 0.0035, 0.0000,  ..., 0.2346, 0.1404, 0.1446],
        [0.0525, 0.0060, 0.0000,  ..., 0.2056, 0.1174, 0.1252],
        ...,
        [0.0629, 0.0135, 0.0244,  ..., 0.1305, 0.0617, 0.0735],
        [0.0612, 0.0121, 0.0094,  ..., 0.1439, 0.0704, 0.0829],
        [0.0612, 0.0121, 0.0094,  ..., 0.1439, 0.0704, 0.0829]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558150.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10405.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7768, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.3250, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5292.9849, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(878.5918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1744],
        [ 0.1846],
        [ 0.1685],
        ...,
        [-1.4066],
        [-1.1626],
        [-1.1613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320328.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0153],
        [1.0211],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369037.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0154],
        [1.0211],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369045.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1281.4424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3680, device='cuda:0')



h[100].sum tensor(-0.9438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0993, device='cuda:0')



h[200].sum tensor(-22.6279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49672.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0189, 0.0828,  ..., 0.0647, 0.0196, 0.0282],
        [0.0690, 0.0189, 0.0825,  ..., 0.0655, 0.0199, 0.0289],
        [0.0681, 0.0183, 0.0751,  ..., 0.0730, 0.0237, 0.0347],
        ...,
        [0.0713, 0.0199, 0.0853,  ..., 0.0673, 0.0204, 0.0296],
        [0.0713, 0.0199, 0.0853,  ..., 0.0673, 0.0204, 0.0296],
        [0.0713, 0.0199, 0.0853,  ..., 0.0673, 0.0204, 0.0296]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548142.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10475.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.5124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5353.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(852.4823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2569],
        [-1.9880],
        [-1.5979],
        ...,
        [-2.4943],
        [-2.4895],
        [-2.4879]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332806., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0154],
        [1.0211],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369045.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0154],
        [1.0211],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369045.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        ...,
        [-0.0012,  0.0073,  0.0017,  ...,  0.0019,  0.0071,  0.0033],
        [-0.0020,  0.0116,  0.0032,  ...,  0.0025,  0.0115,  0.0070],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1418.4893, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0439, device='cuda:0')



h[100].sum tensor(-1.1158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4903, device='cuda:0')



h[200].sum tensor(-22.6071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0649, 0.0191,  ..., 0.0130, 0.0643, 0.0434],
        [0.0000, 0.0244, 0.0063,  ..., 0.0070, 0.0238, 0.0138],
        [0.0000, 0.0139, 0.0033,  ..., 0.0055, 0.0133, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55021.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0656, 0.0163, 0.0542,  ..., 0.0897, 0.0360, 0.0458],
        [0.0684, 0.0184, 0.0775,  ..., 0.0700, 0.0225, 0.0320],
        [0.0696, 0.0192, 0.0834,  ..., 0.0654, 0.0198, 0.0286],
        ...,
        [0.0522, 0.0050, 0.0000,  ..., 0.2225, 0.1274, 0.1370],
        [0.0588, 0.0101, 0.0126,  ..., 0.1676, 0.0887, 0.0993],
        [0.0649, 0.0149, 0.0354,  ..., 0.1184, 0.0551, 0.0650]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576982.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10348.2324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.9338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5157.8667, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(901.6816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8180],
        [-1.4244],
        [-1.9298],
        ...,
        [-0.0257],
        [-0.4354],
        [-1.0748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305861.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0154],
        [1.0211],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369045.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0155],
        [1.0212],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369052.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1209.2891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.9137, device='cuda:0')



h[100].sum tensor(-0.8391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8868, device='cuda:0')



h[200].sum tensor(-22.6399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0024, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0037, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48251.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0189, 0.0828,  ..., 0.0649, 0.0196, 0.0284],
        [0.0692, 0.0190, 0.0830,  ..., 0.0651, 0.0197, 0.0286],
        [0.0696, 0.0192, 0.0834,  ..., 0.0656, 0.0199, 0.0288],
        ...,
        [0.0691, 0.0182, 0.0671,  ..., 0.0837, 0.0304, 0.0412],
        [0.0706, 0.0193, 0.0792,  ..., 0.0729, 0.0237, 0.0336],
        [0.0713, 0.0199, 0.0852,  ..., 0.0675, 0.0204, 0.0298]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542091.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10442.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.3311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.2104, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5279.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(843.4091, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1521],
        [-1.9272],
        [-1.5589],
        ...,
        [-1.7860],
        [-2.0792],
        [-2.3059]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314407.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0155],
        [1.0212],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369052.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0156],
        [1.0213],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369060.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [-0.0035,  0.0201,  0.0062,  ...,  0.0038,  0.0200,  0.0143]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2108.6001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9985, device='cuda:0')



h[100].sum tensor(-1.9343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.3830, device='cuda:0')



h[200].sum tensor(-22.5047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0425, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0038, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0038, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0227, 0.0064,  ..., 0.0068, 0.0222, 0.0148],
        [0.0000, 0.0393, 0.0115,  ..., 0.0093, 0.0388, 0.0265]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72135.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0191, 0.0830,  ..., 0.0651, 0.0197, 0.0283],
        [0.0677, 0.0181, 0.0712,  ..., 0.0761, 0.0265, 0.0360],
        [0.0643, 0.0154, 0.0415,  ..., 0.1112, 0.0517, 0.0599],
        ...,
        [0.0691, 0.0184, 0.0626,  ..., 0.0872, 0.0346, 0.0429],
        [0.0640, 0.0144, 0.0298,  ..., 0.1372, 0.0725, 0.0766],
        [0.0578, 0.0095, 0.0069,  ..., 0.1979, 0.1186, 0.1175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653385.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10063.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.4587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4753.5322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1057.9387, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8475],
        [-1.3469],
        [-0.6598],
        ...,
        [-1.8255],
        [-1.2503],
        [-0.6420]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276149.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0156],
        [1.0213],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369060.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(450.7890, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0157],
        [1.0214],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369067.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1190.2102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.3859, device='cuda:0')



h[100].sum tensor(-0.7954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8097, device='cuda:0')



h[200].sum tensor(-22.6438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0123, 0.0021,  ..., 0.0052, 0.0118, 0.0037],
        [0.0000, 0.0073, 0.0011,  ..., 0.0045, 0.0069, 0.0018],
        [0.0000, 0.0467, 0.0141,  ..., 0.0103, 0.0461, 0.0329],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47269.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0148, 0.0381,  ..., 0.1071, 0.0415, 0.0601],
        [0.0604, 0.0135, 0.0241,  ..., 0.1262, 0.0577, 0.0721],
        [0.0541, 0.0082, 0.0034,  ..., 0.2008, 0.1165, 0.1214],
        ...,
        [0.0714, 0.0202, 0.0856,  ..., 0.0679, 0.0205, 0.0297],
        [0.0714, 0.0202, 0.0856,  ..., 0.0679, 0.0205, 0.0297],
        [0.0714, 0.0202, 0.0856,  ..., 0.0679, 0.0205, 0.0297]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541296.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10547.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.0136, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5465.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(833.2321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9782],
        [-0.4163],
        [-0.0130],
        ...,
        [-2.5285],
        [-2.5234],
        [-2.5217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-354054.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0157],
        [1.0214],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369067.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0158],
        [1.0214],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369075.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0009,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1995.6345, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6746, device='cuda:0')



h[100].sum tensor(-1.7578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0435, device='cuda:0')



h[200].sum tensor(-22.5232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0293, 0.0081,  ..., 0.0078, 0.0287, 0.0181],
        [0.0000, 0.0302, 0.0084,  ..., 0.0079, 0.0297, 0.0189],
        [0.0000, 0.0120, 0.0027,  ..., 0.0052, 0.0115, 0.0058],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0039, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66862.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0575, 0.0111, 0.0069,  ..., 0.1501, 0.0737, 0.0881],
        [0.0579, 0.0112, 0.0066,  ..., 0.1507, 0.0744, 0.0883],
        [0.0622, 0.0143, 0.0270,  ..., 0.1206, 0.0541, 0.0672],
        ...,
        [0.0714, 0.0204, 0.0858,  ..., 0.0681, 0.0205, 0.0296],
        [0.0714, 0.0204, 0.0858,  ..., 0.0681, 0.0205, 0.0296],
        [0.0714, 0.0204, 0.0858,  ..., 0.0681, 0.0205, 0.0296]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627645.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10074.7334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.3261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4710.4932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1016.4794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7736],
        [-0.8617],
        [-1.1882],
        ...,
        [-2.5393],
        [-2.5343],
        [-2.5324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255040.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0158],
        [1.0214],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369075.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0159],
        [1.0215],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369083.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0092,  0.0023,  ...,  0.0022,  0.0090,  0.0049],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1956.6864, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2339, device='cuda:0')



h[100].sum tensor(-1.6711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8330, device='cuda:0')



h[200].sum tensor(-22.5325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0308, 0.0086,  ..., 0.0081, 0.0302, 0.0194],
        [0.0000, 0.0111, 0.0024,  ..., 0.0052, 0.0106, 0.0050],
        [0.0000, 0.0025, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69623.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0556, 0.0103, 0.0086,  ..., 0.1647, 0.0851, 0.0979],
        [0.0603, 0.0136, 0.0189,  ..., 0.1296, 0.0612, 0.0734],
        [0.0630, 0.0153, 0.0263,  ..., 0.1161, 0.0536, 0.0633],
        ...,
        [0.0710, 0.0206, 0.0856,  ..., 0.0682, 0.0204, 0.0297],
        [0.0709, 0.0206, 0.0856,  ..., 0.0682, 0.0204, 0.0297],
        [0.0709, 0.0206, 0.0856,  ..., 0.0682, 0.0204, 0.0297]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654390., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10069.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.5652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4957.5200, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1032.6371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0377],
        [-0.1344],
        [-0.3087],
        ...,
        [-2.5279],
        [-2.5228],
        [-2.5209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300773.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0159],
        [1.0215],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369083.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0160],
        [1.0216],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369090.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0028,  0.0164,  0.0048,  ...,  0.0033,  0.0162,  0.0110],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1495.4302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3185, device='cuda:0')



h[100].sum tensor(-1.0951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5304, device='cuda:0')



h[200].sum tensor(-22.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0756, 0.0235,  ..., 0.0147, 0.0749, 0.0551],
        [0.0000, 0.0294, 0.0081,  ..., 0.0079, 0.0288, 0.0181],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55648.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0425, 0.0020, 0.0000,  ..., 0.3183, 0.2110, 0.1990],
        [0.0495, 0.0057, 0.0000,  ..., 0.2439, 0.1517, 0.1495],
        [0.0554, 0.0096, 0.0000,  ..., 0.1922, 0.1127, 0.1144],
        ...,
        [0.0707, 0.0206, 0.0855,  ..., 0.0683, 0.0203, 0.0299],
        [0.0707, 0.0206, 0.0855,  ..., 0.0683, 0.0203, 0.0299],
        [0.0707, 0.0206, 0.0855,  ..., 0.0683, 0.0203, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575959.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10032.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.3402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4810.5342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(920.8499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1682],
        [ 0.1692],
        [ 0.1706],
        ...,
        [-2.5387],
        [-2.5338],
        [-2.5320]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262007.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0160],
        [1.0216],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369090.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0160],
        [1.0217],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369097.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1986.3639, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6731, device='cuda:0')



h[100].sum tensor(-1.6681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8972, device='cuda:0')



h[200].sum tensor(-22.5298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0378, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0025, 0.0000,  ..., 0.0039, 0.0019, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71018.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0679, 0.0193, 0.0816,  ..., 0.0674, 0.0200, 0.0303],
        [0.0685, 0.0196, 0.0830,  ..., 0.0665, 0.0198, 0.0292],
        [0.0691, 0.0199, 0.0840,  ..., 0.0665, 0.0198, 0.0289],
        ...,
        [0.0709, 0.0206, 0.0859,  ..., 0.0685, 0.0204, 0.0299],
        [0.0709, 0.0206, 0.0858,  ..., 0.0685, 0.0204, 0.0299],
        [0.0709, 0.0206, 0.0858,  ..., 0.0684, 0.0204, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669944.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9996.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.2274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4883.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1047.2797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4274],
        [-1.7926],
        [-2.1168],
        ...,
        [-2.5554],
        [-2.5504],
        [-2.5486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304595.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0160],
        [1.0217],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369097.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0161],
        [1.0218],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369105.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0076,  0.0017,  ...,  0.0020,  0.0074,  0.0035],
        [-0.0034,  0.0199,  0.0060,  ...,  0.0038,  0.0197,  0.0140],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1497.0059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6800, device='cuda:0')



h[100].sum tensor(-1.0856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5832, device='cuda:0')



h[200].sum tensor(-22.6029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1064, 0.0336,  ..., 0.0193, 0.1056, 0.0789],
        [0.0000, 0.0312, 0.0087,  ..., 0.0082, 0.0305, 0.0196],
        [0.0000, 0.0221, 0.0061,  ..., 0.0069, 0.0214, 0.0143],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54292.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0455, 0.0033, 0.0000,  ..., 0.2772, 0.1751, 0.1730],
        [0.0545, 0.0093, 0.0078,  ..., 0.1892, 0.1080, 0.1139],
        [0.0607, 0.0139, 0.0273,  ..., 0.1366, 0.0698, 0.0780],
        ...,
        [0.0710, 0.0206, 0.0860,  ..., 0.0686, 0.0203, 0.0300],
        [0.0710, 0.0206, 0.0860,  ..., 0.0686, 0.0203, 0.0300],
        [0.0710, 0.0206, 0.0860,  ..., 0.0686, 0.0203, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(569529.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10139.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.6719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4945.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(907.6374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1839],
        [-0.0624],
        [-0.5065],
        ...,
        [-2.5539],
        [-2.5594],
        [-2.5598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288862.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0161],
        [1.0218],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369105.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0162],
        [1.0218],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369112.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [-0.0019,  0.0113,  0.0030,  ...,  0.0025,  0.0111,  0.0067],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1745.8135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7202, device='cuda:0')



h[100].sum tensor(-1.3745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3196, device='cuda:0')



h[200].sum tensor(-22.5645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0417, 0.0109,  ..., 0.0097, 0.0410, 0.0238],
        [0.0000, 0.0113, 0.0024,  ..., 0.0053, 0.0107, 0.0051],
        [0.0000, 0.0390, 0.0114,  ..., 0.0093, 0.0383, 0.0263],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59030.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0598, 0.0133, 0.0123,  ..., 0.1309, 0.0599, 0.0745],
        [0.0608, 0.0138, 0.0155,  ..., 0.1360, 0.0681, 0.0768],
        [0.0555, 0.0097, 0.0008,  ..., 0.1975, 0.1167, 0.1178],
        ...,
        [0.0712, 0.0208, 0.0864,  ..., 0.0687, 0.0203, 0.0297],
        [0.0712, 0.0208, 0.0864,  ..., 0.0687, 0.0203, 0.0297],
        [0.0712, 0.0208, 0.0864,  ..., 0.0687, 0.0203, 0.0297]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586188.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10180.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.7139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5010.9111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(947.7192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2704],
        [-0.8110],
        [-0.2926],
        ...,
        [-2.5873],
        [-2.5821],
        [-2.5802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301568.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0162],
        [1.0218],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369112.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0163],
        [1.0219],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369120.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0101,  0.0026,  ...,  0.0024,  0.0099,  0.0057],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1416.7057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1996, device='cuda:0')



h[100].sum tensor(-0.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3669, device='cuda:0')



h[200].sum tensor(-22.6138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0320, 0.0089,  ..., 0.0083, 0.0313, 0.0203],
        [0.0000, 0.0121, 0.0027,  ..., 0.0054, 0.0115, 0.0058],
        [0.0000, 0.0025, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53742.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0513, 0.0067, 0.0069,  ..., 0.2219, 0.1326, 0.1354],
        [0.0588, 0.0122, 0.0187,  ..., 0.1538, 0.0816, 0.0890],
        [0.0639, 0.0158, 0.0300,  ..., 0.1162, 0.0556, 0.0627],
        ...,
        [0.0712, 0.0206, 0.0864,  ..., 0.0689, 0.0202, 0.0300],
        [0.0711, 0.0206, 0.0864,  ..., 0.0689, 0.0202, 0.0300],
        [0.0711, 0.0206, 0.0864,  ..., 0.0689, 0.0202, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571384.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10185.5635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.7436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5080.4233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(903.4351, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1168],
        [ 0.1032],
        [ 0.0900],
        ...,
        [-2.5839],
        [-2.5783],
        [-2.5763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302554.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0163],
        [1.0219],
        ...,
        [1.0036],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369120.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0164],
        [1.0220],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369127.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1596.6763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4977, device='cuda:0')



h[100].sum tensor(-1.1838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8488, device='cuda:0')



h[200].sum tensor(-22.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0192, 0.0044,  ..., 0.0064, 0.0185, 0.0095],
        [0.0000, 0.0075, 0.0011,  ..., 0.0047, 0.0069, 0.0019],
        [0.0000, 0.0117, 0.0025,  ..., 0.0053, 0.0110, 0.0054],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58881.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0583, 0.0126, 0.0179,  ..., 0.1326, 0.0574, 0.0775],
        [0.0600, 0.0137, 0.0218,  ..., 0.1231, 0.0517, 0.0705],
        [0.0600, 0.0134, 0.0154,  ..., 0.1293, 0.0560, 0.0745],
        ...,
        [0.0713, 0.0206, 0.0867,  ..., 0.0690, 0.0202, 0.0300],
        [0.0712, 0.0206, 0.0867,  ..., 0.0690, 0.0202, 0.0300],
        [0.0712, 0.0206, 0.0867,  ..., 0.0690, 0.0202, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597607.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10097.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.4321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4942.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(951.0541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2423],
        [-0.0762],
        [ 0.0167],
        ...,
        [-2.6074],
        [-2.6021],
        [-2.6002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293393.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0164],
        [1.0220],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369127.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0164],
        [1.0221],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369134.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [-0.0013,  0.0079,  0.0018,  ...,  0.0020,  0.0077,  0.0038],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1689.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1939, device='cuda:0')



h[100].sum tensor(-1.2813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0966, device='cuda:0')



h[200].sum tensor(-22.5729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0025, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0224, 0.0055,  ..., 0.0069, 0.0217, 0.0122],
        [0.0000, 0.0290, 0.0071,  ..., 0.0079, 0.0283, 0.0154],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59858.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0595, 0.0127, 0.0091,  ..., 0.1376, 0.0655, 0.0790],
        [0.0569, 0.0109, 0.0072,  ..., 0.1507, 0.0701, 0.0895],
        [0.0540, 0.0089, 0.0062,  ..., 0.1740, 0.0842, 0.1062],
        ...,
        [0.0714, 0.0206, 0.0871,  ..., 0.0692, 0.0203, 0.0299],
        [0.0714, 0.0206, 0.0871,  ..., 0.0692, 0.0203, 0.0299],
        [0.0714, 0.0206, 0.0871,  ..., 0.0692, 0.0203, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598981.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10170.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.1801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5008.7007, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(959.3799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2128],
        [ 0.2174],
        [ 0.2228],
        ...,
        [-2.6254],
        [-2.6200],
        [-2.6180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314758.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0164],
        [1.0221],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369134.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(505.1797, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0165],
        [1.0222],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369142.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0377,  0.0123,  ...,  0.0064,  0.0375,  0.0292],
        [-0.0044,  0.0256,  0.0080,  ...,  0.0047,  0.0254,  0.0189],
        [-0.0023,  0.0139,  0.0039,  ...,  0.0029,  0.0137,  0.0089],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1566.2515, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8308, device='cuda:0')



h[100].sum tensor(-1.1268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7513, device='cuda:0')



h[200].sum tensor(-22.5922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1163, 0.0369,  ..., 0.0207, 0.1154, 0.0874],
        [0.0000, 0.1146, 0.0363,  ..., 0.0205, 0.1137, 0.0859],
        [0.0000, 0.0578, 0.0179,  ..., 0.0121, 0.0570, 0.0423],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57118.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.7783e-02, 0.0000e+00, 0.0000e+00,  ..., 3.7074e-01, 2.5058e-01,
         2.3464e-01],
        [3.9588e-02, 3.2464e-04, 0.0000e+00,  ..., 3.5300e-01, 2.3575e-01,
         2.2289e-01],
        [4.7489e-02, 4.2967e-03, 0.0000e+00,  ..., 2.7345e-01, 1.7292e-01,
         1.6954e-01],
        ...,
        [7.1306e-02, 2.0540e-02, 8.7151e-02,  ..., 6.9347e-02, 2.0342e-02,
         3.0094e-02],
        [7.1289e-02, 2.0534e-02, 8.7132e-02,  ..., 6.9329e-02, 2.0336e-02,
         3.0085e-02],
        [7.1276e-02, 2.0526e-02, 8.7120e-02,  ..., 6.9312e-02, 2.0332e-02,
         3.0076e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587690.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10125.8574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.1311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5064.3564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(938.0757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1928],
        [ 0.1916],
        [ 0.1838],
        ...,
        [-2.6300],
        [-2.6245],
        [-2.6225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309863.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0165],
        [1.0222],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369142.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0166],
        [1.0222],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369150.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1685.0723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1257, device='cuda:0')



h[100].sum tensor(-1.2384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0867, device='cuda:0')



h[200].sum tensor(-22.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59384.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0687, 0.0193, 0.0844,  ..., 0.0667, 0.0195, 0.0291],
        [0.0689, 0.0194, 0.0847,  ..., 0.0670, 0.0196, 0.0292],
        [0.0693, 0.0196, 0.0850,  ..., 0.0675, 0.0197, 0.0295],
        ...,
        [0.0711, 0.0204, 0.0870,  ..., 0.0695, 0.0203, 0.0305],
        [0.0711, 0.0204, 0.0870,  ..., 0.0695, 0.0203, 0.0305],
        [0.0711, 0.0203, 0.0870,  ..., 0.0695, 0.0203, 0.0305]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596390.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10074.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.2878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5006.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(954.8998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7085],
        [-2.6055],
        [-2.4246],
        ...,
        [-2.6297],
        [-2.6243],
        [-2.6223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302144.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0166],
        [1.0222],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369150.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0166],
        [1.0223],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369157.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1431.4814, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8319, device='cuda:0')



h[100].sum tensor(-0.9389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3132, device='cuda:0')



h[200].sum tensor(-22.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0020, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54172.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0662, 0.0177, 0.0655,  ..., 0.0832, 0.0303, 0.0411],
        [0.0684, 0.0191, 0.0828,  ..., 0.0688, 0.0204, 0.0309],
        [0.0690, 0.0195, 0.0849,  ..., 0.0677, 0.0196, 0.0299],
        ...,
        [0.0708, 0.0203, 0.0868,  ..., 0.0697, 0.0202, 0.0309],
        [0.0708, 0.0203, 0.0868,  ..., 0.0697, 0.0202, 0.0309],
        [0.0708, 0.0203, 0.0868,  ..., 0.0697, 0.0202, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574548.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10079.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.6215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5081.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(908.1846, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4009],
        [-1.8251],
        [-2.0649],
        ...,
        [-2.6266],
        [-2.6210],
        [-2.6188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309295.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0166],
        [1.0223],
        ...,
        [1.0036],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369157.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0167],
        [1.0224],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369165.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0135,  0.0038,  ...,  0.0029,  0.0133,  0.0086],
        [-0.0026,  0.0156,  0.0045,  ...,  0.0032,  0.0154,  0.0104],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1771.9307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0613, device='cuda:0')



h[100].sum tensor(-1.2929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2233, device='cuda:0')



h[200].sum tensor(-22.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1325, 0.0425,  ..., 0.0232, 0.1316, 0.1011],
        [0.0000, 0.0480, 0.0144,  ..., 0.0107, 0.0473, 0.0339],
        [0.0000, 0.0179, 0.0046,  ..., 0.0063, 0.0172, 0.0106],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61474.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.7880e-02, 5.1220e-05, 0.0000e+00,  ..., 4.8907e-01, 3.5426e-01,
         3.1288e-01],
        [4.3662e-02, 4.6794e-03, 7.5370e-04,  ..., 3.1342e-01, 2.1096e-01,
         1.9606e-01],
        [5.4993e-02, 9.4662e-03, 1.4351e-02,  ..., 1.9431e-01, 1.1468e-01,
         1.1653e-01],
        ...,
        [7.0705e-02, 2.0308e-02, 8.6813e-02,  ..., 6.9915e-02, 2.0199e-02,
         3.1198e-02],
        [7.0686e-02, 2.0301e-02, 8.6792e-02,  ..., 6.9895e-02, 2.0193e-02,
         3.1188e-02],
        [7.0673e-02, 2.0293e-02, 8.6780e-02,  ..., 6.9877e-02, 2.0188e-02,
         3.1178e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603517.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9899.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.4781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4884.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(975.2748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0357],
        [ 0.0606],
        [ 0.0778],
        ...,
        [-2.6164],
        [-2.6113],
        [-2.6101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282999.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0167],
        [1.0224],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369165.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0168],
        [1.0225],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369173.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1559.8320, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0902, device='cuda:0')



h[100].sum tensor(-1.0520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6431, device='cuda:0')



h[200].sum tensor(-22.5979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0040, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0040, 0.0020, 0.0000],
        [0.0000, 0.0159, 0.0032,  ..., 0.0060, 0.0152, 0.0065],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0021, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0021, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57948.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0595, 0.0136, 0.0238,  ..., 0.1367, 0.0684, 0.0782],
        [0.0565, 0.0113, 0.0133,  ..., 0.1649, 0.0888, 0.0977],
        [0.0519, 0.0083, 0.0073,  ..., 0.2008, 0.1121, 0.1233],
        ...,
        [0.0706, 0.0205, 0.0869,  ..., 0.0701, 0.0202, 0.0313],
        [0.0706, 0.0205, 0.0869,  ..., 0.0701, 0.0202, 0.0313],
        [0.0706, 0.0205, 0.0869,  ..., 0.0701, 0.0202, 0.0312]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591430.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9886.4150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2657, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.9223, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4891.9673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(946.7620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0865],
        [ 0.1029],
        [ 0.1118],
        ...,
        [-2.6418],
        [-2.6363],
        [-2.6341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273677.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0168],
        [1.0225],
        ...,
        [1.0036],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369173.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0168],
        [1.0226],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369180.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [-0.0044,  0.0263,  0.0082,  ...,  0.0048,  0.0261,  0.0195],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1433.8975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8969, device='cuda:0')



h[100].sum tensor(-0.9092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3227, device='cuda:0')



h[200].sum tensor(-22.6165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0020, 0.0000],
        [0.0000, 0.0287, 0.0084,  ..., 0.0079, 0.0280, 0.0198],
        [0.0000, 0.0240, 0.0067,  ..., 0.0072, 0.0233, 0.0158],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0021, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0021, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52363.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0662, 0.0181, 0.0616,  ..., 0.0870, 0.0342, 0.0431],
        [0.0614, 0.0148, 0.0306,  ..., 0.1344, 0.0703, 0.0751],
        [0.0588, 0.0129, 0.0130,  ..., 0.1624, 0.0915, 0.0941],
        ...,
        [0.0707, 0.0207, 0.0872,  ..., 0.0703, 0.0203, 0.0312],
        [0.0707, 0.0207, 0.0872,  ..., 0.0703, 0.0203, 0.0312],
        [0.0707, 0.0207, 0.0872,  ..., 0.0703, 0.0203, 0.0312]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562818.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10042.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.6967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5176.9424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(897.0032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1391],
        [-1.6897],
        [-1.2335],
        ...,
        [-2.6561],
        [-2.6510],
        [-2.6492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300812.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0168],
        [1.0226],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369180.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0169],
        [1.0227],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369187.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0037,  0.0219,  0.0067,  ...,  0.0042,  0.0217,  0.0158],
        [-0.0029,  0.0174,  0.0051,  ...,  0.0035,  0.0172,  0.0119],
        [-0.0080,  0.0468,  0.0154,  ...,  0.0078,  0.0466,  0.0370],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1671.8597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9297, device='cuda:0')



h[100].sum tensor(-1.1465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9119, device='cuda:0')



h[200].sum tensor(-22.5826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0509, 0.0154,  ..., 0.0112, 0.0501, 0.0363],
        [0.0000, 0.1332, 0.0428,  ..., 0.0234, 0.1324, 0.1017],
        [0.0000, 0.1255, 0.0400,  ..., 0.0223, 0.1247, 0.0951],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58707.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0493, 0.0075, 0.0022,  ..., 0.2499, 0.1599, 0.1530],
        [0.0366, 0.0016, 0.0000,  ..., 0.3804, 0.2618, 0.2407],
        [0.0311, 0.0000, 0.0000,  ..., 0.4330, 0.3007, 0.2766],
        ...,
        [0.0708, 0.0208, 0.0875,  ..., 0.0705, 0.0204, 0.0313],
        [0.0708, 0.0208, 0.0875,  ..., 0.0704, 0.0204, 0.0313],
        [0.0707, 0.0208, 0.0874,  ..., 0.0704, 0.0203, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595457.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10024.8848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.8887, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5157.9399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(950.1484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0878],
        [ 0.1441],
        [ 0.1713],
        ...,
        [-2.6701],
        [-2.6648],
        [-2.6628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317655.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0169],
        [1.0227],
        ...,
        [1.0035],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369187.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0170],
        [1.0228],
        ...,
        [1.0035],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369195., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0132,  0.0037,  ...,  0.0029,  0.0131,  0.0084],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [-0.0011,  0.0072,  0.0016,  ...,  0.0020,  0.0071,  0.0033],
        ...,
        [-0.0038,  0.0225,  0.0069,  ...,  0.0043,  0.0223,  0.0163],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1998.1228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6099, device='cuda:0')



h[100].sum tensor(-1.4680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7418, device='cuda:0')



h[200].sum tensor(-22.5363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0363, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0490, 0.0140,  ..., 0.0110, 0.0483, 0.0324],
        [0.0000, 0.0380, 0.0095,  ..., 0.0093, 0.0373, 0.0206],
        [0.0000, 0.0214, 0.0044,  ..., 0.0069, 0.0207, 0.0088],
        ...,
        [0.0000, 0.0213, 0.0058,  ..., 0.0070, 0.0206, 0.0134],
        [0.0000, 0.0255, 0.0072,  ..., 0.0076, 0.0248, 0.0170],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65477.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0442, 0.0046, 0.0000,  ..., 0.2571, 0.1521, 0.1622],
        [0.0475, 0.0065, 0.0000,  ..., 0.2147, 0.1147, 0.1353],
        [0.0517, 0.0094, 0.0000,  ..., 0.1767, 0.0845, 0.1099],
        ...,
        [0.0620, 0.0151, 0.0172,  ..., 0.1436, 0.0737, 0.0816],
        [0.0641, 0.0165, 0.0359,  ..., 0.1272, 0.0621, 0.0702],
        [0.0686, 0.0196, 0.0668,  ..., 0.0882, 0.0334, 0.0435]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624120.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9891.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.7503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5065.4653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1012.7890, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2054],
        [ 0.2236],
        [ 0.2341],
        ...,
        [-1.3507],
        [-1.6815],
        [-2.0877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303338.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0170],
        [1.0228],
        ...,
        [1.0035],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369195., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0171],
        [1.0229],
        ...,
        [1.0035],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369201.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1700.8319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5000, device='cuda:0')



h[100].sum tensor(-1.1497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9952, device='cuda:0')



h[200].sum tensor(-22.5797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61060.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0685, 0.0200, 0.0855,  ..., 0.0679, 0.0198, 0.0299],
        [0.0687, 0.0201, 0.0858,  ..., 0.0681, 0.0198, 0.0300],
        [0.0691, 0.0203, 0.0861,  ..., 0.0686, 0.0200, 0.0303],
        ...,
        [0.0709, 0.0211, 0.0881,  ..., 0.0707, 0.0206, 0.0314],
        [0.0709, 0.0211, 0.0881,  ..., 0.0707, 0.0205, 0.0313],
        [0.0709, 0.0211, 0.0881,  ..., 0.0707, 0.0205, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613752.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9922.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.9623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5026.3701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(978.0880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8344],
        [-2.8449],
        [-2.8376],
        ...,
        [-2.6928],
        [-2.6875],
        [-2.6856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305964.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0171],
        [1.0229],
        ...,
        [1.0035],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369201.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0172],
        [1.0230],
        ...,
        [1.0034],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369208.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1996.7329, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.7064, device='cuda:0')



h[100].sum tensor(-1.4441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7559, device='cuda:0')



h[200].sum tensor(-22.5366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0019, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0019, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66820.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0688, 0.0200, 0.0862,  ..., 0.0680, 0.0200, 0.0298],
        [0.0689, 0.0201, 0.0861,  ..., 0.0686, 0.0202, 0.0302],
        [0.0690, 0.0202, 0.0855,  ..., 0.0700, 0.0207, 0.0315],
        ...,
        [0.0713, 0.0211, 0.0889,  ..., 0.0708, 0.0209, 0.0312],
        [0.0712, 0.0211, 0.0888,  ..., 0.0708, 0.0209, 0.0312],
        [0.0712, 0.0211, 0.0888,  ..., 0.0708, 0.0209, 0.0312]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635404.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9947.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.1557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5025.7729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1031.2479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6634],
        [-2.5086],
        [-2.2790],
        ...,
        [-2.7134],
        [-2.7079],
        [-2.7058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298759.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0172],
        [1.0230],
        ...,
        [1.0034],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369208.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(505.2879, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0173],
        [1.0231],
        ...,
        [1.0034],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369214.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        ...,
        [-0.0019,  0.0120,  0.0032,  ...,  0.0027,  0.0118,  0.0074],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0006, -0.0007,  ...,  0.0010,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1840.7032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1803, device='cuda:0')



h[100].sum tensor(-1.2830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3868, device='cuda:0')



h[200].sum tensor(-22.5581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0018, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0018, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0040, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0386, 0.0110,  ..., 0.0094, 0.0377, 0.0258],
        [0.0000, 0.0145, 0.0034,  ..., 0.0059, 0.0137, 0.0077],
        [0.0000, 0.0027, 0.0000,  ..., 0.0041, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64012.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0684, 0.0191, 0.0775,  ..., 0.0763, 0.0262, 0.0354],
        [0.0656, 0.0170, 0.0523,  ..., 0.0988, 0.0411, 0.0515],
        [0.0621, 0.0144, 0.0255,  ..., 0.1278, 0.0598, 0.0722],
        ...,
        [0.0542, 0.0074, 0.0052,  ..., 0.2260, 0.1348, 0.1383],
        [0.0625, 0.0138, 0.0307,  ..., 0.1505, 0.0785, 0.0864],
        [0.0689, 0.0187, 0.0612,  ..., 0.0954, 0.0387, 0.0482]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631980.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10048.1260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.4657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4986.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1011.1033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8987],
        [-0.3962],
        [-0.0434],
        ...,
        [-0.1458],
        [-0.7723],
        [-1.5775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299999.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0173],
        [1.0231],
        ...,
        [1.0034],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369214.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0173],
        [1.0232],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369220.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0004, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1427.0161, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8655, device='cuda:0')



h[100].sum tensor(-0.8617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3181, device='cuda:0')



h[200].sum tensor(-22.6176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0116, 0.0024,  ..., 0.0053, 0.0108, 0.0053],
        [0.0000, 0.0026, 0.0000,  ..., 0.0039, 0.0018, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0040, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0040, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51969.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0637, 0.0150, 0.0437,  ..., 0.1082, 0.0453, 0.0596],
        [0.0682, 0.0183, 0.0745,  ..., 0.0801, 0.0278, 0.0389],
        [0.0698, 0.0194, 0.0839,  ..., 0.0726, 0.0231, 0.0331],
        ...,
        [0.0722, 0.0206, 0.0900,  ..., 0.0711, 0.0214, 0.0315],
        [0.0722, 0.0206, 0.0900,  ..., 0.0711, 0.0214, 0.0315],
        [0.0722, 0.0206, 0.0900,  ..., 0.0710, 0.0214, 0.0315]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568448.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10409.6807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5380.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(901.4355, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9745],
        [-1.4485],
        [-1.6980],
        ...,
        [-2.7539],
        [-2.7480],
        [-2.7459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344672.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0173],
        [1.0232],
        ...,
        [1.0034],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369220.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0175],
        [1.0233],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369227.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0020,  0.0126,  0.0034,  ...,  0.0027,  0.0124,  0.0079],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [-0.0058,  0.0347,  0.0111,  ...,  0.0060,  0.0344,  0.0267],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1698.4973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4869, device='cuda:0')



h[100].sum tensor(-1.1141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9933, device='cuda:0')



h[200].sum tensor(-22.5799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0027,  ..., 0.0053, 0.0117, 0.0061],
        [0.0000, 0.0692, 0.0202,  ..., 0.0137, 0.0682, 0.0473],
        [0.0000, 0.0359, 0.0100,  ..., 0.0088, 0.0349, 0.0236],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0040, 0.0019, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0040, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59960.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0545, 0.0061, 0.0000,  ..., 0.1989, 0.1144, 0.1226],
        [0.0492, 0.0027, 0.0000,  ..., 0.2462, 0.1484, 0.1559],
        [0.0502, 0.0029, 0.0000,  ..., 0.2436, 0.1475, 0.1536],
        ...,
        [0.0726, 0.0201, 0.0900,  ..., 0.0713, 0.0217, 0.0324],
        [0.0725, 0.0201, 0.0900,  ..., 0.0713, 0.0217, 0.0324],
        [0.0725, 0.0201, 0.0899,  ..., 0.0713, 0.0216, 0.0324]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603150.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10264.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5098.9287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(975.7385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2404],
        [ 0.2460],
        [ 0.2498],
        ...,
        [-2.7607],
        [-2.7549],
        [-2.7528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293087.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0175],
        [1.0233],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369227.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0176],
        [1.0234],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369234.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0168,  0.0049,  ...,  0.0034,  0.0165,  0.0114],
        [-0.0017,  0.0108,  0.0028,  ...,  0.0025,  0.0106,  0.0063],
        [-0.0013,  0.0086,  0.0020,  ...,  0.0022,  0.0084,  0.0044],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1422.8168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9869, device='cuda:0')



h[100].sum tensor(-0.8113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1897, device='cuda:0')



h[200].sum tensor(-22.6231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0534, 0.0146,  ..., 0.0115, 0.0523, 0.0338],
        [0.0000, 0.0487, 0.0130,  ..., 0.0108, 0.0477, 0.0297],
        [0.0000, 0.0197, 0.0044,  ..., 0.0065, 0.0188, 0.0097],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0041, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51947.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0049, 0.0000,  ..., 0.1953, 0.1037, 0.1234],
        [0.0546, 0.0073, 0.0022,  ..., 0.1740, 0.0890, 0.1082],
        [0.0608, 0.0123, 0.0273,  ..., 0.1306, 0.0590, 0.0770],
        ...,
        [0.0720, 0.0202, 0.0898,  ..., 0.0714, 0.0217, 0.0325],
        [0.0719, 0.0201, 0.0898,  ..., 0.0714, 0.0217, 0.0325],
        [0.0719, 0.0201, 0.0898,  ..., 0.0714, 0.0217, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568787.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10326.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.5902, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5261.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(902.8694, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0991],
        [-0.2925],
        [-0.9082],
        ...,
        [-2.7495],
        [-2.7446],
        [-2.7436]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328566.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0176],
        [1.0234],
        ...,
        [1.0034],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369234.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0176],
        [1.0235],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369241.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1713.1211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6856, device='cuda:0')



h[100].sum tensor(-1.0620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8762, device='cuda:0')



h[200].sum tensor(-22.5851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0028, 0.0000,  ..., 0.0041, 0.0019, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0041, 0.0020, 0.0000],
        [0.0000, 0.0381, 0.0108,  ..., 0.0093, 0.0371, 0.0253],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0042, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59409.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0676, 0.0182, 0.0749,  ..., 0.0790, 0.0282, 0.0385],
        [0.0643, 0.0154, 0.0448,  ..., 0.1084, 0.0492, 0.0590],
        [0.0546, 0.0080, 0.0150,  ..., 0.1981, 0.1159, 0.1212],
        ...,
        [0.0713, 0.0204, 0.0896,  ..., 0.0715, 0.0217, 0.0327],
        [0.0713, 0.0203, 0.0895,  ..., 0.0715, 0.0217, 0.0327],
        [0.0713, 0.0203, 0.0895,  ..., 0.0715, 0.0216, 0.0327]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599112.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10042.9189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.3121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4976.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(971.8328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3091],
        [-0.5973],
        [-0.0810],
        ...,
        [-2.7456],
        [-2.7402],
        [-2.7381]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295654.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0176],
        [1.0235],
        ...,
        [1.0033],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369241.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0177],
        [1.0235],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369247.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0091,  0.0022,  ...,  0.0023,  0.0089,  0.0049],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1489.2933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3589, device='cuda:0')



h[100].sum tensor(-0.8159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2441, device='cuda:0')



h[200].sum tensor(-22.6206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0308, 0.0075,  ..., 0.0084, 0.0298, 0.0167],
        [0.0000, 0.0246, 0.0061,  ..., 0.0075, 0.0237, 0.0138],
        [0.0000, 0.0029, 0.0000,  ..., 0.0043, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0043, 0.0020, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0043, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54785.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0546, 0.0095, 0.0071,  ..., 0.1612, 0.0771, 0.0988],
        [0.0579, 0.0119, 0.0214,  ..., 0.1409, 0.0647, 0.0840],
        [0.0641, 0.0167, 0.0541,  ..., 0.0996, 0.0384, 0.0541],
        ...,
        [0.0709, 0.0209, 0.0894,  ..., 0.0717, 0.0216, 0.0326],
        [0.0709, 0.0209, 0.0894,  ..., 0.0717, 0.0216, 0.0326],
        [0.0708, 0.0209, 0.0894,  ..., 0.0717, 0.0216, 0.0326]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587787.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10064.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.7708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5047.3936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(928.2756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0792],
        [-0.3035],
        [-0.6811],
        ...,
        [-2.7448],
        [-2.7395],
        [-2.7374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323618.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0177],
        [1.0235],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369247.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0178],
        [1.0236],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369254.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1788.7240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3068, device='cuda:0')



h[100].sum tensor(-1.0713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9670, device='cuda:0')



h[200].sum tensor(-22.5813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0133, 0.0022,  ..., 0.0059, 0.0123, 0.0042],
        [0.0000, 0.0195, 0.0044,  ..., 0.0069, 0.0186, 0.0095],
        [0.0000, 0.0258, 0.0065,  ..., 0.0078, 0.0248, 0.0148],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0045, 0.0020, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0045, 0.0020, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0045, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60486.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0578, 0.0136, 0.0177,  ..., 0.1317, 0.0564, 0.0776],
        [0.0578, 0.0134, 0.0166,  ..., 0.1372, 0.0615, 0.0808],
        [0.0579, 0.0130, 0.0125,  ..., 0.1444, 0.0676, 0.0851],
        ...,
        [0.0703, 0.0216, 0.0892,  ..., 0.0720, 0.0215, 0.0324],
        [0.0703, 0.0216, 0.0892,  ..., 0.0719, 0.0215, 0.0324],
        [0.0703, 0.0215, 0.0892,  ..., 0.0719, 0.0215, 0.0323]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608471.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9839.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.6095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4963.6182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(982.6411, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0740],
        [-0.0352],
        [-0.2070],
        ...,
        [-2.7424],
        [-2.7371],
        [-2.7350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301237.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0178],
        [1.0236],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369254.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0179],
        [1.0237],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369261.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016,  0.0106,  0.0027,  ...,  0.0026,  0.0103,  0.0061],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [-0.0051,  0.0311,  0.0099,  ...,  0.0056,  0.0308,  0.0235],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1614.3939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6058, device='cuda:0')



h[100].sum tensor(-0.8748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4262, device='cuda:0')



h[200].sum tensor(-22.6098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0111, 0.0021,  ..., 0.0058, 0.0101, 0.0046],
        [0.0000, 0.0438, 0.0128,  ..., 0.0107, 0.0427, 0.0301],
        [0.0000, 0.0282, 0.0081,  ..., 0.0083, 0.0272, 0.0191],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0047, 0.0020, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0047, 0.0020, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0047, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54911.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0569, 0.0139, 0.0068,  ..., 0.1436, 0.0678, 0.0845],
        [0.0548, 0.0121, 0.0000,  ..., 0.1776, 0.0980, 0.1062],
        [0.0564, 0.0131, 0.0020,  ..., 0.1753, 0.0995, 0.1036],
        ...,
        [0.0698, 0.0222, 0.0889,  ..., 0.0721, 0.0214, 0.0322],
        [0.0698, 0.0222, 0.0889,  ..., 0.0721, 0.0214, 0.0322],
        [0.0698, 0.0222, 0.0889,  ..., 0.0721, 0.0214, 0.0322]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578965.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9754.6973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.1052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4956.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(938.8765, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0103],
        [ 0.0083],
        [-0.1447],
        ...,
        [-2.7381],
        [-2.7329],
        [-2.7308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296302.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0179],
        [1.0237],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369261.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0179],
        [1.0237],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369267.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016,  0.0102,  0.0026,  ...,  0.0026,  0.0099,  0.0057],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1629.9091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6627, device='cuda:0')



h[100].sum tensor(-0.8688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4346, device='cuda:0')



h[200].sum tensor(-22.6097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0232, 0.0057,  ..., 0.0077, 0.0222, 0.0126],
        [0.0000, 0.0125, 0.0026,  ..., 0.0061, 0.0116, 0.0058],
        [0.0000, 0.0029, 0.0000,  ..., 0.0047, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0048, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56010.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0124, 0.0184,  ..., 0.1750, 0.0923, 0.1048],
        [0.0605, 0.0171, 0.0398,  ..., 0.1183, 0.0515, 0.0658],
        [0.0649, 0.0202, 0.0698,  ..., 0.0856, 0.0291, 0.0430],
        ...,
        [0.0695, 0.0226, 0.0888,  ..., 0.0723, 0.0213, 0.0322],
        [0.0695, 0.0226, 0.0888,  ..., 0.0723, 0.0212, 0.0322],
        [0.0694, 0.0226, 0.0888,  ..., 0.0722, 0.0212, 0.0322]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586262.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9621.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.2408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4858.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(952.2700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1114],
        [-0.5543],
        [-1.0974],
        ...,
        [-2.7404],
        [-2.7353],
        [-2.7331]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281748.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0179],
        [1.0237],
        ...,
        [1.0032],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369267.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0180],
        [1.0238],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369273.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0094,  0.0024,  ...,  0.0024,  0.0092,  0.0051],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1543.8694, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1163, device='cuda:0')



h[100].sum tensor(-0.7837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2086, device='cuda:0')



h[200].sum tensor(-22.6217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0243, 0.0061,  ..., 0.0078, 0.0233, 0.0135],
        [0.0000, 0.0118, 0.0024,  ..., 0.0060, 0.0109, 0.0052],
        [0.0000, 0.0029, 0.0000,  ..., 0.0047, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0048, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54198.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0556, 0.0138, 0.0170,  ..., 0.1539, 0.0736, 0.0909],
        [0.0603, 0.0170, 0.0322,  ..., 0.1206, 0.0523, 0.0673],
        [0.0635, 0.0192, 0.0524,  ..., 0.1009, 0.0402, 0.0533],
        ...,
        [0.0696, 0.0227, 0.0889,  ..., 0.0725, 0.0212, 0.0324],
        [0.0695, 0.0227, 0.0889,  ..., 0.0725, 0.0212, 0.0324],
        [0.0695, 0.0227, 0.0889,  ..., 0.0724, 0.0212, 0.0324]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580566.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9611.6465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8942, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(116.1899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4813.4438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(940.9655, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0190],
        [-0.1823],
        [-0.2879],
        ...,
        [-2.7528],
        [-2.7480],
        [-2.7463]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276519.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0180],
        [1.0238],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369273.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(504.1944, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0181],
        [1.0239],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369279.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058,  0.0358,  0.0116,  ...,  0.0064,  0.0355,  0.0275],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [-0.0036,  0.0225,  0.0069,  ...,  0.0044,  0.0222,  0.0162],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1928.6135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4016, device='cuda:0')



h[100].sum tensor(-1.1447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2731, device='cuda:0')



h[200].sum tensor(-22.5649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1001, 0.0319,  ..., 0.0191, 0.0990, 0.0757],
        [0.0000, 0.1184, 0.0376,  ..., 0.0218, 0.1173, 0.0889],
        [0.0000, 0.0298, 0.0080,  ..., 0.0086, 0.0289, 0.0182],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0047, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62669.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0314, 0.0011, 0.0000,  ..., 0.4464, 0.3145, 0.2834],
        [0.0372, 0.0030, 0.0000,  ..., 0.3733, 0.2520, 0.2357],
        [0.0481, 0.0085, 0.0000,  ..., 0.2472, 0.1469, 0.1528],
        ...,
        [0.0700, 0.0228, 0.0893,  ..., 0.0727, 0.0213, 0.0324],
        [0.0700, 0.0228, 0.0892,  ..., 0.0727, 0.0213, 0.0324],
        [0.0700, 0.0228, 0.0892,  ..., 0.0726, 0.0212, 0.0324]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618239.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9699.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.2794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4888.8325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1010.9796, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0826],
        [ 0.1198],
        [ 0.1560],
        ...,
        [-2.7811],
        [-2.7758],
        [-2.7735]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303035.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0181],
        [1.0239],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369279.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0182],
        [1.0239],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369285.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [-0.0033,  0.0208,  0.0063,  ...,  0.0041,  0.0206,  0.0148],
        [-0.0033,  0.0208,  0.0063,  ...,  0.0041,  0.0206,  0.0148],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1698.8772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4542, device='cuda:0')



h[100].sum tensor(-0.9328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6963, device='cuda:0')



h[200].sum tensor(-22.5966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0232, 0.0064,  ..., 0.0075, 0.0224, 0.0150],
        [0.0000, 0.0400, 0.0116,  ..., 0.0101, 0.0391, 0.0269],
        [0.0000, 0.1177, 0.0373,  ..., 0.0217, 0.1167, 0.0883],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0046, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0046, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0046, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56421.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0601, 0.0162, 0.0284,  ..., 0.1410, 0.0731, 0.0797],
        [0.0540, 0.0122, 0.0075,  ..., 0.2028, 0.1202, 0.1213],
        [0.0444, 0.0065, 0.0000,  ..., 0.3088, 0.2037, 0.1922],
        ...,
        [0.0703, 0.0226, 0.0892,  ..., 0.0729, 0.0213, 0.0329],
        [0.0703, 0.0226, 0.0892,  ..., 0.0729, 0.0213, 0.0329],
        [0.0703, 0.0225, 0.0892,  ..., 0.0729, 0.0213, 0.0328]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590653.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9774.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.2463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4892.6860, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(959.3704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8476],
        [-0.4076],
        [-0.0180],
        ...,
        [-2.7941],
        [-2.7887],
        [-2.7863]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284969.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0182],
        [1.0239],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369285.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0182],
        [1.0239],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369292.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0021,  0.0136,  0.0038,  ...,  0.0030,  0.0134,  0.0087],
        [-0.0070,  0.0435,  0.0143,  ...,  0.0075,  0.0432,  0.0341],
        [-0.0033,  0.0207,  0.0063,  ...,  0.0041,  0.0205,  0.0147],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.8970, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4492, device='cuda:0')



h[100].sum tensor(-0.7251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1111, device='cuda:0')



h[200].sum tensor(-22.6281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1320, 0.0423,  ..., 0.0237, 0.1310, 0.1006],
        [0.0000, 0.0992, 0.0309,  ..., 0.0188, 0.0983, 0.0727],
        [0.0000, 0.1013, 0.0316,  ..., 0.0191, 0.1004, 0.0744],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52718.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0408, 0.0039, 0.0000,  ..., 0.3327, 0.2187, 0.2097],
        [0.0391, 0.0016, 0.0000,  ..., 0.3452, 0.2256, 0.2186],
        [0.0395, 0.0029, 0.0000,  ..., 0.3442, 0.2243, 0.2179],
        ...,
        [0.0709, 0.0223, 0.0895,  ..., 0.0731, 0.0214, 0.0331],
        [0.0709, 0.0223, 0.0895,  ..., 0.0731, 0.0214, 0.0331],
        [0.0708, 0.0223, 0.0895,  ..., 0.0731, 0.0214, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580932.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10018.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5139.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(923.2006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1644],
        [ 0.1591],
        [ 0.1483],
        ...,
        [-2.8199],
        [-2.8145],
        [-2.8121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321541.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0182],
        [1.0239],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369292.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0162],
        [1.0183],
        [1.0240],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369298.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2024.5940, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7409, device='cuda:0')



h[100].sum tensor(-1.2336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6149, device='cuda:0')



h[200].sum tensor(-22.5467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0121, 0.0025,  ..., 0.0057, 0.0113, 0.0055],
        [0.0000, 0.0028, 0.0000,  ..., 0.0044, 0.0021, 0.0000],
        [0.0000, 0.0386, 0.0111,  ..., 0.0097, 0.0379, 0.0258],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66063.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0584, 0.0144, 0.0099,  ..., 0.1458, 0.0693, 0.0856],
        [0.0579, 0.0140, 0.0133,  ..., 0.1632, 0.0866, 0.0961],
        [0.0481, 0.0079, 0.0057,  ..., 0.2728, 0.1736, 0.1691],
        ...,
        [0.0712, 0.0222, 0.0896,  ..., 0.0733, 0.0215, 0.0333],
        [0.0712, 0.0221, 0.0896,  ..., 0.0733, 0.0215, 0.0333],
        [0.0711, 0.0221, 0.0896,  ..., 0.0733, 0.0215, 0.0333]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645247.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9907.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.7355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4933.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1040.2378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1772],
        [ 0.1426],
        [ 0.1029],
        ...,
        [-2.8372],
        [-2.8317],
        [-2.8293]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315482.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0162],
        [1.0183],
        [1.0240],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369298.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0162],
        [1.0183],
        [1.0240],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369298.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0025,  0.0159,  0.0046,  ...,  0.0033,  0.0157,  0.0107],
        [-0.0012,  0.0081,  0.0019,  ...,  0.0022,  0.0079,  0.0040],
        [-0.0040,  0.0248,  0.0077,  ...,  0.0047,  0.0246,  0.0182],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1740.5183, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7138, device='cuda:0')



h[100].sum tensor(-0.9797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8804, device='cuda:0')



h[200].sum tensor(-22.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0532, 0.0148,  ..., 0.0119, 0.0524, 0.0336],
        [0.0000, 0.0680, 0.0199,  ..., 0.0141, 0.0671, 0.0461],
        [0.0000, 0.0501, 0.0137,  ..., 0.0114, 0.0493, 0.0309],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58047.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0449, 0.0055, 0.0000,  ..., 0.2512, 0.1399, 0.1596],
        [0.0443, 0.0050, 0.0000,  ..., 0.2613, 0.1486, 0.1659],
        [0.0452, 0.0055, 0.0000,  ..., 0.2550, 0.1423, 0.1619],
        ...,
        [0.0712, 0.0222, 0.0896,  ..., 0.0733, 0.0215, 0.0333],
        [0.0712, 0.0221, 0.0896,  ..., 0.0733, 0.0215, 0.0333],
        [0.0711, 0.0221, 0.0896,  ..., 0.0733, 0.0215, 0.0333]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598026.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10035.0850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.7113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5094.7100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(970.6115, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2319],
        [ 0.2082],
        [ 0.1749],
        ...,
        [-2.8364],
        [-2.8316],
        [-2.8293]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325373.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0162],
        [1.0183],
        [1.0240],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369298.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0184],
        [1.0240],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369304.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035,  0.0222,  0.0068,  ...,  0.0043,  0.0220,  0.0160],
        [-0.0050,  0.0313,  0.0100,  ...,  0.0056,  0.0311,  0.0237],
        [-0.0044,  0.0275,  0.0087,  ...,  0.0051,  0.0273,  0.0205],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1827.8474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3276, device='cuda:0')



h[100].sum tensor(-1.0519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1161, device='cuda:0')



h[200].sum tensor(-22.5742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0890, 0.0273,  ..., 0.0172, 0.0881, 0.0640],
        [0.0000, 0.1069, 0.0335,  ..., 0.0199, 0.1060, 0.0792],
        [0.0000, 0.1118, 0.0352,  ..., 0.0206, 0.1109, 0.0834],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59028.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0421, 0.0040, 0.0000,  ..., 0.2993, 0.1853, 0.1898],
        [0.0379, 0.0010, 0.0000,  ..., 0.3446, 0.2211, 0.2200],
        [0.0371, 0.0007, 0.0000,  ..., 0.3595, 0.2335, 0.2296],
        ...,
        [0.0707, 0.0215, 0.0851,  ..., 0.0777, 0.0238, 0.0366],
        [0.0703, 0.0213, 0.0828,  ..., 0.0798, 0.0250, 0.0382],
        [0.0706, 0.0215, 0.0851,  ..., 0.0777, 0.0238, 0.0366]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600968.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10058.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.1813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5099.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(979.2227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1908],
        [ 0.1907],
        [ 0.1820],
        ...,
        [-2.5094],
        [-2.4467],
        [-2.4465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332369.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0184],
        [1.0240],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369304.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0184],
        [1.0240],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369310.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0072,  0.0447,  0.0147,  ...,  0.0076,  0.0445,  0.0352],
        [-0.0028,  0.0177,  0.0053,  ...,  0.0036,  0.0175,  0.0122],
        [-0.0038,  0.0243,  0.0075,  ...,  0.0046,  0.0240,  0.0178],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1654.4646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1256, device='cuda:0')



h[100].sum tensor(-0.8945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6483, device='cuda:0')



h[200].sum tensor(-22.5982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0885, 0.0271,  ..., 0.0171, 0.0876, 0.0636],
        [0.0000, 0.1082, 0.0340,  ..., 0.0200, 0.1073, 0.0804],
        [0.0000, 0.0987, 0.0306,  ..., 0.0186, 0.0978, 0.0722],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56574.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0404, 0.0026, 0.0000,  ..., 0.3404, 0.2258, 0.2155],
        [0.0403, 0.0027, 0.0000,  ..., 0.3379, 0.2219, 0.2143],
        [0.0412, 0.0023, 0.0000,  ..., 0.3178, 0.2011, 0.2021],
        ...,
        [0.0715, 0.0217, 0.0896,  ..., 0.0737, 0.0216, 0.0338],
        [0.0714, 0.0217, 0.0896,  ..., 0.0737, 0.0215, 0.0338],
        [0.0714, 0.0217, 0.0896,  ..., 0.0736, 0.0215, 0.0338]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594756.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10098.0215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.1748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5131.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(958.4614, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2035],
        [ 0.2179],
        [ 0.2315],
        ...,
        [-2.8557],
        [-2.8501],
        [-2.8479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328064.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0184],
        [1.0240],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369310.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0184],
        [1.0241],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369316.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0039,  0.0248,  0.0077,  ...,  0.0046,  0.0245,  0.0182],
        [-0.0032,  0.0205,  0.0062,  ...,  0.0040,  0.0203,  0.0146],
        [-0.0039,  0.0244,  0.0076,  ...,  0.0046,  0.0242,  0.0179],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1506.7378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7790, device='cuda:0')



h[100].sum tensor(-0.7679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3055, device='cuda:0')



h[200].sum tensor(-22.6176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0752, 0.0225,  ..., 0.0151, 0.0744, 0.0524],
        [0.0000, 0.0972, 0.0301,  ..., 0.0184, 0.0963, 0.0710],
        [0.0000, 0.1089, 0.0342,  ..., 0.0201, 0.1080, 0.0809],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52951.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0353, 0.0006, 0.0000,  ..., 0.3712, 0.2423, 0.2379],
        [0.0308, 0.0000, 0.0000,  ..., 0.4265, 0.2882, 0.2743],
        [0.0310, 0.0000, 0.0000,  ..., 0.4379, 0.2993, 0.2814],
        ...,
        [0.0717, 0.0217, 0.0900,  ..., 0.0738, 0.0216, 0.0337],
        [0.0717, 0.0217, 0.0900,  ..., 0.0738, 0.0216, 0.0337],
        [0.0717, 0.0217, 0.0899,  ..., 0.0737, 0.0216, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578947.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10155.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7466, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.3312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5162.4399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(929.8770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0664],
        [ 0.0520],
        [ 0.0506],
        ...,
        [-2.8787],
        [-2.8731],
        [-2.8706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322978.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0184],
        [1.0241],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369316.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0184],
        [1.0241],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369321.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0095,  0.0024,  ...,  0.0024,  0.0093,  0.0052],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2198.5803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6452, device='cuda:0')



h[100].sum tensor(-1.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1853, device='cuda:0')



h[200].sum tensor(-22.5180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0224, 0.0055,  ..., 0.0072, 0.0217, 0.0122],
        [0.0000, 0.0117, 0.0024,  ..., 0.0056, 0.0110, 0.0053],
        [0.0000, 0.0028, 0.0000,  ..., 0.0042, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0043, 0.0021, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0043, 0.0021, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0043, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73889.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0581, 0.0134, 0.0218,  ..., 0.1482, 0.0675, 0.0879],
        [0.0633, 0.0167, 0.0427,  ..., 0.1130, 0.0451, 0.0627],
        [0.0674, 0.0194, 0.0707,  ..., 0.0880, 0.0299, 0.0445],
        ...,
        [0.0721, 0.0220, 0.0908,  ..., 0.0738, 0.0217, 0.0332],
        [0.0716, 0.0217, 0.0854,  ..., 0.0783, 0.0251, 0.0362],
        [0.0699, 0.0205, 0.0678,  ..., 0.0934, 0.0362, 0.0465]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693662.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9997.6104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.5446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4969.5498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1110.3341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3146],
        [-0.8717],
        [-1.4460],
        ...,
        [-2.8027],
        [-2.6008],
        [-2.2457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319504.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0184],
        [1.0241],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369321.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0184],
        [1.0242],
        ...,
        [1.0029],
        [1.0020],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369327.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0010,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1497.3907, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0045, device='cuda:0')



h[100].sum tensor(-0.7684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3384, device='cuda:0')



h[200].sum tensor(-22.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0042, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0043, 0.0020, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0043, 0.0020, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0043, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53170.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0697, 0.0211, 0.0887,  ..., 0.0708, 0.0208, 0.0311],
        [0.0700, 0.0212, 0.0890,  ..., 0.0710, 0.0209, 0.0312],
        [0.0703, 0.0215, 0.0893,  ..., 0.0715, 0.0211, 0.0315],
        ...,
        [0.0723, 0.0223, 0.0915,  ..., 0.0738, 0.0217, 0.0326],
        [0.0723, 0.0223, 0.0915,  ..., 0.0738, 0.0217, 0.0326],
        [0.0722, 0.0223, 0.0914,  ..., 0.0737, 0.0217, 0.0326]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584099.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10272.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7702, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.0629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5335.6187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(931.5640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0666],
        [-3.0985],
        [-3.1142],
        ...,
        [-2.9302],
        [-2.9245],
        [-2.9221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-355832.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0184],
        [1.0242],
        ...,
        [1.0029],
        [1.0020],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369327.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(437.3304, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0185],
        [1.0242],
        ...,
        [1.0029],
        [1.0020],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369333.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [-0.0011,  0.0076,  0.0017,  ...,  0.0021,  0.0074,  0.0036],
        [-0.0011,  0.0076,  0.0017,  ...,  0.0021,  0.0074,  0.0036],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1494.1541, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8469, device='cuda:0')



h[100].sum tensor(-0.7520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3154, device='cuda:0')



h[200].sum tensor(-22.6173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0154, 0.0030,  ..., 0.0062, 0.0146, 0.0062],
        [0.0000, 0.0155, 0.0031,  ..., 0.0062, 0.0146, 0.0063],
        [0.0000, 0.0155, 0.0031,  ..., 0.0062, 0.0147, 0.0063],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0044, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53767.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0160, 0.0349,  ..., 0.1201, 0.0478, 0.0675],
        [0.0620, 0.0165, 0.0431,  ..., 0.1140, 0.0420, 0.0637],
        [0.0627, 0.0169, 0.0458,  ..., 0.1124, 0.0408, 0.0625],
        ...,
        [0.0721, 0.0224, 0.0916,  ..., 0.0738, 0.0217, 0.0324],
        [0.0720, 0.0224, 0.0916,  ..., 0.0738, 0.0217, 0.0324],
        [0.0720, 0.0223, 0.0915,  ..., 0.0737, 0.0217, 0.0324]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591765., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10241.8584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.8871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5381.8091, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(935.5901, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1812],
        [-1.6343],
        [-2.0284],
        ...,
        [-2.9374],
        [-2.9312],
        [-2.9285]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-377550.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0185],
        [1.0242],
        ...,
        [1.0029],
        [1.0020],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369333.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0185],
        [1.0243],
        ...,
        [1.0028],
        [1.0019],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369340., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1790.9124, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3705, device='cuda:0')



h[100].sum tensor(-0.9690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9763, device='cuda:0')



h[200].sum tensor(-22.5800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0045, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0046, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0046, 0.0021, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0045, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61378.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0630, 0.0172, 0.0372,  ..., 0.1155, 0.0525, 0.0625],
        [0.0638, 0.0177, 0.0419,  ..., 0.1115, 0.0498, 0.0595],
        [0.0652, 0.0186, 0.0498,  ..., 0.1045, 0.0452, 0.0545],
        ...,
        [0.0711, 0.0220, 0.0905,  ..., 0.0737, 0.0216, 0.0330],
        [0.0710, 0.0220, 0.0905,  ..., 0.0737, 0.0216, 0.0330],
        [0.0710, 0.0220, 0.0905,  ..., 0.0736, 0.0216, 0.0330]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625037.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9861.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.1671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4982.4604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1002.7122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0579],
        [-0.0982],
        [-0.1857],
        ...,
        [-2.5812],
        [-2.5542],
        [-2.4495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333950.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0185],
        [1.0243],
        ...,
        [1.0028],
        [1.0019],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369340., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0186],
        [1.0244],
        ...,
        [1.0027],
        [1.0018],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369346.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0073,  0.0016,  ...,  0.0021,  0.0071,  0.0033],
        [-0.0016,  0.0105,  0.0027,  ...,  0.0026,  0.0102,  0.0060],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2037.1836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9517, device='cuda:0')



h[100].sum tensor(-1.1404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4996, device='cuda:0')



h[200].sum tensor(-22.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0577, 0.0163,  ..., 0.0127, 0.0567, 0.0372],
        [0.0000, 0.0233, 0.0057,  ..., 0.0076, 0.0223, 0.0125],
        [0.0000, 0.0130, 0.0028,  ..., 0.0061, 0.0121, 0.0061],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66998.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0470, 0.0071, 0.0000,  ..., 0.2109, 0.1143, 0.1315],
        [0.0543, 0.0118, 0.0171,  ..., 0.1615, 0.0805, 0.0968],
        [0.0614, 0.0163, 0.0405,  ..., 0.1165, 0.0504, 0.0647],
        ...,
        [0.0693, 0.0210, 0.0847,  ..., 0.0778, 0.0239, 0.0368],
        [0.0689, 0.0208, 0.0823,  ..., 0.0799, 0.0251, 0.0384],
        [0.0693, 0.0210, 0.0846,  ..., 0.0777, 0.0239, 0.0368]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644602.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9469.7686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.0297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4610.0010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1054.6920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1239],
        [-0.1752],
        [-0.7237],
        ...,
        [-2.5215],
        [-2.4662],
        [-2.5134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292812.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0186],
        [1.0244],
        ...,
        [1.0027],
        [1.0018],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369346.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0186],
        [1.0244],
        ...,
        [1.0026],
        [1.0017],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369353., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0006, -0.0023],
        [-0.0019,  0.0128,  0.0035,  ...,  0.0030,  0.0126,  0.0079],
        [-0.0034,  0.0220,  0.0067,  ...,  0.0043,  0.0218,  0.0158],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0006, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0006, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0006, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1706.8196, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9499, device='cuda:0')



h[100].sum tensor(-0.8362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6226, device='cuda:0')



h[200].sum tensor(-22.6000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0254, 0.0063,  ..., 0.0080, 0.0244, 0.0142],
        [0.0000, 0.0424, 0.0115,  ..., 0.0105, 0.0413, 0.0263],
        [0.0000, 0.0710, 0.0208,  ..., 0.0148, 0.0699, 0.0482],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57065.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0117, 0.0241,  ..., 0.1581, 0.0812, 0.0948],
        [0.0449, 0.0068, 0.0051,  ..., 0.2252, 0.1296, 0.1416],
        [0.0362, 0.0020, 0.0000,  ..., 0.2955, 0.1821, 0.1902],
        ...,
        [0.0691, 0.0210, 0.0882,  ..., 0.0734, 0.0216, 0.0343],
        [0.0691, 0.0210, 0.0882,  ..., 0.0734, 0.0216, 0.0343],
        [0.0691, 0.0209, 0.0881,  ..., 0.0734, 0.0216, 0.0343]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591650.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9497.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.6081, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4725.6084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(961.4097, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4569],
        [-0.0109],
        [ 0.2089],
        ...,
        [-2.8420],
        [-2.8371],
        [-2.8350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309561.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0186],
        [1.0244],
        ...,
        [1.0026],
        [1.0017],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369353., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0187],
        [1.0245],
        ...,
        [1.0025],
        [1.0016],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369358.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0181,  0.0053,  ...,  0.0038,  0.0178,  0.0124],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023],
        [-0.0021,  0.0143,  0.0040,  ...,  0.0032,  0.0140,  0.0091],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.2012, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.0970, device='cuda:0')



h[100].sum tensor(-0.6109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9136, device='cuda:0')



h[200].sum tensor(-22.6375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0186, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0309, 0.0075,  ..., 0.0089, 0.0298, 0.0164],
        [0.0000, 0.0633, 0.0181,  ..., 0.0137, 0.0621, 0.0416],
        [0.0000, 0.0878, 0.0273,  ..., 0.0174, 0.0866, 0.0647],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0024, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0024, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52154.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0398, 0.0029, 0.0000,  ..., 0.2394, 0.1360, 0.1542],
        [0.0308, 0.0007, 0.0000,  ..., 0.3270, 0.2091, 0.2126],
        [0.0224, 0.0000, 0.0000,  ..., 0.4271, 0.2973, 0.2779],
        ...,
        [0.0686, 0.0206, 0.0876,  ..., 0.0733, 0.0216, 0.0346],
        [0.0685, 0.0206, 0.0876,  ..., 0.0732, 0.0216, 0.0346],
        [0.0685, 0.0206, 0.0876,  ..., 0.0732, 0.0216, 0.0346]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571459.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9381.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.7921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4540.8550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(918.9406, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1898],
        [ 0.1579],
        [ 0.1199],
        ...,
        [-2.8249],
        [-2.8201],
        [-2.8180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289295.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0187],
        [1.0245],
        ...,
        [1.0025],
        [1.0016],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369358.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0187],
        [1.0246],
        ...,
        [1.0025],
        [1.0016],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369364.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023],
        [-0.0020,  0.0133,  0.0036,  ...,  0.0031,  0.0129,  0.0083],
        [-0.0013,  0.0092,  0.0022,  ...,  0.0025,  0.0089,  0.0048],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0006, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1587.4259, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2775, device='cuda:0')



h[100].sum tensor(-0.6997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2322, device='cuda:0')



h[200].sum tensor(-22.6213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0160, 0.0037,  ..., 0.0067, 0.0148, 0.0084],
        [0.0000, 0.0223, 0.0052,  ..., 0.0077, 0.0211, 0.0114],
        [0.0000, 0.0777, 0.0231,  ..., 0.0160, 0.0763, 0.0537],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0023, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0023, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53136.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0564, 0.0135, 0.0294,  ..., 0.1295, 0.0606, 0.0754],
        [0.0492, 0.0089, 0.0132,  ..., 0.1777, 0.0939, 0.1096],
        [0.0385, 0.0029, 0.0000,  ..., 0.2604, 0.1561, 0.1667],
        ...,
        [0.0682, 0.0206, 0.0875,  ..., 0.0732, 0.0215, 0.0345],
        [0.0681, 0.0206, 0.0875,  ..., 0.0732, 0.0215, 0.0344],
        [0.0681, 0.0206, 0.0875,  ..., 0.0731, 0.0215, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570343., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9237.7012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.0795, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4514.5454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(931.6653, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0377],
        [ 0.1359],
        [ 0.1828],
        ...,
        [-2.8235],
        [-2.8185],
        [-2.8126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274464.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0187],
        [1.0246],
        ...,
        [1.0025],
        [1.0016],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369364.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0188],
        [1.0247],
        ...,
        [1.0024],
        [1.0015],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369368.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1565.4167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7506, device='cuda:0')



h[100].sum tensor(-0.6801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1552, device='cuda:0')



h[200].sum tensor(-22.6238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0209, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56043.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0196, 0.0854,  ..., 0.0701, 0.0206, 0.0324],
        [0.0658, 0.0195, 0.0831,  ..., 0.0726, 0.0222, 0.0341],
        [0.0639, 0.0183, 0.0649,  ..., 0.0888, 0.0338, 0.0454],
        ...,
        [0.0683, 0.0208, 0.0880,  ..., 0.0731, 0.0214, 0.0340],
        [0.0682, 0.0208, 0.0880,  ..., 0.0731, 0.0214, 0.0340],
        [0.0682, 0.0208, 0.0880,  ..., 0.0731, 0.0214, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592219.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9064.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.0233, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4310.3496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(964.9281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7201],
        [-1.4150],
        [-0.9054],
        ...,
        [-2.8410],
        [-2.8368],
        [-2.8350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257385.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0188],
        [1.0247],
        ...,
        [1.0024],
        [1.0015],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369368.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0188],
        [1.0247],
        ...,
        [1.0024],
        [1.0015],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369368.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1721.5542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8817, device='cuda:0')



h[100].sum tensor(-0.8044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6127, device='cuda:0')



h[200].sum tensor(-22.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58318.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0196, 0.0854,  ..., 0.0701, 0.0206, 0.0324],
        [0.0661, 0.0197, 0.0857,  ..., 0.0704, 0.0207, 0.0325],
        [0.0665, 0.0200, 0.0860,  ..., 0.0709, 0.0208, 0.0328],
        ...,
        [0.0683, 0.0208, 0.0880,  ..., 0.0731, 0.0214, 0.0340],
        [0.0682, 0.0208, 0.0880,  ..., 0.0731, 0.0214, 0.0340],
        [0.0682, 0.0208, 0.0880,  ..., 0.0731, 0.0214, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598767.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9104.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.1101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4350.7783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(982.7653, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7689],
        [-2.7323],
        [-2.6519],
        ...,
        [-2.8424],
        [-2.8374],
        [-2.8351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264892.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0188],
        [1.0247],
        ...,
        [1.0024],
        [1.0015],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369368.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0173],
        [1.0188],
        [1.0247],
        ...,
        [1.0024],
        [1.0015],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369373.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1580.5432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3220, device='cuda:0')



h[100].sum tensor(-0.6959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2387, device='cuda:0')



h[200].sum tensor(-22.6201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0049, 0.0020, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0049, 0.0020, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0050, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55027.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0663, 0.0199, 0.0863,  ..., 0.0701, 0.0206, 0.0317],
        [0.0665, 0.0200, 0.0865,  ..., 0.0703, 0.0206, 0.0318],
        [0.0669, 0.0202, 0.0868,  ..., 0.0708, 0.0208, 0.0321],
        ...,
        [0.0687, 0.0210, 0.0889,  ..., 0.0731, 0.0214, 0.0332],
        [0.0687, 0.0210, 0.0889,  ..., 0.0731, 0.0214, 0.0332],
        [0.0686, 0.0210, 0.0888,  ..., 0.0730, 0.0214, 0.0332]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583158.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9153.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.1023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4380.6108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(959.1602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6261],
        [-2.7616],
        [-2.8806],
        ...,
        [-2.8656],
        [-2.8600],
        [-2.8573]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263218.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0173],
        [1.0188],
        [1.0247],
        ...,
        [1.0024],
        [1.0015],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369373.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0188],
        [1.0248],
        ...,
        [1.0023],
        [1.0014],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369378.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1622.7860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4093, device='cuda:0')



h[100].sum tensor(-0.7371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3975, device='cuda:0')



h[200].sum tensor(-22.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0050, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56981.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0201, 0.0872,  ..., 0.0701, 0.0206, 0.0310],
        [0.0671, 0.0201, 0.0875,  ..., 0.0703, 0.0206, 0.0311],
        [0.0675, 0.0204, 0.0878,  ..., 0.0708, 0.0208, 0.0314],
        ...,
        [0.0693, 0.0212, 0.0899,  ..., 0.0731, 0.0214, 0.0325],
        [0.0693, 0.0212, 0.0899,  ..., 0.0730, 0.0214, 0.0325],
        [0.0693, 0.0212, 0.0899,  ..., 0.0730, 0.0214, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599250.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9317.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.8717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4632.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(973.7260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7085],
        [-2.8727],
        [-2.9784],
        ...,
        [-2.9051],
        [-2.8998],
        [-2.8975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295740.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0188],
        [1.0248],
        ...,
        [1.0023],
        [1.0014],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369378.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(417.6709, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0189],
        [1.0248],
        ...,
        [1.0023],
        [1.0014],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369382.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0022],
        [-0.0014,  0.0100,  0.0025,  ...,  0.0025,  0.0096,  0.0056],
        [-0.0011,  0.0077,  0.0017,  ...,  0.0022,  0.0073,  0.0036],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1425.0951, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3909, device='cuda:0')



h[100].sum tensor(-0.6045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9565, device='cuda:0')



h[200].sum tensor(-22.6344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0125, 0.0026,  ..., 0.0061, 0.0111, 0.0057],
        [0.0000, 0.0235, 0.0057,  ..., 0.0077, 0.0221, 0.0127],
        [0.0000, 0.0734, 0.0217,  ..., 0.0152, 0.0718, 0.0505],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0048, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53374.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0597, 0.0143, 0.0381,  ..., 0.1208, 0.0534, 0.0670],
        [0.0525, 0.0090, 0.0172,  ..., 0.1735, 0.0907, 0.1037],
        [0.0419, 0.0030, 0.0000,  ..., 0.2620, 0.1576, 0.1639],
        ...,
        [0.0704, 0.0210, 0.0911,  ..., 0.0731, 0.0215, 0.0320],
        [0.0704, 0.0210, 0.0910,  ..., 0.0731, 0.0215, 0.0320],
        [0.0704, 0.0210, 0.0910,  ..., 0.0731, 0.0215, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587204., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9727.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.4617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4978.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(936.8426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1067],
        [ 0.0655],
        [ 0.1972],
        ...,
        [-2.9503],
        [-2.9445],
        [-2.9421]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-361721.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0189],
        [1.0248],
        ...,
        [1.0023],
        [1.0014],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369382.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0176],
        [1.0189],
        [1.0249],
        ...,
        [1.0022],
        [1.0013],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369387.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0097,  0.0024,  ...,  0.0025,  0.0093,  0.0054],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1855.3674, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6871, device='cuda:0')



h[100].sum tensor(-0.9512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1687, device='cuda:0')



h[200].sum tensor(-22.5713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0303, 0.0081,  ..., 0.0086, 0.0289, 0.0186],
        [0.0000, 0.0521, 0.0150,  ..., 0.0119, 0.0505, 0.0348],
        [0.0000, 0.0438, 0.0128,  ..., 0.0106, 0.0423, 0.0300],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0047, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61544.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0480, 0.0044, 0.0000,  ..., 0.2131, 0.1192, 0.1308],
        [0.0447, 0.0017, 0.0000,  ..., 0.2502, 0.1508, 0.1550],
        [0.0450, 0.0025, 0.0000,  ..., 0.2573, 0.1588, 0.1589],
        ...,
        [0.0713, 0.0207, 0.0920,  ..., 0.0733, 0.0216, 0.0317],
        [0.0713, 0.0207, 0.0919,  ..., 0.0732, 0.0216, 0.0317],
        [0.0712, 0.0207, 0.0919,  ..., 0.0732, 0.0216, 0.0317]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622032., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9735.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5958, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.2430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4881.9282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1012.0114, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2291],
        [ 0.2183],
        [ 0.2074],
        ...,
        [-2.9882],
        [-2.9823],
        [-2.9798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335323.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0176],
        [1.0189],
        [1.0249],
        ...,
        [1.0022],
        [1.0013],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369387.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0190],
        [1.0249],
        ...,
        [1.0022],
        [1.0013],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369392.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0041,  0.0268,  0.0084,  ...,  0.0050,  0.0264,  0.0199],
        [-0.0058,  0.0379,  0.0123,  ...,  0.0066,  0.0375,  0.0293],
        [-0.0042,  0.0275,  0.0086,  ...,  0.0051,  0.0271,  0.0205],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1862.0369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9014, device='cuda:0')



h[100].sum tensor(-0.9656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2000, device='cuda:0')



h[200].sum tensor(-22.5673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1430, 0.0460,  ..., 0.0253, 0.1412, 0.1098],
        [0.0000, 0.1253, 0.0398,  ..., 0.0226, 0.1235, 0.0947],
        [0.0000, 0.1047, 0.0326,  ..., 0.0196, 0.1030, 0.0772],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0045, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0045, 0.0017, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0045, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63301.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0209, 0.0000, 0.0000,  ..., 0.4838, 0.3448, 0.3120],
        [0.0232, 0.0000, 0.0000,  ..., 0.4591, 0.3228, 0.2957],
        [0.0266, 0.0000, 0.0000,  ..., 0.4224, 0.2899, 0.2718],
        ...,
        [0.0719, 0.0201, 0.0923,  ..., 0.0734, 0.0219, 0.0320],
        [0.0719, 0.0201, 0.0923,  ..., 0.0734, 0.0219, 0.0320],
        [0.0719, 0.0201, 0.0922,  ..., 0.0734, 0.0219, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(626600.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9824.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7589, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1074, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4887.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1027.7194, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1125],
        [ 0.1112],
        [ 0.1132],
        ...,
        [-2.7616],
        [-2.6866],
        [-2.6838]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337432.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0190],
        [1.0249],
        ...,
        [1.0022],
        [1.0013],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369392.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0191],
        [1.0249],
        ...,
        [1.0021],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369397.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1796.0452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8059, device='cuda:0')



h[100].sum tensor(-0.9124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0399, device='cuda:0')



h[200].sum tensor(-22.5757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0043, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0043, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0043, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60894.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0607, 0.0118, 0.0288,  ..., 0.1272, 0.0600, 0.0717],
        [0.0606, 0.0115, 0.0277,  ..., 0.1310, 0.0630, 0.0742],
        [0.0601, 0.0111, 0.0234,  ..., 0.1385, 0.0684, 0.0792],
        ...,
        [0.0719, 0.0194, 0.0918,  ..., 0.0736, 0.0221, 0.0328],
        [0.0718, 0.0194, 0.0917,  ..., 0.0735, 0.0221, 0.0327],
        [0.0718, 0.0194, 0.0917,  ..., 0.0735, 0.0221, 0.0327]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610405.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9744.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.8106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4684.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1011.0163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1086],
        [ 0.1132],
        [ 0.1232],
        ...,
        [-3.0148],
        [-3.0088],
        [-3.0063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291449.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0191],
        [1.0249],
        ...,
        [1.0021],
        [1.0012],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369397.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0178],
        [1.0192],
        [1.0250],
        ...,
        [1.0021],
        [1.0012],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369403.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1326.8643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.8495, device='cuda:0')



h[100].sum tensor(-0.5556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8774, device='cuda:0')



h[200].sum tensor(-22.6399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0042, 0.0019, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0043, 0.0019, 0.0000],
        [0.0000, 0.0140, 0.0030,  ..., 0.0059, 0.0126, 0.0068],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0044, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49792.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0689, 0.0177, 0.0863,  ..., 0.0727, 0.0227, 0.0333],
        [0.0672, 0.0164, 0.0733,  ..., 0.0845, 0.0304, 0.0419],
        [0.0621, 0.0125, 0.0400,  ..., 0.1189, 0.0530, 0.0669],
        ...,
        [0.0717, 0.0190, 0.0912,  ..., 0.0737, 0.0224, 0.0335],
        [0.0717, 0.0190, 0.0912,  ..., 0.0737, 0.0224, 0.0335],
        [0.0717, 0.0190, 0.0912,  ..., 0.0737, 0.0224, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566924.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10141.3545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.4843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5223.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(898.0388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3354],
        [-1.7996],
        [-1.1319],
        ...,
        [-3.0160],
        [-3.0101],
        [-3.0077]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-379288.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0178],
        [1.0192],
        [1.0250],
        ...,
        [1.0021],
        [1.0012],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369403.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0178],
        [1.0193],
        [1.0250],
        ...,
        [1.0021],
        [1.0012],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369408.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [-0.0039,  0.0259,  0.0080,  ...,  0.0048,  0.0255,  0.0191],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1578.9454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2528, device='cuda:0')



h[100].sum tensor(-0.7338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5208, device='cuda:0')



h[200].sum tensor(-22.6061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0042, 0.0021, 0.0000],
        [0.0000, 0.0289, 0.0082,  ..., 0.0080, 0.0276, 0.0194],
        [0.0000, 0.0243, 0.0066,  ..., 0.0074, 0.0230, 0.0155],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0021, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0021, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55651.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0664, 0.0155, 0.0652,  ..., 0.0902, 0.0363, 0.0462],
        [0.0606, 0.0112, 0.0329,  ..., 0.1337, 0.0689, 0.0767],
        [0.0578, 0.0092, 0.0144,  ..., 0.1533, 0.0824, 0.0911],
        ...,
        [0.0715, 0.0185, 0.0907,  ..., 0.0740, 0.0226, 0.0344],
        [0.0715, 0.0185, 0.0907,  ..., 0.0739, 0.0226, 0.0344],
        [0.0714, 0.0185, 0.0906,  ..., 0.0739, 0.0226, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590033.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9879.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9962, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.9774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4873.7461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(954.6514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1593],
        [-1.5123],
        [-0.8876],
        ...,
        [-3.0059],
        [-3.0003],
        [-2.9986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328322.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0178],
        [1.0193],
        [1.0250],
        ...,
        [1.0021],
        [1.0012],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369408.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0193],
        [1.0250],
        ...,
        [1.0020],
        [1.0011],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369414.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [-0.0013,  0.0094,  0.0023,  ...,  0.0024,  0.0091,  0.0051],
        [-0.0012,  0.0085,  0.0020,  ...,  0.0022,  0.0082,  0.0043],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1479.2168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2369, device='cuda:0')



h[100].sum tensor(-0.6430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2262, device='cuda:0')



h[200].sum tensor(-22.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0120, 0.0023,  ..., 0.0056, 0.0108, 0.0051],
        [0.0000, 0.0183, 0.0038,  ..., 0.0066, 0.0171, 0.0082],
        [0.0000, 0.0417, 0.0105,  ..., 0.0101, 0.0404, 0.0234],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54272.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0575, 0.0105, 0.0250,  ..., 0.1346, 0.0611, 0.0797],
        [0.0521, 0.0068, 0.0109,  ..., 0.1659, 0.0796, 0.1030],
        [0.0468, 0.0032, 0.0000,  ..., 0.1984, 0.0994, 0.1269],
        ...,
        [0.0710, 0.0191, 0.0903,  ..., 0.0743, 0.0226, 0.0345],
        [0.0710, 0.0191, 0.0903,  ..., 0.0743, 0.0225, 0.0345],
        [0.0710, 0.0191, 0.0903,  ..., 0.0742, 0.0225, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588079.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9807.1113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8685, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.3994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4897.0068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(941.8177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0104],
        [ 0.1569],
        [ 0.2393],
        ...,
        [-3.0114],
        [-3.0057],
        [-3.0034]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330587., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0193],
        [1.0250],
        ...,
        [1.0020],
        [1.0011],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369414.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0194],
        [1.0249],
        ...,
        [1.0020],
        [1.0011],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369420.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0054,  0.0357,  0.0115,  ...,  0.0063,  0.0354,  0.0274],
        [-0.0072,  0.0479,  0.0158,  ...,  0.0082,  0.0476,  0.0377],
        [-0.0056,  0.0371,  0.0120,  ...,  0.0065,  0.0368,  0.0285],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1769.5203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9168, device='cuda:0')



h[100].sum tensor(-0.8399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9100, device='cuda:0')



h[200].sum tensor(-22.5839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1638, 0.0533,  ..., 0.0285, 0.1623, 0.1270],
        [0.0000, 0.1841, 0.0604,  ..., 0.0316, 0.1826, 0.1442],
        [0.0000, 0.1812, 0.0594,  ..., 0.0312, 0.1797, 0.1418],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61065.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0134, 0.0000, 0.0000,  ..., 0.5249, 0.3784, 0.3410],
        [0.0090, 0.0000, 0.0000,  ..., 0.5747, 0.4219, 0.3738],
        [0.0098, 0.0000, 0.0000,  ..., 0.5690, 0.4161, 0.3701],
        ...,
        [0.0707, 0.0199, 0.0904,  ..., 0.0746, 0.0225, 0.0341],
        [0.0707, 0.0199, 0.0904,  ..., 0.0746, 0.0225, 0.0341],
        [0.0707, 0.0199, 0.0904,  ..., 0.0746, 0.0225, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621334.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9584.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.9197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4800.7241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1003.9216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0486],
        [ 0.0354],
        [ 0.0203],
        ...,
        [-3.0204],
        [-3.0146],
        [-3.0121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306626.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0194],
        [1.0249],
        ...,
        [1.0020],
        [1.0011],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369420.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0195],
        [1.0250],
        ...,
        [1.0019],
        [1.0010],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369425.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1540.9812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.1142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1012, device='cuda:0')



h[100].sum tensor(-0.6664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3525, device='cuda:0')



h[200].sum tensor(-22.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0154, 0.0029,  ..., 0.0064, 0.0142, 0.0057],
        [0.0000, 0.0093, 0.0015,  ..., 0.0055, 0.0082, 0.0029],
        [0.0000, 0.0121, 0.0025,  ..., 0.0059, 0.0110, 0.0052],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54933.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0585, 0.0143, 0.0385,  ..., 0.1185, 0.0447, 0.0677],
        [0.0601, 0.0151, 0.0461,  ..., 0.1118, 0.0412, 0.0627],
        [0.0590, 0.0146, 0.0381,  ..., 0.1199, 0.0457, 0.0687],
        ...,
        [0.0707, 0.0208, 0.0909,  ..., 0.0750, 0.0225, 0.0336],
        [0.0706, 0.0208, 0.0909,  ..., 0.0750, 0.0225, 0.0336],
        [0.0706, 0.0208, 0.0909,  ..., 0.0749, 0.0225, 0.0336]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592781.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9663.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.2535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5011.5259, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(953.4955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9945],
        [-0.8244],
        [-0.6329],
        ...,
        [-3.0359],
        [-3.0303],
        [-3.0282]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326901.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0195],
        [1.0250],
        ...,
        [1.0019],
        [1.0010],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369425.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0196],
        [1.0250],
        ...,
        [1.0019],
        [1.0010],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369430.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023,  0.0159,  0.0047,  ...,  0.0034,  0.0156,  0.0106],
        [-0.0016,  0.0114,  0.0031,  ...,  0.0027,  0.0111,  0.0067],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1640.0786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5440, device='cuda:0')



h[100].sum tensor(-0.7282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5633, device='cuda:0')



h[200].sum tensor(-22.6027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0636, 0.0186,  ..., 0.0137, 0.0624, 0.0421],
        [0.0000, 0.0273, 0.0072,  ..., 0.0083, 0.0262, 0.0159],
        [0.0000, 0.0139, 0.0031,  ..., 0.0063, 0.0128, 0.0069],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56277.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0462, 0.0073, 0.0000,  ..., 0.2249, 0.1261, 0.1373],
        [0.0522, 0.0109, 0.0066,  ..., 0.1784, 0.0908, 0.1061],
        [0.0575, 0.0142, 0.0229,  ..., 0.1421, 0.0644, 0.0813],
        ...,
        [0.0706, 0.0215, 0.0914,  ..., 0.0753, 0.0225, 0.0333],
        [0.0706, 0.0215, 0.0914,  ..., 0.0753, 0.0225, 0.0333],
        [0.0706, 0.0215, 0.0914,  ..., 0.0753, 0.0225, 0.0332]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596978.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9635.0947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.6126, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5049.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(965.6236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1527],
        [ 0.0658],
        [-0.1151],
        ...,
        [-3.0550],
        [-3.0493],
        [-3.0468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323753.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0196],
        [1.0250],
        ...,
        [1.0019],
        [1.0010],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369430.8438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(484.7864, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0197],
        [1.0251],
        ...,
        [1.0018],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369436.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1515.2456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3095, device='cuda:0')



h[100].sum tensor(-0.6350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2369, device='cuda:0')



h[200].sum tensor(-22.6196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0047, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0048, 0.0020, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0048, 0.0020, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0048, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53872.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0682, 0.0208, 0.0890,  ..., 0.0725, 0.0217, 0.0316],
        [0.0684, 0.0209, 0.0893,  ..., 0.0728, 0.0218, 0.0317],
        [0.0684, 0.0209, 0.0871,  ..., 0.0756, 0.0232, 0.0337],
        ...,
        [0.0707, 0.0220, 0.0918,  ..., 0.0756, 0.0226, 0.0331],
        [0.0706, 0.0220, 0.0917,  ..., 0.0756, 0.0226, 0.0331],
        [0.0706, 0.0220, 0.0917,  ..., 0.0756, 0.0226, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590974.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9729.5449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.2831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5254.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(942.5873, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9820],
        [-2.7947],
        [-2.4917],
        ...,
        [-2.7641],
        [-2.9697],
        [-3.0327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351861.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0197],
        [1.0251],
        ...,
        [1.0018],
        [1.0009],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369436.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0198],
        [1.0252],
        ...,
        [1.0018],
        [1.0009],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369441.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1903.5809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1346, device='cuda:0')



h[100].sum tensor(-0.8974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2341, device='cuda:0')



h[200].sum tensor(-22.5675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0047, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0048, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62875.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0615, 0.0176, 0.0487,  ..., 0.1096, 0.0427, 0.0586],
        [0.0639, 0.0189, 0.0677,  ..., 0.0933, 0.0317, 0.0474],
        [0.0653, 0.0198, 0.0748,  ..., 0.0876, 0.0284, 0.0434],
        ...,
        [0.0705, 0.0221, 0.0917,  ..., 0.0760, 0.0228, 0.0334],
        [0.0705, 0.0221, 0.0917,  ..., 0.0759, 0.0228, 0.0334],
        [0.0704, 0.0221, 0.0917,  ..., 0.0759, 0.0227, 0.0334]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630292.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9438.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7116, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.1007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4911.7236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1027.9153, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4046],
        [-0.8396],
        [-1.1739],
        ...,
        [-3.0759],
        [-3.0608],
        [-3.0122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302025.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0198],
        [1.0252],
        ...,
        [1.0018],
        [1.0009],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369441.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0183],
        [1.0199],
        [1.0253],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369445.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0026,  0.0182,  0.0055,  ...,  0.0038,  0.0179,  0.0125],
        [-0.0016,  0.0112,  0.0030,  ...,  0.0027,  0.0109,  0.0066],
        [-0.0035,  0.0240,  0.0075,  ...,  0.0046,  0.0237,  0.0174],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2018.6334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8563, device='cuda:0')



h[100].sum tensor(-0.9773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4856, device='cuda:0')



h[200].sum tensor(-22.5505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0458, 0.0124,  ..., 0.0110, 0.0447, 0.0270],
        [0.0000, 0.0606, 0.0182,  ..., 0.0132, 0.0594, 0.0418],
        [0.0000, 0.0332, 0.0092,  ..., 0.0091, 0.0321, 0.0208],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64708.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0455, 0.0075, 0.0000,  ..., 0.2197, 0.1151, 0.1353],
        [0.0453, 0.0071, 0.0000,  ..., 0.2335, 0.1300, 0.1431],
        [0.0495, 0.0096, 0.0000,  ..., 0.2091, 0.1137, 0.1260],
        ...,
        [0.0707, 0.0219, 0.0920,  ..., 0.0763, 0.0232, 0.0337],
        [0.0707, 0.0219, 0.0920,  ..., 0.0763, 0.0232, 0.0337],
        [0.0707, 0.0218, 0.0920,  ..., 0.0762, 0.0231, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636856.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9528.3135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.1497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5020.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1041.9962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2107],
        [ 0.2499],
        [ 0.2468],
        ...,
        [-3.0605],
        [-3.0409],
        [-3.0324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332406.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0183],
        [1.0199],
        [1.0253],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369445.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0184],
        [1.0200],
        [1.0254],
        ...,
        [1.0017],
        [1.0008],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369449.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0090,  0.0022,  ...,  0.0023,  0.0088,  0.0047],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1436.2241, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4686, device='cuda:0')



h[100].sum tensor(-0.5819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1140, device='cuda:0')



h[200].sum tensor(-22.6271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0315, 0.0086,  ..., 0.0087, 0.0304, 0.0194],
        [0.0000, 0.0116, 0.0023,  ..., 0.0057, 0.0105, 0.0048],
        [0.0000, 0.0032, 0.0000,  ..., 0.0045, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0046, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0046, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0046, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53129.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0503, 0.0089, 0.0096,  ..., 0.2004, 0.1093, 0.1207],
        [0.0583, 0.0139, 0.0300,  ..., 0.1424, 0.0675, 0.0810],
        [0.0646, 0.0179, 0.0593,  ..., 0.1018, 0.0397, 0.0527],
        ...,
        [0.0709, 0.0214, 0.0922,  ..., 0.0766, 0.0236, 0.0342],
        [0.0709, 0.0214, 0.0922,  ..., 0.0766, 0.0236, 0.0342],
        [0.0709, 0.0214, 0.0922,  ..., 0.0765, 0.0236, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588245.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9679.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.4446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5122.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(943.6852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0981],
        [-0.1045],
        [-0.5010],
        ...,
        [-3.1073],
        [-3.1014],
        [-3.0987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308301.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0184],
        [1.0200],
        [1.0254],
        ...,
        [1.0017],
        [1.0008],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369449.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0184],
        [1.0200],
        [1.0254],
        ...,
        [1.0017],
        [1.0008],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369449.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [-0.0024,  0.0169,  0.0050,  ...,  0.0035,  0.0166,  0.0114],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1368.8806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3566, device='cuda:0')



h[100].sum tensor(-0.5360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9515, device='cuda:0')



h[200].sum tensor(-22.6361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0044, 0.0021, 0.0000],
        [0.0000, 0.0262, 0.0068,  ..., 0.0079, 0.0251, 0.0150],
        [0.0000, 0.0248, 0.0056,  ..., 0.0077, 0.0237, 0.0114],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0046, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0046, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0046, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51581.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0621, 0.0165, 0.0422,  ..., 0.1150, 0.0501, 0.0619],
        [0.0575, 0.0136, 0.0229,  ..., 0.1436, 0.0667, 0.0824],
        [0.0539, 0.0114, 0.0114,  ..., 0.1672, 0.0793, 0.0995],
        ...,
        [0.0709, 0.0214, 0.0922,  ..., 0.0766, 0.0236, 0.0342],
        [0.0709, 0.0214, 0.0922,  ..., 0.0766, 0.0236, 0.0342],
        [0.0709, 0.0214, 0.0922,  ..., 0.0765, 0.0236, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585649.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9858.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.8960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5343.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(921.1210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3130],
        [-0.2290],
        [-0.0296],
        ...,
        [-3.0828],
        [-3.0911],
        [-3.0907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-371015.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0184],
        [1.0200],
        [1.0254],
        ...,
        [1.0017],
        [1.0008],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369449.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0185],
        [1.0201],
        [1.0256],
        ...,
        [1.0016],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369453.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1556.4899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8150, device='cuda:0')



h[100].sum tensor(-0.6637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4568, device='cuda:0')



h[200].sum tensor(-22.6098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55078.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0676, 0.0194, 0.0839,  ..., 0.0788, 0.0259, 0.0368],
        [0.0675, 0.0194, 0.0838,  ..., 0.0796, 0.0257, 0.0375],
        [0.0674, 0.0193, 0.0808,  ..., 0.0833, 0.0276, 0.0400],
        ...,
        [0.0705, 0.0208, 0.0885,  ..., 0.0801, 0.0262, 0.0369],
        [0.0710, 0.0211, 0.0923,  ..., 0.0769, 0.0239, 0.0346],
        [0.0710, 0.0211, 0.0923,  ..., 0.0768, 0.0239, 0.0346]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599616.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9837.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.2949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5274.4834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(951.0506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7346],
        [-1.6681],
        [-1.4001],
        ...,
        [-2.8667],
        [-3.0254],
        [-3.0888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-368782., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0185],
        [1.0201],
        [1.0256],
        ...,
        [1.0016],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369453.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0203],
        [1.0257],
        ...,
        [1.0016],
        [1.0007],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369457.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1656.4324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6200, device='cuda:0')



h[100].sum tensor(-0.7357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7205, device='cuda:0')



h[200].sum tensor(-22.5943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0043, 0.0021, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0043, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0043, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58027.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0680, 0.0191, 0.0866,  ..., 0.0772, 0.0247, 0.0360],
        [0.0687, 0.0194, 0.0891,  ..., 0.0753, 0.0239, 0.0343],
        [0.0694, 0.0198, 0.0906,  ..., 0.0748, 0.0237, 0.0338],
        ...,
        [0.0713, 0.0206, 0.0927,  ..., 0.0772, 0.0244, 0.0350],
        [0.0712, 0.0206, 0.0927,  ..., 0.0772, 0.0244, 0.0350],
        [0.0712, 0.0206, 0.0927,  ..., 0.0771, 0.0244, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608900.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9800.5762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.3356, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5109.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(982.1484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8657],
        [-2.3183],
        [-2.6347],
        ...,
        [-3.1308],
        [-3.1248],
        [-3.1224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347120.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0203],
        [1.0257],
        ...,
        [1.0016],
        [1.0007],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369457.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0187],
        [1.0204],
        [1.0258],
        ...,
        [1.0015],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369461.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1474.8362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8205, device='cuda:0')



h[100].sum tensor(-0.6171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3115, device='cuda:0')



h[200].sum tensor(-22.6171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0042, 0.0021, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0042, 0.0021, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0042, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53767.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0690, 0.0193, 0.0904,  ..., 0.0742, 0.0238, 0.0334],
        [0.0692, 0.0194, 0.0907,  ..., 0.0745, 0.0239, 0.0336],
        [0.0696, 0.0196, 0.0911,  ..., 0.0750, 0.0240, 0.0339],
        ...,
        [0.0715, 0.0204, 0.0932,  ..., 0.0774, 0.0248, 0.0351],
        [0.0715, 0.0204, 0.0932,  ..., 0.0774, 0.0248, 0.0351],
        [0.0714, 0.0204, 0.0932,  ..., 0.0773, 0.0248, 0.0351]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592417.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9879.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.5352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5110.6323, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(947.8498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1914],
        [-3.0832],
        [-2.8989],
        ...,
        [-3.1455],
        [-3.1394],
        [-3.1369]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347173.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0187],
        [1.0204],
        [1.0258],
        ...,
        [1.0015],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369461.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0188],
        [1.0205],
        [1.0260],
        ...,
        [1.0015],
        [1.0006],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369464.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [-0.0025,  0.0177,  0.0052,  ...,  0.0036,  0.0174,  0.0121],
        [-0.0027,  0.0188,  0.0056,  ...,  0.0037,  0.0184,  0.0130],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0010,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1648.9468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9939, device='cuda:0')



h[100].sum tensor(-0.7345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7752, device='cuda:0')



h[200].sum tensor(-22.5921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0205, 0.0053,  ..., 0.0068, 0.0192, 0.0123],
        [0.0000, 0.0357, 0.0099,  ..., 0.0091, 0.0343, 0.0230],
        [0.0000, 0.1001, 0.0309,  ..., 0.0187, 0.0985, 0.0730],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0021, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0021, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0043, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58100.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0612, 0.0127, 0.0384,  ..., 0.1348, 0.0683, 0.0752],
        [0.0543, 0.0078, 0.0141,  ..., 0.1897, 0.1092, 0.1132],
        [0.0436, 0.0016, 0.0000,  ..., 0.2836, 0.1828, 0.1774],
        ...,
        [0.0718, 0.0205, 0.0940,  ..., 0.0775, 0.0251, 0.0348],
        [0.0718, 0.0205, 0.0940,  ..., 0.0775, 0.0251, 0.0348],
        [0.0717, 0.0205, 0.0940,  ..., 0.0775, 0.0251, 0.0348]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616180.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9870.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.8295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5054.2935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(987.6934, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2818],
        [-0.8163],
        [-0.2956],
        ...,
        [-3.1656],
        [-3.1594],
        [-3.1569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343467.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0188],
        [1.0205],
        [1.0260],
        ...,
        [1.0015],
        [1.0006],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369464.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0207],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369469.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1920.6467, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5703, device='cuda:0')



h[100].sum tensor(-0.9048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4438, device='cuda:0')



h[200].sum tensor(-22.5558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0043, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0043, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0043, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0020, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0044, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65673.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0693, 0.0198, 0.0918,  ..., 0.0745, 0.0243, 0.0327],
        [0.0691, 0.0196, 0.0899,  ..., 0.0767, 0.0254, 0.0344],
        [0.0679, 0.0186, 0.0810,  ..., 0.0858, 0.0301, 0.0412],
        ...,
        [0.0718, 0.0209, 0.0947,  ..., 0.0776, 0.0253, 0.0343],
        [0.0718, 0.0209, 0.0947,  ..., 0.0776, 0.0253, 0.0343],
        [0.0717, 0.0209, 0.0946,  ..., 0.0776, 0.0253, 0.0343]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647861.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9644.9551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9903, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.1087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4913.2041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1063.9304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9535],
        [-1.7889],
        [-1.3832],
        ...,
        [-3.1650],
        [-3.1653],
        [-3.1648]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314010.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0207],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369469.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(443.6292, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0208],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369474.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1595.2296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0013, device='cuda:0')



h[100].sum tensor(-0.6852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6301, device='cuda:0')



h[200].sum tensor(-22.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59382.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0693, 0.0206, 0.0926,  ..., 0.0745, 0.0242, 0.0319],
        [0.0695, 0.0207, 0.0928,  ..., 0.0747, 0.0243, 0.0321],
        [0.0699, 0.0209, 0.0932,  ..., 0.0753, 0.0245, 0.0324],
        ...,
        [0.0718, 0.0218, 0.0954,  ..., 0.0777, 0.0252, 0.0335],
        [0.0718, 0.0218, 0.0954,  ..., 0.0777, 0.0252, 0.0335],
        [0.0717, 0.0217, 0.0954,  ..., 0.0776, 0.0252, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633900.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9932.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.0071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5388.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1000.1323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6130],
        [-2.7851],
        [-2.8962],
        ...,
        [-3.1933],
        [-3.1871],
        [-3.1845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-399612.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0208],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369474.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0208],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369474.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1625.8774, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3156, device='cuda:0')



h[100].sum tensor(-0.7049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6761, device='cuda:0')



h[200].sum tensor(-22.5958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0044, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57245.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0688, 0.0204, 0.0911,  ..., 0.0759, 0.0246, 0.0333],
        [0.0677, 0.0195, 0.0841,  ..., 0.0828, 0.0281, 0.0386],
        [0.0654, 0.0177, 0.0660,  ..., 0.1000, 0.0387, 0.0510],
        ...,
        [0.0718, 0.0218, 0.0954,  ..., 0.0777, 0.0252, 0.0335],
        [0.0718, 0.0218, 0.0954,  ..., 0.0777, 0.0252, 0.0335],
        [0.0717, 0.0217, 0.0954,  ..., 0.0776, 0.0252, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606979.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9762.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.8325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5078.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(994.9089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2953],
        [-1.8575],
        [-1.3280],
        ...,
        [-3.1933],
        [-3.1871],
        [-3.1845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330555.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0208],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369474.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0208],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369474.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0189,  0.0057,  ...,  0.0038,  0.0185,  0.0131],
        [-0.0049,  0.0341,  0.0109,  ...,  0.0061,  0.0336,  0.0260],
        [-0.0060,  0.0411,  0.0134,  ...,  0.0071,  0.0406,  0.0319],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1493.4192, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2133, device='cuda:0')



h[100].sum tensor(-0.6198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3689, device='cuda:0')



h[200].sum tensor(-22.6134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1216, 0.0386,  ..., 0.0222, 0.1197, 0.0913],
        [0.0000, 0.1604, 0.0521,  ..., 0.0281, 0.1584, 0.1242],
        [0.0000, 0.1602, 0.0520,  ..., 0.0280, 0.1582, 0.1240],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0045, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55356.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0236, 0.0000, 0.0000,  ..., 0.4632, 0.3254, 0.2962],
        [0.0188, 0.0000, 0.0000,  ..., 0.5200, 0.3752, 0.3337],
        [0.0171, 0.0000, 0.0000,  ..., 0.5415, 0.3934, 0.3479],
        ...,
        [0.0718, 0.0218, 0.0954,  ..., 0.0777, 0.0252, 0.0335],
        [0.0718, 0.0218, 0.0954,  ..., 0.0777, 0.0252, 0.0335],
        [0.0717, 0.0217, 0.0954,  ..., 0.0776, 0.0252, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604875.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9845.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.8642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5172.4272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(973.9697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2403],
        [ 0.2365],
        [ 0.2378],
        ...,
        [-3.1913],
        [-3.1846],
        [-3.1816]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347114.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0208],
        [1.0261],
        ...,
        [1.0014],
        [1.0005],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369474.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0209],
        [1.0262],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369479.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023,  0.0165,  0.0048,  ...,  0.0035,  0.0161,  0.0111],
        [-0.0019,  0.0136,  0.0038,  ...,  0.0031,  0.0131,  0.0086],
        [-0.0043,  0.0298,  0.0095,  ...,  0.0055,  0.0293,  0.0224],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1181.6268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(10.6566, device='cuda:0')



h[100].sum tensor(-0.4011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.5570, device='cuda:0')



h[200].sum tensor(-22.6582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0292, 0.0077,  ..., 0.0085, 0.0276, 0.0175],
        [0.0000, 0.0856, 0.0261,  ..., 0.0170, 0.0838, 0.0607],
        [0.0000, 0.0890, 0.0273,  ..., 0.0176, 0.0871, 0.0635],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47550.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0560, 0.0107, 0.0198,  ..., 0.1714, 0.0937, 0.0980],
        [0.0466, 0.0043, 0.0000,  ..., 0.2513, 0.1552, 0.1520],
        [0.0433, 0.0013, 0.0000,  ..., 0.2829, 0.1795, 0.1734],
        ...,
        [0.0712, 0.0224, 0.0954,  ..., 0.0777, 0.0251, 0.0332],
        [0.0712, 0.0224, 0.0954,  ..., 0.0776, 0.0251, 0.0332],
        [0.0712, 0.0224, 0.0953,  ..., 0.0776, 0.0251, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570825.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9976.3291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.2480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(106.7542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5584.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(900.9258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0828],
        [-0.3542],
        [ 0.1033],
        ...,
        [-3.1872],
        [-3.1810],
        [-3.1785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-408184.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0209],
        [1.0262],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369479.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0210],
        [1.0262],
        ...,
        [1.0013],
        [1.0004],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369485.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0090,  0.0022,  ...,  0.0024,  0.0086,  0.0047],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [-0.0012,  0.0090,  0.0022,  ...,  0.0024,  0.0086,  0.0047],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1476.6420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1279, device='cuda:0')



h[100].sum tensor(-0.5667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2103, device='cuda:0')



h[200].sum tensor(-22.6225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0017,  ..., 0.0058, 0.0085, 0.0035],
        [0.0000, 0.0337, 0.0080,  ..., 0.0094, 0.0320, 0.0166],
        [0.0000, 0.0101, 0.0017,  ..., 0.0059, 0.0085, 0.0035],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0050, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0050, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0050, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53215.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0630, 0.0179, 0.0608,  ..., 0.1034, 0.0400, 0.0524],
        [0.0597, 0.0155, 0.0384,  ..., 0.1245, 0.0519, 0.0675],
        [0.0636, 0.0183, 0.0612,  ..., 0.1044, 0.0404, 0.0530],
        ...,
        [0.0704, 0.0228, 0.0948,  ..., 0.0778, 0.0250, 0.0331],
        [0.0704, 0.0228, 0.0948,  ..., 0.0778, 0.0250, 0.0331],
        [0.0704, 0.0227, 0.0948,  ..., 0.0777, 0.0250, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589324.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9647.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(105.1784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5316.0957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(957.1692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1793],
        [-2.2512],
        [-2.4772],
        ...,
        [-3.1694],
        [-3.1635],
        [-3.1612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-371829.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0210],
        [1.0262],
        ...,
        [1.0013],
        [1.0004],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369485.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0211],
        [1.0262],
        ...,
        [1.0013],
        [1.0004],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369491.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [-0.0017,  0.0121,  0.0033,  ...,  0.0029,  0.0117,  0.0073],
        [-0.0062,  0.0430,  0.0141,  ...,  0.0076,  0.0424,  0.0334],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2818.5950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.4992, device='cuda:0')



h[100].sum tensor(-1.3808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4790, device='cuda:0')



h[200].sum tensor(-22.4483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0337, 0.0093,  ..., 0.0096, 0.0319, 0.0211],
        [0.0000, 0.0928, 0.0292,  ..., 0.0185, 0.0907, 0.0688],
        [0.0000, 0.1233, 0.0392,  ..., 0.0231, 0.1211, 0.0923],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0051, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85290.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0436, 0.0063, 0.0104,  ..., 0.2642, 0.1696, 0.1598],
        [0.0270, 0.0012, 0.0000,  ..., 0.4200, 0.2980, 0.2635],
        [0.0143, 0.0000, 0.0000,  ..., 0.5481, 0.4038, 0.3487],
        ...,
        [0.0694, 0.0228, 0.0939,  ..., 0.0779, 0.0249, 0.0334],
        [0.0694, 0.0228, 0.0939,  ..., 0.0779, 0.0249, 0.0333],
        [0.0694, 0.0228, 0.0939,  ..., 0.0778, 0.0249, 0.0333]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(742374.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8930.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.9472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.0639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4684.8262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1238.9082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1123],
        [ 0.1115],
        [ 0.0882],
        ...,
        [-3.1420],
        [-3.1364],
        [-3.1341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317505.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0211],
        [1.0262],
        ...,
        [1.0013],
        [1.0004],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369491.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0263],
        ...,
        [1.0013],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369496.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1347.0391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0996, device='cuda:0')



h[100].sum tensor(-0.4453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7678, device='cuda:0')



h[200].sum tensor(-22.6466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0051, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0053, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50648.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0213, 0.0901,  ..., 0.0748, 0.0239, 0.0323],
        [0.0663, 0.0214, 0.0904,  ..., 0.0750, 0.0239, 0.0324],
        [0.0662, 0.0213, 0.0869,  ..., 0.0789, 0.0265, 0.0349],
        ...,
        [0.0685, 0.0225, 0.0929,  ..., 0.0780, 0.0249, 0.0339],
        [0.0684, 0.0225, 0.0929,  ..., 0.0780, 0.0249, 0.0339],
        [0.0684, 0.0225, 0.0928,  ..., 0.0779, 0.0248, 0.0338]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579266.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9309.6592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(100.1311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5186.6978, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(929.1140, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9346],
        [-2.7111],
        [-2.3364],
        ...,
        [-3.1162],
        [-3.1108],
        [-3.1087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-368424.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0263],
        ...,
        [1.0013],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369496.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0196],
        [1.0213],
        [1.0263],
        ...,
        [1.0012],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369501.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1649.4941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9281, device='cuda:0')



h[100].sum tensor(-0.6147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4733, device='cuda:0')



h[200].sum tensor(-22.6091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0052, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0052, 0.0020, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0053, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0054, 0.0020, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0054, 0.0020, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0054, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56424.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0209, 0.0893,  ..., 0.0749, 0.0238, 0.0328],
        [0.0655, 0.0210, 0.0895,  ..., 0.0752, 0.0239, 0.0329],
        [0.0659, 0.0213, 0.0899,  ..., 0.0758, 0.0241, 0.0333],
        ...,
        [0.0676, 0.0221, 0.0920,  ..., 0.0781, 0.0248, 0.0344],
        [0.0676, 0.0221, 0.0919,  ..., 0.0781, 0.0248, 0.0344],
        [0.0676, 0.0221, 0.0919,  ..., 0.0781, 0.0248, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597375., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8838.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1412, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(101.7178, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4666.6958, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(988.6025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9004],
        [-2.9537],
        [-2.9470],
        ...,
        [-3.0936],
        [-3.0885],
        [-3.0864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287100.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0196],
        [1.0213],
        [1.0263],
        ...,
        [1.0012],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369501.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0197],
        [1.0214],
        [1.0263],
        ...,
        [1.0012],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369506.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0449,  0.0147,  ...,  0.0079,  0.0443,  0.0350],
        [-0.0042,  0.0300,  0.0095,  ...,  0.0057,  0.0294,  0.0223],
        [-0.0022,  0.0161,  0.0046,  ...,  0.0036,  0.0156,  0.0105],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2175.9998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1494, device='cuda:0')



h[100].sum tensor(-0.9236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6745, device='cuda:0')



h[200].sum tensor(-22.5407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1363, 0.0436,  ..., 0.0253, 0.1340, 0.1029],
        [0.0000, 0.1008, 0.0312,  ..., 0.0199, 0.0986, 0.0728],
        [0.0000, 0.0581, 0.0170,  ..., 0.0135, 0.0561, 0.0389],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0054, 0.0021, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0054, 0.0021, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0054, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69146.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0172, 0.0000, 0.0000,  ..., 0.4405, 0.3106, 0.2827],
        [0.0228, 0.0000, 0.0000,  ..., 0.3899, 0.2667, 0.2490],
        [0.0304, 0.0000, 0.0000,  ..., 0.3251, 0.2109, 0.2057],
        ...,
        [0.0671, 0.0216, 0.0914,  ..., 0.0782, 0.0248, 0.0350],
        [0.0671, 0.0216, 0.0914,  ..., 0.0782, 0.0248, 0.0350],
        [0.0671, 0.0216, 0.0913,  ..., 0.0781, 0.0248, 0.0349]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657102.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8391.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(105.0685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4172.1040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1103.1317, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2452],
        [ 0.2512],
        [ 0.2589],
        ...,
        [-3.0851],
        [-3.0800],
        [-3.0780]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241761.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0197],
        [1.0214],
        [1.0263],
        ...,
        [1.0012],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369506.4375, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:25.741297
evaluation loss: 512.8623657226562
epoch: 0 mean loss: 495.67620849609375
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0214],
        [1.0264],
        ...,
        [1.0012],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369511., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0199,  0.0060,  ...,  0.0041,  0.0194,  0.0138],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1988.9705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4687, device='cuda:0')



h[100].sum tensor(-0.8073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2829, device='cuda:0')



h[200].sum tensor(-22.5646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0767, 0.0234,  ..., 0.0162, 0.0746, 0.0547],
        [0.0000, 0.0639, 0.0189,  ..., 0.0143, 0.0619, 0.0438],
        [0.0000, 0.0179, 0.0042,  ..., 0.0073, 0.0161, 0.0096],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0053, 0.0022, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0053, 0.0022, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0053, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64113.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0267, 0.0000, 0.0000,  ..., 0.3520, 0.2370, 0.2240],
        [0.0320, 0.0009, 0.0000,  ..., 0.3135, 0.2063, 0.1977],
        [0.0392, 0.0008, 0.0000,  ..., 0.2676, 0.1720, 0.1658],
        ...,
        [0.0671, 0.0211, 0.0912,  ..., 0.0782, 0.0247, 0.0354],
        [0.0671, 0.0211, 0.0912,  ..., 0.0782, 0.0247, 0.0354],
        [0.0670, 0.0211, 0.0911,  ..., 0.0782, 0.0247, 0.0354]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628227.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8537.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.5632, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4374.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1054.9280, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2154],
        [ 0.2030],
        [ 0.1875],
        ...,
        [-3.0927],
        [-3.0877],
        [-3.0857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256251.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0214],
        [1.0264],
        ...,
        [1.0012],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369511., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 0.0 event: 0 loss: tensor(54.6167, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0199],
        [1.0215],
        [1.0265],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369515.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1862.5696, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6695, device='cuda:0')



h[100].sum tensor(-0.7385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0200, device='cuda:0')



h[200].sum tensor(-22.5784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0021, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0021, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0052, 0.0022, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0052, 0.0022, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0052, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62554.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0196, 0.0889,  ..., 0.0750, 0.0237, 0.0336],
        [0.0655, 0.0197, 0.0892,  ..., 0.0752, 0.0238, 0.0337],
        [0.0658, 0.0198, 0.0892,  ..., 0.0761, 0.0241, 0.0343],
        ...,
        [0.0676, 0.0207, 0.0916,  ..., 0.0782, 0.0247, 0.0352],
        [0.0676, 0.0207, 0.0916,  ..., 0.0782, 0.0247, 0.0352],
        [0.0676, 0.0207, 0.0915,  ..., 0.0781, 0.0247, 0.0352]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628529.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8684.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.0042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4405.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1036.6359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9931],
        [-2.8390],
        [-2.6022],
        ...,
        [-3.1223],
        [-3.1171],
        [-3.1147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276573.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0199],
        [1.0215],
        [1.0265],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369515.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0200],
        [1.0216],
        [1.0265],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369519.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1477.0172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6886, device='cuda:0')



h[100].sum tensor(-0.5207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1461, device='cuda:0')



h[200].sum tensor(-22.6257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0685, 0.0206,  ..., 0.0146, 0.0666, 0.0479],
        [0.0000, 0.0468, 0.0137,  ..., 0.0114, 0.0450, 0.0318],
        [0.0000, 0.0395, 0.0111,  ..., 0.0103, 0.0377, 0.0256],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0021, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0021, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55405.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0321, 0.0000, 0.0000,  ..., 0.3093, 0.1980, 0.1954],
        [0.0386, 0.0008, 0.0000,  ..., 0.2642, 0.1630, 0.1644],
        [0.0449, 0.0033, 0.0029,  ..., 0.2209, 0.1286, 0.1348],
        ...,
        [0.0684, 0.0206, 0.0924,  ..., 0.0782, 0.0247, 0.0348],
        [0.0684, 0.0206, 0.0924,  ..., 0.0782, 0.0246, 0.0348],
        [0.0684, 0.0206, 0.0923,  ..., 0.0781, 0.0246, 0.0348]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604512.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8992.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.3401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4693.0674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(970.2289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2855],
        [ 0.2703],
        [ 0.1192],
        ...,
        [-3.1651],
        [-3.1598],
        [-3.1577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316332.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0200],
        [1.0216],
        [1.0265],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369519.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0200],
        [1.0216],
        [1.0266],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369523.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0079,  0.0018,  ...,  0.0022,  0.0075,  0.0037],
        [-0.0024,  0.0179,  0.0053,  ...,  0.0037,  0.0175,  0.0122],
        [-0.0018,  0.0133,  0.0037,  ...,  0.0030,  0.0129,  0.0083],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1542.0576, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1480, device='cuda:0')



h[100].sum tensor(-0.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3594, device='cuda:0')



h[200].sum tensor(-22.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0654, 0.0189,  ..., 0.0141, 0.0636, 0.0432],
        [0.0000, 0.0436, 0.0113,  ..., 0.0108, 0.0418, 0.0247],
        [0.0000, 0.0373, 0.0098,  ..., 0.0098, 0.0355, 0.0216],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55645.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0408, 0.0004, 0.0000,  ..., 0.2381, 0.1339, 0.1477],
        [0.0426, 0.0007, 0.0000,  ..., 0.2229, 0.1204, 0.1380],
        [0.0462, 0.0038, 0.0041,  ..., 0.2034, 0.1074, 0.1241],
        ...,
        [0.0692, 0.0207, 0.0932,  ..., 0.0782, 0.0245, 0.0343],
        [0.0692, 0.0207, 0.0932,  ..., 0.0782, 0.0245, 0.0343],
        [0.0692, 0.0207, 0.0932,  ..., 0.0782, 0.0245, 0.0343]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602062.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9162.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.6485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4874.6318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(972.9487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2862],
        [ 0.2891],
        [ 0.2558],
        ...,
        [-3.2074],
        [-3.2015],
        [-3.1991]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327821.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0200],
        [1.0216],
        [1.0266],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369523.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0201],
        [1.0217],
        [1.0266],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369527.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1452.4081, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0358, device='cuda:0')



h[100].sum tensor(-0.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1969, device='cuda:0')



h[200].sum tensor(-22.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53574.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0676, 0.0197, 0.0913,  ..., 0.0750, 0.0234, 0.0322],
        [0.0675, 0.0196, 0.0895,  ..., 0.0771, 0.0246, 0.0336],
        [0.0664, 0.0187, 0.0812,  ..., 0.0856, 0.0292, 0.0398],
        ...,
        [0.0700, 0.0209, 0.0940,  ..., 0.0782, 0.0244, 0.0338],
        [0.0700, 0.0209, 0.0940,  ..., 0.0782, 0.0244, 0.0338],
        [0.0700, 0.0209, 0.0940,  ..., 0.0782, 0.0244, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594075.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9465.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.0182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5157.5239, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(950.5298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3563],
        [-2.1423],
        [-1.6394],
        ...,
        [-3.1359],
        [-3.2130],
        [-3.2298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370187.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0201],
        [1.0217],
        [1.0266],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369527.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0202],
        [1.0217],
        [1.0267],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369531.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0128,  0.0035,  ...,  0.0029,  0.0124,  0.0079],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1850.7532, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5460, device='cuda:0')



h[100].sum tensor(-0.7519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1481, device='cuda:0')



h[200].sum tensor(-22.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0488, 0.0146,  ..., 0.0114, 0.0471, 0.0340],
        [0.0000, 0.0155, 0.0036,  ..., 0.0064, 0.0139, 0.0080],
        [0.0000, 0.0033, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63931.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0480, 0.0057, 0.0083,  ..., 0.2185, 0.1284, 0.1304],
        [0.0588, 0.0122, 0.0370,  ..., 0.1404, 0.0697, 0.0770],
        [0.0661, 0.0180, 0.0716,  ..., 0.0940, 0.0363, 0.0447],
        ...,
        [0.0706, 0.0211, 0.0947,  ..., 0.0782, 0.0242, 0.0333],
        [0.0706, 0.0211, 0.0947,  ..., 0.0782, 0.0242, 0.0333],
        [0.0705, 0.0211, 0.0946,  ..., 0.0782, 0.0242, 0.0333]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647110.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9295.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4893.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1045.8876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2634],
        [-1.0176],
        [-1.8987],
        ...,
        [-3.2800],
        [-3.2739],
        [-3.2715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338593.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0202],
        [1.0217],
        [1.0267],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369531.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0218],
        [1.0267],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369536., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1702.7338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0152, device='cuda:0')



h[100].sum tensor(-0.6635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7783, device='cuda:0')



h[200].sum tensor(-22.5887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0045, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0045, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0046, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60517.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0643, 0.0167, 0.0633,  ..., 0.1006, 0.0396, 0.0495],
        [0.0676, 0.0193, 0.0864,  ..., 0.0808, 0.0263, 0.0356],
        [0.0690, 0.0203, 0.0928,  ..., 0.0759, 0.0234, 0.0319],
        ...,
        [0.0709, 0.0212, 0.0950,  ..., 0.0782, 0.0241, 0.0331],
        [0.0709, 0.0212, 0.0950,  ..., 0.0782, 0.0241, 0.0331],
        [0.0708, 0.0212, 0.0949,  ..., 0.0782, 0.0241, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632599.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9532.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5176, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.2959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5291.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1010.9046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2921],
        [-2.0516],
        [-2.5914],
        ...,
        [-3.2998],
        [-3.2936],
        [-3.2912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-386768.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0218],
        [1.0267],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369536., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0219],
        [1.0268],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369540.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0031,  0.0228,  0.0071,  ...,  0.0045,  0.0224,  0.0164],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1464.0630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0962, device='cuda:0')



h[100].sum tensor(-0.5149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2057, device='cuda:0')



h[200].sum tensor(-22.6221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0435, 0.0128,  ..., 0.0107, 0.0419, 0.0295],
        [0.0000, 0.0257, 0.0072,  ..., 0.0080, 0.0242, 0.0167],
        [0.0000, 0.0033, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54449.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0050, 0.0073,  ..., 0.2723, 0.1755, 0.1646],
        [0.0554, 0.0107, 0.0295,  ..., 0.1744, 0.0990, 0.0988],
        [0.0634, 0.0160, 0.0531,  ..., 0.1098, 0.0467, 0.0558],
        ...,
        [0.0704, 0.0210, 0.0944,  ..., 0.0783, 0.0238, 0.0335],
        [0.0704, 0.0210, 0.0944,  ..., 0.0783, 0.0238, 0.0335],
        [0.0704, 0.0210, 0.0943,  ..., 0.0782, 0.0238, 0.0334]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599938.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9381.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.3550, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5083.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(962.9276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0203],
        [-0.2735],
        [-0.5265],
        ...,
        [-3.2865],
        [-3.2798],
        [-3.2769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345959.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0219],
        [1.0268],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369540.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0219],
        [1.0268],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369540.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1294.3854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.4578, device='cuda:0')



h[100].sum tensor(-0.4200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8202, device='cuda:0')



h[200].sum tensor(-22.6442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0350, 0.0098,  ..., 0.0094, 0.0334, 0.0223],
        [0.0000, 0.0192, 0.0049,  ..., 0.0070, 0.0176, 0.0112],
        [0.0000, 0.0312, 0.0091,  ..., 0.0089, 0.0296, 0.0213],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50876.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0539, 0.0083, 0.0052,  ..., 0.1680, 0.0873, 0.0960],
        [0.0559, 0.0098, 0.0058,  ..., 0.1590, 0.0822, 0.0894],
        [0.0546, 0.0085, 0.0000,  ..., 0.1772, 0.0983, 0.1012],
        ...,
        [0.0704, 0.0210, 0.0944,  ..., 0.0783, 0.0238, 0.0335],
        [0.0704, 0.0210, 0.0944,  ..., 0.0783, 0.0238, 0.0335],
        [0.0704, 0.0210, 0.0943,  ..., 0.0782, 0.0238, 0.0334]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588188., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9528.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.5762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.7612, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5311.3882, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(927.2693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6600],
        [-0.3957],
        [-0.1288],
        ...,
        [-3.2921],
        [-3.2860],
        [-3.2837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-387796.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0219],
        [1.0268],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369540.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0269],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369545.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [-0.0012,  0.0096,  0.0024,  ...,  0.0025,  0.0092,  0.0052],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1250.2233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.1847, device='cuda:0')



h[100].sum tensor(-0.3775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.6342, device='cuda:0')



h[200].sum tensor(-22.6534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0123, 0.0025,  ..., 0.0061, 0.0108, 0.0052],
        [0.0000, 0.0160, 0.0038,  ..., 0.0067, 0.0145, 0.0084],
        [0.0000, 0.0467, 0.0125,  ..., 0.0113, 0.0451, 0.0274],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49957.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0595, 0.0141, 0.0460,  ..., 0.1153, 0.0464, 0.0619],
        [0.0561, 0.0116, 0.0315,  ..., 0.1352, 0.0585, 0.0762],
        [0.0506, 0.0074, 0.0023,  ..., 0.1691, 0.0791, 0.1003],
        ...,
        [0.0696, 0.0207, 0.0934,  ..., 0.0783, 0.0236, 0.0341],
        [0.0696, 0.0207, 0.0934,  ..., 0.0783, 0.0236, 0.0341],
        [0.0696, 0.0207, 0.0934,  ..., 0.0783, 0.0236, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582730.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9274.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.4870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.7908, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5098.9233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(921.4930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0171],
        [-0.6044],
        [-0.2794],
        ...,
        [-3.2703],
        [-3.2644],
        [-3.2592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344338.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0269],
        ...,
        [1.0011],
        [1.0003],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369545.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0270],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369550.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [-0.0041,  0.0298,  0.0095,  ...,  0.0056,  0.0294,  0.0223],
        [-0.0041,  0.0302,  0.0096,  ...,  0.0056,  0.0298,  0.0226],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1565.4655, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1166, device='cuda:0')



h[100].sum tensor(-0.5409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3548, device='cuda:0')



h[200].sum tensor(-22.6139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0422, 0.0122,  ..., 0.0106, 0.0406, 0.0281],
        [0.0000, 0.0576, 0.0169,  ..., 0.0130, 0.0559, 0.0388],
        [0.0000, 0.1172, 0.0371,  ..., 0.0220, 0.1153, 0.0869],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0021, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0021, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56082.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0455, 0.0055, 0.0071,  ..., 0.2128, 0.1220, 0.1290],
        [0.0358, 0.0018, 0.0000,  ..., 0.2904, 0.1828, 0.1811],
        [0.0222, 0.0000, 0.0000,  ..., 0.4014, 0.2709, 0.2555],
        ...,
        [0.0689, 0.0203, 0.0926,  ..., 0.0784, 0.0235, 0.0349],
        [0.0689, 0.0203, 0.0926,  ..., 0.0784, 0.0235, 0.0348],
        [0.0689, 0.0203, 0.0926,  ..., 0.0783, 0.0235, 0.0348]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604108.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8987.5137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.5548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4826.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(974.8054, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2079],
        [ 0.1940],
        [ 0.1738],
        ...,
        [-3.2507],
        [-3.2450],
        [-3.2429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317464.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0270],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369550.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0270],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369550.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0111,  0.0029,  ...,  0.0027,  0.0107,  0.0064],
        [-0.0017,  0.0133,  0.0037,  ...,  0.0031,  0.0129,  0.0082],
        [-0.0029,  0.0213,  0.0065,  ...,  0.0043,  0.0208,  0.0150],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1972.0291, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6501, device='cuda:0')



h[100].sum tensor(-0.7639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3094, device='cuda:0')



h[200].sum tensor(-22.5610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0578, 0.0164,  ..., 0.0130, 0.0562, 0.0367],
        [0.0000, 0.0621, 0.0179,  ..., 0.0137, 0.0605, 0.0404],
        [0.0000, 0.0443, 0.0116,  ..., 0.0110, 0.0427, 0.0252],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0021, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0021, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63708.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.6186e-02, 3.9905e-03, 0.0000e+00,  ..., 1.9712e-01, 1.0396e-01,
         1.1910e-01],
        [4.1118e-02, 1.5242e-03, 0.0000e+00,  ..., 2.3174e-01, 1.2881e-01,
         1.4311e-01],
        [3.6237e-02, 1.9270e-04, 0.0000e+00,  ..., 2.6820e-01, 1.5521e-01,
         1.6835e-01],
        ...,
        [6.8934e-02, 2.0279e-02, 9.2634e-02,  ..., 7.8388e-02, 2.3541e-02,
         3.4852e-02],
        [6.8923e-02, 2.0275e-02, 9.2620e-02,  ..., 7.8375e-02, 2.3537e-02,
         3.4846e-02],
        [6.8894e-02, 2.0262e-02, 9.2586e-02,  ..., 7.8335e-02, 2.3525e-02,
         3.4825e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630764.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8915.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.9831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4724.6763, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1040.6582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2147],
        [ 0.2192],
        [ 0.2088],
        ...,
        [-3.2507],
        [-3.2450],
        [-3.2429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307395., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0270],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369550.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0207],
        [1.0222],
        [1.0270],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369555.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2504.8335, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.5512, device='cuda:0')



h[100].sum tensor(-1.0433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6099, device='cuda:0')



h[200].sum tensor(-22.4927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0182, 0.0044,  ..., 0.0070, 0.0167, 0.0100],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74506.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0657, 0.0183, 0.0864,  ..., 0.0779, 0.0245, 0.0357],
        [0.0640, 0.0170, 0.0744,  ..., 0.0889, 0.0314, 0.0435],
        [0.0591, 0.0131, 0.0414,  ..., 0.1200, 0.0517, 0.0656],
        ...,
        [0.0685, 0.0198, 0.0921,  ..., 0.0784, 0.0236, 0.0355],
        [0.0685, 0.0197, 0.0921,  ..., 0.0784, 0.0236, 0.0355],
        [0.0685, 0.0197, 0.0920,  ..., 0.0784, 0.0236, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678577.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8621.0918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.9614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4471.7119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1134.1305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4507],
        [-1.9329],
        [-1.2629],
        ...,
        [-3.2380],
        [-3.2324],
        [-3.2303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290357.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0207],
        [1.0222],
        [1.0270],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369555.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0223],
        [1.0271],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369559.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0041,  0.0300,  0.0095,  ...,  0.0056,  0.0296,  0.0224],
        [-0.0059,  0.0433,  0.0141,  ...,  0.0076,  0.0428,  0.0336],
        [-0.0042,  0.0312,  0.0099,  ...,  0.0058,  0.0307,  0.0234],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1992.3715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0775, device='cuda:0')



h[100].sum tensor(-0.7558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3718, device='cuda:0')



h[200].sum tensor(-22.5599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1364, 0.0436,  ..., 0.0249, 0.1345, 0.1031],
        [0.0000, 0.1592, 0.0516,  ..., 0.0284, 0.1572, 0.1224],
        [0.0000, 0.1330, 0.0425,  ..., 0.0244, 0.1311, 0.1002],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65240.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.0000, 0.0000,  ..., 0.4170, 0.2899, 0.2677],
        [0.0097, 0.0000, 0.0000,  ..., 0.4927, 0.3511, 0.3188],
        [0.0105, 0.0000, 0.0000,  ..., 0.4823, 0.3400, 0.3122],
        ...,
        [0.0682, 0.0194, 0.0918,  ..., 0.0785, 0.0236, 0.0360],
        [0.0682, 0.0194, 0.0918,  ..., 0.0785, 0.0236, 0.0360],
        [0.0682, 0.0193, 0.0917,  ..., 0.0785, 0.0236, 0.0360]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640337.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8682.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9793, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.3884, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4515.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1051.8141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1584],
        [ 0.1563],
        [ 0.1452],
        ...,
        [-3.2304],
        [-3.2255],
        [-3.2241]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291347.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0223],
        [1.0271],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369559.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0223],
        [1.0271],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369559.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1942.1008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8862, device='cuda:0')



h[100].sum tensor(-0.7288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1978, device='cuda:0')



h[200].sum tensor(-22.5664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0022, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0022, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0022, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65063.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0183, 0.0891,  ..., 0.0753, 0.0227, 0.0343],
        [0.0661, 0.0184, 0.0894,  ..., 0.0756, 0.0228, 0.0344],
        [0.0665, 0.0186, 0.0897,  ..., 0.0762, 0.0229, 0.0348],
        ...,
        [0.0682, 0.0194, 0.0918,  ..., 0.0785, 0.0236, 0.0360],
        [0.0682, 0.0194, 0.0918,  ..., 0.0785, 0.0236, 0.0360],
        [0.0682, 0.0193, 0.0917,  ..., 0.0785, 0.0236, 0.0360]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646520.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8689.9395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.8565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4501.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1049.5790, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0195],
        [-2.9406],
        [-2.7512],
        ...,
        [-2.9380],
        [-3.1395],
        [-3.2013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290672.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0223],
        [1.0271],
        ...,
        [1.0011],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369559.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0224],
        [1.0272],
        ...,
        [1.0010],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369563.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0018,  0.0137,  0.0038,  ...,  0.0031,  0.0133,  0.0086],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [-0.0030,  0.0227,  0.0069,  ...,  0.0045,  0.0222,  0.0162],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1793.8301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8589, device='cuda:0')



h[100].sum tensor(-0.6481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9016, device='cuda:0')



h[200].sum tensor(-22.5845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0143, 0.0030,  ..., 0.0064, 0.0127, 0.0067],
        [0.0000, 0.0603, 0.0171,  ..., 0.0134, 0.0586, 0.0387],
        [0.0000, 0.0355, 0.0091,  ..., 0.0096, 0.0338, 0.0199],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0049, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62256.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0560, 0.0105, 0.0302,  ..., 0.1313, 0.0590, 0.0746],
        [0.0456, 0.0040, 0.0000,  ..., 0.1904, 0.0978, 0.1172],
        [0.0440, 0.0021, 0.0005,  ..., 0.1954, 0.0977, 0.1219],
        ...,
        [0.0684, 0.0193, 0.0920,  ..., 0.0786, 0.0236, 0.0360],
        [0.0684, 0.0193, 0.0920,  ..., 0.0786, 0.0236, 0.0360],
        [0.0684, 0.0193, 0.0919,  ..., 0.0785, 0.0236, 0.0359]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628741.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8609.6309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4312.3774, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1033.4651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0152],
        [-0.3194],
        [ 0.0688],
        ...,
        [-3.2502],
        [-3.2446],
        [-3.2426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250774.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0224],
        [1.0272],
        ...,
        [1.0010],
        [1.0002],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369563.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0225],
        [1.0274],
        ...,
        [1.0010],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369566.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0016,  0.0122,  0.0033,  ...,  0.0029,  0.0118,  0.0073],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [-0.0016,  0.0122,  0.0033,  ...,  0.0029,  0.0118,  0.0073],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1491.2771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3110, device='cuda:0')



h[100].sum tensor(-0.4948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2371, device='cuda:0')



h[200].sum tensor(-22.6208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0026,  ..., 0.0061, 0.0114, 0.0057],
        [0.0000, 0.0456, 0.0120,  ..., 0.0111, 0.0439, 0.0264],
        [0.0000, 0.0131, 0.0026,  ..., 0.0062, 0.0115, 0.0057],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55359.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0597, 0.0131, 0.0472,  ..., 0.1135, 0.0462, 0.0614],
        [0.0549, 0.0093, 0.0161,  ..., 0.1415, 0.0638, 0.0815],
        [0.0602, 0.0134, 0.0474,  ..., 0.1148, 0.0467, 0.0621],
        ...,
        [0.0691, 0.0196, 0.0928,  ..., 0.0785, 0.0235, 0.0354],
        [0.0691, 0.0196, 0.0928,  ..., 0.0785, 0.0235, 0.0354],
        [0.0690, 0.0196, 0.0927,  ..., 0.0785, 0.0235, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604493.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9084.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.8033, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4823.3452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(966.1418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5087],
        [-2.3604],
        [-2.4141],
        ...,
        [-3.2873],
        [-3.2815],
        [-3.2793]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343851.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0225],
        [1.0274],
        ...,
        [1.0010],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369566.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0210],
        [1.0226],
        [1.0275],
        ...,
        [1.0010],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369569.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0034,  0.0254,  0.0079,  ...,  0.0049,  0.0250,  0.0186],
        [-0.0031,  0.0236,  0.0073,  ...,  0.0046,  0.0231,  0.0170],
        [-0.0048,  0.0359,  0.0116,  ...,  0.0064,  0.0353,  0.0274],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1662.8064, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4009, device='cuda:0')



h[100].sum tensor(-0.5874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6885, device='cuda:0')



h[200].sum tensor(-22.5969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1031, 0.0321,  ..., 0.0198, 0.1012, 0.0752],
        [0.0000, 0.1331, 0.0425,  ..., 0.0243, 0.1310, 0.1005],
        [0.0000, 0.1604, 0.0521,  ..., 0.0284, 0.1582, 0.1236],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58158.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0214, 0.0000, 0.0000,  ..., 0.4008, 0.2702, 0.2558],
        [0.0146, 0.0000, 0.0000,  ..., 0.4652, 0.3251, 0.2984],
        [0.0072, 0.0000, 0.0000,  ..., 0.5401, 0.3889, 0.3479],
        ...,
        [0.0695, 0.0200, 0.0935,  ..., 0.0785, 0.0234, 0.0348],
        [0.0695, 0.0200, 0.0935,  ..., 0.0785, 0.0234, 0.0348],
        [0.0695, 0.0200, 0.0935,  ..., 0.0785, 0.0234, 0.0348]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612511.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9107.8506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.6720, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4796.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(994.5375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2185],
        [ 0.2050],
        [ 0.1929],
        ...,
        [-3.3154],
        [-3.3094],
        [-3.3072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332436.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0210],
        [1.0226],
        [1.0275],
        ...,
        [1.0010],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369569.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0211],
        [1.0227],
        [1.0276],
        ...,
        [1.0009],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369573.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0095,  0.0024,  ...,  0.0025,  0.0091,  0.0051],
        [-0.0011,  0.0089,  0.0022,  ...,  0.0024,  0.0085,  0.0046],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2304.0605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7335, device='cuda:0')



h[100].sum tensor(-0.9187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1982, device='cuda:0')



h[200].sum tensor(-22.5130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0717, 0.0212,  ..., 0.0150, 0.0698, 0.0487],
        [0.0000, 0.0190, 0.0041,  ..., 0.0070, 0.0173, 0.0086],
        [0.0000, 0.0374, 0.0098,  ..., 0.0098, 0.0356, 0.0218],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74124.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0383, 0.0017, 0.0000,  ..., 0.2618, 0.1553, 0.1628],
        [0.0482, 0.0051, 0.0010,  ..., 0.1881, 0.0969, 0.1131],
        [0.0463, 0.0040, 0.0000,  ..., 0.2014, 0.1049, 0.1222],
        ...,
        [0.0699, 0.0203, 0.0942,  ..., 0.0785, 0.0234, 0.0344],
        [0.0699, 0.0203, 0.0942,  ..., 0.0785, 0.0234, 0.0344],
        [0.0699, 0.0203, 0.0941,  ..., 0.0785, 0.0234, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690558.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8756.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.0576, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4346.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1142.5350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2810],
        [ 0.2900],
        [ 0.2992],
        ...,
        [-3.1887],
        [-3.1818],
        [-3.2043]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299016.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0211],
        [1.0227],
        [1.0276],
        ...,
        [1.0009],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369573.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0211],
        [1.0228],
        [1.0277],
        ...,
        [1.0009],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369577.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1858.5883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6314, device='cuda:0')



h[100].sum tensor(-0.6858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1605, device='cuda:0')



h[200].sum tensor(-22.5696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62846.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.0193, 0.0919,  ..., 0.0753, 0.0224, 0.0326],
        [0.0681, 0.0194, 0.0922,  ..., 0.0756, 0.0225, 0.0327],
        [0.0684, 0.0197, 0.0925,  ..., 0.0762, 0.0227, 0.0331],
        ...,
        [0.0702, 0.0205, 0.0946,  ..., 0.0785, 0.0234, 0.0342],
        [0.0702, 0.0204, 0.0946,  ..., 0.0785, 0.0234, 0.0342],
        [0.0702, 0.0204, 0.0946,  ..., 0.0785, 0.0234, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634685.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8946.8994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7264, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.1365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4471.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1048.4615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3818],
        [-3.4066],
        [-3.3867],
        ...,
        [-3.3467],
        [-3.3122],
        [-3.2486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288112.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0211],
        [1.0228],
        [1.0277],
        ...,
        [1.0009],
        [1.0001],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369577.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0229],
        [1.0278],
        ...,
        [1.0009],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369580.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0116,  0.0031,  ...,  0.0028,  0.0112,  0.0069],
        [-0.0016,  0.0122,  0.0033,  ...,  0.0028,  0.0117,  0.0074],
        [-0.0015,  0.0116,  0.0031,  ...,  0.0028,  0.0112,  0.0069],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2079.6538, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5290, device='cuda:0')



h[100].sum tensor(-0.7966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7300, device='cuda:0')



h[200].sum tensor(-22.5401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0362, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0542, 0.0151,  ..., 0.0122, 0.0524, 0.0340],
        [0.0000, 0.0529, 0.0146,  ..., 0.0121, 0.0511, 0.0329],
        [0.0000, 0.0240, 0.0059,  ..., 0.0077, 0.0222, 0.0129],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0047, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69172., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0483, 0.0046, 0.0000,  ..., 0.1841, 0.0909, 0.1107],
        [0.0479, 0.0044, 0.0000,  ..., 0.1863, 0.0915, 0.1123],
        [0.0521, 0.0077, 0.0108,  ..., 0.1631, 0.0749, 0.0962],
        ...,
        [0.0706, 0.0205, 0.0951,  ..., 0.0785, 0.0234, 0.0341],
        [0.0706, 0.0205, 0.0950,  ..., 0.0785, 0.0234, 0.0341],
        [0.0705, 0.0204, 0.0950,  ..., 0.0785, 0.0234, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670341.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9026.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3488, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.8975, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4572.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1097.9049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2976],
        [-0.0065],
        [ 0.0643],
        ...,
        [-3.3418],
        [-3.3300],
        [-3.3330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314712.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0229],
        [1.0278],
        ...,
        [1.0009],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369580.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 20.0 event: 100 loss: tensor(816.7343, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0229],
        [1.0279],
        ...,
        [1.0009],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369584.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0090,  0.0022,  ...,  0.0023,  0.0086,  0.0047],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1596.8456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7474, device='cuda:0')



h[100].sum tensor(-0.5452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5931, device='cuda:0')



h[200].sum tensor(-22.6026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0356, 0.0099,  ..., 0.0094, 0.0338, 0.0227],
        [0.0000, 0.0118, 0.0022,  ..., 0.0058, 0.0101, 0.0048],
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57386.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0476, 0.0049, 0.0086,  ..., 0.2032, 0.1116, 0.1222],
        [0.0575, 0.0107, 0.0349,  ..., 0.1406, 0.0665, 0.0789],
        [0.0652, 0.0167, 0.0690,  ..., 0.0970, 0.0369, 0.0480],
        ...,
        [0.0705, 0.0202, 0.0949,  ..., 0.0786, 0.0235, 0.0346],
        [0.0705, 0.0202, 0.0949,  ..., 0.0786, 0.0235, 0.0345],
        [0.0704, 0.0202, 0.0948,  ..., 0.0785, 0.0235, 0.0345]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613295.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9280.7549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.2891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4830.8149, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(989.7536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0929],
        [-0.7162],
        [-1.5810],
        ...,
        [-3.3844],
        [-3.3782],
        [-3.3759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344250.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0229],
        [1.0279],
        ...,
        [1.0009],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369584.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0230],
        [1.0280],
        ...,
        [1.0009],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369589.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1465.7715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1503, device='cuda:0')



h[100].sum tensor(-0.4649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2136, device='cuda:0')



h[200].sum tensor(-22.6222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0046, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0046, 0.0019, 0.0000],
        [0.0000, 0.0239, 0.0064,  ..., 0.0077, 0.0220, 0.0148],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0047, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0047, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0047, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54006.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0178, 0.0841,  ..., 0.0819, 0.0263, 0.0387],
        [0.0643, 0.0162, 0.0681,  ..., 0.0960, 0.0370, 0.0481],
        [0.0583, 0.0115, 0.0362,  ..., 0.1392, 0.0690, 0.0777],
        ...,
        [0.0697, 0.0198, 0.0940,  ..., 0.0787, 0.0236, 0.0354],
        [0.0697, 0.0198, 0.0940,  ..., 0.0787, 0.0236, 0.0354],
        [0.0697, 0.0198, 0.0939,  ..., 0.0786, 0.0236, 0.0354]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597286.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9308.9746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.1236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4893.9482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(952.5644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6959],
        [-1.7128],
        [-1.4956],
        ...,
        [-3.3645],
        [-3.3584],
        [-3.3562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-373617.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0230],
        [1.0280],
        ...,
        [1.0009],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369589.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0231],
        [1.0280],
        ...,
        [1.0008],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369594.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1920.5444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0207, device='cuda:0')



h[100].sum tensor(-0.6745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2174, device='cuda:0')



h[200].sum tensor(-22.5666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0047, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0048, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64744.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.0141, 0.0545,  ..., 0.1066, 0.0441, 0.0568],
        [0.0655, 0.0172, 0.0811,  ..., 0.0843, 0.0284, 0.0408],
        [0.0668, 0.0183, 0.0890,  ..., 0.0783, 0.0240, 0.0367],
        ...,
        [0.0690, 0.0194, 0.0931,  ..., 0.0787, 0.0237, 0.0364],
        [0.0690, 0.0193, 0.0931,  ..., 0.0787, 0.0237, 0.0364],
        [0.0690, 0.0193, 0.0931,  ..., 0.0787, 0.0236, 0.0363]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643496.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8711.5371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.6397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4212.9541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1055.8937, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6946],
        [-2.1107],
        [-2.1489],
        ...,
        [-3.3431],
        [-3.3372],
        [-3.3351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274305.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0231],
        [1.0280],
        ...,
        [1.0008],
        [1.0000],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369594.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0232],
        [1.0281],
        ...,
        [1.0008],
        [1.0000],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369598.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0080,  0.0018,  ...,  0.0022,  0.0075,  0.0037],
        [-0.0012,  0.0099,  0.0024,  ...,  0.0025,  0.0095,  0.0054],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        ...,
        [-0.0016,  0.0125,  0.0033,  ...,  0.0029,  0.0120,  0.0076],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1790.8528, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7759, device='cuda:0')



h[100].sum tensor(-0.5962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8894, device='cuda:0')



h[200].sum tensor(-22.5856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0718, 0.0209,  ..., 0.0149, 0.0698, 0.0484],
        [0.0000, 0.0312, 0.0081,  ..., 0.0088, 0.0293, 0.0186],
        [0.0000, 0.0131, 0.0025,  ..., 0.0061, 0.0113, 0.0055],
        ...,
        [0.0000, 0.0916, 0.0284,  ..., 0.0180, 0.0895, 0.0671],
        [0.0000, 0.0816, 0.0249,  ..., 0.0165, 0.0795, 0.0587],
        [0.0000, 0.0596, 0.0180,  ..., 0.0132, 0.0576, 0.0424]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60302.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0370, 0.0000, 0.0000,  ..., 0.2458, 0.1408, 0.1562],
        [0.0465, 0.0049, 0.0104,  ..., 0.1897, 0.1003, 0.1168],
        [0.0563, 0.0107, 0.0355,  ..., 0.1346, 0.0613, 0.0777],
        ...,
        [0.0107, 0.0000, 0.0000,  ..., 0.4861, 0.3453, 0.3169],
        [0.0208, 0.0000, 0.0000,  ..., 0.4068, 0.2792, 0.2633],
        [0.0343, 0.0007, 0.0000,  ..., 0.3076, 0.1993, 0.1953]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622643.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8857.2451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4996, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.6091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4425.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1008.7070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2024],
        [-0.1371],
        [-0.7835],
        ...,
        [ 0.2408],
        [ 0.2570],
        [ 0.1906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308204.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0232],
        [1.0281],
        ...,
        [1.0008],
        [1.0000],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369598.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0232],
        [1.0282],
        ...,
        [1.0008],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369603.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [-0.0018,  0.0146,  0.0041,  ...,  0.0032,  0.0141,  0.0093],
        [-0.0018,  0.0146,  0.0041,  ...,  0.0032,  0.0141,  0.0093],
        ...,
        [ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1639.8866, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0298, device='cuda:0')



h[100].sum tensor(-0.5103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4882, device='cuda:0')



h[200].sum tensor(-22.6070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0291, 0.0074,  ..., 0.0085, 0.0273, 0.0168],
        [0.0000, 0.0406, 0.0107,  ..., 0.0103, 0.0387, 0.0242],
        [0.0000, 0.0574, 0.0165,  ..., 0.0128, 0.0555, 0.0384],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0048, 0.0022, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0048, 0.0022, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0048, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57631.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0442, 0.0034, 0.0045,  ..., 0.2076, 0.1166, 0.1285],
        [0.0362, 0.0012, 0.0000,  ..., 0.2646, 0.1609, 0.1679],
        [0.0272, 0.0000, 0.0000,  ..., 0.3347, 0.2183, 0.2155],
        ...,
        [0.0684, 0.0194, 0.0924,  ..., 0.0788, 0.0236, 0.0371],
        [0.0684, 0.0194, 0.0924,  ..., 0.0788, 0.0236, 0.0370],
        [0.0683, 0.0193, 0.0924,  ..., 0.0788, 0.0236, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614345.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8927.6533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.4041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4575.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(981.4437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2249],
        [ 0.2016],
        [ 0.1776],
        ...,
        [-3.3329],
        [-3.3270],
        [-3.3250]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345061.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0232],
        [1.0282],
        ...,
        [1.0008],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369603.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0215],
        [1.0233],
        [1.0283],
        ...,
        [1.0008],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369607.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [-0.0011,  0.0095,  0.0023,  ...,  0.0025,  0.0090,  0.0050],
        ...,
        [ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0010, -0.0007,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1695.2396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8833, device='cuda:0')



h[100].sum tensor(-0.5252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6129, device='cuda:0')



h[200].sum tensor(-22.6019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0048, 0.0021, 0.0000],
        [0.0000, 0.0126, 0.0023,  ..., 0.0061, 0.0108, 0.0051],
        [0.0000, 0.0240, 0.0056,  ..., 0.0078, 0.0221, 0.0124],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0049, 0.0022, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0049, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59650.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0620, 0.0161, 0.0717,  ..., 0.0922, 0.0311, 0.0484],
        [0.0557, 0.0114, 0.0357,  ..., 0.1290, 0.0553, 0.0747],
        [0.0478, 0.0058, 0.0138,  ..., 0.1790, 0.0894, 0.1099],
        ...,
        [0.0683, 0.0197, 0.0924,  ..., 0.0789, 0.0235, 0.0370],
        [0.0683, 0.0197, 0.0924,  ..., 0.0789, 0.0235, 0.0370],
        [0.0683, 0.0197, 0.0924,  ..., 0.0789, 0.0234, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622392.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8680.8994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4275, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.5088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4208.5010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1008.5403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8046],
        [-0.2295],
        [ 0.0787],
        ...,
        [-3.3293],
        [-3.3235],
        [-3.3217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277459.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0215],
        [1.0233],
        [1.0283],
        ...,
        [1.0008],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369607.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0215],
        [1.0234],
        [1.0284],
        ...,
        [1.0008],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369611.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032,  0.0248,  0.0077,  ...,  0.0048,  0.0243,  0.0179],
        [-0.0033,  0.0256,  0.0079,  ...,  0.0049,  0.0251,  0.0186],
        [-0.0028,  0.0218,  0.0066,  ...,  0.0043,  0.0213,  0.0154],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1553.4495, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2353, device='cuda:0')



h[100].sum tensor(-0.4487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2260, device='cuda:0')



h[200].sum tensor(-22.6213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0953, 0.0292,  ..., 0.0186, 0.0932, 0.0682],
        [0.0000, 0.1025, 0.0317,  ..., 0.0197, 0.1004, 0.0743],
        [0.0000, 0.1261, 0.0399,  ..., 0.0233, 0.1239, 0.0942],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0049, 0.0021, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0049, 0.0021, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0049, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55042.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0254, 0.0000, 0.0000,  ..., 0.3397, 0.2188, 0.2179],
        [0.0222, 0.0000, 0.0000,  ..., 0.3651, 0.2381, 0.2350],
        [0.0191, 0.0000, 0.0000,  ..., 0.3927, 0.2601, 0.2536],
        ...,
        [0.0685, 0.0202, 0.0928,  ..., 0.0790, 0.0233, 0.0367],
        [0.0685, 0.0202, 0.0928,  ..., 0.0790, 0.0233, 0.0366],
        [0.0684, 0.0202, 0.0928,  ..., 0.0789, 0.0233, 0.0366]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602351.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8924.0693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2202, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4545.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(964.5800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2781],
        [ 0.2719],
        [ 0.2677],
        ...,
        [-3.3579],
        [-3.3520],
        [-3.3499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330287.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0215],
        [1.0234],
        [1.0284],
        ...,
        [1.0008],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369611.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0235],
        [1.0284],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369616.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [-0.0012,  0.0101,  0.0026,  ...,  0.0026,  0.0096,  0.0055],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1721.5245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5778, device='cuda:0')



h[100].sum tensor(-0.5135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5683, device='cuda:0')



h[200].sum tensor(-22.6026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0443, 0.0115,  ..., 0.0110, 0.0423, 0.0251],
        [0.0000, 0.0114, 0.0020,  ..., 0.0061, 0.0096, 0.0042],
        [0.0000, 0.0132, 0.0026,  ..., 0.0064, 0.0113, 0.0056],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0050, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58490.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0463, 0.0060, 0.0017,  ..., 0.1868, 0.0917, 0.1135],
        [0.0540, 0.0115, 0.0254,  ..., 0.1433, 0.0637, 0.0829],
        [0.0590, 0.0150, 0.0466,  ..., 0.1165, 0.0461, 0.0642],
        ...,
        [0.0684, 0.0209, 0.0931,  ..., 0.0790, 0.0231, 0.0363],
        [0.0684, 0.0209, 0.0930,  ..., 0.0790, 0.0231, 0.0363],
        [0.0684, 0.0209, 0.0930,  ..., 0.0790, 0.0231, 0.0363]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617371.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8775.0576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3124, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.5348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4454.4131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1001.5812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0571],
        [-0.1560],
        [-0.5545],
        ...,
        [-3.3676],
        [-3.3631],
        [-3.3609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311559.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0235],
        [1.0284],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369616.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0236],
        [1.0285],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369620.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1935.1383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8433, device='cuda:0')



h[100].sum tensor(-0.6035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0454, device='cuda:0')



h[200].sum tensor(-22.5768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0202, 0.0045,  ..., 0.0074, 0.0183, 0.0093],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0158, 0.0035,  ..., 0.0069, 0.0139, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62674.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0651, 0.0195, 0.0844,  ..., 0.0818, 0.0253, 0.0386],
        [0.0609, 0.0165, 0.0535,  ..., 0.1093, 0.0435, 0.0577],
        [0.0491, 0.0089, 0.0242,  ..., 0.1907, 0.1018, 0.1135],
        ...,
        [0.0683, 0.0211, 0.0909,  ..., 0.0814, 0.0245, 0.0376],
        [0.0647, 0.0186, 0.0649,  ..., 0.1038, 0.0400, 0.0531],
        [0.0567, 0.0131, 0.0312,  ..., 0.1521, 0.0725, 0.0867]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634874.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8687.1709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.1891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4328.4111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1042.2336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0066],
        [-0.4248],
        [-0.1196],
        ...,
        [-2.7240],
        [-1.9695],
        [-1.0572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285466.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0236],
        [1.0285],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369620.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0236],
        [1.0285],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369620.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1520.6241, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1855, device='cuda:0')



h[100].sum tensor(-0.4120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0726, device='cuda:0')



h[200].sum tensor(-22.6292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53658.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0660, 0.0200, 0.0888,  ..., 0.0777, 0.0232, 0.0356],
        [0.0666, 0.0203, 0.0911,  ..., 0.0762, 0.0222, 0.0344],
        [0.0670, 0.0206, 0.0915,  ..., 0.0768, 0.0224, 0.0348],
        ...,
        [0.0687, 0.0214, 0.0935,  ..., 0.0791, 0.0230, 0.0360],
        [0.0687, 0.0214, 0.0935,  ..., 0.0790, 0.0230, 0.0360],
        [0.0686, 0.0214, 0.0934,  ..., 0.0790, 0.0230, 0.0359]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598177., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9014.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.0912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4725.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(955.0461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3271],
        [-2.7928],
        [-3.1121],
        ...,
        [-3.3906],
        [-3.3845],
        [-3.3822]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-352646.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0236],
        [1.0285],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369620.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0236],
        [1.0285],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369620.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1744.2552, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8411, device='cuda:0')



h[100].sum tensor(-0.5153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6067, device='cuda:0')



h[200].sum tensor(-22.6009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58662.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0656, 0.0199, 0.0890,  ..., 0.0776, 0.0225, 0.0361],
        [0.0664, 0.0202, 0.0905,  ..., 0.0767, 0.0223, 0.0350],
        [0.0670, 0.0206, 0.0915,  ..., 0.0768, 0.0224, 0.0348],
        ...,
        [0.0687, 0.0214, 0.0935,  ..., 0.0791, 0.0230, 0.0360],
        [0.0687, 0.0214, 0.0935,  ..., 0.0790, 0.0230, 0.0360],
        [0.0686, 0.0214, 0.0934,  ..., 0.0790, 0.0230, 0.0359]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617675.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8803.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.5040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4437.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1004.8726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8003],
        [-2.1607],
        [-2.4754],
        ...,
        [-3.3802],
        [-3.3729],
        [-3.3699]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306095.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0236],
        [1.0285],
        ...,
        [1.0007],
        [0.9999],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369620.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0237],
        [1.0286],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369623.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [-0.0013,  0.0109,  0.0029,  ...,  0.0027,  0.0104,  0.0062],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1901.1870, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8387, device='cuda:0')



h[100].sum tensor(-0.5827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8986, device='cuda:0')



h[200].sum tensor(-22.5811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        [0.0000, 0.0217, 0.0051,  ..., 0.0076, 0.0199, 0.0108],
        [0.0000, 0.0491, 0.0140,  ..., 0.0118, 0.0472, 0.0316],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62933.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0597, 0.0160, 0.0482,  ..., 0.1145, 0.0453, 0.0618],
        [0.0504, 0.0096, 0.0227,  ..., 0.1772, 0.0884, 0.1048],
        [0.0387, 0.0041, 0.0011,  ..., 0.2649, 0.1543, 0.1638],
        ...,
        [0.0691, 0.0216, 0.0941,  ..., 0.0791, 0.0229, 0.0357],
        [0.0691, 0.0216, 0.0940,  ..., 0.0791, 0.0229, 0.0357],
        [0.0691, 0.0216, 0.0940,  ..., 0.0790, 0.0229, 0.0357]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645212.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8900.5791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.7425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4557.8315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1039.5201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8411],
        [-0.2388],
        [ 0.0929],
        ...,
        [-3.3040],
        [-3.3819],
        [-3.3990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326611.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0237],
        [1.0286],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369623.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0218],
        [1.0238],
        [1.0287],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369627.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1524.9948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1110, device='cuda:0')



h[100].sum tensor(-0.4052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0617, device='cuda:0')



h[200].sum tensor(-22.6292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0050, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53633.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0205, 0.0915,  ..., 0.0759, 0.0220, 0.0341],
        [0.0671, 0.0206, 0.0918,  ..., 0.0762, 0.0221, 0.0343],
        [0.0675, 0.0208, 0.0922,  ..., 0.0768, 0.0223, 0.0347],
        ...,
        [0.0693, 0.0216, 0.0942,  ..., 0.0791, 0.0229, 0.0358],
        [0.0692, 0.0216, 0.0942,  ..., 0.0791, 0.0229, 0.0358],
        [0.0692, 0.0216, 0.0941,  ..., 0.0791, 0.0229, 0.0358]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598800.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9047.0488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.8754, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4687.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(959.8883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4171],
        [-3.5067],
        [-3.5854],
        ...,
        [-3.4297],
        [-3.4230],
        [-3.4203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342113.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0218],
        [1.0238],
        [1.0287],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369627.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0218],
        [1.0239],
        [1.0287],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369631.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0049,  0.0379,  0.0123,  ...,  0.0068,  0.0373,  0.0291],
        [-0.0052,  0.0404,  0.0132,  ...,  0.0072,  0.0398,  0.0312],
        [-0.0042,  0.0324,  0.0104,  ...,  0.0059,  0.0319,  0.0245],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2237.7168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0712, device='cuda:0')



h[100].sum tensor(-0.7199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6631, device='cuda:0')



h[200].sum tensor(-22.5397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1606, 0.0523,  ..., 0.0285, 0.1583, 0.1237],
        [0.0000, 0.1659, 0.0541,  ..., 0.0294, 0.1636, 0.1282],
        [0.0000, 0.1821, 0.0598,  ..., 0.0318, 0.1798, 0.1419],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71194.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5884e-03, 0.0000e+00, 0.0000e+00,  ..., 5.5678e-01, 3.9933e-01,
         3.5782e-01],
        [2.0092e-03, 0.0000e+00, 0.0000e+00,  ..., 6.0137e-01, 4.3744e-01,
         3.8722e-01],
        [3.5936e-04, 0.0000e+00, 0.0000e+00,  ..., 6.3155e-01, 4.6364e-01,
         4.0701e-01],
        ...,
        [6.9417e-02, 2.1486e-02, 9.4323e-02,  ..., 7.9212e-02, 2.2990e-02,
         3.6081e-02],
        [6.9395e-02, 2.1477e-02, 9.4295e-02,  ..., 7.9185e-02, 2.2982e-02,
         3.6067e-02],
        [6.9363e-02, 2.1462e-02, 9.4258e-02,  ..., 7.9142e-02, 2.2969e-02,
         3.6044e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693987.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8841.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.0904, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4478.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1109.0382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0844],
        [ 0.0601],
        [ 0.0523],
        ...,
        [-3.4472],
        [-3.4408],
        [-3.4384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344523.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0218],
        [1.0239],
        [1.0287],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369631.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0219],
        [1.0240],
        [1.0288],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369634.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0107,  0.0028,  ...,  0.0026,  0.0102,  0.0061],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0011,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1536.4102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5383, device='cuda:0')



h[100].sum tensor(-0.4065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1242, device='cuda:0')



h[200].sum tensor(-22.6268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0206, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0022,  ..., 0.0059, 0.0100, 0.0046],
        [0.0000, 0.0137, 0.0028,  ..., 0.0061, 0.0118, 0.0062],
        [0.0000, 0.0037, 0.0000,  ..., 0.0047, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0048, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54070.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0613, 0.0155, 0.0545,  ..., 0.1102, 0.0416, 0.0595],
        [0.0629, 0.0166, 0.0629,  ..., 0.1029, 0.0377, 0.0541],
        [0.0666, 0.0191, 0.0833,  ..., 0.0854, 0.0274, 0.0413],
        ...,
        [0.0698, 0.0209, 0.0945,  ..., 0.0794, 0.0233, 0.0365],
        [0.0698, 0.0209, 0.0944,  ..., 0.0793, 0.0232, 0.0365],
        [0.0697, 0.0209, 0.0944,  ..., 0.0793, 0.0232, 0.0365]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601693.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9166.8145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.7421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4631.8525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(961.4410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7481],
        [-2.8130],
        [-2.8281],
        ...,
        [-3.4652],
        [-3.4586],
        [-3.4562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340129.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0219],
        [1.0240],
        [1.0288],
        ...,
        [1.0007],
        [0.9999],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369634.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0220],
        [1.0241],
        [1.0288],
        ...,
        [1.0007],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369638.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023,  0.0188,  0.0056,  ...,  0.0038,  0.0183,  0.0129],
        [-0.0027,  0.0214,  0.0065,  ...,  0.0042,  0.0208,  0.0151],
        [-0.0075,  0.0582,  0.0193,  ...,  0.0097,  0.0575,  0.0463],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2008.2281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6788, device='cuda:0')



h[100].sum tensor(-0.6123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1675, device='cuda:0')



h[200].sum tensor(-22.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1459, 0.0469,  ..., 0.0260, 0.1436, 0.1114],
        [0.0000, 0.1655, 0.0537,  ..., 0.0289, 0.1630, 0.1279],
        [0.0000, 0.1025, 0.0317,  ..., 0.0194, 0.1003, 0.0746],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0020, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66352.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0000, 0.0000,  ..., 0.5196, 0.3680, 0.3387],
        [0.0054, 0.0000, 0.0000,  ..., 0.5531, 0.3965, 0.3610],
        [0.0131, 0.0000, 0.0000,  ..., 0.4769, 0.3313, 0.3099],
        ...,
        [0.0701, 0.0205, 0.0945,  ..., 0.0795, 0.0235, 0.0370],
        [0.0701, 0.0205, 0.0945,  ..., 0.0795, 0.0235, 0.0370],
        [0.0700, 0.0205, 0.0944,  ..., 0.0795, 0.0234, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665357.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9056.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.8040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4436.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1066.6039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1998],
        [ 0.1945],
        [ 0.2044],
        ...,
        [-3.4780],
        [-3.4713],
        [-3.4688]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328110., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0220],
        [1.0241],
        [1.0288],
        ...,
        [1.0007],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369638.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0220],
        [1.0241],
        [1.0289],
        ...,
        [1.0007],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369641.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1939.1199, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1020, device='cuda:0')



h[100].sum tensor(-0.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0832, device='cuda:0')



h[200].sum tensor(-22.5755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0044, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0045, 0.0019, 0.0000],
        [0.0000, 0.0287, 0.0073,  ..., 0.0082, 0.0268, 0.0167],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63546.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0179, 0.0795,  ..., 0.0874, 0.0299, 0.0433],
        [0.0639, 0.0162, 0.0666,  ..., 0.0999, 0.0364, 0.0526],
        [0.0557, 0.0101, 0.0273,  ..., 0.1496, 0.0671, 0.0886],
        ...,
        [0.0703, 0.0203, 0.0948,  ..., 0.0797, 0.0236, 0.0372],
        [0.0703, 0.0203, 0.0947,  ..., 0.0796, 0.0235, 0.0372],
        [0.0703, 0.0203, 0.0947,  ..., 0.0796, 0.0235, 0.0371]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646229.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9060.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1036, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4329.6826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1046.8741, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9452],
        [-1.5652],
        [-0.9565],
        ...,
        [-3.4941],
        [-3.4873],
        [-3.4848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317081.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0220],
        [1.0241],
        [1.0289],
        ...,
        [1.0007],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369641.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0221],
        [1.0242],
        [1.0289],
        ...,
        [1.0006],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369645.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0110,  0.0028,  ...,  0.0026,  0.0105,  0.0063],
        [-0.0008,  0.0074,  0.0016,  ...,  0.0021,  0.0069,  0.0033],
        [-0.0022,  0.0174,  0.0051,  ...,  0.0036,  0.0169,  0.0118],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1531.6423, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4661, device='cuda:0')



h[100].sum tensor(-0.3950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1136, device='cuda:0')



h[200].sum tensor(-22.6271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0240, 0.0057,  ..., 0.0075, 0.0220, 0.0128],
        [0.0000, 0.0598, 0.0168,  ..., 0.0129, 0.0577, 0.0386],
        [0.0000, 0.0377, 0.0098,  ..., 0.0096, 0.0357, 0.0221],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0046, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54536.6289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0526, 0.0077, 0.0165,  ..., 0.1693, 0.0815, 0.1017],
        [0.0456, 0.0024, 0.0000,  ..., 0.2152, 0.1125, 0.1340],
        [0.0477, 0.0039, 0.0046,  ..., 0.2033, 0.1034, 0.1258],
        ...,
        [0.0705, 0.0206, 0.0953,  ..., 0.0797, 0.0236, 0.0370],
        [0.0705, 0.0206, 0.0953,  ..., 0.0797, 0.0236, 0.0369],
        [0.0705, 0.0206, 0.0952,  ..., 0.0796, 0.0236, 0.0369]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608161.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9416.8389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.1013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4788.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(961.2819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1027],
        [ 0.1283],
        [ 0.0694],
        ...,
        [-3.4145],
        [-3.3446],
        [-3.2897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-392015.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0221],
        [1.0242],
        [1.0289],
        ...,
        [1.0006],
        [0.9998],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369645.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0222],
        [1.0242],
        [1.0289],
        ...,
        [1.0006],
        [0.9998],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369650.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0011,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2459.6084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2594, device='cuda:0')



h[100].sum tensor(-0.7731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1289, device='cuda:0')



h[200].sum tensor(-22.5151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0093, 0.0012,  ..., 0.0056, 0.0074, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72594.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0675, 0.0199, 0.0923,  ..., 0.0765, 0.0225, 0.0352],
        [0.0678, 0.0200, 0.0926,  ..., 0.0769, 0.0226, 0.0354],
        [0.0677, 0.0199, 0.0898,  ..., 0.0803, 0.0247, 0.0377],
        ...,
        [0.0687, 0.0203, 0.0894,  ..., 0.0851, 0.0258, 0.0411],
        [0.0670, 0.0191, 0.0812,  ..., 0.0929, 0.0293, 0.0471],
        [0.0641, 0.0173, 0.0678,  ..., 0.1057, 0.0349, 0.0570]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684454.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8998.6689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.7599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4448.8506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1122.0819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3076],
        [-3.0518],
        [-2.6323],
        ...,
        [-3.2103],
        [-2.9722],
        [-2.7550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332173.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0222],
        [1.0242],
        [1.0289],
        ...,
        [1.0006],
        [0.9998],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369650.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0222],
        [1.0243],
        [1.0289],
        ...,
        [1.0006],
        [0.9998],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369655.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1896.5901, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.1738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3863, device='cuda:0')



h[100].sum tensor(-0.5122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8325, device='cuda:0')



h[200].sum tensor(-22.5903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0237, 0.0063,  ..., 0.0079, 0.0217, 0.0147],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60741.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0552, 0.0118, 0.0189,  ..., 0.1554, 0.0788, 0.0895],
        [0.0608, 0.0159, 0.0434,  ..., 0.1192, 0.0526, 0.0645],
        [0.0636, 0.0180, 0.0649,  ..., 0.1021, 0.0390, 0.0528],
        ...,
        [0.0692, 0.0216, 0.0948,  ..., 0.0799, 0.0233, 0.0369],
        [0.0692, 0.0216, 0.0948,  ..., 0.0798, 0.0233, 0.0368],
        [0.0691, 0.0216, 0.0948,  ..., 0.0798, 0.0233, 0.0368]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631237.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8919.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.2739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4470.9146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1025.2561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4137],
        [-0.7443],
        [-1.0092],
        ...,
        [-3.4889],
        [-3.4822],
        [-3.4798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340359.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0222],
        [1.0243],
        [1.0289],
        ...,
        [1.0006],
        [0.9998],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369655.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 40.0 event: 200 loss: tensor(871.6470, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0244],
        [1.0290],
        ...,
        [1.0006],
        [0.9998],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369660.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0077,  0.0018,  ...,  0.0023,  0.0072,  0.0035],
        [-0.0009,  0.0077,  0.0018,  ...,  0.0023,  0.0072,  0.0035],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1827.9681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3015, device='cuda:0')



h[100].sum tensor(-0.4588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5279, device='cuda:0')



h[200].sum tensor(-22.6050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0163, 0.0032,  ..., 0.0070, 0.0143, 0.0061],
        [0.0000, 0.0164, 0.0032,  ..., 0.0071, 0.0143, 0.0061],
        [0.0000, 0.0164, 0.0032,  ..., 0.0071, 0.0144, 0.0062],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0053, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58591.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0541, 0.0135, 0.0278,  ..., 0.1368, 0.0518, 0.0794],
        [0.0569, 0.0153, 0.0431,  ..., 0.1233, 0.0444, 0.0695],
        [0.0597, 0.0171, 0.0560,  ..., 0.1121, 0.0389, 0.0610],
        ...,
        [0.0683, 0.0223, 0.0943,  ..., 0.0800, 0.0232, 0.0369],
        [0.0682, 0.0223, 0.0943,  ..., 0.0800, 0.0231, 0.0368],
        [0.0682, 0.0223, 0.0943,  ..., 0.0799, 0.0231, 0.0368]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617119.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8729.9639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.5367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4347.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1010.3542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8409],
        [-1.5072],
        [-2.1572],
        ...,
        [-3.4664],
        [-3.4599],
        [-3.4575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308696.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0244],
        [1.0290],
        ...,
        [1.0006],
        [0.9998],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369660.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0224],
        [1.0244],
        [1.0291],
        ...,
        [1.0006],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369665.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [-0.0039,  0.0317,  0.0102,  ...,  0.0060,  0.0311,  0.0238],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2299.0339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6149, device='cuda:0')



h[100].sum tensor(-0.6324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4503, device='cuda:0')



h[200].sum tensor(-22.5514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0529, 0.0160,  ..., 0.0128, 0.0506, 0.0370],
        [0.0000, 0.0653, 0.0203,  ..., 0.0147, 0.0630, 0.0475],
        [0.0000, 0.1365, 0.0439,  ..., 0.0255, 0.1339, 0.1030],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0055, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68865.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0458, 0.0091, 0.0065,  ..., 0.2208, 0.1305, 0.1325],
        [0.0398, 0.0046, 0.0000,  ..., 0.2697, 0.1685, 0.1654],
        [0.0349, 0.0019, 0.0000,  ..., 0.3147, 0.2050, 0.1955],
        ...,
        [0.0676, 0.0228, 0.0940,  ..., 0.0801, 0.0230, 0.0369],
        [0.0675, 0.0228, 0.0939,  ..., 0.0801, 0.0230, 0.0369],
        [0.0675, 0.0228, 0.0939,  ..., 0.0800, 0.0230, 0.0369]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(667709.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8374.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.0917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4162.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1103.4229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5002],
        [-0.1245],
        [-0.1270],
        ...,
        [-3.4509],
        [-3.4445],
        [-3.4422]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291124.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0224],
        [1.0244],
        [1.0291],
        ...,
        [1.0006],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369665.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0225],
        [1.0245],
        [1.0292],
        ...,
        [1.0005],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369669.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0512,  0.0169,  ...,  0.0090,  0.0504,  0.0402],
        [-0.0061,  0.0484,  0.0160,  ...,  0.0085,  0.0477,  0.0379],
        [-0.0061,  0.0490,  0.0162,  ...,  0.0086,  0.0483,  0.0384],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2089.6116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2490, device='cuda:0')



h[100].sum tensor(-0.5295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9586, device='cuda:0')



h[200].sum tensor(-22.5811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1971, 0.0651,  ..., 0.0348, 0.1941, 0.1542],
        [0.0000, 0.2150, 0.0713,  ..., 0.0375, 0.2120, 0.1693],
        [0.0000, 0.2116, 0.0701,  ..., 0.0370, 0.2086, 0.1664],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0056, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63401.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.6856, 0.5137, 0.4426],
        [0.0000, 0.0000, 0.0000,  ..., 0.7084, 0.5334, 0.4577],
        [0.0012, 0.0000, 0.0000,  ..., 0.6800, 0.5076, 0.4389],
        ...,
        [0.0670, 0.0232, 0.0941,  ..., 0.0799, 0.0231, 0.0370],
        [0.0670, 0.0231, 0.0940,  ..., 0.0799, 0.0231, 0.0370],
        [0.0670, 0.0231, 0.0940,  ..., 0.0798, 0.0230, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(638886.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8269.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.7383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4115.5537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1066.8263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0990],
        [ 0.0979],
        [ 0.1067],
        ...,
        [-3.4495],
        [-3.4431],
        [-3.4408]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280440.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0225],
        [1.0245],
        [1.0292],
        ...,
        [1.0005],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369669.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0245],
        [1.0293],
        ...,
        [1.0005],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369673.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2284.1794, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3194, device='cuda:0')



h[100].sum tensor(-0.5971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4072, device='cuda:0')



h[200].sum tensor(-22.5589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0330, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0054, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0056, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67297.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0603, 0.0195, 0.0671,  ..., 0.0982, 0.0353, 0.0516],
        [0.0633, 0.0214, 0.0867,  ..., 0.0815, 0.0243, 0.0399],
        [0.0642, 0.0219, 0.0898,  ..., 0.0796, 0.0231, 0.0383],
        ...,
        [0.0666, 0.0232, 0.0942,  ..., 0.0796, 0.0232, 0.0374],
        [0.0665, 0.0232, 0.0942,  ..., 0.0795, 0.0232, 0.0373],
        [0.0665, 0.0231, 0.0941,  ..., 0.0795, 0.0232, 0.0373]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660580.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8256.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.2487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4287.7803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1101.3929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3530],
        [-1.9557],
        [-2.4308],
        ...,
        [-3.4505],
        [-3.4441],
        [-3.4418]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309423.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0245],
        [1.0293],
        ...,
        [1.0005],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369673.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0246],
        [1.0294],
        ...,
        [1.0005],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369677.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [-0.0017,  0.0144,  0.0041,  ...,  0.0034,  0.0139,  0.0091],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1820.8945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7727, device='cuda:0')



h[100].sum tensor(-0.4041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3045, device='cuda:0')



h[200].sum tensor(-22.6170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0278, 0.0078,  ..., 0.0090, 0.0257, 0.0180],
        [0.0000, 0.0176, 0.0042,  ..., 0.0075, 0.0155, 0.0093],
        [0.0000, 0.0446, 0.0130,  ..., 0.0116, 0.0424, 0.0298],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0055, 0.0019, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0055, 0.0019, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0055, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57658.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0471, 0.0107, 0.0000,  ..., 0.1931, 0.1071, 0.1162],
        [0.0481, 0.0114, 0.0000,  ..., 0.1872, 0.1017, 0.1123],
        [0.0429, 0.0084, 0.0000,  ..., 0.2314, 0.1367, 0.1421],
        ...,
        [0.0662, 0.0230, 0.0944,  ..., 0.0792, 0.0233, 0.0378],
        [0.0662, 0.0230, 0.0944,  ..., 0.0791, 0.0233, 0.0378],
        [0.0662, 0.0230, 0.0943,  ..., 0.0791, 0.0233, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616220.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8275.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.2297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4363.2490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1028.0261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2852],
        [ 0.2825],
        [ 0.2765],
        ...,
        [-3.4563],
        [-3.4498],
        [-3.4474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310736.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0246],
        [1.0294],
        ...,
        [1.0005],
        [0.9997],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369677.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0227],
        [1.0246],
        [1.0295],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369681.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0021,  0.0176,  0.0052,  ...,  0.0038,  0.0171,  0.0118],
        [-0.0010,  0.0090,  0.0022,  ...,  0.0025,  0.0085,  0.0046],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2415.2314, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8119, device='cuda:0')



h[100].sum tensor(-0.6362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6252, device='cuda:0')



h[200].sum tensor(-22.5434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0766, 0.0229,  ..., 0.0163, 0.0743, 0.0522],
        [0.0000, 0.0475, 0.0140,  ..., 0.0119, 0.0454, 0.0323],
        [0.0000, 0.0122, 0.0023,  ..., 0.0066, 0.0101, 0.0047],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0054, 0.0019, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0054, 0.0019, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0054, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69675.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0306, 0.0018, 0.0000,  ..., 0.3024, 0.1872, 0.1926],
        [0.0391, 0.0057, 0.0000,  ..., 0.2445, 0.1441, 0.1528],
        [0.0479, 0.0112, 0.0044,  ..., 0.1839, 0.0968, 0.1116],
        ...,
        [0.0661, 0.0228, 0.0948,  ..., 0.0787, 0.0235, 0.0383],
        [0.0661, 0.0228, 0.0947,  ..., 0.0787, 0.0235, 0.0382],
        [0.0660, 0.0228, 0.0947,  ..., 0.0787, 0.0234, 0.0382]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663590.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7971.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(161.0419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4075.4553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1145.0516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1993],
        [ 0.2240],
        [ 0.2431],
        ...,
        [-3.4681],
        [-3.4616],
        [-3.4592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274514.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0227],
        [1.0246],
        [1.0295],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369681.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0227],
        [1.0246],
        [1.0295],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369684.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0013,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1829.4175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0072, device='cuda:0')



h[100].sum tensor(-0.4043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3388, device='cuda:0')



h[200].sum tensor(-22.6147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0051, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0052, 0.0019, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0052, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0053, 0.0020, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0053, 0.0020, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0053, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57073.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0528, 0.0142, 0.0385,  ..., 0.1546, 0.0832, 0.0914],
        [0.0609, 0.0192, 0.0675,  ..., 0.0974, 0.0385, 0.0524],
        [0.0634, 0.0210, 0.0878,  ..., 0.0811, 0.0255, 0.0415],
        ...,
        [0.0663, 0.0225, 0.0955,  ..., 0.0783, 0.0237, 0.0386],
        [0.0662, 0.0225, 0.0954,  ..., 0.0782, 0.0236, 0.0386],
        [0.0662, 0.0225, 0.0954,  ..., 0.0782, 0.0236, 0.0386]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605847.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8262.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.6526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4452.6499, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1043.3984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2111],
        [-0.6746],
        [-0.9939],
        ...,
        [-3.4830],
        [-3.4776],
        [-3.4761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308030.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0227],
        [1.0246],
        [1.0295],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369684.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0228],
        [1.0247],
        [1.0296],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369688.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [-0.0012,  0.0104,  0.0027,  ...,  0.0027,  0.0099,  0.0057],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1999.7026, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0224, device='cuda:0')



h[100].sum tensor(-0.4719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7793, device='cuda:0')



h[200].sum tensor(-22.5921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0224, 0.0059,  ..., 0.0078, 0.0205, 0.0135],
        [0.0000, 0.0134, 0.0027,  ..., 0.0065, 0.0115, 0.0059],
        [0.0000, 0.0252, 0.0062,  ..., 0.0083, 0.0232, 0.0135],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0051, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60487.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0534, 0.0143, 0.0267,  ..., 0.1395, 0.0659, 0.0826],
        [0.0526, 0.0137, 0.0225,  ..., 0.1435, 0.0659, 0.0860],
        [0.0475, 0.0104, 0.0128,  ..., 0.1760, 0.0864, 0.1090],
        ...,
        [0.0665, 0.0225, 0.0964,  ..., 0.0779, 0.0238, 0.0386],
        [0.0665, 0.0225, 0.0963,  ..., 0.0778, 0.0238, 0.0386],
        [0.0665, 0.0225, 0.0963,  ..., 0.0778, 0.0238, 0.0385]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621375.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8271.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(184.4867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4455.3311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1081.5826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2735],
        [-0.0506],
        [ 0.1418],
        ...,
        [-3.5202],
        [-3.5131],
        [-3.5107]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326051.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0228],
        [1.0247],
        [1.0296],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369688.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0228],
        [1.0247],
        [1.0297],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369691.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1850.9646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9333, device='cuda:0')



h[100].sum tensor(-0.4182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4741, device='cuda:0')



h[200].sum tensor(-22.6080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0184, 0.0045,  ..., 0.0071, 0.0165, 0.0101],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0050, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57627.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0632, 0.0205, 0.0873,  ..., 0.0810, 0.0266, 0.0419],
        [0.0622, 0.0198, 0.0766,  ..., 0.0908, 0.0338, 0.0483],
        [0.0574, 0.0166, 0.0458,  ..., 0.1258, 0.0583, 0.0725],
        ...,
        [0.0670, 0.0227, 0.0975,  ..., 0.0774, 0.0240, 0.0384],
        [0.0669, 0.0226, 0.0974,  ..., 0.0774, 0.0240, 0.0384],
        [0.0669, 0.0226, 0.0974,  ..., 0.0774, 0.0240, 0.0384]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610473.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8366.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(195.1977, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4550.8975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1066.8727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0365],
        [-2.1479],
        [-1.9766],
        ...,
        [-3.5617],
        [-3.5542],
        [-3.5513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328050., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0228],
        [1.0247],
        [1.0297],
        ...,
        [1.0004],
        [0.9996],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369691.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0248],
        [1.0299],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369695.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [-0.0024,  0.0199,  0.0060,  ...,  0.0040,  0.0194,  0.0138],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1801.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5752, device='cuda:0')



h[100].sum tensor(-0.4041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4218, device='cuda:0')



h[200].sum tensor(-22.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0312, 0.0083,  ..., 0.0089, 0.0293, 0.0187],
        [0.0000, 0.0194, 0.0049,  ..., 0.0071, 0.0176, 0.0111],
        [0.0000, 0.0950, 0.0293,  ..., 0.0186, 0.0929, 0.0680],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0048, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0048, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58157.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0495, 0.0117, 0.0051,  ..., 0.1689, 0.0860, 0.1035],
        [0.0504, 0.0123, 0.0114,  ..., 0.1699, 0.0899, 0.1032],
        [0.0405, 0.0068, 0.0000,  ..., 0.2448, 0.1478, 0.1541],
        ...,
        [0.0676, 0.0229, 0.0985,  ..., 0.0773, 0.0241, 0.0380],
        [0.0676, 0.0229, 0.0984,  ..., 0.0772, 0.0241, 0.0380],
        [0.0675, 0.0229, 0.0984,  ..., 0.0772, 0.0241, 0.0380]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620620.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8452.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(200.2161, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4579.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1073.6144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0112],
        [-0.1209],
        [-0.1717],
        ...,
        [-3.6001],
        [-3.5924],
        [-3.5896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333336.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0248],
        [1.0299],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369695.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 50.0 event: 250 loss: tensor(499.4881, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0248],
        [1.0300],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369699., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [-0.0015,  0.0125,  0.0034,  ...,  0.0029,  0.0121,  0.0076],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3306.9150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.4630, device='cuda:0')



h[100].sum tensor(-0.9733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7659, device='cuda:0')



h[200].sum tensor(-22.4243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        [0.0000, 0.0219, 0.0051,  ..., 0.0074, 0.0201, 0.0109],
        [0.0000, 0.0466, 0.0130,  ..., 0.0112, 0.0447, 0.0294],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0047, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91043.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0594, 0.0180, 0.0579,  ..., 0.1087, 0.0438, 0.0611],
        [0.0507, 0.0127, 0.0286,  ..., 0.1637, 0.0804, 0.0997],
        [0.0401, 0.0067, 0.0043,  ..., 0.2369, 0.1329, 0.1503],
        ...,
        [0.0682, 0.0231, 0.0991,  ..., 0.0772, 0.0241, 0.0378],
        [0.0681, 0.0231, 0.0991,  ..., 0.0771, 0.0241, 0.0378],
        [0.0681, 0.0230, 0.0990,  ..., 0.0771, 0.0241, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(779553.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8006.5166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.5518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(207.5712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4138.4116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1360.6830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0208],
        [-0.4920],
        [-0.0334],
        ...,
        [-3.6308],
        [-3.6227],
        [-3.6196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297325.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0248],
        [1.0300],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369699., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0248],
        [1.0301],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369703.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2063.6853, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6649, device='cuda:0')



h[100].sum tensor(-0.5022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0193, device='cuda:0')



h[200].sum tensor(-22.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0046, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0047, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66080.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0627, 0.0201, 0.0792,  ..., 0.0903, 0.0312, 0.0478],
        [0.0638, 0.0207, 0.0841,  ..., 0.0864, 0.0291, 0.0448],
        [0.0659, 0.0219, 0.0930,  ..., 0.0789, 0.0253, 0.0392],
        ...,
        [0.0684, 0.0232, 0.0991,  ..., 0.0774, 0.0241, 0.0377],
        [0.0683, 0.0232, 0.0991,  ..., 0.0773, 0.0241, 0.0377],
        [0.0683, 0.0232, 0.0990,  ..., 0.0773, 0.0241, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674437.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8600.7461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(197.8280, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4744.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1130.4979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6970],
        [-1.9595],
        [-2.2519],
        ...,
        [-3.6432],
        [-3.6351],
        [-3.6320]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-402275.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0248],
        [1.0301],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369703.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0302],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369707.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0029,  0.0243,  0.0076,  ...,  0.0047,  0.0239,  0.0175],
        [-0.0015,  0.0126,  0.0035,  ...,  0.0029,  0.0122,  0.0077],
        [-0.0015,  0.0126,  0.0035,  ...,  0.0029,  0.0122,  0.0077],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1964.8712, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2620, device='cuda:0')



h[100].sum tensor(-0.4594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8143, device='cuda:0')



h[200].sum tensor(-22.5894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0666, 0.0195,  ..., 0.0143, 0.0648, 0.0440],
        [0.0000, 0.0690, 0.0203,  ..., 0.0147, 0.0671, 0.0460],
        [0.0000, 0.0254, 0.0064,  ..., 0.0080, 0.0236, 0.0138],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61967.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0401, 0.0078, 0.0000,  ..., 0.2316, 0.1314, 0.1453],
        [0.0429, 0.0094, 0.0000,  ..., 0.2166, 0.1204, 0.1349],
        [0.0512, 0.0140, 0.0205,  ..., 0.1639, 0.0806, 0.0989],
        ...,
        [0.0684, 0.0234, 0.0990,  ..., 0.0776, 0.0241, 0.0377],
        [0.0684, 0.0234, 0.0989,  ..., 0.0775, 0.0241, 0.0376],
        [0.0683, 0.0234, 0.0989,  ..., 0.0775, 0.0241, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640629.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8536.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(192.0631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4557.0830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1095.7068, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.4169e-01],
        [-1.5575e-03],
        [-4.8670e-01],
        ...,
        [-3.6358e+00],
        [-3.6298e+00],
        [-3.6291e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343634.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0302],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369707.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0302],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369707.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030,  0.0250,  0.0078,  ...,  0.0048,  0.0245,  0.0181],
        [-0.0021,  0.0176,  0.0052,  ...,  0.0037,  0.0171,  0.0118],
        [-0.0029,  0.0245,  0.0076,  ...,  0.0047,  0.0240,  0.0177],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1924.5730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7741, device='cuda:0')



h[100].sum tensor(-0.4446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7431, device='cuda:0')



h[200].sum tensor(-22.5943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0644, 0.0187,  ..., 0.0139, 0.0626, 0.0422],
        [0.0000, 0.0721, 0.0214,  ..., 0.0151, 0.0702, 0.0486],
        [0.0000, 0.0464, 0.0131,  ..., 0.0112, 0.0446, 0.0292],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60307.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0305, 0.0028, 0.0000,  ..., 0.2881, 0.1699, 0.1847],
        [0.0319, 0.0039, 0.0000,  ..., 0.2833, 0.1679, 0.1811],
        [0.0373, 0.0066, 0.0000,  ..., 0.2499, 0.1428, 0.1584],
        ...,
        [0.0684, 0.0234, 0.0990,  ..., 0.0776, 0.0241, 0.0377],
        [0.0684, 0.0234, 0.0989,  ..., 0.0775, 0.0241, 0.0376],
        [0.0683, 0.0234, 0.0989,  ..., 0.0775, 0.0241, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629706.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8563.4756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(192.0141, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4610.2603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1082.0439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3021],
        [ 0.3093],
        [ 0.3111],
        ...,
        [-3.6475],
        [-3.6394],
        [-3.6361]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351032.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0302],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369707.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0303],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369712.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1789.7229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1134, device='cuda:0')



h[100].sum tensor(-0.3845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3543, device='cuda:0')



h[200].sum tensor(-22.6132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0048, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57339.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0657, 0.0222, 0.0954,  ..., 0.0747, 0.0231, 0.0361],
        [0.0660, 0.0224, 0.0957,  ..., 0.0750, 0.0232, 0.0362],
        [0.0664, 0.0226, 0.0962,  ..., 0.0756, 0.0233, 0.0366],
        ...,
        [0.0681, 0.0235, 0.0983,  ..., 0.0779, 0.0240, 0.0379],
        [0.0680, 0.0235, 0.0982,  ..., 0.0778, 0.0240, 0.0378],
        [0.0680, 0.0235, 0.0982,  ..., 0.0778, 0.0240, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619675.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8625.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2520, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(181.4661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4703.7065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1044.4052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3548],
        [-3.2780],
        [-3.1066],
        ...,
        [-3.6339],
        [-3.6259],
        [-3.6228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-374540.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0303],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369712.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0304],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369717.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2033.3275, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8044, device='cuda:0')



h[100].sum tensor(-0.4622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8936, device='cuda:0')



h[200].sum tensor(-22.5857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60538.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0222, 0.0946,  ..., 0.0750, 0.0231, 0.0363],
        [0.0656, 0.0223, 0.0949,  ..., 0.0753, 0.0232, 0.0365],
        [0.0640, 0.0216, 0.0810,  ..., 0.0883, 0.0324, 0.0454],
        ...,
        [0.0676, 0.0234, 0.0975,  ..., 0.0782, 0.0240, 0.0382],
        [0.0676, 0.0234, 0.0974,  ..., 0.0781, 0.0240, 0.0381],
        [0.0675, 0.0234, 0.0974,  ..., 0.0781, 0.0240, 0.0381]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(626806.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8529.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(171.4821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4650.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1061.8497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7658],
        [-2.1078],
        [-2.0844],
        ...,
        [-3.6167],
        [-3.6091],
        [-3.6063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-365080.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0249],
        [1.0304],
        ...,
        [1.0003],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369717.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0250],
        [1.0305],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369722.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2516.9297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7880, device='cuda:0')



h[100].sum tensor(-0.6252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9140, device='cuda:0')



h[200].sum tensor(-22.5286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0051, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72198.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0585, 0.0189, 0.0610,  ..., 0.1055, 0.0399, 0.0589],
        [0.0608, 0.0200, 0.0725,  ..., 0.0958, 0.0340, 0.0517],
        [0.0628, 0.0211, 0.0804,  ..., 0.0893, 0.0306, 0.0468],
        ...,
        [0.0672, 0.0233, 0.0967,  ..., 0.0785, 0.0241, 0.0385],
        [0.0672, 0.0233, 0.0966,  ..., 0.0784, 0.0240, 0.0385],
        [0.0671, 0.0233, 0.0966,  ..., 0.0784, 0.0240, 0.0384]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681564.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8009.4580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(165.9066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4071.3130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1162.1925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6516],
        [-1.1082],
        [-1.5581],
        ...,
        [-3.5977],
        [-3.5903],
        [-3.5876]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292894.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0250],
        [1.0305],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369722.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0250],
        [1.0306],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369727.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040,  0.0335,  0.0108,  ...,  0.0062,  0.0330,  0.0251],
        [-0.0011,  0.0100,  0.0026,  ...,  0.0026,  0.0095,  0.0053],
        [-0.0035,  0.0294,  0.0094,  ...,  0.0056,  0.0289,  0.0217],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3006.0315, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9936, device='cuda:0')



h[100].sum tensor(-0.7904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.9668, device='cuda:0')



h[200].sum tensor(-22.4698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0530, 0.0154,  ..., 0.0126, 0.0512, 0.0345],
        [0.0000, 0.1258, 0.0402,  ..., 0.0237, 0.1238, 0.0934],
        [0.0000, 0.0816, 0.0248,  ..., 0.0170, 0.0797, 0.0561],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0052, 0.0020, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0052, 0.0020, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0052, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85833.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8557e-02, 5.7828e-03, 0.0000e+00,  ..., 2.9334e-01, 1.8287e-01,
         1.8733e-01],
        [1.2665e-02, 9.2851e-04, 0.0000e+00,  ..., 4.0467e-01, 2.7355e-01,
         2.6209e-01],
        [1.0774e-02, 9.4232e-05, 0.0000e+00,  ..., 4.1780e-01, 2.8238e-01,
         2.7106e-01],
        ...,
        [6.7106e-02, 2.3097e-02, 9.6258e-02,  ..., 7.8721e-02, 2.4146e-02,
         3.8721e-02],
        [6.5498e-02, 2.2345e-02, 8.5919e-02,  ..., 8.7540e-02, 3.0408e-02,
         4.4840e-02],
        [6.1199e-02, 2.0439e-02, 5.5193e-02,  ..., 1.1364e-01, 4.9995e-02,
         6.2737e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(768715., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7826.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.0087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(160.0859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3947.8696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1270.8228, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1696],
        [ 0.1528],
        [ 0.1333],
        ...,
        [-2.9560],
        [-2.2959],
        [-1.4337]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286022.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0250],
        [1.0306],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369727.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0250],
        [1.0307],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369731.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2393.0928, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7955, device='cuda:0')



h[100].sum tensor(-0.5701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6228, device='cuda:0')



h[200].sum tensor(-22.5439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0051, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69243.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0640, 0.0212, 0.0898,  ..., 0.0790, 0.0246, 0.0399],
        [0.0651, 0.0217, 0.0932,  ..., 0.0765, 0.0235, 0.0377],
        [0.0657, 0.0220, 0.0942,  ..., 0.0765, 0.0235, 0.0375],
        ...,
        [0.0674, 0.0229, 0.0962,  ..., 0.0788, 0.0242, 0.0388],
        [0.0674, 0.0229, 0.0962,  ..., 0.0787, 0.0241, 0.0387],
        [0.0673, 0.0229, 0.0961,  ..., 0.0787, 0.0241, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672042.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8107.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3777, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.8663, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4019.3896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1125.0394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5915],
        [-3.0015],
        [-3.3107],
        ...,
        [-3.6015],
        [-3.5942],
        [-3.5916]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287270.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0250],
        [1.0307],
        ...,
        [1.0004],
        [0.9995],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369731.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0251],
        [1.0308],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369735.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1912.0464, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7102, device='cuda:0')



h[100].sum tensor(-0.3997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5876, device='cuda:0')



h[200].sum tensor(-22.6019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0050, 0.0020, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0051, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60409.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0653, 0.0214, 0.0933,  ..., 0.0756, 0.0232, 0.0370],
        [0.0656, 0.0216, 0.0937,  ..., 0.0759, 0.0233, 0.0372],
        [0.0659, 0.0218, 0.0941,  ..., 0.0765, 0.0235, 0.0376],
        ...,
        [0.0676, 0.0227, 0.0961,  ..., 0.0788, 0.0241, 0.0389],
        [0.0675, 0.0226, 0.0961,  ..., 0.0787, 0.0241, 0.0389],
        [0.0675, 0.0226, 0.0960,  ..., 0.0787, 0.0241, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632501.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8351.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(152.1259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4223.5752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1044.9753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6266],
        [-3.5593],
        [-3.4336],
        ...,
        [-3.6079],
        [-3.6006],
        [-3.5979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308310.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0251],
        [1.0308],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369735.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 60.0 event: 300 loss: tensor(450.9087, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0234],
        [1.0251],
        [1.0309],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369739.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0012,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2136.5806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2915, device='cuda:0')



h[100].sum tensor(-0.4779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1109, device='cuda:0')



h[200].sum tensor(-22.5730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0049, 0.0019, 0.0000],
        [0.0000, 0.0129, 0.0027,  ..., 0.0064, 0.0113, 0.0055],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0051, 0.0020, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0051, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63614.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0654, 0.0213, 0.0914,  ..., 0.0777, 0.0242, 0.0382],
        [0.0626, 0.0200, 0.0757,  ..., 0.0924, 0.0331, 0.0489],
        [0.0567, 0.0174, 0.0434,  ..., 0.1238, 0.0518, 0.0717],
        ...,
        [0.0682, 0.0228, 0.0965,  ..., 0.0788, 0.0240, 0.0384],
        [0.0681, 0.0227, 0.0965,  ..., 0.0787, 0.0240, 0.0384],
        [0.0681, 0.0227, 0.0964,  ..., 0.0787, 0.0240, 0.0384]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643742.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8504.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.7760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4323.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1068.0852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3424],
        [-1.7882],
        [-1.1432],
        ...,
        [-3.6357],
        [-3.6282],
        [-3.6255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342522.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0234],
        [1.0251],
        [1.0309],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369739.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0235],
        [1.0252],
        [1.0310],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369743.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0026,  0.0226,  0.0070,  ...,  0.0045,  0.0222,  0.0160],
        [-0.0035,  0.0296,  0.0094,  ...,  0.0056,  0.0292,  0.0219],
        [-0.0017,  0.0151,  0.0044,  ...,  0.0034,  0.0146,  0.0096],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1612.2908, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5684, device='cuda:0')



h[100].sum tensor(-0.3026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9825, device='cuda:0')



h[200].sum tensor(-22.6341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1362, 0.0439,  ..., 0.0252, 0.1343, 0.1023],
        [0.0000, 0.1147, 0.0364,  ..., 0.0219, 0.1128, 0.0841],
        [0.0000, 0.0965, 0.0306,  ..., 0.0191, 0.0946, 0.0712],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54511.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9107e-03, 0.0000e+00, 0.0000e+00,  ..., 5.5105e-01, 4.0165e-01,
         3.5791e-01],
        [5.0417e-03, 3.3121e-04, 0.0000e+00,  ..., 5.0882e-01, 3.6593e-01,
         3.2967e-01],
        [1.5499e-02, 3.9412e-03, 0.0000e+00,  ..., 4.1913e-01, 2.9348e-01,
         2.6905e-01],
        ...,
        [6.9035e-02, 2.2781e-02, 9.7174e-02,  ..., 7.8750e-02, 2.3911e-02,
         3.7906e-02],
        [6.8980e-02, 2.2757e-02, 9.7103e-02,  ..., 7.8681e-02, 2.3891e-02,
         3.7869e-02],
        [6.8949e-02, 2.2739e-02, 9.7068e-02,  ..., 7.8638e-02, 2.3878e-02,
         3.7844e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610567.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8776.0654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9196, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.1298, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4449.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(990.9692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0170],
        [ 0.0247],
        [ 0.0407],
        ...,
        [-3.6707],
        [-3.6634],
        [-3.6609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340061.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0235],
        [1.0252],
        [1.0310],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369743.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0236],
        [1.0253],
        [1.0311],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369746.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0078,  0.0018,  ...,  0.0022,  0.0074,  0.0035],
        [-0.0021,  0.0179,  0.0054,  ...,  0.0038,  0.0175,  0.0120],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2583.0942, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.8414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1042, device='cuda:0')



h[100].sum tensor(-0.6336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.2524, device='cuda:0')



h[200].sum tensor(-22.5138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0656, 0.0193,  ..., 0.0143, 0.0639, 0.0430],
        [0.0000, 0.0307, 0.0077,  ..., 0.0089, 0.0291, 0.0159],
        [0.0000, 0.0266, 0.0069,  ..., 0.0083, 0.0250, 0.0149],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0049, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0049, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0049, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71291.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0412, 0.0099, 0.0000,  ..., 0.2171, 0.1167, 0.1351],
        [0.0466, 0.0122, 0.0087,  ..., 0.1850, 0.0924, 0.1134],
        [0.0524, 0.0149, 0.0262,  ..., 0.1576, 0.0755, 0.0940],
        ...,
        [0.0701, 0.0228, 0.0981,  ..., 0.0786, 0.0239, 0.0372],
        [0.0700, 0.0227, 0.0981,  ..., 0.0786, 0.0239, 0.0372],
        [0.0700, 0.0227, 0.0980,  ..., 0.0785, 0.0239, 0.0372]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682109.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9025.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5673, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.9603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4731.3833, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1126.7860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0605],
        [-0.1699],
        [-0.6115],
        ...,
        [-3.6953],
        [-3.6450],
        [-3.5624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-417151.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0236],
        [1.0253],
        [1.0311],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369746.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0253],
        [1.0311],
        ...,
        [1.0004],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369749.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1715.5004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6905, device='cuda:0')



h[100].sum tensor(-0.3433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2925, device='cuda:0')



h[200].sum tensor(-22.6173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0016, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0016, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0047, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55706.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0208, 0.0907,  ..., 0.0801, 0.0253, 0.0390],
        [0.0682, 0.0214, 0.0954,  ..., 0.0763, 0.0234, 0.0360],
        [0.0687, 0.0217, 0.0964,  ..., 0.0764, 0.0234, 0.0360],
        ...,
        [0.0705, 0.0225, 0.0985,  ..., 0.0786, 0.0240, 0.0372],
        [0.0704, 0.0225, 0.0985,  ..., 0.0785, 0.0240, 0.0371],
        [0.0704, 0.0225, 0.0984,  ..., 0.0785, 0.0240, 0.0371]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615041.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9163.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.0951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4679.9536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1000.6315, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6629],
        [-2.8332],
        [-2.9553],
        ...,
        [-3.7326],
        [-3.7244],
        [-3.7214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-391770.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0253],
        [1.0311],
        ...,
        [1.0004],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369749.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0254],
        [1.0312],
        ...,
        [1.0004],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369753.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0011,  0.0004, -0.0023],
        [-0.0050,  0.0429,  0.0140,  ...,  0.0076,  0.0423,  0.0331],
        [-0.0024,  0.0206,  0.0063,  ...,  0.0042,  0.0202,  0.0144],
        ...,
        [-0.0018,  0.0159,  0.0046,  ...,  0.0035,  0.0155,  0.0104],
        [-0.0013,  0.0114,  0.0031,  ...,  0.0028,  0.0110,  0.0066],
        [-0.0018,  0.0159,  0.0046,  ...,  0.0035,  0.0155,  0.0104]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1948.9132, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4652, device='cuda:0')



h[100].sum tensor(-0.4174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8440, device='cuda:0')



h[200].sum tensor(-22.5889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1473, 0.0476,  ..., 0.0266, 0.1451, 0.1117],
        [0.0000, 0.0587, 0.0174,  ..., 0.0132, 0.0569, 0.0395],
        [0.0000, 0.1495, 0.0483,  ..., 0.0270, 0.1473, 0.1135],
        ...,
        [0.0000, 0.0275, 0.0071,  ..., 0.0085, 0.0257, 0.0154],
        [0.0000, 0.0701, 0.0206,  ..., 0.0150, 0.0682, 0.0464],
        [0.0000, 0.0568, 0.0160,  ..., 0.0130, 0.0549, 0.0352]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59041.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0121, 0.0008, 0.0000,  ..., 0.4498, 0.3221, 0.2914],
        [0.0231, 0.0021, 0.0000,  ..., 0.3646, 0.2484, 0.2341],
        [0.0097, 0.0008, 0.0000,  ..., 0.4659, 0.3341, 0.3025],
        ...,
        [0.0555, 0.0149, 0.0230,  ..., 0.1618, 0.0810, 0.0961],
        [0.0467, 0.0107, 0.0000,  ..., 0.2131, 0.1187, 0.1319],
        [0.0445, 0.0098, 0.0000,  ..., 0.2263, 0.1279, 0.1412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(626510.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9219.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3610, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.5016, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4751.9233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1024.4005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1505],
        [ 0.1591],
        [ 0.1427],
        ...,
        [-1.6939],
        [-0.9966],
        [-0.4555]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-416882.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0254],
        [1.0312],
        ...,
        [1.0004],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369753.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0239],
        [1.0255],
        [1.0314],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369757.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1727.4803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8425, device='cuda:0')



h[100].sum tensor(-0.3402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3147, device='cuda:0')



h[200].sum tensor(-22.6161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0047, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0048, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56541.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0603, 0.0171, 0.0469,  ..., 0.1178, 0.0533, 0.0659],
        [0.0650, 0.0194, 0.0787,  ..., 0.0910, 0.0332, 0.0473],
        [0.0666, 0.0203, 0.0890,  ..., 0.0830, 0.0269, 0.0419],
        ...,
        [0.0702, 0.0221, 0.0982,  ..., 0.0788, 0.0243, 0.0377],
        [0.0702, 0.0220, 0.0981,  ..., 0.0787, 0.0243, 0.0377],
        [0.0701, 0.0220, 0.0981,  ..., 0.0786, 0.0243, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620967.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9143.2178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.1413, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4670.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1002.7410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8486],
        [-1.3947],
        [-1.6095],
        ...,
        [-3.7306],
        [-3.7225],
        [-3.7196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-407243.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0239],
        [1.0255],
        [1.0314],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369757.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0256],
        [1.0315],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369762.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1909.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3280, device='cuda:0')



h[100].sum tensor(-0.3898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6779, device='cuda:0')



h[200].sum tensor(-22.5964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0048, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0050, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60410.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0635, 0.0190, 0.0677,  ..., 0.0986, 0.0415, 0.0523],
        [0.0664, 0.0204, 0.0877,  ..., 0.0823, 0.0283, 0.0410],
        [0.0663, 0.0203, 0.0861,  ..., 0.0848, 0.0293, 0.0429],
        ...,
        [0.0696, 0.0219, 0.0975,  ..., 0.0789, 0.0244, 0.0383],
        [0.0695, 0.0219, 0.0975,  ..., 0.0788, 0.0244, 0.0382],
        [0.0695, 0.0219, 0.0974,  ..., 0.0788, 0.0244, 0.0382]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632505., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8786.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.7994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4255.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1038.5398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0060],
        [-2.0790],
        [-1.6928],
        ...,
        [-3.7039],
        [-3.6979],
        [-3.6964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334234., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0256],
        [1.0315],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369762.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0241],
        [1.0258],
        [1.0316],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369766.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0012,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1548.3983, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.7147, device='cuda:0')



h[100].sum tensor(-0.2673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8577, device='cuda:0')



h[200].sum tensor(-22.6413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0049, 0.0017, 0.0000],
        [0.0000, 0.0096, 0.0015,  ..., 0.0059, 0.0079, 0.0028],
        [0.0000, 0.0159, 0.0030,  ..., 0.0069, 0.0141, 0.0056],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0051, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0051, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0051, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51968.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0609, 0.0178, 0.0651,  ..., 0.1028, 0.0386, 0.0569],
        [0.0570, 0.0158, 0.0444,  ..., 0.1222, 0.0501, 0.0714],
        [0.0538, 0.0143, 0.0302,  ..., 0.1398, 0.0608, 0.0843],
        ...,
        [0.0693, 0.0220, 0.0972,  ..., 0.0790, 0.0244, 0.0383],
        [0.0693, 0.0220, 0.0972,  ..., 0.0790, 0.0244, 0.0383],
        [0.0693, 0.0220, 0.0971,  ..., 0.0789, 0.0244, 0.0383]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597997.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9102.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.6720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.0552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4667.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(954.4994, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8929],
        [-0.4599],
        [-0.1925],
        ...,
        [-3.7042],
        [-3.6964],
        [-3.6937]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-408992.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0241],
        [1.0258],
        [1.0316],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369766.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0242],
        [1.0259],
        [1.0318],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369770.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0174,  0.0051,  ...,  0.0037,  0.0169,  0.0115],
        [-0.0008,  0.0077,  0.0017,  ...,  0.0023,  0.0072,  0.0034],
        [-0.0011,  0.0106,  0.0027,  ...,  0.0027,  0.0101,  0.0058],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0012,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1945.1743, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5244, device='cuda:0')



h[100].sum tensor(-0.3866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7066, device='cuda:0')



h[200].sum tensor(-22.5950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0313, 0.0077,  ..., 0.0093, 0.0295, 0.0162],
        [0.0000, 0.0667, 0.0194,  ..., 0.0147, 0.0647, 0.0435],
        [0.0000, 0.0316, 0.0085,  ..., 0.0093, 0.0297, 0.0188],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59627.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0452, 0.0105, 0.0084,  ..., 0.1858, 0.0957, 0.1161],
        [0.0401, 0.0080, 0.0000,  ..., 0.2200, 0.1221, 0.1397],
        [0.0457, 0.0110, 0.0002,  ..., 0.1935, 0.1042, 0.1206],
        ...,
        [0.0693, 0.0223, 0.0971,  ..., 0.0792, 0.0245, 0.0382],
        [0.0692, 0.0223, 0.0971,  ..., 0.0792, 0.0244, 0.0381],
        [0.0692, 0.0223, 0.0970,  ..., 0.0791, 0.0244, 0.0381]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632063.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8967.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.2115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4577.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1018.9647, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1191],
        [ 0.2708],
        [ 0.3049],
        ...,
        [-3.7016],
        [-3.6957],
        [-3.6946]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-410623.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0242],
        [1.0259],
        [1.0318],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369770.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0260],
        [1.0319],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369775.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1903.5209, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5876, device='cuda:0')



h[100].sum tensor(-0.3626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5697, device='cuda:0')



h[200].sum tensor(-22.6028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0053, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0054, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60168.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0621, 0.0195, 0.0717,  ..., 0.0967, 0.0353, 0.0515],
        [0.0656, 0.0212, 0.0884,  ..., 0.0822, 0.0266, 0.0406],
        [0.0672, 0.0220, 0.0946,  ..., 0.0773, 0.0237, 0.0368],
        ...,
        [0.0689, 0.0229, 0.0967,  ..., 0.0796, 0.0244, 0.0381],
        [0.0688, 0.0228, 0.0966,  ..., 0.0795, 0.0244, 0.0380],
        [0.0688, 0.0228, 0.0966,  ..., 0.0794, 0.0244, 0.0380]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634956.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8606.5244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.0117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4143.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1032.0919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0339],
        [-1.7360],
        [-2.3393],
        ...,
        [-3.6976],
        [-3.6900],
        [-3.6873]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314535.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0260],
        [1.0319],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369775.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 70.0 event: 350 loss: tensor(487.1082, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0245],
        [1.0262],
        [1.0320],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369779.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0029,  0.0253,  0.0079,  ...,  0.0050,  0.0247,  0.0182],
        [-0.0038,  0.0335,  0.0108,  ...,  0.0063,  0.0329,  0.0251],
        [-0.0049,  0.0429,  0.0140,  ...,  0.0077,  0.0422,  0.0330],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2380.5659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4147, device='cuda:0')



h[100].sum tensor(-0.5023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5672, device='cuda:0')



h[200].sum tensor(-22.5474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1155, 0.0365,  ..., 0.0224, 0.1131, 0.0845],
        [0.0000, 0.1265, 0.0403,  ..., 0.0241, 0.1240, 0.0937],
        [0.0000, 0.1503, 0.0486,  ..., 0.0278, 0.1477, 0.1137],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0055, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0055, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0055, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67968.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0194, 0.0043, 0.0000,  ..., 0.3743, 0.2586, 0.2412],
        [0.0116, 0.0025, 0.0000,  ..., 0.4336, 0.3089, 0.2815],
        [0.0051, 0.0007, 0.0000,  ..., 0.4869, 0.3530, 0.3179],
        ...,
        [0.0689, 0.0235, 0.0967,  ..., 0.0798, 0.0243, 0.0377],
        [0.0688, 0.0235, 0.0966,  ..., 0.0798, 0.0243, 0.0376],
        [0.0688, 0.0235, 0.0966,  ..., 0.0797, 0.0243, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665054.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8507.7432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.6092, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4159.8706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1099.2596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2346],
        [ 0.2358],
        [ 0.2332],
        ...,
        [-3.7025],
        [-3.6949],
        [-3.6922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316170.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0245],
        [1.0262],
        [1.0320],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369779.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0263],
        [1.0321],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369783.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2017.7256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1304, device='cuda:0')



h[100].sum tensor(-0.3874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7951, device='cuda:0')



h[200].sum tensor(-22.5905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0271, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0250, 0.0063,  ..., 0.0087, 0.0231, 0.0133],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0056, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61828.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0515, 0.0160, 0.0221,  ..., 0.1550, 0.0728, 0.0919],
        [0.0599, 0.0196, 0.0557,  ..., 0.1126, 0.0450, 0.0616],
        [0.0643, 0.0217, 0.0789,  ..., 0.0924, 0.0324, 0.0470],
        ...,
        [0.0692, 0.0240, 0.0971,  ..., 0.0799, 0.0243, 0.0373],
        [0.0692, 0.0239, 0.0970,  ..., 0.0798, 0.0243, 0.0373],
        [0.0691, 0.0239, 0.0970,  ..., 0.0798, 0.0242, 0.0372]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644116.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8672.5762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.1011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4286.4575, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1048.2567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6171],
        [-1.2294],
        [-1.8741],
        ...,
        [-3.7231],
        [-3.7151],
        [-3.7097]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323997.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0263],
        [1.0321],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369783.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0247],
        [1.0264],
        [1.0322],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369786.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [-0.0016,  0.0144,  0.0041,  ...,  0.0034,  0.0139,  0.0090],
        [-0.0011,  0.0106,  0.0028,  ...,  0.0028,  0.0102,  0.0059],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2007.8213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1818, device='cuda:0')



h[100].sum tensor(-0.3847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8026, device='cuda:0')



h[200].sum tensor(-22.5902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0362, 0.0102,  ..., 0.0104, 0.0343, 0.0228],
        [0.0000, 0.0330, 0.0091,  ..., 0.0099, 0.0311, 0.0201],
        [0.0000, 0.0726, 0.0217,  ..., 0.0160, 0.0704, 0.0484],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62948.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0158, 0.0028,  ..., 0.1807, 0.0990, 0.1079],
        [0.0477, 0.0150, 0.0024,  ..., 0.1902, 0.1038, 0.1149],
        [0.0396, 0.0120, 0.0000,  ..., 0.2409, 0.1409, 0.1502],
        ...,
        [0.0697, 0.0242, 0.0975,  ..., 0.0799, 0.0243, 0.0372],
        [0.0696, 0.0241, 0.0974,  ..., 0.0798, 0.0243, 0.0371],
        [0.0696, 0.0241, 0.0974,  ..., 0.0798, 0.0243, 0.0371]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654465.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8769.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.0502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4389.8081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1056.8988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1929],
        [ 0.2051],
        [ 0.2542],
        ...,
        [-3.7467],
        [-3.7388],
        [-3.7360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342162., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0247],
        [1.0264],
        [1.0322],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369786.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0247],
        [1.0264],
        [1.0322],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369786.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [-0.0007,  0.0072,  0.0016,  ...,  0.0023,  0.0068,  0.0030],
        [-0.0007,  0.0072,  0.0016,  ...,  0.0023,  0.0068,  0.0030],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2069.0684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2662, device='cuda:0')



h[100].sum tensor(-0.4032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9611, device='cuda:0')



h[200].sum tensor(-22.5829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0287, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0152, 0.0029,  ..., 0.0072, 0.0134, 0.0051],
        [0.0000, 0.0152, 0.0030,  ..., 0.0072, 0.0135, 0.0052],
        [0.0000, 0.0153, 0.0030,  ..., 0.0072, 0.0135, 0.0052],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62164.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0594, 0.0196, 0.0607,  ..., 0.1091, 0.0393, 0.0598],
        [0.0561, 0.0183, 0.0412,  ..., 0.1277, 0.0512, 0.0731],
        [0.0503, 0.0165, 0.0223,  ..., 0.1672, 0.0820, 0.1003],
        ...,
        [0.0697, 0.0242, 0.0975,  ..., 0.0799, 0.0243, 0.0372],
        [0.0696, 0.0241, 0.0974,  ..., 0.0798, 0.0243, 0.0371],
        [0.0696, 0.0241, 0.0974,  ..., 0.0798, 0.0243, 0.0371]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645131.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8783.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6412, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.0650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4362.5308, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1050.0199, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9442],
        [-1.0226],
        [-0.2876],
        ...,
        [-3.7467],
        [-3.7388],
        [-3.7360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332797.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0247],
        [1.0264],
        [1.0322],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369786.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0247],
        [1.0265],
        [1.0323],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369790.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1754.4406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5362, device='cuda:0')



h[100].sum tensor(-0.3102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2700, device='cuda:0')



h[200].sum tensor(-22.6185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0053, 0.0016, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0053, 0.0016, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0053, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55775.5742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0677, 0.0228, 0.0950,  ..., 0.0767, 0.0235, 0.0356],
        [0.0680, 0.0230, 0.0953,  ..., 0.0771, 0.0236, 0.0358],
        [0.0683, 0.0233, 0.0957,  ..., 0.0777, 0.0238, 0.0361],
        ...,
        [0.0700, 0.0241, 0.0978,  ..., 0.0799, 0.0244, 0.0373],
        [0.0700, 0.0241, 0.0977,  ..., 0.0798, 0.0244, 0.0373],
        [0.0699, 0.0241, 0.0977,  ..., 0.0798, 0.0244, 0.0373]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621597.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9212.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.4910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4856.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(984.4949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8835],
        [-3.8512],
        [-3.7849],
        ...,
        [-3.7680],
        [-3.7600],
        [-3.7572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-439834.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0247],
        [1.0265],
        [1.0323],
        ...,
        [1.0005],
        [0.9996],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369790.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0266],
        [1.0325],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369793.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1935.0785, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5394, device='cuda:0')



h[100].sum tensor(-0.3644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7088, device='cuda:0')



h[200].sum tensor(-22.5956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0302, 0.0081,  ..., 0.0093, 0.0284, 0.0177],
        [0.0000, 0.0032, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63272.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0158, 0.0237,  ..., 0.1672, 0.0840, 0.1014],
        [0.0604, 0.0198, 0.0565,  ..., 0.1124, 0.0452, 0.0625],
        [0.0652, 0.0218, 0.0822,  ..., 0.0904, 0.0303, 0.0465],
        ...,
        [0.0702, 0.0239, 0.0978,  ..., 0.0799, 0.0246, 0.0377],
        [0.0701, 0.0239, 0.0978,  ..., 0.0799, 0.0246, 0.0377],
        [0.0701, 0.0239, 0.0977,  ..., 0.0798, 0.0245, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672860.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9013.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.6967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4569.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1049.7096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0019],
        [-1.7755],
        [-2.4671],
        ...,
        [-3.7776],
        [-3.7694],
        [-3.7662]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-382189.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0266],
        [1.0325],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369793.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0266],
        [1.0325],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369793.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1801.7078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5221, device='cuda:0')



h[100].sum tensor(-0.3249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4140, device='cuda:0')



h[200].sum tensor(-22.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0162, 0.0039,  ..., 0.0072, 0.0146, 0.0085],
        [0.0000, 0.0032, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58691.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0592, 0.0194, 0.0477,  ..., 0.1261, 0.0590, 0.0708],
        [0.0657, 0.0218, 0.0809,  ..., 0.0901, 0.0324, 0.0455],
        [0.0680, 0.0229, 0.0930,  ..., 0.0801, 0.0256, 0.0382],
        ...,
        [0.0702, 0.0239, 0.0978,  ..., 0.0799, 0.0246, 0.0377],
        [0.0701, 0.0239, 0.0978,  ..., 0.0799, 0.0246, 0.0377],
        [0.0701, 0.0239, 0.0977,  ..., 0.0798, 0.0245, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640211.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9002.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.0447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4516.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1014.3947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5680],
        [-2.4463],
        [-3.0714],
        ...,
        [-3.7795],
        [-3.7715],
        [-3.7687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370121.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0266],
        [1.0325],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369793.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0249],
        [1.0267],
        [1.0326],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369798., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1671.2177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6414, device='cuda:0')



h[100].sum tensor(-0.2848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1392, device='cuda:0')



h[200].sum tensor(-22.6265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0017, 0.0000],
        [0.0000, 0.0329, 0.0090,  ..., 0.0097, 0.0312, 0.0200],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0053, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55556.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0663, 0.0219, 0.0858,  ..., 0.0850, 0.0291, 0.0423],
        [0.0616, 0.0203, 0.0543,  ..., 0.1132, 0.0494, 0.0623],
        [0.0495, 0.0161, 0.0225,  ..., 0.1836, 0.0998, 0.1121],
        ...,
        [0.0702, 0.0237, 0.0976,  ..., 0.0801, 0.0246, 0.0383],
        [0.0701, 0.0237, 0.0976,  ..., 0.0800, 0.0246, 0.0382],
        [0.0701, 0.0237, 0.0975,  ..., 0.0800, 0.0246, 0.0382]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623038.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9099.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.0420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4544.5698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(984.0815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3908],
        [-1.4562],
        [-0.4953],
        ...,
        [-3.7826],
        [-3.7746],
        [-3.7718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-375888.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0249],
        [1.0267],
        [1.0326],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369798., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0250],
        [1.0268],
        [1.0327],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369802., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0091,  0.0023,  ...,  0.0025,  0.0087,  0.0046],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1672.3331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5517, device='cuda:0')



h[100].sum tensor(-0.2824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1261, device='cuda:0')



h[200].sum tensor(-22.6264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0206, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0314, 0.0085,  ..., 0.0094, 0.0298, 0.0187],
        [0.0000, 0.0119, 0.0023,  ..., 0.0065, 0.0103, 0.0047],
        [0.0000, 0.0501, 0.0150,  ..., 0.0123, 0.0484, 0.0344],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0053, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55437.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0428, 0.0137, 0.0000,  ..., 0.2151, 0.1221, 0.1351],
        [0.0480, 0.0158, 0.0000,  ..., 0.1913, 0.1068, 0.1177],
        [0.0407, 0.0144, 0.0000,  ..., 0.2450, 0.1529, 0.1542],
        ...,
        [0.0702, 0.0235, 0.0974,  ..., 0.0803, 0.0246, 0.0388],
        [0.0701, 0.0235, 0.0974,  ..., 0.0802, 0.0246, 0.0387],
        [0.0701, 0.0234, 0.0973,  ..., 0.0802, 0.0246, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620164.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9072.3613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9697, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.8833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4473.1558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(983.8227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3075],
        [ 0.3020],
        [ 0.2860],
        ...,
        [-3.7726],
        [-3.7650],
        [-3.7636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-367017.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0250],
        [1.0268],
        [1.0327],
        ...,
        [1.0005],
        [0.9996],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369802., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0251],
        [1.0269],
        [1.0328],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369805.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1715.5820, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3485, device='cuda:0')



h[100].sum tensor(-0.2936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2426, device='cuda:0')



h[200].sum tensor(-22.6207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0051, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0051, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57119.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0681, 0.0225, 0.0951,  ..., 0.0772, 0.0237, 0.0368],
        [0.0680, 0.0225, 0.0935,  ..., 0.0794, 0.0249, 0.0384],
        [0.0653, 0.0216, 0.0764,  ..., 0.0957, 0.0353, 0.0501],
        ...,
        [0.0704, 0.0237, 0.0979,  ..., 0.0804, 0.0246, 0.0386],
        [0.0704, 0.0237, 0.0978,  ..., 0.0803, 0.0246, 0.0386],
        [0.0704, 0.0237, 0.0978,  ..., 0.0803, 0.0246, 0.0386]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634156.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9235.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1429, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.5951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4696.9429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(991.4495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2867],
        [-2.8314],
        [-2.1506],
        ...,
        [-3.8049],
        [-3.7968],
        [-3.7940]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-412948.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0251],
        [1.0269],
        [1.0328],
        ...,
        [1.0004],
        [0.9995],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369805.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 80.0 event: 400 loss: tensor(384.5262, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0252],
        [1.0270],
        [1.0329],
        ...,
        [1.0004],
        [0.9995],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369809.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [-0.0020,  0.0183,  0.0055,  ...,  0.0039,  0.0179,  0.0123],
        [-0.0010,  0.0096,  0.0025,  ...,  0.0026,  0.0092,  0.0050],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1960.5247, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6863, device='cuda:0')



h[100].sum tensor(-0.3576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7302, device='cuda:0')



h[200].sum tensor(-22.5929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0285, 0.0075,  ..., 0.0090, 0.0269, 0.0163],
        [0.0000, 0.0322, 0.0081,  ..., 0.0096, 0.0306, 0.0169],
        [0.0000, 0.1077, 0.0338,  ..., 0.0212, 0.1058, 0.0780],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58975.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0504, 0.0170, 0.0265,  ..., 0.1704, 0.0876, 0.1042],
        [0.0394, 0.0141, 0.0090,  ..., 0.2385, 0.1383, 0.1519],
        [0.0201, 0.0104, 0.0000,  ..., 0.3699, 0.2466, 0.2419],
        ...,
        [0.0703, 0.0238, 0.0978,  ..., 0.0806, 0.0245, 0.0388],
        [0.0702, 0.0238, 0.0977,  ..., 0.0805, 0.0245, 0.0388],
        [0.0702, 0.0238, 0.0977,  ..., 0.0805, 0.0245, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630722.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9044.7080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.5979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4454.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1014.5380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1003e-01],
        [ 2.7334e-03],
        [ 2.3888e-01],
        ...,
        [-3.8066e+00],
        [-3.7986e+00],
        [-3.7958e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-364149.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0252],
        [1.0270],
        [1.0329],
        ...,
        [1.0004],
        [0.9995],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369809.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0253],
        [1.0271],
        [1.0331],
        ...,
        [1.0004],
        [0.9995],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369813.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2188.5020, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0830, device='cuda:0')



h[100].sum tensor(-0.4157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2265, device='cuda:0')



h[200].sum tensor(-22.5670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0052, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67178.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.0227, 0.0950,  ..., 0.0775, 0.0235, 0.0371],
        [0.0681, 0.0229, 0.0954,  ..., 0.0779, 0.0236, 0.0373],
        [0.0685, 0.0231, 0.0957,  ..., 0.0785, 0.0238, 0.0377],
        ...,
        [0.0702, 0.0240, 0.0978,  ..., 0.0807, 0.0244, 0.0389],
        [0.0701, 0.0240, 0.0977,  ..., 0.0806, 0.0244, 0.0389],
        [0.0701, 0.0240, 0.0977,  ..., 0.0806, 0.0244, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678204.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8840.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.3615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4263.4131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1087.4857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0904],
        [-3.2766],
        [-3.4364],
        ...,
        [-3.8115],
        [-3.8036],
        [-3.8008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340912.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0253],
        [1.0271],
        [1.0331],
        ...,
        [1.0004],
        [0.9995],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369813.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0253],
        [1.0271],
        [1.0332],
        ...,
        [1.0004],
        [0.9995],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369817.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2464.5984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8089, device='cuda:0')



h[100].sum tensor(-0.4838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7709, device='cuda:0')



h[200].sum tensor(-22.5364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0016,  ..., 0.0063, 0.0082, 0.0030],
        [0.0000, 0.0162, 0.0032,  ..., 0.0073, 0.0147, 0.0060],
        [0.0000, 0.0098, 0.0016,  ..., 0.0063, 0.0083, 0.0030],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0054, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0054, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0054, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70104.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0573, 0.0194, 0.0491,  ..., 0.1210, 0.0460, 0.0702],
        [0.0561, 0.0190, 0.0424,  ..., 0.1282, 0.0495, 0.0756],
        [0.0571, 0.0195, 0.0444,  ..., 0.1268, 0.0495, 0.0744],
        ...,
        [0.0699, 0.0243, 0.0977,  ..., 0.0809, 0.0243, 0.0390],
        [0.0698, 0.0242, 0.0976,  ..., 0.0808, 0.0243, 0.0390],
        [0.0698, 0.0242, 0.0976,  ..., 0.0808, 0.0243, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689618., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8819.0020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.3944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4397.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1109.7815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4278],
        [-0.5217],
        [-0.4399],
        ...,
        [-3.8101],
        [-3.8023],
        [-3.7994]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370130.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0253],
        [1.0271],
        [1.0332],
        ...,
        [1.0004],
        [0.9995],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369817.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0254],
        [1.0272],
        [1.0333],
        ...,
        [1.0003],
        [0.9994],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369821.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1891.0247, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3678, device='cuda:0')



h[100].sum tensor(-0.3194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5376, device='cuda:0')



h[200].sum tensor(-22.6051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59500.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0552, 0.0200, 0.0289,  ..., 0.1462, 0.0715, 0.0855],
        [0.0601, 0.0212, 0.0487,  ..., 0.1198, 0.0519, 0.0669],
        [0.0628, 0.0223, 0.0621,  ..., 0.1084, 0.0446, 0.0586],
        ...,
        [0.0697, 0.0245, 0.0978,  ..., 0.0810, 0.0242, 0.0390],
        [0.0697, 0.0245, 0.0977,  ..., 0.0810, 0.0242, 0.0390],
        [0.0696, 0.0244, 0.0977,  ..., 0.0809, 0.0242, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641673.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8893.9512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.8862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4430.4229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1020.2335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1108],
        [ 0.0775],
        [ 0.0619],
        ...,
        [-3.7930],
        [-3.7843],
        [-3.7733]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-366380.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0254],
        [1.0272],
        [1.0333],
        ...,
        [1.0003],
        [0.9994],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369821.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0255],
        [1.0273],
        [1.0334],
        ...,
        [1.0003],
        [0.9994],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369825.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0055,  0.0502,  0.0166,  ...,  0.0089,  0.0496,  0.0391],
        [-0.0040,  0.0363,  0.0117,  ...,  0.0068,  0.0358,  0.0274],
        [-0.0021,  0.0195,  0.0059,  ...,  0.0042,  0.0191,  0.0133],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1879.1904, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8809, device='cuda:0')



h[100].sum tensor(-0.3110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4664, device='cuda:0')



h[200].sum tensor(-22.6074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1579, 0.0514,  ..., 0.0291, 0.1557, 0.1201],
        [0.0000, 0.1459, 0.0472,  ..., 0.0273, 0.1437, 0.1100],
        [0.0000, 0.1217, 0.0387,  ..., 0.0236, 0.1196, 0.0896],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0056, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57866.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0141, 0.0000,  ..., 0.4916, 0.3559, 0.3214],
        [0.0120, 0.0147, 0.0000,  ..., 0.4427, 0.3138, 0.2882],
        [0.0201, 0.0151, 0.0000,  ..., 0.3787, 0.2565, 0.2453],
        ...,
        [0.0696, 0.0247, 0.0979,  ..., 0.0812, 0.0241, 0.0390],
        [0.0696, 0.0246, 0.0978,  ..., 0.0811, 0.0241, 0.0390],
        [0.0695, 0.0246, 0.0978,  ..., 0.0811, 0.0241, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631980.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8976.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.6820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4550.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1003.7331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1796],
        [ 0.1993],
        [ 0.2222],
        ...,
        [-3.8180],
        [-3.8102],
        [-3.8073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-386263.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0255],
        [1.0273],
        [1.0334],
        ...,
        [1.0003],
        [0.9994],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369825.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0256],
        [1.0274],
        [1.0336],
        ...,
        [1.0003],
        [0.9994],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369829.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0563,  0.0187,  ...,  0.0099,  0.0557,  0.0442],
        [-0.0019,  0.0177,  0.0053,  ...,  0.0039,  0.0173,  0.0118],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1878.2279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9123, device='cuda:0')



h[100].sum tensor(-0.3054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4710, device='cuda:0')



h[200].sum tensor(-22.6086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0985, 0.0307,  ..., 0.0201, 0.0965, 0.0702],
        [0.0000, 0.1167, 0.0376,  ..., 0.0229, 0.1146, 0.0879],
        [0.0000, 0.0487, 0.0146,  ..., 0.0125, 0.0470, 0.0333],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0056, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58716.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0187, 0.0147, 0.0000,  ..., 0.3836, 0.2599, 0.2482],
        [0.0193, 0.0152, 0.0000,  ..., 0.3846, 0.2620, 0.2487],
        [0.0310, 0.0167, 0.0000,  ..., 0.3064, 0.1972, 0.1955],
        ...,
        [0.0695, 0.0247, 0.0979,  ..., 0.0814, 0.0240, 0.0391],
        [0.0694, 0.0247, 0.0978,  ..., 0.0813, 0.0240, 0.0391],
        [0.0694, 0.0247, 0.0978,  ..., 0.0812, 0.0240, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637058.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8851.0322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.8699, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4449.4556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1015.6121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1907],
        [ 0.2128],
        [ 0.2269],
        ...,
        [-3.8194],
        [-3.8116],
        [-3.8088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-365624.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0256],
        [1.0274],
        [1.0336],
        ...,
        [1.0003],
        [0.9994],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369829.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0256],
        [1.0275],
        [1.0337],
        ...,
        [1.0003],
        [0.9993],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369833.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2497.0273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2124, device='cuda:0')



h[100].sum tensor(-0.4642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8299, device='cuda:0')



h[200].sum tensor(-22.5372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0371, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0056, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70595.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0668, 0.0233, 0.0948,  ..., 0.0783, 0.0231, 0.0376],
        [0.0671, 0.0234, 0.0952,  ..., 0.0787, 0.0232, 0.0378],
        [0.0675, 0.0237, 0.0956,  ..., 0.0793, 0.0233, 0.0382],
        ...,
        [0.0691, 0.0246, 0.0976,  ..., 0.0815, 0.0240, 0.0394],
        [0.0691, 0.0246, 0.0975,  ..., 0.0815, 0.0240, 0.0394],
        [0.0691, 0.0245, 0.0975,  ..., 0.0814, 0.0240, 0.0393]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(693323., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8519.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.1001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4185.8857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1121.0907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6794],
        [-3.7683],
        [-3.8144],
        ...,
        [-3.8011],
        [-3.7930],
        [-3.7897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324980.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0256],
        [1.0275],
        [1.0337],
        ...,
        [1.0003],
        [0.9993],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369833.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0276],
        [1.0338],
        ...,
        [1.0002],
        [0.9993],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369837., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0209,  0.0064,  ...,  0.0044,  0.0205,  0.0145],
        [-0.0046,  0.0427,  0.0140,  ...,  0.0078,  0.0421,  0.0328],
        [-0.0037,  0.0343,  0.0110,  ...,  0.0065,  0.0337,  0.0257],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0005, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0005, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0014,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2335.0486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1389, device='cuda:0')



h[100].sum tensor(-0.4152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3808, device='cuda:0')



h[200].sum tensor(-22.5572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1084, 0.0340,  ..., 0.0217, 0.1064, 0.0784],
        [0.0000, 0.1154, 0.0365,  ..., 0.0228, 0.1134, 0.0843],
        [0.0000, 0.1424, 0.0459,  ..., 0.0269, 0.1402, 0.1069],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67918.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0257, 0.0154, 0.0000,  ..., 0.3305, 0.2156, 0.2130],
        [0.0170, 0.0141, 0.0000,  ..., 0.3882, 0.2609, 0.2529],
        [0.0112, 0.0136, 0.0000,  ..., 0.4302, 0.2949, 0.2817],
        ...,
        [0.0689, 0.0244, 0.0973,  ..., 0.0818, 0.0239, 0.0398],
        [0.0689, 0.0243, 0.0972,  ..., 0.0817, 0.0239, 0.0397],
        [0.0689, 0.0243, 0.0972,  ..., 0.0817, 0.0239, 0.0397]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674066.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8459.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.3275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4064.4517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1100.9835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0627],
        [ 0.1680],
        [ 0.1444],
        ...,
        [-3.8087],
        [-3.8012],
        [-3.7984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300936.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0276],
        [1.0338],
        ...,
        [1.0002],
        [0.9993],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369837., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0339],
        ...,
        [1.0002],
        [0.9993],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369839.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0087,  0.0021,  ...,  0.0025,  0.0083,  0.0042],
        [-0.0029,  0.0268,  0.0084,  ...,  0.0053,  0.0263,  0.0195],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0005, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0005, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0005, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0013,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2762.0264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7464, device='cuda:0')



h[100].sum tensor(-0.5265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.3462, device='cuda:0')



h[200].sum tensor(-22.5054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0886, 0.0270,  ..., 0.0185, 0.0867, 0.0618],
        [0.0000, 0.0385, 0.0102,  ..., 0.0109, 0.0368, 0.0221],
        [0.0000, 0.0611, 0.0174,  ..., 0.0143, 0.0593, 0.0386],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0056, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0056, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0056, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75086.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0249, 0.0118, 0.0000,  ..., 0.3255, 0.2048, 0.2125],
        [0.0334, 0.0133, 0.0006,  ..., 0.2719, 0.1612, 0.1755],
        [0.0376, 0.0146, 0.0027,  ..., 0.2496, 0.1442, 0.1599],
        ...,
        [0.0694, 0.0237, 0.0975,  ..., 0.0819, 0.0240, 0.0401],
        [0.0694, 0.0237, 0.0974,  ..., 0.0819, 0.0240, 0.0401],
        [0.0693, 0.0237, 0.0974,  ..., 0.0818, 0.0239, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703175.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8438.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.1985, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3967.5288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1163.1172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2164],
        [ 0.2096],
        [ 0.1897],
        ...,
        [-3.8233],
        [-3.8157],
        [-3.8129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306359.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0339],
        ...,
        [1.0002],
        [0.9993],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369839.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0339],
        ...,
        [1.0002],
        [0.9992],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369841.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0005, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0005, -0.0024],
        [-0.0009,  0.0092,  0.0022,  ...,  0.0026,  0.0088,  0.0047],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0005, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0005, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0005, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2627.1304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8523, device='cuda:0')



h[100].sum tensor(-0.4915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0695, device='cuda:0')



h[200].sum tensor(-22.5190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0018, 0.0000],
        [0.0000, 0.0119, 0.0023,  ..., 0.0067, 0.0104, 0.0048],
        [0.0000, 0.0324, 0.0087,  ..., 0.0099, 0.0308, 0.0195],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74347.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0636, 0.0206, 0.0730,  ..., 0.0989, 0.0357, 0.0532],
        [0.0567, 0.0182, 0.0388,  ..., 0.1358, 0.0586, 0.0803],
        [0.0480, 0.0155, 0.0134,  ..., 0.1854, 0.0920, 0.1160],
        ...,
        [0.0700, 0.0233, 0.0979,  ..., 0.0821, 0.0240, 0.0402],
        [0.0700, 0.0233, 0.0978,  ..., 0.0821, 0.0240, 0.0402],
        [0.0699, 0.0232, 0.0978,  ..., 0.0820, 0.0240, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(707058.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8614.2021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.8700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4055.7275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1155.6499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8679],
        [-1.0123],
        [-0.2930],
        ...,
        [-3.8464],
        [-3.8388],
        [-3.8359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323133.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0339],
        ...,
        [1.0002],
        [0.9992],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369841.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 90.0 event: 450 loss: tensor(506.4784, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0340],
        ...,
        [1.0001],
        [0.9992],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369843.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1983.8186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6321, device='cuda:0')



h[100].sum tensor(-0.3264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7223, device='cuda:0')



h[200].sum tensor(-22.5926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0053, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0055, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63805.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0660, 0.0209, 0.0865,  ..., 0.0876, 0.0273, 0.0451],
        [0.0669, 0.0213, 0.0907,  ..., 0.0846, 0.0251, 0.0429],
        [0.0646, 0.0206, 0.0760,  ..., 0.0986, 0.0341, 0.0532],
        ...,
        [0.0705, 0.0231, 0.0984,  ..., 0.0823, 0.0239, 0.0402],
        [0.0704, 0.0230, 0.0983,  ..., 0.0822, 0.0239, 0.0401],
        [0.0704, 0.0230, 0.0983,  ..., 0.0822, 0.0239, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669527.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8848.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.5847, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4159.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1066.3567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4542],
        [-1.6020],
        [-1.3271],
        ...,
        [-3.8662],
        [-3.8586],
        [-3.8557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333234.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0340],
        ...,
        [1.0001],
        [0.9992],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369843.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0341],
        ...,
        [1.0000],
        [0.9991],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369846.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1808.4790, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5610, device='cuda:0')



h[100].sum tensor(-0.2817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4197, device='cuda:0')



h[200].sum tensor(-22.6118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0053, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0054, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0055, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57420.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0647, 0.0206, 0.0696,  ..., 0.1023, 0.0403, 0.0543],
        [0.0660, 0.0209, 0.0807,  ..., 0.0939, 0.0323, 0.0488],
        [0.0623, 0.0195, 0.0590,  ..., 0.1147, 0.0448, 0.0643],
        ...,
        [0.0709, 0.0231, 0.0990,  ..., 0.0824, 0.0238, 0.0399],
        [0.0708, 0.0231, 0.0989,  ..., 0.0824, 0.0238, 0.0398],
        [0.0708, 0.0231, 0.0989,  ..., 0.0823, 0.0238, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627844., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9226.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.0019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4518.0283, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1007.6906, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4199],
        [-1.2907],
        [-0.7219],
        ...,
        [-3.8768],
        [-3.8649],
        [-3.8352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-392641.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0341],
        ...,
        [1.0000],
        [0.9991],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369846.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0342],
        ...,
        [1.0000],
        [0.9991],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369849.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0099,  0.0025,  ...,  0.0027,  0.0095,  0.0053],
        [-0.0008,  0.0081,  0.0018,  ...,  0.0024,  0.0076,  0.0037],
        [-0.0018,  0.0172,  0.0050,  ...,  0.0038,  0.0167,  0.0114],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0013,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1646.1287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.8976, device='cuda:0')



h[100].sum tensor(-0.2355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0306, device='cuda:0')



h[200].sum tensor(-22.6322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0310, 0.0082,  ..., 0.0097, 0.0292, 0.0185],
        [0.0000, 0.0643, 0.0184,  ..., 0.0148, 0.0623, 0.0415],
        [0.0000, 0.0313, 0.0076,  ..., 0.0098, 0.0295, 0.0162],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0056, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0056, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0056, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54553.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0488, 0.0143, 0.0159,  ..., 0.1798, 0.0880, 0.1117],
        [0.0417, 0.0116, 0.0000,  ..., 0.2226, 0.1174, 0.1422],
        [0.0471, 0.0136, 0.0090,  ..., 0.1915, 0.0938, 0.1203],
        ...,
        [0.0706, 0.0232, 0.0989,  ..., 0.0825, 0.0236, 0.0399],
        [0.0705, 0.0232, 0.0988,  ..., 0.0825, 0.0236, 0.0398],
        [0.0705, 0.0231, 0.0988,  ..., 0.0824, 0.0236, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616443., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9159.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.6818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4464.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(987.1295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2389],
        [-0.0064],
        [-0.2456],
        ...,
        [-3.8493],
        [-3.8088],
        [-3.6869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-374428.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0277],
        [1.0342],
        ...,
        [1.0000],
        [0.9991],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369849.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0342],
        ...,
        [0.9999],
        [0.9990],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369853.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [-0.0011,  0.0108,  0.0028,  ...,  0.0029,  0.0103,  0.0061],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2205.6213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2335, device='cuda:0')



h[100].sum tensor(-0.3694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2485, device='cuda:0')



h[200].sum tensor(-22.5680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0334, 0.0098,  ..., 0.0101, 0.0315, 0.0229],
        [0.0000, 0.0136, 0.0029,  ..., 0.0071, 0.0118, 0.0062],
        [0.0000, 0.0212, 0.0048,  ..., 0.0083, 0.0194, 0.0102],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0057, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65121.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0512, 0.0172, 0.0179,  ..., 0.1864, 0.1054, 0.1128],
        [0.0562, 0.0181, 0.0209,  ..., 0.1495, 0.0722, 0.0880],
        [0.0527, 0.0165, 0.0230,  ..., 0.1672, 0.0809, 0.1016],
        ...,
        [0.0704, 0.0234, 0.0990,  ..., 0.0825, 0.0234, 0.0397],
        [0.0703, 0.0234, 0.0990,  ..., 0.0824, 0.0234, 0.0396],
        [0.0703, 0.0234, 0.0989,  ..., 0.0824, 0.0234, 0.0396]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664318.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8944.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.2015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4376.8682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1078.9567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2297],
        [-0.3944],
        [-0.2368],
        ...,
        [-3.8774],
        [-3.8698],
        [-3.8671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-382564.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0342],
        ...,
        [0.9999],
        [0.9990],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369853.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0343],
        ...,
        [0.9999],
        [0.9989],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369856.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [-0.0011,  0.0113,  0.0029,  ...,  0.0030,  0.0108,  0.0064],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2344.6272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6619, device='cuda:0')



h[100].sum tensor(-0.3946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4572, device='cuda:0')



h[200].sum tensor(-22.5544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0057, 0.0016, 0.0000],
        [0.0000, 0.0141, 0.0030,  ..., 0.0073, 0.0122, 0.0066],
        [0.0000, 0.0122, 0.0023,  ..., 0.0071, 0.0103, 0.0049],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0058, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0058, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0058, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69263.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0649, 0.0213, 0.0838,  ..., 0.0904, 0.0283, 0.0467],
        [0.0611, 0.0197, 0.0619,  ..., 0.1113, 0.0408, 0.0621],
        [0.0580, 0.0186, 0.0442,  ..., 0.1284, 0.0507, 0.0749],
        ...,
        [0.0698, 0.0237, 0.0987,  ..., 0.0824, 0.0232, 0.0397],
        [0.0698, 0.0236, 0.0986,  ..., 0.0824, 0.0232, 0.0397],
        [0.0698, 0.0236, 0.0986,  ..., 0.0824, 0.0231, 0.0396]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686860.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8654.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.1369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4149.8721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1120.1791, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4297],
        [-1.7548],
        [-1.0391],
        ...,
        [-3.8604],
        [-3.8531],
        [-3.8504]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349367.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0343],
        ...,
        [0.9999],
        [0.9989],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369856.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0343],
        ...,
        [0.9998],
        [0.9989],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369861., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2709.2334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1293, device='cuda:0')



h[100].sum tensor(-0.4707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1099, device='cuda:0')



h[200].sum tensor(-22.5160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0061, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74257.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0667, 0.0226, 0.0950,  ..., 0.0791, 0.0221, 0.0380],
        [0.0670, 0.0228, 0.0954,  ..., 0.0795, 0.0222, 0.0383],
        [0.0674, 0.0230, 0.0958,  ..., 0.0801, 0.0223, 0.0386],
        ...,
        [0.0690, 0.0239, 0.0978,  ..., 0.0824, 0.0229, 0.0399],
        [0.0690, 0.0239, 0.0977,  ..., 0.0824, 0.0229, 0.0399],
        [0.0689, 0.0239, 0.0977,  ..., 0.0823, 0.0229, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699239.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8412.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8381, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.0847, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3946.5874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1166.9523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8352],
        [-3.8014],
        [-3.7142],
        ...,
        [-3.8091],
        [-3.8075],
        [-3.8065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303997.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0343],
        ...,
        [0.9998],
        [0.9989],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369861., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0344],
        ...,
        [0.9998],
        [0.9989],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369864.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0095,  0.0023,  ...,  0.0028,  0.0089,  0.0048],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2035.1506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3920, device='cuda:0')



h[100].sum tensor(-0.2999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6872, device='cuda:0')



h[200].sum tensor(-22.5967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0329, 0.0088,  ..., 0.0105, 0.0309, 0.0198],
        [0.0000, 0.0124, 0.0024,  ..., 0.0074, 0.0104, 0.0049],
        [0.0000, 0.0035, 0.0000,  ..., 0.0061, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61279.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0459, 0.0156, 0.0128,  ..., 0.1855, 0.0926, 0.1151],
        [0.0555, 0.0189, 0.0390,  ..., 0.1356, 0.0583, 0.0793],
        [0.0626, 0.0217, 0.0709,  ..., 0.1019, 0.0367, 0.0547],
        ...,
        [0.0684, 0.0240, 0.0973,  ..., 0.0823, 0.0228, 0.0401],
        [0.0684, 0.0240, 0.0972,  ..., 0.0823, 0.0228, 0.0401],
        [0.0684, 0.0239, 0.0972,  ..., 0.0822, 0.0228, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639491.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8529.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.1200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4120.1299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1053.4518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2901],
        [-1.0166],
        [-1.8054],
        ...,
        [-3.8084],
        [-3.8016],
        [-3.7991]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319620.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0344],
        ...,
        [0.9998],
        [0.9989],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369864.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0344],
        ...,
        [0.9998],
        [0.9988],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369867.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0054,  0.0509,  0.0167,  ...,  0.0091,  0.0501,  0.0395],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [-0.0032,  0.0305,  0.0096,  ...,  0.0060,  0.0299,  0.0225],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2222.7839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6870, device='cuda:0')



h[100].sum tensor(-0.3387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0226, device='cuda:0')



h[200].sum tensor(-22.5763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0935, 0.0292,  ..., 0.0198, 0.0911, 0.0681],
        [0.0000, 0.1350, 0.0428,  ..., 0.0262, 0.1323, 0.1003],
        [0.0000, 0.0285, 0.0080,  ..., 0.0099, 0.0264, 0.0184],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65185.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0157, 0.0118, 0.0000,  ..., 0.4058, 0.2839, 0.2655],
        [0.0247, 0.0136, 0.0000,  ..., 0.3449, 0.2339, 0.2231],
        [0.0467, 0.0184, 0.0126,  ..., 0.2044, 0.1203, 0.1257],
        ...,
        [0.0681, 0.0238, 0.0969,  ..., 0.0822, 0.0227, 0.0404],
        [0.0680, 0.0238, 0.0969,  ..., 0.0822, 0.0227, 0.0403],
        [0.0680, 0.0238, 0.0968,  ..., 0.0821, 0.0227, 0.0403]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663680.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8348.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.4349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4072.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1086.8151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1974],
        [ 0.0183],
        [-0.5273],
        ...,
        [-3.8003],
        [-3.7936],
        [-3.7911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339056.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0344],
        ...,
        [0.9998],
        [0.9988],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369867.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9988],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369871.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [-0.0015,  0.0149,  0.0042,  ...,  0.0036,  0.0144,  0.0094],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2480.2993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0418, device='cuda:0')



h[100].sum tensor(-0.3934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5127, device='cuda:0')



h[200].sum tensor(-22.5477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0180, 0.0043,  ..., 0.0083, 0.0160, 0.0096],
        [0.0000, 0.0539, 0.0160,  ..., 0.0138, 0.0516, 0.0372],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0063, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0063, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72057.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0582, 0.0201, 0.0491,  ..., 0.1184, 0.0496, 0.0671],
        [0.0461, 0.0172, 0.0250,  ..., 0.1953, 0.1094, 0.1208],
        [0.0265, 0.0135, 0.0034,  ..., 0.3254, 0.2151, 0.2108],
        ...,
        [0.0677, 0.0236, 0.0965,  ..., 0.0821, 0.0226, 0.0407],
        [0.0677, 0.0236, 0.0965,  ..., 0.0821, 0.0226, 0.0407],
        [0.0676, 0.0236, 0.0964,  ..., 0.0820, 0.0226, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(694638.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8138.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6364, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.5691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3851.5525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1146.0048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4975e-01],
        [ 2.0449e-03],
        [ 9.3864e-02],
        ...,
        [-3.7925e+00],
        [-3.7859e+00],
        [-3.7835e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299692.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9988],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369871.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369874.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1728.9904, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.5482, device='cuda:0')



h[100].sum tensor(-0.2143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9795, device='cuda:0')



h[200].sum tensor(-22.6352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0018, 0.0000],
        [0.0000, 0.0350, 0.0102,  ..., 0.0109, 0.0329, 0.0238],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55454.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0644, 0.0219, 0.0870,  ..., 0.0843, 0.0262, 0.0430],
        [0.0616, 0.0213, 0.0657,  ..., 0.1034, 0.0409, 0.0564],
        [0.0525, 0.0195, 0.0284,  ..., 0.1631, 0.0882, 0.0981],
        ...,
        [0.0675, 0.0233, 0.0961,  ..., 0.0820, 0.0226, 0.0411],
        [0.0675, 0.0233, 0.0961,  ..., 0.0820, 0.0226, 0.0411],
        [0.0674, 0.0233, 0.0961,  ..., 0.0820, 0.0226, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615909.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8440.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.5768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4072.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(998.4997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7905],
        [-2.0222],
        [-1.0698],
        ...,
        [-3.5112],
        [-3.7085],
        [-3.7601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329365.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369874.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 100.0 event: 500 loss: tensor(472.1522, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369877.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [-0.0009,  0.0094,  0.0022,  ...,  0.0028,  0.0089,  0.0047],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1796.6371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4099, device='cuda:0')



h[100].sum tensor(-0.2277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1054, device='cuda:0')



h[200].sum tensor(-22.6274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0018, 0.0000],
        [0.0000, 0.0125, 0.0023,  ..., 0.0075, 0.0104, 0.0049],
        [0.0000, 0.0109, 0.0018,  ..., 0.0072, 0.0089, 0.0035],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56532.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0636, 0.0213, 0.0854,  ..., 0.0861, 0.0259, 0.0450],
        [0.0602, 0.0201, 0.0682,  ..., 0.1028, 0.0353, 0.0575],
        [0.0584, 0.0195, 0.0595,  ..., 0.1119, 0.0399, 0.0645],
        ...,
        [0.0675, 0.0232, 0.0960,  ..., 0.0820, 0.0225, 0.0413],
        [0.0675, 0.0232, 0.0960,  ..., 0.0819, 0.0225, 0.0413],
        [0.0672, 0.0231, 0.0951,  ..., 0.0827, 0.0229, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619528.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8373.1074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1198, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.2585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3965.0181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1009.2973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6942],
        [-2.2459],
        [-1.6719],
        ...,
        [-3.7469],
        [-3.6671],
        [-3.5632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318809.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369877.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369877.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0101,  0.0025,  ...,  0.0029,  0.0096,  0.0053],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [-0.0010,  0.0101,  0.0025,  ...,  0.0029,  0.0096,  0.0053],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2657.6665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1754, device='cuda:0')



h[100].sum tensor(-0.4247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9706, device='cuda:0')



h[200].sum tensor(-22.5282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0328, 0.0086,  ..., 0.0105, 0.0306, 0.0194],
        [0.0000, 0.0520, 0.0138,  ..., 0.0135, 0.0497, 0.0306],
        [0.0000, 0.0185, 0.0044,  ..., 0.0084, 0.0164, 0.0099],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74866.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.1816e-02, 1.3772e-02, 1.8661e-03,  ..., 1.9491e-01, 9.7982e-02,
         1.2466e-01],
        [4.1407e-02, 1.3437e-02, 9.6839e-05,  ..., 1.9624e-01, 9.6555e-02,
         1.2609e-01],
        [5.0842e-02, 1.6938e-02, 2.5514e-02,  ..., 1.5076e-01, 6.6583e-02,
         9.2884e-02],
        ...,
        [6.7504e-02, 2.3188e-02, 9.6012e-02,  ..., 8.1967e-02, 2.2543e-02,
         4.1317e-02],
        [6.7478e-02, 2.3177e-02, 9.5978e-02,  ..., 8.1933e-02, 2.2533e-02,
         4.1298e-02],
        [6.7453e-02, 2.3161e-02, 9.5949e-02,  ..., 8.1894e-02, 2.2524e-02,
         4.1275e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701055.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7823.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.7328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3413.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1176.3354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2547],
        [ 0.0355],
        [-0.4726],
        ...,
        [-3.7921],
        [-3.7855],
        [-3.7829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238737.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369877.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369877.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2319.6982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0133, device='cuda:0')



h[100].sum tensor(-0.3474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2163, device='cuda:0')



h[200].sum tensor(-22.5671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0061, 0.0018, 0.0000],
        [0.0000, 0.0118, 0.0021,  ..., 0.0074, 0.0097, 0.0043],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66666.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0612, 0.0203, 0.0792,  ..., 0.0923, 0.0271, 0.0507],
        [0.0612, 0.0205, 0.0749,  ..., 0.0965, 0.0312, 0.0532],
        [0.0580, 0.0197, 0.0527,  ..., 0.1171, 0.0457, 0.0678],
        ...,
        [0.0671, 0.0230, 0.0936,  ..., 0.0841, 0.0239, 0.0429],
        [0.0675, 0.0232, 0.0960,  ..., 0.0819, 0.0225, 0.0413],
        [0.0675, 0.0232, 0.0959,  ..., 0.0819, 0.0225, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663713.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8160.6289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1052, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(144.0447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3824.3159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1099.0436, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7717],
        [-1.6655],
        [-1.3538],
        ...,
        [-3.5910],
        [-3.7183],
        [-3.7698]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302955.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0345],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369877.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0346],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369879.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [-0.0014,  0.0139,  0.0038,  ...,  0.0035,  0.0133,  0.0085],
        [-0.0012,  0.0120,  0.0032,  ...,  0.0032,  0.0115,  0.0069],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1671.1136, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.6932, device='cuda:0')



h[100].sum tensor(-0.1987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8546, device='cuda:0')



h[200].sum tensor(-22.6410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0169, 0.0039,  ..., 0.0081, 0.0149, 0.0087],
        [0.0000, 0.0353, 0.0095,  ..., 0.0109, 0.0331, 0.0216],
        [0.0000, 0.0821, 0.0243,  ..., 0.0181, 0.0796, 0.0559],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0062, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0062, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0062, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54534.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0498, 0.0171, 0.0047,  ..., 0.1661, 0.0843, 0.1017],
        [0.0428, 0.0151, 0.0032,  ..., 0.2079, 0.1146, 0.1316],
        [0.0307, 0.0120, 0.0000,  ..., 0.2810, 0.1694, 0.1834],
        ...,
        [0.0678, 0.0233, 0.0964,  ..., 0.0820, 0.0224, 0.0412],
        [0.0678, 0.0233, 0.0963,  ..., 0.0819, 0.0224, 0.0412],
        [0.0677, 0.0233, 0.0963,  ..., 0.0819, 0.0224, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613235.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8504.8818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.9192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4110.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(991.3134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1769],
        [ 0.2377],
        [ 0.2538],
        ...,
        [-3.8182],
        [-3.8092],
        [-3.7902]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337212.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0278],
        [1.0346],
        ...,
        [0.9997],
        [0.9987],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369879.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0279],
        [1.0346],
        ...,
        [0.9996],
        [0.9987],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369882.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0175,  0.0051,  ...,  0.0040,  0.0169,  0.0115],
        [-0.0030,  0.0291,  0.0091,  ...,  0.0058,  0.0284,  0.0212],
        [-0.0031,  0.0302,  0.0095,  ...,  0.0060,  0.0295,  0.0222],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1923.2322, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6005, device='cuda:0')



h[100].sum tensor(-0.2549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4255, device='cuda:0')



h[200].sum tensor(-22.6112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0931, 0.0282,  ..., 0.0198, 0.0905, 0.0653],
        [0.0000, 0.1233, 0.0387,  ..., 0.0244, 0.1206, 0.0906],
        [0.0000, 0.1098, 0.0340,  ..., 0.0224, 0.1071, 0.0792],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60269.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0144, 0.0081, 0.0000,  ..., 0.3836, 0.2503, 0.2543],
        [0.0130, 0.0087, 0.0000,  ..., 0.4026, 0.2697, 0.2666],
        [0.0213, 0.0110, 0.0000,  ..., 0.3537, 0.2317, 0.2325],
        ...,
        [0.0681, 0.0236, 0.0969,  ..., 0.0820, 0.0222, 0.0411],
        [0.0681, 0.0236, 0.0968,  ..., 0.0819, 0.0222, 0.0410],
        [0.0681, 0.0236, 0.0968,  ..., 0.0819, 0.0222, 0.0410]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645894., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8446.4795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4763, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.2284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4066.9348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1042.4464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2486],
        [ 0.2409],
        [ 0.2231],
        ...,
        [-3.8444],
        [-3.8377],
        [-3.8353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338521.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0279],
        [1.0346],
        ...,
        [0.9996],
        [0.9987],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369882.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0279],
        [1.0347],
        ...,
        [0.9996],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369885.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0223,  0.0067,  ...,  0.0048,  0.0217,  0.0156],
        [-0.0013,  0.0135,  0.0037,  ...,  0.0034,  0.0129,  0.0082],
        [-0.0047,  0.0454,  0.0148,  ...,  0.0083,  0.0446,  0.0350],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1979.0342, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4404, device='cuda:0')



h[100].sum tensor(-0.2666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5482, device='cuda:0')



h[200].sum tensor(-22.6039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0681, 0.0196,  ..., 0.0160, 0.0657, 0.0445],
        [0.0000, 0.1177, 0.0368,  ..., 0.0236, 0.1150, 0.0860],
        [0.0000, 0.0540, 0.0153,  ..., 0.0138, 0.0516, 0.0350],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60295.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0393, 0.0148, 0.0000,  ..., 0.2389, 0.1386, 0.1521],
        [0.0310, 0.0135, 0.0000,  ..., 0.2999, 0.1896, 0.1937],
        [0.0373, 0.0148, 0.0000,  ..., 0.2594, 0.1559, 0.1658],
        ...,
        [0.0685, 0.0240, 0.0975,  ..., 0.0820, 0.0220, 0.0408],
        [0.0685, 0.0240, 0.0975,  ..., 0.0820, 0.0220, 0.0408],
        [0.0685, 0.0240, 0.0974,  ..., 0.0819, 0.0220, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639553., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8491.4482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4693, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.4798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4063.2554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1046.1322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6149],
        [-0.1141],
        [ 0.1497],
        ...,
        [-3.8737],
        [-3.8669],
        [-3.8644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330107.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0279],
        [1.0347],
        ...,
        [0.9996],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369885.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9996],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369888.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1864.2769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9117, device='cuda:0')



h[100].sum tensor(-0.2418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3248, device='cuda:0')



h[200].sum tensor(-22.6155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0137, 0.0028,  ..., 0.0076, 0.0117, 0.0062],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0061, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57927.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0577, 0.0194, 0.0481,  ..., 0.1220, 0.0470, 0.0710],
        [0.0634, 0.0216, 0.0781,  ..., 0.0953, 0.0305, 0.0512],
        [0.0664, 0.0228, 0.0922,  ..., 0.0834, 0.0231, 0.0423],
        ...,
        [0.0689, 0.0241, 0.0980,  ..., 0.0821, 0.0219, 0.0407],
        [0.0689, 0.0241, 0.0980,  ..., 0.0821, 0.0219, 0.0406],
        [0.0689, 0.0240, 0.0980,  ..., 0.0820, 0.0219, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628776.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8635.7861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.4606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4167.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1026.0405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0915],
        [-1.6993],
        [-2.1000],
        ...,
        [-3.9048],
        [-3.8978],
        [-3.8953]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-341478.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9996],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369888.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9996],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369888.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0078,  0.0017,  ...,  0.0025,  0.0073,  0.0035],
        [-0.0007,  0.0078,  0.0017,  ...,  0.0025,  0.0073,  0.0035],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2931.6504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3783, device='cuda:0')



h[100].sum tensor(-0.4757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5846, device='cuda:0')



h[200].sum tensor(-22.4931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0445, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0490, 0.0129,  ..., 0.0130, 0.0467, 0.0286],
        [0.0000, 0.0305, 0.0072,  ..., 0.0102, 0.0283, 0.0154],
        [0.0000, 0.0576, 0.0159,  ..., 0.0143, 0.0552, 0.0357],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81288.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0372, 0.0119, 0.0000,  ..., 0.2321, 0.1194, 0.1503],
        [0.0400, 0.0130, 0.0000,  ..., 0.2182, 0.1096, 0.1402],
        [0.0371, 0.0124, 0.0000,  ..., 0.2422, 0.1291, 0.1565],
        ...,
        [0.0689, 0.0241, 0.0980,  ..., 0.0821, 0.0219, 0.0407],
        [0.0689, 0.0241, 0.0980,  ..., 0.0821, 0.0219, 0.0406],
        [0.0689, 0.0240, 0.0980,  ..., 0.0820, 0.0219, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(749512.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8395.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.5178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.2222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4038.4290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1223.5741, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3229],
        [ 0.3431],
        [ 0.2912],
        ...,
        [-3.9048],
        [-3.8979],
        [-3.8953]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-352888.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9996],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369888.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9995],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369890.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2218.7139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2711, device='cuda:0')



h[100].sum tensor(-0.3197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1079, device='cuda:0')



h[200].sum tensor(-22.5732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0059, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0015, 0.0000],
        [0.0000, 0.0354, 0.0104,  ..., 0.0109, 0.0332, 0.0244],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0061, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0061, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0061, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65113.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0635, 0.0214, 0.0732,  ..., 0.0990, 0.0346, 0.0532],
        [0.0605, 0.0208, 0.0478,  ..., 0.1216, 0.0524, 0.0688],
        [0.0513, 0.0187, 0.0215,  ..., 0.1854, 0.1019, 0.1131],
        ...,
        [0.0694, 0.0239, 0.0984,  ..., 0.0822, 0.0219, 0.0406],
        [0.0693, 0.0239, 0.0983,  ..., 0.0821, 0.0219, 0.0406],
        [0.0693, 0.0239, 0.0983,  ..., 0.0821, 0.0219, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661905.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8603.9160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.9377, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4103.9575, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1088.6375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7193],
        [-0.4694],
        [-0.1076],
        ...,
        [-3.7635],
        [-3.8137],
        [-3.8611]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344683.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9995],
        [0.9986],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369890.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369893.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        ...,
        [-0.0020,  0.0197,  0.0058,  ...,  0.0043,  0.0191,  0.0135],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0014,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2518.1484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.5385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4715, device='cuda:0')



h[100].sum tensor(-0.3837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7216, device='cuda:0')



h[200].sum tensor(-22.5374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0058, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0355, 0.0089,  ..., 0.0109, 0.0334, 0.0196],
        [0.0000, 0.0314, 0.0082,  ..., 0.0103, 0.0293, 0.0185],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73415.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0217, 0.0879,  ..., 0.0861, 0.0258, 0.0439],
        [0.0676, 0.0223, 0.0961,  ..., 0.0795, 0.0214, 0.0390],
        [0.0680, 0.0226, 0.0964,  ..., 0.0801, 0.0216, 0.0394],
        ...,
        [0.0452, 0.0147, 0.0068,  ..., 0.2163, 0.1121, 0.1374],
        [0.0521, 0.0174, 0.0243,  ..., 0.1796, 0.0887, 0.1106],
        [0.0637, 0.0213, 0.0620,  ..., 0.1149, 0.0442, 0.0641]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710104.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8508.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.1079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3975.4753, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1158.8610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2265],
        [-2.7430],
        [-3.0387],
        ...,
        [-0.2956],
        [-1.0784],
        [-2.1410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338906.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0279],
        [1.0347],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369893.8438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 110.0 event: 550 loss: tensor(322.0868, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0280],
        [1.0348],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369897.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0078,  0.0017,  ...,  0.0025,  0.0073,  0.0036],
        [-0.0009,  0.0098,  0.0024,  ...,  0.0028,  0.0093,  0.0052],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2200.8130, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3902, device='cuda:0')



h[100].sum tensor(-0.3128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1253, device='cuda:0')



h[200].sum tensor(-22.5736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0743, 0.0217,  ..., 0.0166, 0.0720, 0.0500],
        [0.0000, 0.0327, 0.0086,  ..., 0.0103, 0.0306, 0.0197],
        [0.0000, 0.0127, 0.0025,  ..., 0.0072, 0.0107, 0.0054],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0059, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64542.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0361, 0.0113, 0.0000,  ..., 0.2544, 0.1445, 0.1647],
        [0.0464, 0.0147, 0.0123,  ..., 0.1970, 0.1031, 0.1237],
        [0.0569, 0.0183, 0.0392,  ..., 0.1404, 0.0627, 0.0832],
        ...,
        [0.0696, 0.0229, 0.0981,  ..., 0.0824, 0.0223, 0.0409],
        [0.0696, 0.0229, 0.0981,  ..., 0.0824, 0.0223, 0.0409],
        [0.0696, 0.0229, 0.0981,  ..., 0.0823, 0.0223, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659908.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8618.6260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.7538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4012.7041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1083.0632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1476],
        [-0.2368],
        [-0.9683],
        ...,
        [-3.9590],
        [-3.9517],
        [-3.9490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345342.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0280],
        [1.0348],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369897.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0259],
        [1.0280],
        [1.0348],
        ...,
        [0.9994],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369901.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [-0.0007,  0.0079,  0.0017,  ...,  0.0025,  0.0074,  0.0036],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2609.1006, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.7210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8675, device='cuda:0')



h[100].sum tensor(-0.3943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9256, device='cuda:0')



h[200].sum tensor(-22.5277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0093, 0.0013,  ..., 0.0067, 0.0074, 0.0025],
        [0.0000, 0.0337, 0.0082,  ..., 0.0104, 0.0317, 0.0182],
        [0.0000, 0.0559, 0.0160,  ..., 0.0138, 0.0537, 0.0368],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0059, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0059, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0059, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74586.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0550, 0.0168, 0.0369,  ..., 0.1355, 0.0562, 0.0813],
        [0.0442, 0.0130, 0.0108,  ..., 0.1943, 0.0959, 0.1239],
        [0.0369, 0.0109, 0.0000,  ..., 0.2412, 0.1306, 0.1568],
        ...,
        [0.0694, 0.0225, 0.0977,  ..., 0.0825, 0.0226, 0.0411],
        [0.0694, 0.0225, 0.0977,  ..., 0.0824, 0.0226, 0.0411],
        [0.0694, 0.0224, 0.0977,  ..., 0.0824, 0.0226, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(709482.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8314.2725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.9251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3700.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1172.4525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0326],
        [-0.2579],
        [ 0.1643],
        ...,
        [-3.9578],
        [-3.9505],
        [-3.9478]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305484.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0259],
        [1.0280],
        [1.0348],
        ...,
        [0.9994],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369901.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0259],
        [1.0280],
        [1.0348],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369905.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1916.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9364, device='cuda:0')



h[100].sum tensor(-0.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4746, device='cuda:0')



h[200].sum tensor(-22.6077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0058, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0058, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0058, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0059, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0059, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0059, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60084.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0670, 0.0209, 0.0947,  ..., 0.0793, 0.0220, 0.0393],
        [0.0672, 0.0211, 0.0951,  ..., 0.0796, 0.0221, 0.0396],
        [0.0676, 0.0213, 0.0954,  ..., 0.0803, 0.0223, 0.0399],
        ...,
        [0.0693, 0.0221, 0.0974,  ..., 0.0825, 0.0229, 0.0413],
        [0.0692, 0.0221, 0.0974,  ..., 0.0825, 0.0229, 0.0412],
        [0.0692, 0.0221, 0.0973,  ..., 0.0824, 0.0229, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643659.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8716.6025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4162, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.4406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4089.4692, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1038.6903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7779],
        [-3.5672],
        [-3.2439],
        ...,
        [-3.9579],
        [-3.9506],
        [-3.9479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-361581.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0259],
        [1.0280],
        [1.0348],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369905.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0349],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369909.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [-0.0009,  0.0094,  0.0022,  ...,  0.0027,  0.0089,  0.0048],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2198.6802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8790, device='cuda:0')



h[100].sum tensor(-0.2969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0506, device='cuda:0')



h[200].sum tensor(-22.5774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0058, 0.0018, 0.0000],
        [0.0000, 0.0124, 0.0023,  ..., 0.0072, 0.0105, 0.0050],
        [0.0000, 0.0167, 0.0038,  ..., 0.0079, 0.0148, 0.0086],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0060, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0060, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0060, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65546.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0640, 0.0200, 0.0811,  ..., 0.0916, 0.0297, 0.0484],
        [0.0582, 0.0180, 0.0497,  ..., 0.1212, 0.0480, 0.0703],
        [0.0528, 0.0162, 0.0275,  ..., 0.1492, 0.0652, 0.0910],
        ...,
        [0.0689, 0.0222, 0.0971,  ..., 0.0827, 0.0231, 0.0413],
        [0.0689, 0.0222, 0.0971,  ..., 0.0826, 0.0231, 0.0412],
        [0.0688, 0.0222, 0.0971,  ..., 0.0826, 0.0231, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665638.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8361.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.9114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3723.2065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1093.9402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6928],
        [-1.9322],
        [-1.2048],
        ...,
        [-3.9554],
        [-3.9482],
        [-3.9454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292265.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0349],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369909.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0349],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369912.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2091.0476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2729, device='cuda:0')



h[100].sum tensor(-0.2719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8159, device='cuda:0')



h[200].sum tensor(-22.5899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0059, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0059, 0.0017, 0.0000],
        [0.0000, 0.0200, 0.0042,  ..., 0.0084, 0.0181, 0.0090],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0060, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0060, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0060, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61780.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0655, 0.0208, 0.0890,  ..., 0.0849, 0.0254, 0.0430],
        [0.0624, 0.0197, 0.0718,  ..., 0.1013, 0.0354, 0.0551],
        [0.0544, 0.0169, 0.0323,  ..., 0.1424, 0.0607, 0.0854],
        ...,
        [0.0690, 0.0224, 0.0974,  ..., 0.0828, 0.0233, 0.0409],
        [0.0689, 0.0224, 0.0974,  ..., 0.0828, 0.0233, 0.0409],
        [0.0689, 0.0224, 0.0973,  ..., 0.0828, 0.0233, 0.0409]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647397.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8533.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.2924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3946.9473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1059.5326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8960],
        [-2.2182],
        [-1.4721],
        ...,
        [-3.9265],
        [-3.9080],
        [-3.9001]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331563.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0349],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369912.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369916.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0046,  0.0463,  0.0151,  ...,  0.0084,  0.0456,  0.0358],
        [-0.0012,  0.0122,  0.0033,  ...,  0.0032,  0.0117,  0.0073],
        [-0.0038,  0.0378,  0.0122,  ...,  0.0071,  0.0371,  0.0287],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2911.1084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7877, device='cuda:0')



h[100].sum tensor(-0.4325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.4983, device='cuda:0')



h[200].sum tensor(-22.4976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0436, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0868, 0.0261,  ..., 0.0187, 0.0845, 0.0605],
        [0.0000, 0.1914, 0.0625,  ..., 0.0348, 0.1886, 0.1482],
        [0.0000, 0.1231, 0.0388,  ..., 0.0243, 0.1206, 0.0909],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79620.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0156, 0.0114, 0.0000,  ..., 0.4054, 0.2739, 0.2640],
        [0.0024, 0.0112, 0.0000,  ..., 0.5178, 0.3724, 0.3391],
        [0.0070, 0.0115, 0.0000,  ..., 0.4847, 0.3432, 0.3168],
        ...,
        [0.0690, 0.0228, 0.0976,  ..., 0.0830, 0.0233, 0.0405],
        [0.0690, 0.0228, 0.0975,  ..., 0.0830, 0.0232, 0.0405],
        [0.0690, 0.0228, 0.0975,  ..., 0.0830, 0.0232, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731825.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8144.2065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.3048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.1967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3612.1997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1219.2815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1698],
        [ 0.1405],
        [ 0.1128],
        ...,
        [-3.9563],
        [-3.9282],
        [-3.9041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295777.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369916.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369920.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [-0.0027,  0.0271,  0.0085,  ...,  0.0055,  0.0265,  0.0197],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1982.2507, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6093, device='cuda:0')



h[100].sum tensor(-0.2424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5729, device='cuda:0')



h[200].sum tensor(-22.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0304, 0.0086,  ..., 0.0101, 0.0284, 0.0202],
        [0.0000, 0.0256, 0.0070,  ..., 0.0094, 0.0236, 0.0161],
        [0.0000, 0.1018, 0.0314,  ..., 0.0211, 0.0994, 0.0730],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60497.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0564, 0.0195, 0.0368,  ..., 0.1465, 0.0737, 0.0843],
        [0.0535, 0.0189, 0.0170,  ..., 0.1663, 0.0883, 0.0980],
        [0.0442, 0.0172, 0.0000,  ..., 0.2287, 0.1360, 0.1409],
        ...,
        [0.0691, 0.0229, 0.0977,  ..., 0.0831, 0.0233, 0.0403],
        [0.0691, 0.0229, 0.0976,  ..., 0.0831, 0.0233, 0.0402],
        [0.0691, 0.0229, 0.0976,  ..., 0.0831, 0.0233, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645797.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8618.8740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.6849, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4093.6538, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1050.5784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1905],
        [-1.7108],
        [-1.3062],
        ...,
        [-3.9976],
        [-3.9900],
        [-3.9871]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-353753.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369920.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369924.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2028.7443, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9125, device='cuda:0')



h[100].sum tensor(-0.2479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6172, device='cuda:0')



h[200].sum tensor(-22.5992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60366.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0218, 0.0952,  ..., 0.0799, 0.0225, 0.0381],
        [0.0669, 0.0218, 0.0948,  ..., 0.0810, 0.0228, 0.0390],
        [0.0666, 0.0218, 0.0933,  ..., 0.0834, 0.0235, 0.0411],
        ...,
        [0.0692, 0.0230, 0.0979,  ..., 0.0832, 0.0234, 0.0400],
        [0.0692, 0.0230, 0.0979,  ..., 0.0832, 0.0234, 0.0400],
        [0.0692, 0.0230, 0.0978,  ..., 0.0831, 0.0234, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643178.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8600.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.7705, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4089.0916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1052.8978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2186],
        [-3.3349],
        [-3.3398],
        ...,
        [-3.9320],
        [-3.9877],
        [-3.9974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347142.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369924.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369924.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [-0.0007,  0.0079,  0.0018,  ...,  0.0025,  0.0074,  0.0036],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1910.6816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5411, device='cuda:0')



h[100].sum tensor(-0.2247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4168, device='cuda:0')



h[200].sum tensor(-22.6125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        [0.0000, 0.0166, 0.0032,  ..., 0.0080, 0.0146, 0.0063],
        [0.0000, 0.0222, 0.0044,  ..., 0.0089, 0.0202, 0.0086],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57889.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0630, 0.0204, 0.0758,  ..., 0.0980, 0.0328, 0.0515],
        [0.0559, 0.0179, 0.0386,  ..., 0.1333, 0.0533, 0.0773],
        [0.0510, 0.0162, 0.0177,  ..., 0.1576, 0.0664, 0.0953],
        ...,
        [0.0692, 0.0230, 0.0979,  ..., 0.0832, 0.0234, 0.0400],
        [0.0692, 0.0230, 0.0979,  ..., 0.0832, 0.0234, 0.0400],
        [0.0692, 0.0230, 0.0978,  ..., 0.0831, 0.0234, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633964.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8828.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.1504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4358.0713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1023.5178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1567],
        [-1.4227],
        [-0.8329],
        ...,
        [-4.0104],
        [-4.0028],
        [-3.9998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-401721.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0281],
        [1.0350],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369924.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0280],
        [1.0351],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369927.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [-0.0008,  0.0093,  0.0023,  ...,  0.0028,  0.0088,  0.0048],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [-0.0023,  0.0234,  0.0072,  ...,  0.0049,  0.0229,  0.0167],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2345.3135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5168, device='cuda:0')



h[100].sum tensor(-0.3052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2899, device='cuda:0')



h[200].sum tensor(-22.5644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        [0.0000, 0.0122, 0.0023,  ..., 0.0073, 0.0103, 0.0049],
        [0.0000, 0.0325, 0.0087,  ..., 0.0105, 0.0305, 0.0196],
        ...,
        [0.0000, 0.0274, 0.0076,  ..., 0.0098, 0.0253, 0.0175],
        [0.0000, 0.0230, 0.0060,  ..., 0.0091, 0.0210, 0.0139],
        [0.0000, 0.0898, 0.0272,  ..., 0.0194, 0.0875, 0.0628]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67709.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0197, 0.0556,  ..., 0.1156, 0.0465, 0.0635],
        [0.0554, 0.0180, 0.0329,  ..., 0.1421, 0.0625, 0.0828],
        [0.0479, 0.0159, 0.0131,  ..., 0.1865, 0.0923, 0.1141],
        ...,
        [0.0594, 0.0207, 0.0420,  ..., 0.1444, 0.0693, 0.0822],
        [0.0564, 0.0199, 0.0224,  ..., 0.1619, 0.0820, 0.0944],
        [0.0474, 0.0178, 0.0000,  ..., 0.2179, 0.1241, 0.1330]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678143., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8329.9736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.4463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3780.8638, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1121.9103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0572],
        [ 0.1008],
        [ 0.1436],
        ...,
        [-2.4654],
        [-1.9209],
        [-1.6315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300886.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0280],
        [1.0351],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369927.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 120.0 event: 600 loss: tensor(456.3234, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0280],
        [1.0351],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369931.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2083.7070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5393, device='cuda:0')



h[100].sum tensor(-0.2494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7088, device='cuda:0')



h[200].sum tensor(-22.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0062, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63656.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0658, 0.0212, 0.0898,  ..., 0.0847, 0.0258, 0.0418],
        [0.0670, 0.0216, 0.0950,  ..., 0.0807, 0.0229, 0.0387],
        [0.0674, 0.0219, 0.0956,  ..., 0.0811, 0.0230, 0.0388],
        ...,
        [0.0691, 0.0228, 0.0977,  ..., 0.0834, 0.0236, 0.0401],
        [0.0691, 0.0227, 0.0976,  ..., 0.0833, 0.0236, 0.0401],
        [0.0690, 0.0227, 0.0976,  ..., 0.0833, 0.0236, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666459.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8573.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.4387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4067.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1079.3944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0252],
        [-3.5477],
        [-3.8705],
        ...,
        [-4.0126],
        [-4.0050],
        [-4.0020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-363984.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0280],
        [1.0351],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369931.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0279],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369935.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1915.5883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7639, device='cuda:0')



h[100].sum tensor(-0.2115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3032, device='cuda:0')



h[200].sum tensor(-22.6165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0123, 0.0023,  ..., 0.0074, 0.0104, 0.0050],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0211, 0.0054,  ..., 0.0088, 0.0191, 0.0123],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59632.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0610, 0.0193, 0.0659,  ..., 0.1068, 0.0386, 0.0580],
        [0.0628, 0.0201, 0.0714,  ..., 0.1019, 0.0370, 0.0539],
        [0.0590, 0.0194, 0.0469,  ..., 0.1282, 0.0564, 0.0720],
        ...,
        [0.0688, 0.0225, 0.0973,  ..., 0.0835, 0.0237, 0.0404],
        [0.0688, 0.0225, 0.0972,  ..., 0.0834, 0.0237, 0.0404],
        [0.0688, 0.0225, 0.0972,  ..., 0.0834, 0.0237, 0.0403]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645592.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8565.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.5001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4039.7944, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1046.7040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0614],
        [-3.0660],
        [-2.8461],
        ...,
        [-4.0013],
        [-3.9933],
        [-3.9899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-341970.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0279],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369935.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0279],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369938.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2336.9258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5061, device='cuda:0')



h[100].sum tensor(-0.2860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1422, device='cuda:0')



h[200].sum tensor(-22.5707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0018, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65498.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0660, 0.0208, 0.0922,  ..., 0.0822, 0.0240, 0.0403],
        [0.0657, 0.0207, 0.0912,  ..., 0.0839, 0.0245, 0.0418],
        [0.0638, 0.0200, 0.0821,  ..., 0.0934, 0.0292, 0.0492],
        ...,
        [0.0687, 0.0222, 0.0969,  ..., 0.0836, 0.0238, 0.0407],
        [0.0687, 0.0221, 0.0969,  ..., 0.0835, 0.0238, 0.0406],
        [0.0686, 0.0221, 0.0969,  ..., 0.0835, 0.0238, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660421.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8240.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.6608, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3602.9087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1106.4875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0367],
        [-2.6713],
        [-2.0845],
        ...,
        [-3.9943],
        [-3.9872],
        [-3.9848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277590.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0279],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369938.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0278],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369941.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2045.9929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4406, device='cuda:0')



h[100].sum tensor(-0.2282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5482, device='cuda:0')



h[200].sum tensor(-22.6040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0790, 0.0242,  ..., 0.0177, 0.0766, 0.0562],
        [0.0000, 0.0159, 0.0036,  ..., 0.0080, 0.0139, 0.0079],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60378.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0321, 0.0122, 0.0000,  ..., 0.2967, 0.1866, 0.1883],
        [0.0450, 0.0150, 0.0085,  ..., 0.2144, 0.1220, 0.1318],
        [0.0516, 0.0170, 0.0180,  ..., 0.1777, 0.0947, 0.1061],
        ...,
        [0.0690, 0.0219, 0.0972,  ..., 0.0837, 0.0239, 0.0405],
        [0.0690, 0.0218, 0.0971,  ..., 0.0836, 0.0239, 0.0405],
        [0.0689, 0.0218, 0.0971,  ..., 0.0836, 0.0239, 0.0405]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641769.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8533.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.7518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3921.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1057.7773, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2324],
        [ 0.2123],
        [ 0.1818],
        ...,
        [-4.0130],
        [-4.0055],
        [-4.0025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326856.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0278],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369941.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0278],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369941.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2059.0405, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6562, device='cuda:0')



h[100].sum tensor(-0.2306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5797, device='cuda:0')



h[200].sum tensor(-22.6025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0063, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61796.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0645, 0.0198, 0.0829,  ..., 0.0911, 0.0295, 0.0466],
        [0.0664, 0.0206, 0.0921,  ..., 0.0833, 0.0247, 0.0408],
        [0.0673, 0.0211, 0.0952,  ..., 0.0813, 0.0233, 0.0392],
        ...,
        [0.0690, 0.0219, 0.0972,  ..., 0.0837, 0.0239, 0.0405],
        [0.0690, 0.0218, 0.0971,  ..., 0.0836, 0.0239, 0.0405],
        [0.0689, 0.0218, 0.0971,  ..., 0.0836, 0.0239, 0.0405]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(651748.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8466.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.0630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3816.2104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1070.4070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7996],
        [-3.3800],
        [-3.7679],
        ...,
        [-4.0123],
        [-4.0054],
        [-4.0025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313407.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0278],
        [1.0352],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369941.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369944.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2098.5383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2300, device='cuda:0')



h[100].sum tensor(-0.2369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6636, device='cuda:0')



h[200].sum tensor(-22.5973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61100.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0611, 0.0180, 0.0596,  ..., 0.1133, 0.0436, 0.0620],
        [0.0630, 0.0187, 0.0697,  ..., 0.1047, 0.0379, 0.0558],
        [0.0649, 0.0196, 0.0802,  ..., 0.0961, 0.0318, 0.0496],
        ...,
        [0.0696, 0.0216, 0.0978,  ..., 0.0838, 0.0240, 0.0402],
        [0.0696, 0.0216, 0.0977,  ..., 0.0837, 0.0239, 0.0402],
        [0.0695, 0.0216, 0.0977,  ..., 0.0837, 0.0239, 0.0401]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642791., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8602.5615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.6486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3858.8301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1068.1052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7750],
        [-0.8932],
        [-1.0647],
        ...,
        [-4.0340],
        [-4.0285],
        [-4.0281]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316269.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369944.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0262],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369947., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2060.5737, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7669, device='cuda:0')



h[100].sum tensor(-0.2290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5959, device='cuda:0')



h[200].sum tensor(-22.6007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0252, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60208.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.0203, 0.0957,  ..., 0.0805, 0.0232, 0.0380],
        [0.0681, 0.0204, 0.0960,  ..., 0.0809, 0.0233, 0.0382],
        [0.0685, 0.0206, 0.0964,  ..., 0.0815, 0.0234, 0.0386],
        ...,
        [0.0702, 0.0214, 0.0984,  ..., 0.0839, 0.0241, 0.0399],
        [0.0701, 0.0214, 0.0984,  ..., 0.0838, 0.0241, 0.0398],
        [0.0701, 0.0214, 0.0983,  ..., 0.0838, 0.0241, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644486.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8919.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.9194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4201.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1052.9001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5256],
        [-3.6065],
        [-3.6492],
        ...,
        [-4.0745],
        [-4.0666],
        [-4.0635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-398472.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0262],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369947., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0262],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369950.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2055.4502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7331, device='cuda:0')



h[100].sum tensor(-0.2267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5910, device='cuda:0')



h[200].sum tensor(-22.6008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60800.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0197, 0.0906,  ..., 0.0856, 0.0267, 0.0415],
        [0.0648, 0.0189, 0.0724,  ..., 0.1026, 0.0387, 0.0533],
        [0.0618, 0.0180, 0.0507,  ..., 0.1232, 0.0528, 0.0678],
        ...,
        [0.0706, 0.0212, 0.0988,  ..., 0.0839, 0.0242, 0.0397],
        [0.0706, 0.0212, 0.0988,  ..., 0.0838, 0.0242, 0.0397],
        [0.0705, 0.0212, 0.0988,  ..., 0.0838, 0.0242, 0.0397]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647410.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8864.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.9391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3990.7646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1063.4875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1035],
        [-0.6210],
        [-0.2106],
        ...,
        [-4.0958],
        [-4.0878],
        [-4.0846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358161.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0262],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369950.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0262],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369953.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2057.9331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9141, device='cuda:0')



h[100].sum tensor(-0.2238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6174, device='cuda:0')



h[200].sum tensor(-22.6012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60350.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0670, 0.0194, 0.0925,  ..., 0.0842, 0.0245, 0.0414],
        [0.0674, 0.0195, 0.0931,  ..., 0.0843, 0.0245, 0.0414],
        [0.0675, 0.0197, 0.0907,  ..., 0.0874, 0.0267, 0.0433],
        ...,
        [0.0693, 0.0206, 0.0905,  ..., 0.0916, 0.0296, 0.0453],
        [0.0666, 0.0196, 0.0736,  ..., 0.1068, 0.0400, 0.0561],
        [0.0652, 0.0191, 0.0652,  ..., 0.1144, 0.0452, 0.0615]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643753.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8996.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.9945, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4132.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1056.1298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1239],
        [-2.9344],
        [-2.5864],
        ...,
        [-3.3333],
        [-2.8435],
        [-2.5298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-389014.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0262],
        [1.0277],
        [1.0353],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369953.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0354],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369957.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [-0.0021,  0.0221,  0.0067,  ...,  0.0047,  0.0215,  0.0156],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0014,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3019.3884, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1767, device='cuda:0')



h[100].sum tensor(-0.3882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5552, device='cuda:0')



h[200].sum tensor(-22.4952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0059, 0.0016, 0.0000],
        [0.0000, 0.0432, 0.0124,  ..., 0.0120, 0.0410, 0.0286],
        [0.0000, 0.0433, 0.0125,  ..., 0.0121, 0.0411, 0.0287],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0061, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77555.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0569, 0.0160, 0.0280,  ..., 0.1420, 0.0659, 0.0824],
        [0.0487, 0.0141, 0.0124,  ..., 0.1984, 0.1105, 0.1208],
        [0.0444, 0.0133, 0.0065,  ..., 0.2302, 0.1353, 0.1425],
        ...,
        [0.0705, 0.0211, 0.0987,  ..., 0.0839, 0.0244, 0.0403],
        [0.0705, 0.0211, 0.0986,  ..., 0.0838, 0.0244, 0.0402],
        [0.0704, 0.0210, 0.0986,  ..., 0.0838, 0.0244, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(721173., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8599.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.1019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.0121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3756.0991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1206.9474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1418],
        [ 0.1998],
        [ 0.2358],
        ...,
        [-4.0920],
        [-4.0837],
        [-4.0803]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331601.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0354],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369957.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 130.0 event: 650 loss: tensor(459.5785, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0354],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369961.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0080,  0.0018,  ...,  0.0025,  0.0074,  0.0037],
        [-0.0030,  0.0320,  0.0102,  ...,  0.0062,  0.0313,  0.0238],
        [-0.0047,  0.0491,  0.0161,  ...,  0.0089,  0.0484,  0.0382],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2518.2402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.9249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9528, device='cuda:0')



h[100].sum tensor(-0.2947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4997, device='cuda:0')



h[200].sum tensor(-22.5528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1077, 0.0336,  ..., 0.0219, 0.1052, 0.0782],
        [0.0000, 0.1206, 0.0380,  ..., 0.0239, 0.1180, 0.0889],
        [0.0000, 0.1594, 0.0515,  ..., 0.0299, 0.1565, 0.1213],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70547.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0064, 0.0000,  ..., 0.4470, 0.3106, 0.2928],
        [0.0069, 0.0061, 0.0000,  ..., 0.4788, 0.3375, 0.3145],
        [0.0014, 0.0058, 0.0000,  ..., 0.5296, 0.3816, 0.3489],
        ...,
        [0.0704, 0.0209, 0.0985,  ..., 0.0838, 0.0245, 0.0407],
        [0.0703, 0.0209, 0.0984,  ..., 0.0838, 0.0244, 0.0407],
        [0.0703, 0.0209, 0.0984,  ..., 0.0838, 0.0244, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(694499.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8655.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.5948, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3805.4658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1147.0876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2427],
        [ 0.2411],
        [ 0.2369],
        ...,
        [-4.0899],
        [-4.0820],
        [-4.0788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335075.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0354],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369961.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369965.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0187,  0.0056,  ...,  0.0042,  0.0181,  0.0127],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [-0.0047,  0.0497,  0.0163,  ...,  0.0090,  0.0490,  0.0387],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2966.4915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8832, device='cuda:0')



h[100].sum tensor(-0.3673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.3662, device='cuda:0')



h[200].sum tensor(-22.5039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0423, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0185, 0.0045,  ..., 0.0082, 0.0164, 0.0102],
        [0.0000, 0.1136, 0.0356,  ..., 0.0229, 0.1111, 0.0830],
        [0.0000, 0.1066, 0.0338,  ..., 0.0218, 0.1041, 0.0794],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0061, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74850.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.0146, 0.0240,  ..., 0.1891, 0.1047, 0.1160],
        [0.0281, 0.0105, 0.0000,  ..., 0.3277, 0.2154, 0.2118],
        [0.0188, 0.0093, 0.0000,  ..., 0.3937, 0.2697, 0.2571],
        ...,
        [0.0704, 0.0208, 0.0984,  ..., 0.0838, 0.0245, 0.0411],
        [0.0686, 0.0201, 0.0893,  ..., 0.0922, 0.0296, 0.0474],
        [0.0661, 0.0191, 0.0761,  ..., 0.1046, 0.0370, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697259.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8717.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.4819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3826.9006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1181.5626, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4066],
        [ 0.1083],
        [ 0.2844],
        ...,
        [-3.8293],
        [-3.4564],
        [-2.9207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346446.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369965.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369968.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [-0.0017,  0.0182,  0.0054,  ...,  0.0041,  0.0176,  0.0123],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1834.4539, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9988, device='cuda:0')



h[100].sum tensor(-0.1697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0453, device='cuda:0')



h[200].sum tensor(-22.6309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0060, 0.0017, 0.0000],
        [0.0000, 0.0279, 0.0071,  ..., 0.0098, 0.0258, 0.0158],
        [0.0000, 0.0262, 0.0059,  ..., 0.0095, 0.0241, 0.0121],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56172.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0635, 0.0180, 0.0717,  ..., 0.1028, 0.0373, 0.0558],
        [0.0542, 0.0148, 0.0304,  ..., 0.1502, 0.0675, 0.0906],
        [0.0498, 0.0131, 0.0123,  ..., 0.1708, 0.0780, 0.1064],
        ...,
        [0.0705, 0.0209, 0.0984,  ..., 0.0837, 0.0245, 0.0412],
        [0.0704, 0.0209, 0.0983,  ..., 0.0837, 0.0245, 0.0412],
        [0.0704, 0.0209, 0.0983,  ..., 0.0837, 0.0245, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625780.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9024.0957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.2212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4066.3491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1019.5912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0253],
        [-1.2163],
        [-0.5544],
        ...,
        [-4.0939],
        [-4.0862],
        [-4.0833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-375224.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0277],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369968.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369972.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030,  0.0321,  0.0102,  ...,  0.0063,  0.0315,  0.0239],
        [-0.0030,  0.0323,  0.0103,  ...,  0.0063,  0.0317,  0.0241],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2478.8789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.7290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9818, device='cuda:0')



h[100].sum tensor(-0.2747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3578, device='cuda:0')



h[200].sum tensor(-22.5603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1347, 0.0430,  ..., 0.0262, 0.1321, 0.1007],
        [0.0000, 0.0983, 0.0310,  ..., 0.0207, 0.0959, 0.0724],
        [0.0000, 0.0909, 0.0278,  ..., 0.0195, 0.0885, 0.0639],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0062, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68457.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0174, 0.0103, 0.0000,  ..., 0.3953, 0.2696, 0.2584],
        [0.0166, 0.0098, 0.0000,  ..., 0.3992, 0.2705, 0.2615],
        [0.0178, 0.0093, 0.0000,  ..., 0.3876, 0.2567, 0.2544],
        ...,
        [0.0704, 0.0211, 0.0984,  ..., 0.0837, 0.0245, 0.0412],
        [0.0704, 0.0211, 0.0983,  ..., 0.0836, 0.0245, 0.0411],
        [0.0703, 0.0211, 0.0983,  ..., 0.0836, 0.0244, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681976.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8716.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.6999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3806.2534, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1129.3247, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2618],
        [ 0.2696],
        [ 0.2740],
        ...,
        [-4.0959],
        [-4.0881],
        [-4.0853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334790.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369972.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369976.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0076,  0.0017,  ...,  0.0025,  0.0071,  0.0034],
        [-0.0009,  0.0101,  0.0026,  ...,  0.0029,  0.0096,  0.0055],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1992.1449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7613, device='cuda:0')



h[100].sum tensor(-0.1891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3029, device='cuda:0')



h[200].sum tensor(-22.6158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0654, 0.0190,  ..., 0.0157, 0.0631, 0.0426],
        [0.0000, 0.0239, 0.0058,  ..., 0.0093, 0.0218, 0.0124],
        [0.0000, 0.0131, 0.0027,  ..., 0.0077, 0.0110, 0.0056],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0063, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0063, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0063, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58738.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0366, 0.0121, 0.0000,  ..., 0.2586, 0.1498, 0.1653],
        [0.0499, 0.0151, 0.0235,  ..., 0.1794, 0.0903, 0.1100],
        [0.0598, 0.0180, 0.0488,  ..., 0.1273, 0.0536, 0.0729],
        ...,
        [0.0704, 0.0217, 0.0987,  ..., 0.0836, 0.0245, 0.0408],
        [0.0703, 0.0216, 0.0987,  ..., 0.0836, 0.0245, 0.0408],
        [0.0703, 0.0216, 0.0986,  ..., 0.0835, 0.0245, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637795.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8888.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.9089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3990.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1045.7018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1475],
        [-0.2061],
        [-0.8685],
        ...,
        [-4.1055],
        [-4.0978],
        [-4.0950]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-357252.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0355],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369976.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369980.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0015,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2103.7664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3731, device='cuda:0')



h[100].sum tensor(-0.2022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5384, device='cuda:0')



h[200].sum tensor(-22.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0063, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0063, 0.0016, 0.0000],
        [0.0000, 0.0154, 0.0035,  ..., 0.0082, 0.0134, 0.0076],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0065, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0065, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0065, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61111.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0634, 0.0196, 0.0697,  ..., 0.1046, 0.0394, 0.0562],
        [0.0608, 0.0190, 0.0519,  ..., 0.1215, 0.0509, 0.0680],
        [0.0554, 0.0177, 0.0299,  ..., 0.1539, 0.0726, 0.0911],
        ...,
        [0.0702, 0.0222, 0.0988,  ..., 0.0836, 0.0244, 0.0406],
        [0.0701, 0.0222, 0.0987,  ..., 0.0835, 0.0244, 0.0406],
        [0.0701, 0.0221, 0.0987,  ..., 0.0835, 0.0244, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654474., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8804.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.9152, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3968.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1065.7688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4552],
        [-0.2414],
        [-0.0309],
        ...,
        [-4.1058],
        [-4.0981],
        [-4.0954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-355807.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369980.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369984.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2105.6262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8724, device='cuda:0')



h[100].sum tensor(-0.1982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4652, device='cuda:0')



h[200].sum tensor(-22.6070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0064, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0064, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0064, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0065, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0065, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0065, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60233.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0661, 0.0207, 0.0895,  ..., 0.0867, 0.0265, 0.0438],
        [0.0673, 0.0212, 0.0942,  ..., 0.0829, 0.0244, 0.0407],
        [0.0682, 0.0217, 0.0965,  ..., 0.0815, 0.0238, 0.0395],
        ...,
        [0.0701, 0.0226, 0.0989,  ..., 0.0835, 0.0243, 0.0405],
        [0.0700, 0.0225, 0.0989,  ..., 0.0835, 0.0243, 0.0404],
        [0.0700, 0.0225, 0.0988,  ..., 0.0835, 0.0243, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644811.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8820.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.3521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4000.0444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1059.3279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5188],
        [-2.2345],
        [-2.8498],
        ...,
        [-4.0806],
        [-4.0859],
        [-4.0896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358347.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0276],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369984.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369988.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0163,  0.0048,  ...,  0.0039,  0.0157,  0.0106],
        [-0.0017,  0.0183,  0.0055,  ...,  0.0043,  0.0177,  0.0123],
        [-0.0017,  0.0187,  0.0056,  ...,  0.0043,  0.0181,  0.0127],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2154.0691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6160, device='cuda:0')



h[100].sum tensor(-0.2030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5739, device='cuda:0')



h[200].sum tensor(-22.6023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0613, 0.0177,  ..., 0.0153, 0.0590, 0.0391],
        [0.0000, 0.0709, 0.0211,  ..., 0.0168, 0.0686, 0.0471],
        [0.0000, 0.0889, 0.0274,  ..., 0.0196, 0.0865, 0.0622],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61359.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0214, 0.0139, 0.0000,  ..., 0.3616, 0.2311, 0.2341],
        [0.0246, 0.0139, 0.0000,  ..., 0.3391, 0.2109, 0.2193],
        [0.0292, 0.0145, 0.0000,  ..., 0.3123, 0.1896, 0.2008],
        ...,
        [0.0701, 0.0228, 0.0990,  ..., 0.0835, 0.0243, 0.0405],
        [0.0701, 0.0228, 0.0990,  ..., 0.0834, 0.0243, 0.0405],
        [0.0700, 0.0227, 0.0990,  ..., 0.0834, 0.0243, 0.0405]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652405.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8769.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.8174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3951.3381, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1069.8468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1632],
        [ 0.1911],
        [ 0.2001],
        ...,
        [-4.1159],
        [-4.1079],
        [-4.1045]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346117.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369988.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369991.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0172,  0.0051,  ...,  0.0041,  0.0166,  0.0114],
        [-0.0007,  0.0078,  0.0018,  ...,  0.0026,  0.0072,  0.0035],
        [-0.0009,  0.0103,  0.0027,  ...,  0.0030,  0.0098,  0.0057],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2039.0760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8579, device='cuda:0')



h[100].sum tensor(-0.1819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3170, device='cuda:0')



h[200].sum tensor(-22.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0312, 0.0078,  ..., 0.0107, 0.0290, 0.0162],
        [0.0000, 0.0656, 0.0192,  ..., 0.0161, 0.0633, 0.0427],
        [0.0000, 0.0314, 0.0085,  ..., 0.0108, 0.0292, 0.0187],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58425.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0468, 0.0163, 0.0097,  ..., 0.1897, 0.0935, 0.1176],
        [0.0420, 0.0157, 0.0000,  ..., 0.2231, 0.1188, 0.1405],
        [0.0497, 0.0176, 0.0173,  ..., 0.1813, 0.0896, 0.1110],
        ...,
        [0.0700, 0.0229, 0.0990,  ..., 0.0835, 0.0243, 0.0408],
        [0.0700, 0.0228, 0.0990,  ..., 0.0834, 0.0243, 0.0407],
        [0.0700, 0.0228, 0.0990,  ..., 0.0834, 0.0243, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640462.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8955.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.6155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4238.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1038.4502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2466],
        [-0.1664],
        [-0.6177],
        ...,
        [-4.1209],
        [-4.1133],
        [-4.1108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-402390.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369991.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369991.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0084,  0.0020,  ...,  0.0027,  0.0079,  0.0040],
        [-0.0016,  0.0179,  0.0053,  ...,  0.0042,  0.0173,  0.0120],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2789.3682, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4925, device='cuda:0')



h[100].sum tensor(-0.3013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8708, device='cuda:0')



h[200].sum tensor(-22.5326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0375, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0640, 0.0187,  ..., 0.0158, 0.0617, 0.0414],
        [0.0000, 0.0312, 0.0078,  ..., 0.0107, 0.0290, 0.0162],
        [0.0000, 0.0274, 0.0071,  ..., 0.0102, 0.0252, 0.0153],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77212.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0155, 0.0000,  ..., 0.2226, 0.1182, 0.1402],
        [0.0474, 0.0164, 0.0103,  ..., 0.1890, 0.0929, 0.1170],
        [0.0540, 0.0185, 0.0301,  ..., 0.1577, 0.0736, 0.0943],
        ...,
        [0.0700, 0.0229, 0.0990,  ..., 0.0835, 0.0243, 0.0408],
        [0.0700, 0.0228, 0.0990,  ..., 0.0834, 0.0243, 0.0407],
        [0.0700, 0.0228, 0.0990,  ..., 0.0834, 0.0243, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(738975.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8440.6221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.0805, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.7588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3691.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1205.6781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0530],
        [-0.2109],
        [-0.8548],
        ...,
        [-4.1196],
        [-4.1112],
        [-4.1076]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322786.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369991.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 140.0 event: 700 loss: tensor(427.0093, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369994.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0033,  0.0356,  0.0115,  ...,  0.0069,  0.0350,  0.0268],
        [-0.0065,  0.0703,  0.0236,  ...,  0.0123,  0.0695,  0.0558],
        [-0.0033,  0.0358,  0.0116,  ...,  0.0070,  0.0352,  0.0270],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2003.0070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4601, device='cuda:0')



h[100].sum tensor(-0.1747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2589, device='cuda:0')



h[200].sum tensor(-22.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1770, 0.0580,  ..., 0.0333, 0.1742, 0.1359],
        [0.0000, 0.1656, 0.0540,  ..., 0.0315, 0.1627, 0.1262],
        [0.0000, 0.1803, 0.0591,  ..., 0.0338, 0.1773, 0.1385],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57921.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0176, 0.0000,  ..., 0.4789, 0.3412, 0.3142],
        [0.0039, 0.0174, 0.0000,  ..., 0.5061, 0.3630, 0.3328],
        [0.0054, 0.0173, 0.0000,  ..., 0.5082, 0.3634, 0.3343],
        ...,
        [0.0699, 0.0227, 0.0989,  ..., 0.0834, 0.0245, 0.0413],
        [0.0699, 0.0227, 0.0988,  ..., 0.0833, 0.0245, 0.0412],
        [0.0698, 0.0226, 0.0988,  ..., 0.0833, 0.0245, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637041.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8881.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.8896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4084.4580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1036.1044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2041],
        [ 0.1883],
        [ 0.1920],
        ...,
        [-4.1204],
        [-4.1129],
        [-4.1104]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-378078.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369994.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369994.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [-0.0008,  0.0094,  0.0024,  ...,  0.0029,  0.0089,  0.0049],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2097.2703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7546, device='cuda:0')



h[100].sum tensor(-0.1895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4480, device='cuda:0')



h[200].sum tensor(-22.6090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0491, 0.0134,  ..., 0.0135, 0.0469, 0.0289],
        [0.0000, 0.0108, 0.0019,  ..., 0.0076, 0.0087, 0.0037],
        [0.0000, 0.0124, 0.0024,  ..., 0.0078, 0.0103, 0.0050],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59628.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0484, 0.0163, 0.0106,  ..., 0.1779, 0.0851, 0.1103],
        [0.0581, 0.0188, 0.0450,  ..., 0.1288, 0.0531, 0.0749],
        [0.0600, 0.0197, 0.0512,  ..., 0.1235, 0.0508, 0.0707],
        ...,
        [0.0699, 0.0227, 0.0989,  ..., 0.0834, 0.0245, 0.0413],
        [0.0699, 0.0227, 0.0988,  ..., 0.0833, 0.0245, 0.0412],
        [0.0698, 0.0226, 0.0988,  ..., 0.0833, 0.0245, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643425.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8902.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3745, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.7380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4108.1260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1048.1312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7561],
        [-2.1926],
        [-2.3819],
        ...,
        [-4.1204],
        [-4.1129],
        [-4.1104]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-381823.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0275],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369994.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0274],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369998.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [-0.0021,  0.0229,  0.0071,  ...,  0.0050,  0.0223,  0.0161],
        [-0.0031,  0.0338,  0.0109,  ...,  0.0067,  0.0331,  0.0253],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2320.6230, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8992, device='cuda:0')



h[100].sum tensor(-0.2228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9074, device='cuda:0')



h[200].sum tensor(-22.5840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0462, 0.0130,  ..., 0.0130, 0.0440, 0.0287],
        [0.0000, 0.0647, 0.0194,  ..., 0.0159, 0.0624, 0.0441],
        [0.0000, 0.0973, 0.0301,  ..., 0.0209, 0.0948, 0.0690],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0017, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0017, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63972.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0272, 0.0149, 0.0000,  ..., 0.3180, 0.2016, 0.2074],
        [0.0208, 0.0149, 0.0000,  ..., 0.3628, 0.2387, 0.2380],
        [0.0126, 0.0145, 0.0000,  ..., 0.4187, 0.2835, 0.2764],
        ...,
        [0.0697, 0.0223, 0.0985,  ..., 0.0833, 0.0247, 0.0420],
        [0.0696, 0.0223, 0.0985,  ..., 0.0832, 0.0247, 0.0419],
        [0.0696, 0.0223, 0.0984,  ..., 0.0832, 0.0247, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659780.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8644.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7855, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.4980, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3789.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1090.5714, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0179],
        [-0.0132],
        [-0.0113],
        ...,
        [-4.1098],
        [-4.1017],
        [-4.0980]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332526.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0274],
        [1.0356],
        ...,
        [0.9996],
        [0.9985],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369998.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370001.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [-0.0009,  0.0101,  0.0026,  ...,  0.0030,  0.0095,  0.0054],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2106.9805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0079, device='cuda:0')



h[100].sum tensor(-0.1878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4850, device='cuda:0')



h[200].sum tensor(-22.6075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0064, 0.0017, 0.0000],
        [0.0000, 0.0131, 0.0026,  ..., 0.0079, 0.0111, 0.0055],
        [0.0000, 0.0553, 0.0167,  ..., 0.0144, 0.0530, 0.0384],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60558.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0605, 0.0193, 0.0557,  ..., 0.1163, 0.0490, 0.0667],
        [0.0518, 0.0177, 0.0294,  ..., 0.1654, 0.0838, 0.1019],
        [0.0374, 0.0161, 0.0029,  ..., 0.2557, 0.1537, 0.1652],
        ...,
        [0.0695, 0.0221, 0.0982,  ..., 0.0833, 0.0249, 0.0425],
        [0.0694, 0.0221, 0.0981,  ..., 0.0832, 0.0248, 0.0425],
        [0.0694, 0.0220, 0.0981,  ..., 0.0832, 0.0248, 0.0425]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647829.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8680.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.7865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3818.2319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1058.6334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7045],
        [-0.1680],
        [ 0.1171],
        ...,
        [-4.1068],
        [-4.0994],
        [-4.0971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344404.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370001.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370004.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1752.4614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0059, device='cuda:0')



h[100].sum tensor(-0.1325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7542, device='cuda:0')



h[200].sum tensor(-22.6462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0170, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0464, 0.0136,  ..., 0.0130, 0.0442, 0.0311],
        [0.0000, 0.0037, 0.0000,  ..., 0.0065, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0065, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0018, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0066, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54766.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0492, 0.0178, 0.0184,  ..., 0.1849, 0.1025, 0.1148],
        [0.0614, 0.0199, 0.0567,  ..., 0.1153, 0.0501, 0.0655],
        [0.0656, 0.0208, 0.0816,  ..., 0.0939, 0.0338, 0.0504],
        ...,
        [0.0695, 0.0220, 0.0981,  ..., 0.0833, 0.0248, 0.0427],
        [0.0694, 0.0220, 0.0981,  ..., 0.0833, 0.0248, 0.0427],
        [0.0694, 0.0220, 0.0981,  ..., 0.0833, 0.0248, 0.0427]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623976.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8763.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.8867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.2867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3844.7190, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1010.5275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7037],
        [-1.7781],
        [-2.7436],
        ...,
        [-4.1120],
        [-4.1046],
        [-4.1024]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-341674.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370004.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370007.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0188,  0.0056,  ...,  0.0043,  0.0183,  0.0128],
        [-0.0034,  0.0371,  0.0120,  ...,  0.0072,  0.0364,  0.0281],
        [-0.0049,  0.0534,  0.0177,  ...,  0.0097,  0.0527,  0.0417],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1835.4323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4390, device='cuda:0')



h[100].sum tensor(-0.1454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9636, device='cuda:0')



h[200].sum tensor(-22.6358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1238, 0.0393,  ..., 0.0250, 0.1212, 0.0913],
        [0.0000, 0.1749, 0.0571,  ..., 0.0329, 0.1721, 0.1340],
        [0.0000, 0.2574, 0.0859,  ..., 0.0457, 0.2541, 0.2030],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0066, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55324.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0075, 0.0148, 0.0000,  ..., 0.5125, 0.3672, 0.3417],
        [0.0015, 0.0172, 0.0000,  ..., 0.6891, 0.5237, 0.4610],
        [0.0000, 0.0202, 0.0000,  ..., 0.8752, 0.6891, 0.5866],
        ...,
        [0.0699, 0.0221, 0.0986,  ..., 0.0835, 0.0246, 0.0424],
        [0.0699, 0.0221, 0.0986,  ..., 0.0834, 0.0246, 0.0423],
        [0.0699, 0.0220, 0.0985,  ..., 0.0834, 0.0246, 0.0423]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625675.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8972.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.9496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.1754, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4079.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1011.2925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0802],
        [-0.0058],
        [-0.0831],
        ...,
        [-4.1377],
        [-4.1303],
        [-4.1280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-384957.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370007.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370009.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2107.4617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4258, device='cuda:0')



h[100].sum tensor(-0.1852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5461, device='cuda:0')



h[200].sum tensor(-22.6053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0140, 0.0030,  ..., 0.0081, 0.0120, 0.0065],
        [0.0000, 0.0036, 0.0000,  ..., 0.0065, 0.0016, 0.0000],
        [0.0000, 0.0165, 0.0039,  ..., 0.0085, 0.0145, 0.0085],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0066, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61409.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0620, 0.0194, 0.0641,  ..., 0.1108, 0.0421, 0.0623],
        [0.0641, 0.0201, 0.0738,  ..., 0.1021, 0.0371, 0.0558],
        [0.0602, 0.0194, 0.0496,  ..., 0.1277, 0.0551, 0.0739],
        ...,
        [0.0703, 0.0224, 0.0991,  ..., 0.0837, 0.0244, 0.0418],
        [0.0703, 0.0223, 0.0991,  ..., 0.0836, 0.0244, 0.0418],
        [0.0702, 0.0223, 0.0991,  ..., 0.0836, 0.0244, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657853.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8845.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.7244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3980.2739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1068.6549, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6288],
        [-2.3468],
        [-1.7443],
        ...,
        [-4.1647],
        [-4.1571],
        [-4.1548]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-376388.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0264],
        [1.0274],
        [1.0356],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370009.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0265],
        [1.0274],
        [1.0357],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370011.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0076,  0.0017,  ...,  0.0026,  0.0070,  0.0034],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2541.2017, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0812, device='cuda:0')



h[100].sum tensor(-0.2487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3724, device='cuda:0')



h[200].sum tensor(-22.5565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0274, 0.0071,  ..., 0.0101, 0.0253, 0.0155],
        [0.0000, 0.0473, 0.0134,  ..., 0.0132, 0.0451, 0.0298],
        [0.0000, 0.0768, 0.0237,  ..., 0.0178, 0.0744, 0.0544],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0066, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0066, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0066, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69188.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0385, 0.0157, 0.0000,  ..., 0.2559, 0.1490, 0.1635],
        [0.0305, 0.0153, 0.0000,  ..., 0.3137, 0.1960, 0.2030],
        [0.0223, 0.0157, 0.0000,  ..., 0.3780, 0.2506, 0.2465],
        ...,
        [0.0709, 0.0226, 0.0999,  ..., 0.0838, 0.0243, 0.0412],
        [0.0709, 0.0225, 0.0998,  ..., 0.0838, 0.0243, 0.0412],
        [0.0709, 0.0225, 0.0998,  ..., 0.0838, 0.0243, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691252.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8850.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7508, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3983.2241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1137.0338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1096],
        [ 0.0954],
        [ 0.0790],
        ...,
        [-4.1895],
        [-4.1815],
        [-4.1790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-376485.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0265],
        [1.0274],
        [1.0357],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370011.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0265],
        [1.0274],
        [1.0357],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370014.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1875.6772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1191, device='cuda:0')



h[100].sum tensor(-0.1490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0629, device='cuda:0')



h[200].sum tensor(-22.6298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0143, 0.0026,  ..., 0.0081, 0.0123, 0.0046],
        [0.0000, 0.0089, 0.0013,  ..., 0.0073, 0.0069, 0.0023],
        [0.0000, 0.0034, 0.0000,  ..., 0.0065, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0066, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0066, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0066, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57295.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0597, 0.0180, 0.0626,  ..., 0.1147, 0.0382, 0.0662],
        [0.0626, 0.0191, 0.0730,  ..., 0.1051, 0.0340, 0.0585],
        [0.0661, 0.0205, 0.0852,  ..., 0.0939, 0.0291, 0.0495],
        ...,
        [0.0710, 0.0224, 0.0999,  ..., 0.0839, 0.0242, 0.0412],
        [0.0710, 0.0224, 0.0998,  ..., 0.0839, 0.0242, 0.0412],
        [0.0710, 0.0224, 0.0998,  ..., 0.0839, 0.0242, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639386.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9019.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4053.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1037.4630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1880],
        [-3.3236],
        [-3.5170],
        ...,
        [-4.2072],
        [-4.1993],
        [-4.1970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-371223.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0265],
        [1.0274],
        [1.0357],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370014.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0357],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370017.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0093,  0.0023,  ...,  0.0029,  0.0088,  0.0048],
        [-0.0008,  0.0091,  0.0023,  ...,  0.0029,  0.0085,  0.0047],
        [-0.0015,  0.0175,  0.0052,  ...,  0.0042,  0.0169,  0.0117],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0016,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2218.7251, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6664, device='cuda:0')



h[100].sum tensor(-0.1950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7273, device='cuda:0')



h[200].sum tensor(-22.5936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0316, 0.0086,  ..., 0.0108, 0.0294, 0.0190],
        [0.0000, 0.0620, 0.0180,  ..., 0.0156, 0.0597, 0.0398],
        [0.0000, 0.0318, 0.0080,  ..., 0.0109, 0.0296, 0.0168],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0067, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62252.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0477, 0.0157, 0.0129,  ..., 0.1932, 0.0983, 0.1206],
        [0.0391, 0.0137, 0.0000,  ..., 0.2433, 0.1332, 0.1563],
        [0.0418, 0.0144, 0.0000,  ..., 0.2279, 0.1209, 0.1458],
        ...,
        [0.0707, 0.0221, 0.0994,  ..., 0.0840, 0.0241, 0.0415],
        [0.0707, 0.0221, 0.0994,  ..., 0.0840, 0.0241, 0.0415],
        [0.0707, 0.0221, 0.0994,  ..., 0.0840, 0.0241, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654121.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8859.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.1454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3931.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1082.1608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1857],
        [ 0.2569],
        [ 0.2864],
        ...,
        [-4.1941],
        [-4.1866],
        [-4.1847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-355136.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0357],
        ...,
        [0.9995],
        [0.9985],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370017.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 150.0 event: 750 loss: tensor(411.7148, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0357],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370020.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2137.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6807, device='cuda:0')



h[100].sum tensor(-0.1800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5833, device='cuda:0')



h[200].sum tensor(-22.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0065, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0065, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0065, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61495.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0681, 0.0205, 0.0963,  ..., 0.0808, 0.0232, 0.0399],
        [0.0681, 0.0205, 0.0959,  ..., 0.0819, 0.0235, 0.0408],
        [0.0678, 0.0204, 0.0943,  ..., 0.0843, 0.0242, 0.0429],
        ...,
        [0.0705, 0.0217, 0.0990,  ..., 0.0841, 0.0241, 0.0419],
        [0.0704, 0.0216, 0.0989,  ..., 0.0841, 0.0241, 0.0418],
        [0.0704, 0.0216, 0.0989,  ..., 0.0841, 0.0241, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658686., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8993.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5631, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.6817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4178.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1068.7697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2772],
        [-3.3190],
        [-3.3165],
        ...,
        [-4.1784],
        [-4.1731],
        [-4.1731]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-399005.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0357],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370020.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0358],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370023.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1889.9497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9535, device='cuda:0')



h[100].sum tensor(-0.1423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0387, device='cuda:0')



h[200].sum tensor(-22.6315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0013,  ..., 0.0074, 0.0069, 0.0023],
        [0.0000, 0.0145, 0.0026,  ..., 0.0082, 0.0124, 0.0046],
        [0.0000, 0.0091, 0.0013,  ..., 0.0074, 0.0070, 0.0023],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57882.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0594, 0.0169, 0.0617,  ..., 0.1142, 0.0395, 0.0663],
        [0.0573, 0.0161, 0.0524,  ..., 0.1239, 0.0441, 0.0738],
        [0.0604, 0.0173, 0.0643,  ..., 0.1133, 0.0386, 0.0655],
        ...,
        [0.0702, 0.0211, 0.0987,  ..., 0.0843, 0.0242, 0.0422],
        [0.0702, 0.0211, 0.0986,  ..., 0.0842, 0.0242, 0.0422],
        [0.0702, 0.0211, 0.0986,  ..., 0.0842, 0.0242, 0.0422]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642436.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8809.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2003, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.2605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3931.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1043.5128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9592],
        [-1.6642],
        [-1.5392],
        ...,
        [-4.1779],
        [-4.1704],
        [-4.1682]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-354832., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0358],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370023.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0358],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370026.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [-0.0016,  0.0185,  0.0055,  ...,  0.0043,  0.0179,  0.0125],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2494.6433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1906, device='cuda:0')



h[100].sum tensor(-0.2254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2422, device='cuda:0')



h[200].sum tensor(-22.5652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0134, 0.0028,  ..., 0.0080, 0.0113, 0.0059],
        [0.0000, 0.0571, 0.0168,  ..., 0.0148, 0.0548, 0.0379],
        [0.0000, 0.0846, 0.0263,  ..., 0.0190, 0.0821, 0.0608],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0067, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0067, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0067, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67305.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0497, 0.0149, 0.0281,  ..., 0.1806, 0.0940, 0.1123],
        [0.0308, 0.0121, 0.0022,  ..., 0.2993, 0.1875, 0.1950],
        [0.0157, 0.0110, 0.0000,  ..., 0.4007, 0.2711, 0.2649],
        ...,
        [0.0703, 0.0206, 0.0986,  ..., 0.0844, 0.0243, 0.0424],
        [0.0702, 0.0206, 0.0985,  ..., 0.0843, 0.0243, 0.0423],
        [0.0702, 0.0206, 0.0985,  ..., 0.0843, 0.0242, 0.0423]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674014.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8687.4863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1193, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.4098, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3806.8936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1125.4425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0853],
        [ 0.1761],
        [ 0.2148],
        ...,
        [-4.1792],
        [-4.1719],
        [-4.1701]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345814.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0358],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370026.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0359],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370029.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0126,  0.0035,  ...,  0.0034,  0.0121,  0.0076],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1898.1604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1810, device='cuda:0')



h[100].sum tensor(-0.1413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0720, device='cuda:0')



h[200].sum tensor(-22.6301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0420, 0.0121,  ..., 0.0124, 0.0397, 0.0275],
        [0.0000, 0.0156, 0.0036,  ..., 0.0084, 0.0135, 0.0078],
        [0.0000, 0.0036, 0.0000,  ..., 0.0065, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0067, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56846.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0404, 0.0124, 0.0111,  ..., 0.2392, 0.1410, 0.1525],
        [0.0545, 0.0154, 0.0351,  ..., 0.1570, 0.0773, 0.0947],
        [0.0636, 0.0178, 0.0692,  ..., 0.1074, 0.0401, 0.0595],
        ...,
        [0.0704, 0.0203, 0.0987,  ..., 0.0845, 0.0243, 0.0423],
        [0.0703, 0.0203, 0.0986,  ..., 0.0844, 0.0243, 0.0422],
        [0.0703, 0.0203, 0.0986,  ..., 0.0844, 0.0243, 0.0422]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630591.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8933.0557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.6106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3988.5088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1034.6820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1116],
        [-0.2286],
        [-0.8638],
        ...,
        [-4.1884],
        [-4.1808],
        [-4.1787]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370171.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0275],
        [1.0359],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370029.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0275],
        [1.0359],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370032.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [-0.0028,  0.0323,  0.0103,  ...,  0.0064,  0.0316,  0.0240],
        [-0.0033,  0.0377,  0.0122,  ...,  0.0073,  0.0370,  0.0285],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0004, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1947.7996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6837, device='cuda:0')



h[100].sum tensor(-0.1460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1454, device='cuda:0')



h[200].sum tensor(-22.6252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0357, 0.0106,  ..., 0.0115, 0.0334, 0.0246],
        [0.0000, 0.0747, 0.0235,  ..., 0.0175, 0.0722, 0.0548],
        [0.0000, 0.1726, 0.0563,  ..., 0.0327, 0.1696, 0.1320],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58457.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0472, 0.0153, 0.0227,  ..., 0.2064, 0.1219, 0.1278],
        [0.0303, 0.0125, 0.0000,  ..., 0.3135, 0.2077, 0.2025],
        [0.0105, 0.0097, 0.0000,  ..., 0.4430, 0.3125, 0.2924],
        ...,
        [0.0703, 0.0203, 0.0987,  ..., 0.0846, 0.0243, 0.0420],
        [0.0703, 0.0203, 0.0986,  ..., 0.0845, 0.0243, 0.0419],
        [0.0703, 0.0203, 0.0986,  ..., 0.0845, 0.0243, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643037.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8958.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.8774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4144.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1046.8939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1142],
        [ 0.2085],
        [ 0.3033],
        ...,
        [-4.1914],
        [-4.1838],
        [-4.1817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-402952.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0275],
        [1.0359],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370032.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0276],
        [1.0359],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370035.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2506.6738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3252, device='cuda:0')



h[100].sum tensor(-0.2181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2619, device='cuda:0')



h[200].sum tensor(-22.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0066, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0067, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0068, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0068, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0068, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65900.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0611, 0.0168, 0.0626,  ..., 0.1122, 0.0425, 0.0630],
        [0.0655, 0.0184, 0.0857,  ..., 0.0916, 0.0288, 0.0479],
        [0.0678, 0.0194, 0.0950,  ..., 0.0838, 0.0240, 0.0418],
        ...,
        [0.0701, 0.0204, 0.0986,  ..., 0.0847, 0.0242, 0.0418],
        [0.0701, 0.0204, 0.0986,  ..., 0.0846, 0.0242, 0.0417],
        [0.0700, 0.0204, 0.0986,  ..., 0.0846, 0.0242, 0.0417]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662427.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8655.4893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3818.7163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1119.1288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0382],
        [-1.8566],
        [-2.5569],
        ...,
        [-4.1724],
        [-4.1659],
        [-4.1654]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339349.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0276],
        [1.0359],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370035.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0276],
        [1.0360],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370038.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0016,  0.0003, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2052.9761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0176, device='cuda:0')



h[100].sum tensor(-0.1550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3403, device='cuda:0')



h[200].sum tensor(-22.6154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0067, 0.0013, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0067, 0.0013, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0068, 0.0013, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59304.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0677, 0.0195, 0.0961,  ..., 0.0814, 0.0233, 0.0395],
        [0.0672, 0.0194, 0.0911,  ..., 0.0864, 0.0270, 0.0430],
        [0.0665, 0.0194, 0.0846,  ..., 0.0931, 0.0319, 0.0476],
        ...,
        [0.0701, 0.0206, 0.0988,  ..., 0.0848, 0.0242, 0.0415],
        [0.0700, 0.0206, 0.0987,  ..., 0.0847, 0.0241, 0.0414],
        [0.0700, 0.0206, 0.0987,  ..., 0.0847, 0.0241, 0.0414]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639434.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8754.9014, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.9661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3924.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1063.8954, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6842],
        [-3.4578],
        [-2.9545],
        ...,
        [-4.1916],
        [-4.1841],
        [-4.1821]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349171.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0276],
        [1.0360],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370038.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0276],
        [1.0360],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370041.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0106,  0.0028,  ...,  0.0032,  0.0100,  0.0059],
        [-0.0020,  0.0238,  0.0074,  ...,  0.0052,  0.0231,  0.0169],
        [-0.0026,  0.0299,  0.0095,  ...,  0.0061,  0.0291,  0.0220],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3341.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6702, device='cuda:0')



h[100].sum tensor(-0.3226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7734, device='cuda:0')



h[200].sum tensor(-22.4756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0463, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0479, 0.0143,  ..., 0.0136, 0.0454, 0.0325],
        [0.0000, 0.1032, 0.0323,  ..., 0.0222, 0.1004, 0.0740],
        [0.0000, 0.1243, 0.0396,  ..., 0.0254, 0.1213, 0.0915],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0013, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0013, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84079.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0350, 0.0108, 0.0033,  ..., 0.2697, 0.1634, 0.1717],
        [0.0167, 0.0075, 0.0000,  ..., 0.3872, 0.2586, 0.2528],
        [0.0064, 0.0058, 0.0000,  ..., 0.4711, 0.3286, 0.3102],
        ...,
        [0.0701, 0.0208, 0.0990,  ..., 0.0848, 0.0241, 0.0411],
        [0.0701, 0.0207, 0.0989,  ..., 0.0847, 0.0241, 0.0411],
        [0.0701, 0.0207, 0.0989,  ..., 0.0847, 0.0241, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(760864.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8585.4941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.7913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.1442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3942.2280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1271.8856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2444],
        [ 0.1993],
        [ 0.1494],
        ...,
        [-4.1661],
        [-4.1762],
        [-4.1798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-369664.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0276],
        [1.0360],
        ...,
        [0.9995],
        [0.9984],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370041.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0277],
        [1.0361],
        ...,
        [0.9995],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370044.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024,  0.0283,  0.0089,  ...,  0.0059,  0.0275,  0.0206],
        [-0.0042,  0.0489,  0.0161,  ...,  0.0091,  0.0481,  0.0378],
        [-0.0023,  0.0269,  0.0085,  ...,  0.0057,  0.0262,  0.0194],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2024.1533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5429, device='cuda:0')



h[100].sum tensor(-0.1471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2709, device='cuda:0')



h[200].sum tensor(-22.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1410, 0.0454,  ..., 0.0281, 0.1379, 0.1054],
        [0.0000, 0.1461, 0.0472,  ..., 0.0289, 0.1429, 0.1096],
        [0.0000, 0.1546, 0.0501,  ..., 0.0302, 0.1514, 0.1167],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0013, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0013, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58535.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0160, 0.0081, 0.0000,  ..., 0.3956, 0.2715, 0.2575],
        [0.0077, 0.0059, 0.0000,  ..., 0.4505, 0.3135, 0.2959],
        [0.0059, 0.0054, 0.0000,  ..., 0.4618, 0.3208, 0.3043],
        ...,
        [0.0699, 0.0207, 0.0988,  ..., 0.0848, 0.0241, 0.0412],
        [0.0699, 0.0207, 0.0988,  ..., 0.0847, 0.0241, 0.0411],
        [0.0699, 0.0207, 0.0988,  ..., 0.0847, 0.0241, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636370.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8804.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.5998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4037.0637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1056.1599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1512],
        [ 0.2711],
        [ 0.2957],
        ...,
        [-4.1957],
        [-4.1883],
        [-4.1863]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-369301.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0277],
        [1.0361],
        ...,
        [0.9995],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370044.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0277],
        [1.0361],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370047.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0031,  0.0357,  0.0115,  ...,  0.0071,  0.0349,  0.0268],
        [-0.0060,  0.0686,  0.0230,  ...,  0.0122,  0.0677,  0.0543],
        [-0.0058,  0.0672,  0.0225,  ...,  0.0119,  0.0662,  0.0531],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2074.2451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9603, device='cuda:0')



h[100].sum tensor(-0.1504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3319, device='cuda:0')



h[200].sum tensor(-22.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1790, 0.0586,  ..., 0.0340, 0.1757, 0.1370],
        [0.0000, 0.2365, 0.0786,  ..., 0.0429, 0.2328, 0.1850],
        [0.0000, 0.2480, 0.0826,  ..., 0.0447, 0.2442, 0.1945],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0071, 0.0013, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0071, 0.0013, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0071, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60329.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0094, 0.0069, 0.0000,  ..., 0.5319, 0.3936, 0.3495],
        [0.0000, 0.0042, 0.0000,  ..., 0.6670, 0.5076, 0.4419],
        [0.0000, 0.0035, 0.0000,  ..., 0.6915, 0.5261, 0.4592],
        ...,
        [0.0696, 0.0207, 0.0985,  ..., 0.0848, 0.0241, 0.0413],
        [0.0696, 0.0207, 0.0984,  ..., 0.0848, 0.0241, 0.0413],
        [0.0695, 0.0207, 0.0984,  ..., 0.0847, 0.0241, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647120.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8641.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.4775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3902.8342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1073.3529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1638],
        [ 0.2089],
        [ 0.2238],
        ...,
        [-4.1817],
        [-4.1738],
        [-4.1691]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347267.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0277],
        [1.0361],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370047.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 160.0 event: 800 loss: tensor(482.7585, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0278],
        [1.0362],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370050.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0153,  0.0044,  ...,  0.0039,  0.0146,  0.0097],
        [-0.0030,  0.0355,  0.0114,  ...,  0.0070,  0.0347,  0.0266],
        [-0.0026,  0.0305,  0.0097,  ...,  0.0063,  0.0298,  0.0225],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2594.7661, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0314, device='cuda:0')



h[100].sum tensor(-0.2143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3651, device='cuda:0')



h[200].sum tensor(-22.5596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1024, 0.0319,  ..., 0.0222, 0.0995, 0.0731],
        [0.0000, 0.1020, 0.0318,  ..., 0.0222, 0.0991, 0.0727],
        [0.0000, 0.1064, 0.0333,  ..., 0.0229, 0.1035, 0.0763],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0071, 0.0014, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0071, 0.0014, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0071, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68596.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0282, 0.0086, 0.0000,  ..., 0.3061, 0.1952, 0.1968],
        [0.0237, 0.0075, 0.0000,  ..., 0.3342, 0.2167, 0.2165],
        [0.0241, 0.0077, 0.0000,  ..., 0.3340, 0.2158, 0.2164],
        ...,
        [0.0694, 0.0206, 0.0982,  ..., 0.0848, 0.0242, 0.0415],
        [0.0694, 0.0206, 0.0982,  ..., 0.0848, 0.0241, 0.0415],
        [0.0693, 0.0205, 0.0982,  ..., 0.0848, 0.0241, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683376.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8422.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.5080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3743.2429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1144.6379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3088],
        [ 0.3270],
        [ 0.3243],
        ...,
        [-4.1755],
        [-4.1721],
        [-4.1720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331444.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0278],
        [1.0362],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370050.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0278],
        [1.0362],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370053.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0202,  0.0061,  ...,  0.0047,  0.0196,  0.0139],
        [-0.0015,  0.0183,  0.0055,  ...,  0.0044,  0.0177,  0.0123],
        [-0.0017,  0.0199,  0.0060,  ...,  0.0046,  0.0192,  0.0136],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [-0.0019,  0.0226,  0.0070,  ...,  0.0050,  0.0219,  0.0159],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2421.4575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9021, device='cuda:0')



h[100].sum tensor(-0.1910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0540, device='cuda:0')



h[200].sum tensor(-22.5777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0720, 0.0213,  ..., 0.0175, 0.0693, 0.0477],
        [0.0000, 0.0740, 0.0220,  ..., 0.0178, 0.0713, 0.0493],
        [0.0000, 0.0719, 0.0213,  ..., 0.0175, 0.0693, 0.0476],
        ...,
        [0.0000, 0.0266, 0.0073,  ..., 0.0106, 0.0241, 0.0167],
        [0.0000, 0.0224, 0.0059,  ..., 0.0100, 0.0200, 0.0132],
        [0.0000, 0.0868, 0.0264,  ..., 0.0199, 0.0840, 0.0597]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65137.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0308, 0.0065, 0.0000,  ..., 0.2759, 0.1619, 0.1783],
        [0.0314, 0.0070, 0.0000,  ..., 0.2756, 0.1630, 0.1778],
        [0.0320, 0.0074, 0.0000,  ..., 0.2741, 0.1615, 0.1768],
        ...,
        [0.0589, 0.0171, 0.0437,  ..., 0.1441, 0.0688, 0.0833],
        [0.0557, 0.0160, 0.0252,  ..., 0.1611, 0.0811, 0.0954],
        [0.0460, 0.0130, 0.0000,  ..., 0.2156, 0.1221, 0.1338]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661463.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8414.4424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9259, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.3366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3578.5688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1118.3326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4037],
        [ 0.4060],
        [ 0.4053],
        ...,
        [-2.2610],
        [-1.9445],
        [-1.7811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296508.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0278],
        [1.0362],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370053.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0279],
        [1.0363],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370056.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2648.2065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8026, device='cuda:0')



h[100].sum tensor(-0.2184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4778, device='cuda:0')



h[200].sum tensor(-22.5522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0068, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0070, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69280.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0629, 0.0170, 0.0740,  ..., 0.1016, 0.0356, 0.0549],
        [0.0655, 0.0180, 0.0867,  ..., 0.0906, 0.0282, 0.0468],
        [0.0668, 0.0186, 0.0914,  ..., 0.0870, 0.0259, 0.0441],
        ...,
        [0.0697, 0.0199, 0.0983,  ..., 0.0847, 0.0243, 0.0418],
        [0.0697, 0.0199, 0.0982,  ..., 0.0847, 0.0243, 0.0417],
        [0.0697, 0.0199, 0.0982,  ..., 0.0847, 0.0243, 0.0417]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678653.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8385.3623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3224, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.3888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3491.8882, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1152.8701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2165],
        [-1.8780],
        [-2.4513],
        ...,
        [-4.2097],
        [-4.2023],
        [-4.2002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302762.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0279],
        [1.0363],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370056.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0279],
        [1.0363],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370059.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [-0.0015,  0.0181,  0.0054,  ...,  0.0043,  0.0174,  0.0121],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2539.0623, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4879, device='cuda:0')



h[100].sum tensor(-0.2033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2857, device='cuda:0')



h[200].sum tensor(-22.5635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0068, 0.0014, 0.0000],
        [0.0000, 0.0357, 0.0099,  ..., 0.0118, 0.0333, 0.0221],
        [0.0000, 0.0481, 0.0136,  ..., 0.0137, 0.0456, 0.0301],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0275, 0.0070,  ..., 0.0107, 0.0250, 0.0151]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66160.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0605, 0.0164, 0.0531,  ..., 0.1202, 0.0513, 0.0674],
        [0.0477, 0.0122, 0.0211,  ..., 0.1914, 0.1034, 0.1179],
        [0.0367, 0.0086, 0.0035,  ..., 0.2523, 0.1464, 0.1617],
        ...,
        [0.0676, 0.0191, 0.0859,  ..., 0.0965, 0.0318, 0.0502],
        [0.0626, 0.0171, 0.0603,  ..., 0.1202, 0.0471, 0.0677],
        [0.0516, 0.0130, 0.0230,  ..., 0.1756, 0.0846, 0.1078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665055.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8601.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.3065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3618.7393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1122.7601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0498],
        [-0.3553],
        [ 0.1086],
        ...,
        [-3.0383],
        [-2.0475],
        [-1.0405]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318171.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0279],
        [1.0363],
        ...,
        [0.9994],
        [0.9984],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370059.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0280],
        [1.0364],
        ...,
        [0.9994],
        [0.9983],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370062.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0165,  0.0048,  ...,  0.0041,  0.0159,  0.0108],
        [-0.0036,  0.0426,  0.0139,  ...,  0.0081,  0.0418,  0.0325],
        [-0.0006,  0.0079,  0.0018,  ...,  0.0027,  0.0073,  0.0036],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2032.0076, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7372, device='cuda:0')



h[100].sum tensor(-0.1402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2993, device='cuda:0')



h[200].sum tensor(-22.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1551, 0.0502,  ..., 0.0302, 0.1520, 0.1171],
        [0.0000, 0.0802, 0.0242,  ..., 0.0187, 0.0776, 0.0546],
        [0.0000, 0.0891, 0.0273,  ..., 0.0201, 0.0864, 0.0620],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57552.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0084, 0.0040, 0.0000,  ..., 0.4270, 0.2933, 0.2820],
        [0.0194, 0.0051, 0.0000,  ..., 0.3548, 0.2291, 0.2328],
        [0.0272, 0.0070, 0.0000,  ..., 0.3117, 0.1942, 0.2027],
        ...,
        [0.0704, 0.0202, 0.0990,  ..., 0.0847, 0.0242, 0.0413],
        [0.0704, 0.0202, 0.0989,  ..., 0.0846, 0.0242, 0.0413],
        [0.0704, 0.0202, 0.0989,  ..., 0.0846, 0.0242, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632431.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8999.6748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.5657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4001.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1042.6846, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2962],
        [ 0.2995],
        [ 0.2744],
        ...,
        [-4.2589],
        [-4.2515],
        [-4.2496]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-397575.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0280],
        [1.0364],
        ...,
        [0.9994],
        [0.9983],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370062.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0280],
        [1.0364],
        ...,
        [0.9994],
        [0.9983],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370065.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0146,  0.0042,  ...,  0.0038,  0.0140,  0.0092],
        [-0.0024,  0.0285,  0.0090,  ...,  0.0059,  0.0278,  0.0208],
        [-0.0012,  0.0147,  0.0042,  ...,  0.0038,  0.0141,  0.0093],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2667.7651, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.5406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0974, device='cuda:0')



h[100].sum tensor(-0.2146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5208, device='cuda:0')



h[200].sum tensor(-22.5497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0967, 0.0306,  ..., 0.0212, 0.0940, 0.0707],
        [0.0000, 0.0970, 0.0300,  ..., 0.0213, 0.0942, 0.0686],
        [0.0000, 0.0963, 0.0304,  ..., 0.0212, 0.0935, 0.0703],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71338.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0057, 0.0000,  ..., 0.4677, 0.3331, 0.3082],
        [0.0062, 0.0042, 0.0000,  ..., 0.5071, 0.3637, 0.3358],
        [0.0075, 0.0052, 0.0000,  ..., 0.4915, 0.3518, 0.3247],
        ...,
        [0.0706, 0.0204, 0.0990,  ..., 0.0846, 0.0241, 0.0413],
        [0.0705, 0.0203, 0.0990,  ..., 0.0845, 0.0241, 0.0412],
        [0.0705, 0.0203, 0.0990,  ..., 0.0845, 0.0241, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698499.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8705.9111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5214, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.2467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3716.2236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1164.2266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0773],
        [ 0.0617],
        [ 0.0593],
        ...,
        [-4.2698],
        [-4.2625],
        [-4.2606]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-354552.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0280],
        [1.0364],
        ...,
        [0.9994],
        [0.9983],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370065.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0281],
        [1.0365],
        ...,
        [0.9994],
        [0.9983],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370069.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2116.2588, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6688, device='cuda:0')



h[100].sum tensor(-0.1458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4355, device='cuda:0')



h[200].sum tensor(-22.6100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0071, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0071, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0071, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60532.8633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0669, 0.0187, 0.0923,  ..., 0.0849, 0.0246, 0.0426],
        [0.0681, 0.0193, 0.0958,  ..., 0.0822, 0.0233, 0.0403],
        [0.0687, 0.0197, 0.0967,  ..., 0.0822, 0.0233, 0.0400],
        ...,
        [0.0704, 0.0204, 0.0988,  ..., 0.0846, 0.0239, 0.0414],
        [0.0704, 0.0204, 0.0987,  ..., 0.0845, 0.0239, 0.0414],
        [0.0704, 0.0204, 0.0987,  ..., 0.0845, 0.0239, 0.0414]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649926.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8791.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.3531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3717.7192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1072.7548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6895],
        [-3.2728],
        [-3.7071],
        ...,
        [-4.2707],
        [-4.2635],
        [-4.2616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350134.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0281],
        [1.0365],
        ...,
        [0.9994],
        [0.9983],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370069.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0281],
        [1.0366],
        ...,
        [0.9994],
        [0.9983],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370073., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2018.3809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0616, device='cuda:0')



h[100].sum tensor(-0.1317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2006, device='cuda:0')



h[200].sum tensor(-22.6215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0069, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0070, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0071, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0071, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0071, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58040.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0679, 0.0192, 0.0958,  ..., 0.0812, 0.0230, 0.0396],
        [0.0682, 0.0193, 0.0961,  ..., 0.0816, 0.0231, 0.0398],
        [0.0685, 0.0196, 0.0964,  ..., 0.0822, 0.0232, 0.0402],
        ...,
        [0.0702, 0.0204, 0.0984,  ..., 0.0845, 0.0239, 0.0416],
        [0.0702, 0.0204, 0.0984,  ..., 0.0845, 0.0238, 0.0415],
        [0.0702, 0.0203, 0.0984,  ..., 0.0845, 0.0238, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637286.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8835.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2188, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.4042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3753.2622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1048.9099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0579],
        [-4.1972],
        [-4.3106],
        ...,
        [-4.2671],
        [-4.2600],
        [-4.2583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-362231.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0281],
        [1.0366],
        ...,
        [0.9994],
        [0.9983],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370073., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0282],
        [1.0367],
        ...,
        [0.9993],
        [0.9983],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370076.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0204,  0.0062,  ...,  0.0047,  0.0198,  0.0140],
        [-0.0028,  0.0337,  0.0108,  ...,  0.0068,  0.0330,  0.0251],
        [-0.0019,  0.0233,  0.0072,  ...,  0.0052,  0.0226,  0.0164],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3669.0378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2860, device='cuda:0')



h[100].sum tensor(-0.3206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4478, device='cuda:0')



h[200].sum tensor(-22.4455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0588, 0.0174,  ..., 0.0155, 0.0564, 0.0390],
        [0.0000, 0.0848, 0.0258,  ..., 0.0196, 0.0822, 0.0582],
        [0.0000, 0.1596, 0.0518,  ..., 0.0312, 0.1566, 0.1205],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94259.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0250, 0.0087, 0.0000,  ..., 0.3117, 0.1937, 0.2033],
        [0.0124, 0.0069, 0.0000,  ..., 0.4040, 0.2704, 0.2669],
        [0.0023, 0.0059, 0.0000,  ..., 0.5338, 0.3832, 0.3553],
        ...,
        [0.0700, 0.0203, 0.0980,  ..., 0.0846, 0.0238, 0.0418],
        [0.0700, 0.0203, 0.0980,  ..., 0.0845, 0.0238, 0.0417],
        [0.0700, 0.0203, 0.0980,  ..., 0.0845, 0.0238, 0.0417]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(843143.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8111.1963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.7465, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.5938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3254.4060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1364.4194, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2264],
        [ 0.1647],
        [ 0.1016],
        ...,
        [-4.2621],
        [-4.2551],
        [-4.2535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302478.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0282],
        [1.0367],
        ...,
        [0.9993],
        [0.9983],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370076.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0283],
        [1.0368],
        ...,
        [0.9993],
        [0.9982],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370080.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0180,  0.0054,  ...,  0.0044,  0.0174,  0.0120],
        [-0.0007,  0.0087,  0.0021,  ...,  0.0029,  0.0081,  0.0042],
        [-0.0025,  0.0307,  0.0098,  ...,  0.0063,  0.0300,  0.0225],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2577.5376, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2969, device='cuda:0')



h[100].sum tensor(-0.1912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2578, device='cuda:0')



h[200].sum tensor(-22.5634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0429, 0.0112,  ..., 0.0131, 0.0406, 0.0233],
        [0.0000, 0.0741, 0.0220,  ..., 0.0180, 0.0716, 0.0492],
        [0.0000, 0.0433, 0.0119,  ..., 0.0132, 0.0410, 0.0259],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70790.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0302, 0.0074, 0.0000,  ..., 0.2690, 0.1530, 0.1757],
        [0.0311, 0.0077, 0.0000,  ..., 0.2683, 0.1534, 0.1747],
        [0.0372, 0.0094, 0.0000,  ..., 0.2381, 0.1309, 0.1532],
        ...,
        [0.0698, 0.0201, 0.0977,  ..., 0.0847, 0.0238, 0.0420],
        [0.0698, 0.0201, 0.0976,  ..., 0.0847, 0.0238, 0.0420],
        [0.0698, 0.0201, 0.0976,  ..., 0.0847, 0.0238, 0.0420]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(705128.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8324.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.7658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3283.8960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1164.8300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3279],
        [ 0.3559],
        [ 0.3691],
        ...,
        [-4.2576],
        [-4.2507],
        [-4.2492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294514.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0283],
        [1.0368],
        ...,
        [0.9993],
        [0.9982],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370080.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 170.0 event: 850 loss: tensor(517.0771, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0284],
        [1.0369],
        ...,
        [0.9993],
        [0.9982],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370083.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0136,  0.0038,  ...,  0.0037,  0.0130,  0.0083],
        [-0.0011,  0.0139,  0.0039,  ...,  0.0037,  0.0133,  0.0085],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2536.5640, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9632, device='cuda:0')



h[100].sum tensor(-0.1843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2090, device='cuda:0')



h[200].sum tensor(-22.5680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0756, 0.0225,  ..., 0.0182, 0.0730, 0.0505],
        [0.0000, 0.0275, 0.0071,  ..., 0.0108, 0.0253, 0.0152],
        [0.0000, 0.0170, 0.0040,  ..., 0.0092, 0.0148, 0.0087],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0072, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69214.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0381, 0.0104, 0.0000,  ..., 0.2415, 0.1397, 0.1534],
        [0.0508, 0.0137, 0.0247,  ..., 0.1721, 0.0867, 0.1046],
        [0.0591, 0.0164, 0.0494,  ..., 0.1301, 0.0562, 0.0746],
        ...,
        [0.0698, 0.0201, 0.0978,  ..., 0.0849, 0.0238, 0.0419],
        [0.0698, 0.0201, 0.0977,  ..., 0.0848, 0.0238, 0.0419],
        [0.0698, 0.0201, 0.0977,  ..., 0.0848, 0.0238, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698972.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8453.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.7953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3445.9197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1147.4009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5949],
        [-1.5238],
        [-2.5074],
        ...,
        [-4.2678],
        [-4.2610],
        [-4.2596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322123.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0284],
        [1.0369],
        ...,
        [0.9993],
        [0.9982],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370083.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0270],
        [1.0284],
        [1.0370],
        ...,
        [0.9992],
        [0.9981],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370086.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [-0.0033,  0.0398,  0.0129,  ...,  0.0077,  0.0390,  0.0301],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0017,  0.0004, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2487.6726, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3524, device='cuda:0')



h[100].sum tensor(-0.1783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1198, device='cuda:0')



h[200].sum tensor(-22.5719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0940, 0.0290,  ..., 0.0211, 0.0913, 0.0658],
        [0.0000, 0.0362, 0.0101,  ..., 0.0121, 0.0339, 0.0225],
        [0.0000, 0.0983, 0.0304,  ..., 0.0218, 0.0957, 0.0694],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0072, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0072, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0072, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67742.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0393, 0.0107, 0.0000,  ..., 0.2431, 0.1436, 0.1529],
        [0.0450, 0.0121, 0.0000,  ..., 0.2119, 0.1191, 0.1312],
        [0.0391, 0.0109, 0.0000,  ..., 0.2498, 0.1487, 0.1574],
        ...,
        [0.0703, 0.0203, 0.0984,  ..., 0.0849, 0.0237, 0.0415],
        [0.0702, 0.0203, 0.0983,  ..., 0.0848, 0.0237, 0.0415],
        [0.0702, 0.0203, 0.0983,  ..., 0.0848, 0.0237, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684194.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8648.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.0756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3631.0322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1132.6532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9996],
        [-1.0218],
        [-1.1434],
        ...,
        [-4.2685],
        [-4.2626],
        [-4.2647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-357249.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0270],
        [1.0284],
        [1.0370],
        ...,
        [0.9992],
        [0.9981],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370086.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0270],
        [1.0285],
        [1.0370],
        ...,
        [0.9992],
        [0.9981],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370089.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0109,  0.0029,  ...,  0.0033,  0.0103,  0.0060],
        [-0.0025,  0.0308,  0.0098,  ...,  0.0064,  0.0301,  0.0226],
        [-0.0031,  0.0377,  0.0122,  ...,  0.0074,  0.0370,  0.0284],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2236.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1519, device='cuda:0')



h[100].sum tensor(-0.1498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6522, device='cuda:0')



h[200].sum tensor(-22.5976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0479, 0.0142,  ..., 0.0139, 0.0456, 0.0323],
        [0.0000, 0.1087, 0.0341,  ..., 0.0234, 0.1060, 0.0781],
        [0.0000, 0.1783, 0.0582,  ..., 0.0342, 0.1751, 0.1360],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0072, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0072, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0072, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62594.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0343, 0.0097, 0.0025,  ..., 0.2771, 0.1700, 0.1760],
        [0.0156, 0.0057, 0.0000,  ..., 0.4098, 0.2784, 0.2668],
        [0.0036, 0.0028, 0.0000,  ..., 0.5545, 0.4009, 0.3651],
        ...,
        [0.0706, 0.0205, 0.0989,  ..., 0.0849, 0.0237, 0.0412],
        [0.0705, 0.0204, 0.0988,  ..., 0.0848, 0.0237, 0.0412],
        [0.0705, 0.0204, 0.0988,  ..., 0.0848, 0.0237, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659391.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8725.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.8485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3569.8354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1092.1884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2816],
        [ 0.2342],
        [ 0.1844],
        ...,
        [-4.3216],
        [-4.3146],
        [-4.3135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336903.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0270],
        [1.0285],
        [1.0370],
        ...,
        [0.9992],
        [0.9981],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370089.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0271],
        [1.0286],
        [1.0371],
        ...,
        [0.9991],
        [0.9980],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370092., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0161,  0.0047,  ...,  0.0041,  0.0155,  0.0104],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2178.7363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6714, device='cuda:0')



h[100].sum tensor(-0.1425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5819, device='cuda:0')



h[200].sum tensor(-22.6032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0350, 0.0097,  ..., 0.0119, 0.0328, 0.0216],
        [0.0000, 0.0191, 0.0048,  ..., 0.0095, 0.0170, 0.0107],
        [0.0000, 0.0034, 0.0000,  ..., 0.0071, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0072, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0072, 0.0014, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0072, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60828.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0481, 0.0128, 0.0172,  ..., 0.1962, 0.1066, 0.1196],
        [0.0566, 0.0152, 0.0386,  ..., 0.1468, 0.0678, 0.0855],
        [0.0633, 0.0173, 0.0698,  ..., 0.1087, 0.0382, 0.0592],
        ...,
        [0.0708, 0.0205, 0.0992,  ..., 0.0848, 0.0237, 0.0411],
        [0.0708, 0.0205, 0.0992,  ..., 0.0848, 0.0237, 0.0410],
        [0.0708, 0.0205, 0.0992,  ..., 0.0848, 0.0237, 0.0410]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(651884.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8958.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4763, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.9895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3799.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1070.0013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7070],
        [-1.3996],
        [-2.0311],
        ...,
        [-4.3410],
        [-4.3340],
        [-4.3330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-387391., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0271],
        [1.0286],
        [1.0371],
        ...,
        [0.9991],
        [0.9980],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370092., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0271],
        [1.0286],
        [1.0372],
        ...,
        [0.9990],
        [0.9980],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370095.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0174,  0.0052,  ...,  0.0043,  0.0168,  0.0115],
        [-0.0006,  0.0081,  0.0019,  ...,  0.0028,  0.0075,  0.0037],
        [-0.0008,  0.0102,  0.0026,  ...,  0.0032,  0.0097,  0.0055],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2079.9736, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1479, device='cuda:0')



h[100].sum tensor(-0.1298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3593, device='cuda:0')



h[200].sum tensor(-22.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0315, 0.0079,  ..., 0.0114, 0.0293, 0.0162],
        [0.0000, 0.0656, 0.0191,  ..., 0.0167, 0.0632, 0.0422],
        [0.0000, 0.0317, 0.0086,  ..., 0.0115, 0.0295, 0.0188],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59039.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0464, 0.0110, 0.0097,  ..., 0.1924, 0.0944, 0.1189],
        [0.0407, 0.0092, 0.0000,  ..., 0.2272, 0.1199, 0.1430],
        [0.0485, 0.0121, 0.0161,  ..., 0.1864, 0.0909, 0.1145],
        ...,
        [0.0707, 0.0207, 0.0993,  ..., 0.0848, 0.0237, 0.0410],
        [0.0707, 0.0207, 0.0993,  ..., 0.0848, 0.0237, 0.0410],
        [0.0707, 0.0207, 0.0993,  ..., 0.0848, 0.0237, 0.0410]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647006.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8995.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.7040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3903.3911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1053.1812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3148],
        [-0.0194],
        [-0.2187],
        ...,
        [-4.3489],
        [-4.3419],
        [-4.3410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-406820.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0271],
        [1.0286],
        [1.0372],
        ...,
        [0.9990],
        [0.9980],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370095.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0286],
        [1.0372],
        ...,
        [0.9990],
        [0.9979],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370098.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0091,  0.0023,  ...,  0.0030,  0.0086,  0.0046],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2182.7334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4442, device='cuda:0')



h[100].sum tensor(-0.1385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5487, device='cuda:0')



h[200].sum tensor(-22.6043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0247, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0322, 0.0088,  ..., 0.0116, 0.0301, 0.0192],
        [0.0000, 0.0119, 0.0023,  ..., 0.0085, 0.0099, 0.0047],
        [0.0000, 0.0034, 0.0000,  ..., 0.0072, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61372.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0475, 0.0116, 0.0148,  ..., 0.1879, 0.0931, 0.1156],
        [0.0567, 0.0149, 0.0399,  ..., 0.1396, 0.0596, 0.0816],
        [0.0639, 0.0179, 0.0731,  ..., 0.1051, 0.0368, 0.0569],
        ...,
        [0.0706, 0.0207, 0.0992,  ..., 0.0848, 0.0238, 0.0412],
        [0.0705, 0.0207, 0.0991,  ..., 0.0847, 0.0237, 0.0412],
        [0.0706, 0.0207, 0.0991,  ..., 0.0847, 0.0237, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654479.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8852.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.3487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3710.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1075.9965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2529],
        [-0.9071],
        [-1.6380],
        ...,
        [-4.2807],
        [-4.3144],
        [-4.3258]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370489.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0286],
        [1.0372],
        ...,
        [0.9990],
        [0.9979],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370098.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0287],
        [1.0372],
        ...,
        [0.9989],
        [0.9979],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370101.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0087,  0.0021,  ...,  0.0030,  0.0082,  0.0042],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2223.2446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6253, device='cuda:0')



h[100].sum tensor(-0.1406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5752, device='cuda:0')



h[200].sum tensor(-22.6007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0236, 0.0051,  ..., 0.0103, 0.0215, 0.0096],
        [0.0000, 0.0181, 0.0039,  ..., 0.0095, 0.0161, 0.0075],
        [0.0000, 0.0034, 0.0000,  ..., 0.0072, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63571.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0502, 0.0118, 0.0151,  ..., 0.1639, 0.0705, 0.1001],
        [0.0553, 0.0140, 0.0349,  ..., 0.1413, 0.0571, 0.0836],
        [0.0632, 0.0174, 0.0727,  ..., 0.1061, 0.0357, 0.0580],
        ...,
        [0.0706, 0.0208, 0.0992,  ..., 0.0847, 0.0238, 0.0412],
        [0.0705, 0.0207, 0.0992,  ..., 0.0847, 0.0238, 0.0412],
        [0.0705, 0.0208, 0.0992,  ..., 0.0847, 0.0238, 0.0412]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671867., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8785.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.9790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3725.0916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1093.8976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5759],
        [-1.0611],
        [-1.7720],
        ...,
        [-4.3550],
        [-4.3482],
        [-4.3475]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-373068.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0287],
        [1.0372],
        ...,
        [0.9989],
        [0.9979],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370101.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0287],
        [1.0373],
        ...,
        [0.9989],
        [0.9978],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370104.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0029,  0.0358,  0.0115,  ...,  0.0072,  0.0351,  0.0268],
        [-0.0046,  0.0571,  0.0189,  ...,  0.0105,  0.0563,  0.0444],
        [-0.0033,  0.0408,  0.0133,  ...,  0.0079,  0.0400,  0.0309],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0017,  0.0003, -0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1904.4272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3852, device='cuda:0')



h[100].sum tensor(-0.1050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9557, device='cuda:0')



h[200].sum tensor(-22.6354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1582, 0.0512,  ..., 0.0312, 0.1552, 0.1191],
        [0.0000, 0.2025, 0.0666,  ..., 0.0381, 0.1992, 0.1559],
        [0.0000, 0.2147, 0.0709,  ..., 0.0400, 0.2114, 0.1660],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56038.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0099, 0.0047, 0.0000,  ..., 0.4894, 0.3512, 0.3193],
        [0.0000, 0.0016, 0.0000,  ..., 0.5961, 0.4394, 0.3917],
        [0.0000, 0.0008, 0.0000,  ..., 0.6163, 0.4534, 0.4058],
        ...,
        [0.0705, 0.0207, 0.0992,  ..., 0.0846, 0.0238, 0.0413],
        [0.0705, 0.0207, 0.0992,  ..., 0.0846, 0.0238, 0.0413],
        [0.0705, 0.0207, 0.0992,  ..., 0.0846, 0.0238, 0.0413]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633892.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9003.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.9582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3931.1860, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1026.6219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2617],
        [ 0.2669],
        [ 0.2707],
        ...,
        [-4.3609],
        [-4.3541],
        [-4.3534]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-402161.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0287],
        [1.0373],
        ...,
        [0.9989],
        [0.9978],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370104.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0288],
        [1.0374],
        ...,
        [0.9989],
        [0.9978],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370108.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0175,  0.0052,  ...,  0.0044,  0.0169,  0.0115],
        [-0.0007,  0.0089,  0.0022,  ...,  0.0030,  0.0084,  0.0043],
        [-0.0007,  0.0095,  0.0024,  ...,  0.0031,  0.0089,  0.0048],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2389.4236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4434, device='cuda:0')



h[100].sum tensor(-0.1509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8409, device='cuda:0')



h[200].sum tensor(-22.5871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0316, 0.0079,  ..., 0.0116, 0.0294, 0.0162],
        [0.0000, 0.0627, 0.0181,  ..., 0.0165, 0.0603, 0.0396],
        [0.0000, 0.0318, 0.0086,  ..., 0.0117, 0.0296, 0.0187],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65143.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0458, 0.0109, 0.0097,  ..., 0.1890, 0.0922, 0.1173],
        [0.0407, 0.0093, 0.0000,  ..., 0.2199, 0.1148, 0.1387],
        [0.0488, 0.0125, 0.0189,  ..., 0.1804, 0.0878, 0.1106],
        ...,
        [0.0700, 0.0208, 0.0987,  ..., 0.0846, 0.0237, 0.0416],
        [0.0700, 0.0207, 0.0987,  ..., 0.0846, 0.0237, 0.0416],
        [0.0700, 0.0207, 0.0987,  ..., 0.0846, 0.0237, 0.0416]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671251.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8553.3232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.3546, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3513.6277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1111.6263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3847],
        [-0.3718],
        [-0.9007],
        ...,
        [-4.3432],
        [-4.3366],
        [-4.3360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327130.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0288],
        [1.0374],
        ...,
        [0.9989],
        [0.9978],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370108.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0288],
        [1.0374],
        ...,
        [0.9988],
        [0.9978],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370112.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0278,  0.0088,  ...,  0.0060,  0.0272,  0.0201],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2518.0020, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7379, device='cuda:0')



h[100].sum tensor(-0.1590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0300, device='cuda:0')



h[200].sum tensor(-22.5770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1053, 0.0336,  ..., 0.0232, 0.1027, 0.0774],
        [0.0000, 0.0727, 0.0222,  ..., 0.0182, 0.0703, 0.0503],
        [0.0000, 0.0450, 0.0126,  ..., 0.0139, 0.0428, 0.0272],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66586.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0092, 0.0050, 0.0000,  ..., 0.4274, 0.2918, 0.2781],
        [0.0162, 0.0059, 0.0000,  ..., 0.3752, 0.2462, 0.2429],
        [0.0241, 0.0067, 0.0000,  ..., 0.3249, 0.2009, 0.2094],
        ...,
        [0.0695, 0.0210, 0.0983,  ..., 0.0846, 0.0235, 0.0417],
        [0.0695, 0.0210, 0.0983,  ..., 0.0845, 0.0235, 0.0417],
        [0.0695, 0.0210, 0.0983,  ..., 0.0845, 0.0235, 0.0417]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675112.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8401.2012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.2039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3426.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1123.8257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2534],
        [ 0.2641],
        [ 0.2751],
        ...,
        [-4.3291],
        [-4.3227],
        [-4.3223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309134.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0288],
        [1.0374],
        ...,
        [0.9988],
        [0.9978],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370112.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 180.0 event: 900 loss: tensor(508.5697, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0289],
        [1.0375],
        ...,
        [0.9988],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370116.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2419.4290, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4213, device='cuda:0')



h[100].sum tensor(-0.1452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8376, device='cuda:0')



h[200].sum tensor(-22.5898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0125, 0.0025,  ..., 0.0089, 0.0104, 0.0050],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64827.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0658, 0.0195, 0.0912,  ..., 0.0857, 0.0244, 0.0433],
        [0.0637, 0.0187, 0.0799,  ..., 0.0971, 0.0313, 0.0515],
        [0.0577, 0.0167, 0.0466,  ..., 0.1293, 0.0517, 0.0745],
        ...,
        [0.0693, 0.0213, 0.0983,  ..., 0.0845, 0.0234, 0.0416],
        [0.0693, 0.0213, 0.0982,  ..., 0.0844, 0.0233, 0.0416],
        [0.0693, 0.0213, 0.0982,  ..., 0.0845, 0.0233, 0.0416]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673587.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8517.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.2891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3646.8682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1103.7531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5427],
        [-1.8286],
        [-1.0463],
        ...,
        [-4.3310],
        [-4.3246],
        [-4.3244]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358450.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0289],
        [1.0375],
        ...,
        [0.9988],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370116.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0277],
        [1.0290],
        [1.0376],
        ...,
        [0.9988],
        [0.9977],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370120.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0111,  0.0030,  ...,  0.0034,  0.0105,  0.0061],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [-0.0008,  0.0111,  0.0030,  ...,  0.0034,  0.0105,  0.0061],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3760.2261, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.1699, device='cuda:0')



h[100].sum tensor(-0.2758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2847, device='cuda:0')



h[200].sum tensor(-22.4497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0251, 0.0064,  ..., 0.0109, 0.0230, 0.0131],
        [0.0000, 0.0483, 0.0133,  ..., 0.0145, 0.0460, 0.0274],
        [0.0000, 0.0121, 0.0024,  ..., 0.0089, 0.0100, 0.0047],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94297.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0475, 0.0132, 0.0169,  ..., 0.1760, 0.0821, 0.1075],
        [0.0450, 0.0123, 0.0036,  ..., 0.1904, 0.0912, 0.1177],
        [0.0537, 0.0155, 0.0326,  ..., 0.1488, 0.0639, 0.0883],
        ...,
        [0.0693, 0.0214, 0.0984,  ..., 0.0844, 0.0232, 0.0415],
        [0.0693, 0.0214, 0.0984,  ..., 0.0843, 0.0232, 0.0415],
        [0.0693, 0.0214, 0.0984,  ..., 0.0843, 0.0232, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(842487.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8385.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.7665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.9222, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3660.5137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1348.5774, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4097e-03],
        [-1.2206e-02],
        [-3.4122e-01],
        ...,
        [-4.3403e+00],
        [-4.3340e+00],
        [-4.3339e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-367846.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0277],
        [1.0290],
        [1.0376],
        ...,
        [0.9988],
        [0.9977],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370120.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0278],
        [1.0291],
        [1.0377],
        ...,
        [0.9988],
        [0.9977],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370123.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0088,  0.0022,  ...,  0.0031,  0.0083,  0.0042],
        [-0.0006,  0.0088,  0.0022,  ...,  0.0031,  0.0083,  0.0042],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2496.2471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9339, device='cuda:0')



h[100].sum tensor(-0.1472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9125, device='cuda:0')



h[200].sum tensor(-22.5844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0294, 0.0073,  ..., 0.0115, 0.0273, 0.0142],
        [0.0000, 0.0239, 0.0054,  ..., 0.0107, 0.0218, 0.0096],
        [0.0000, 0.0183, 0.0040,  ..., 0.0099, 0.0162, 0.0074],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64921.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0443, 0.0110, 0.0043,  ..., 0.1851, 0.0833, 0.1147],
        [0.0475, 0.0122, 0.0127,  ..., 0.1700, 0.0727, 0.1041],
        [0.0526, 0.0143, 0.0285,  ..., 0.1483, 0.0594, 0.0884],
        ...,
        [0.0693, 0.0213, 0.0985,  ..., 0.0844, 0.0232, 0.0415],
        [0.0692, 0.0213, 0.0984,  ..., 0.0844, 0.0232, 0.0415],
        [0.0692, 0.0213, 0.0984,  ..., 0.0844, 0.0232, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670111., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8423.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.5594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3545.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1107.8256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0356],
        [-0.2322],
        [-0.7261],
        ...,
        [-4.3429],
        [-4.3361],
        [-4.3354]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331601.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0278],
        [1.0291],
        [1.0377],
        ...,
        [0.9988],
        [0.9977],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370123.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0292],
        [1.0378],
        ...,
        [0.9988],
        [0.9977],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370125.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [-0.0017,  0.0215,  0.0066,  ...,  0.0050,  0.0209,  0.0148],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2236.1895, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4436, device='cuda:0')



h[100].sum tensor(-0.1208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4026, device='cuda:0')



h[200].sum tensor(-22.6113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0479, 0.0131,  ..., 0.0143, 0.0456, 0.0271],
        [0.0000, 0.0208, 0.0048,  ..., 0.0101, 0.0187, 0.0095],
        [0.0000, 0.0575, 0.0164,  ..., 0.0158, 0.0552, 0.0350],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60721.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0422, 0.0097, 0.0005,  ..., 0.2028, 0.0997, 0.1261],
        [0.0448, 0.0106, 0.0078,  ..., 0.1914, 0.0918, 0.1180],
        [0.0390, 0.0087, 0.0000,  ..., 0.2264, 0.1170, 0.1421],
        ...,
        [0.0695, 0.0209, 0.0987,  ..., 0.0844, 0.0233, 0.0415],
        [0.0695, 0.0209, 0.0987,  ..., 0.0844, 0.0233, 0.0415],
        [0.0695, 0.0209, 0.0987,  ..., 0.0844, 0.0233, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652494.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8564.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.2089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3616.7449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1070.0061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3194],
        [ 0.3193],
        [ 0.3242],
        ...,
        [-4.3583],
        [-4.3498],
        [-4.3453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351823.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0292],
        [1.0378],
        ...,
        [0.9988],
        [0.9977],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370125.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0293],
        [1.0379],
        ...,
        [0.9987],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370128.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0197,  0.0060,  ...,  0.0047,  0.0191,  0.0133],
        [-0.0013,  0.0175,  0.0052,  ...,  0.0044,  0.0169,  0.0115],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2472.6379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9027, device='cuda:0')



h[100].sum tensor(-0.1430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9080, device='cuda:0')



h[200].sum tensor(-22.5856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0814, 0.0246,  ..., 0.0194, 0.0789, 0.0550],
        [0.0000, 0.0368, 0.0104,  ..., 0.0125, 0.0347, 0.0229],
        [0.0000, 0.0206, 0.0053,  ..., 0.0100, 0.0185, 0.0118],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63568.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0383, 0.0085, 0.0000,  ..., 0.2411, 0.1397, 0.1499],
        [0.0479, 0.0119, 0.0166,  ..., 0.1889, 0.1001, 0.1141],
        [0.0562, 0.0152, 0.0404,  ..., 0.1449, 0.0672, 0.0840],
        ...,
        [0.0697, 0.0206, 0.0989,  ..., 0.0845, 0.0234, 0.0416],
        [0.0697, 0.0206, 0.0989,  ..., 0.0844, 0.0234, 0.0416],
        [0.0697, 0.0206, 0.0989,  ..., 0.0845, 0.0234, 0.0416]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660229.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8691.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7483, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.3101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3739.3684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1088.4371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9665],
        [-1.2700],
        [-1.7028],
        ...,
        [-4.3808],
        [-4.3746],
        [-4.3747]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-388236.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0293],
        [1.0379],
        ...,
        [0.9987],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370128.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0293],
        [1.0379],
        ...,
        [0.9987],
        [0.9976],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370131.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0084,  0.0020,  ...,  0.0029,  0.0079,  0.0039],
        [-0.0015,  0.0193,  0.0058,  ...,  0.0046,  0.0187,  0.0129],
        [-0.0008,  0.0108,  0.0028,  ...,  0.0033,  0.0102,  0.0058],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2146.5203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2969, device='cuda:0')



h[100].sum tensor(-0.1102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2350, device='cuda:0')



h[200].sum tensor(-22.6202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0580, 0.0164,  ..., 0.0157, 0.0557, 0.0355],
        [0.0000, 0.0369, 0.0091,  ..., 0.0125, 0.0347, 0.0179],
        [0.0000, 0.0592, 0.0168,  ..., 0.0159, 0.0569, 0.0364],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59251.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0078, 0.0008,  ..., 0.2046, 0.1025, 0.1276],
        [0.0400, 0.0068, 0.0000,  ..., 0.2108, 0.1036, 0.1327],
        [0.0351, 0.0049, 0.0000,  ..., 0.2418, 0.1271, 0.1538],
        ...,
        [0.0692, 0.0200, 0.0964,  ..., 0.0867, 0.0250, 0.0434],
        [0.0695, 0.0202, 0.0986,  ..., 0.0845, 0.0234, 0.0418],
        [0.0695, 0.0202, 0.0987,  ..., 0.0845, 0.0234, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645470.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8613.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.9799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3565.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1054.6749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3418],
        [-0.8175],
        [-0.5071],
        ...,
        [-3.6222],
        [-3.9774],
        [-4.2168]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349792.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0293],
        [1.0379],
        ...,
        [0.9987],
        [0.9976],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370131.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0282],
        [1.0294],
        [1.0380],
        ...,
        [0.9987],
        [0.9976],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370134.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2009.5630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3547, device='cuda:0')



h[100].sum tensor(-0.0949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9512, device='cuda:0')



h[200].sum tensor(-22.6360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0073, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57116.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0672, 0.0192, 0.0960,  ..., 0.0813, 0.0224, 0.0399],
        [0.0670, 0.0191, 0.0945,  ..., 0.0835, 0.0236, 0.0414],
        [0.0668, 0.0191, 0.0921,  ..., 0.0868, 0.0252, 0.0438],
        ...,
        [0.0694, 0.0203, 0.0986,  ..., 0.0846, 0.0233, 0.0418],
        [0.0694, 0.0203, 0.0985,  ..., 0.0845, 0.0233, 0.0418],
        [0.0694, 0.0203, 0.0985,  ..., 0.0846, 0.0233, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636538.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8636.4053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.5281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3585.0991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1035.0492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7713],
        [-3.3689],
        [-2.8086],
        ...,
        [-4.3525],
        [-4.3301],
        [-4.3138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-361293.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0282],
        [1.0294],
        [1.0380],
        ...,
        [0.9987],
        [0.9976],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370134.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0294],
        [1.0380],
        ...,
        [0.9986],
        [0.9976],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370136.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2143.2295, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9513, device='cuda:0')



h[100].sum tensor(-0.1056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1845, device='cuda:0')



h[200].sum tensor(-22.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58883.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0672, 0.0194, 0.0962,  ..., 0.0814, 0.0222, 0.0396],
        [0.0675, 0.0195, 0.0965,  ..., 0.0818, 0.0223, 0.0398],
        [0.0679, 0.0198, 0.0968,  ..., 0.0824, 0.0224, 0.0402],
        ...,
        [0.0695, 0.0205, 0.0988,  ..., 0.0846, 0.0230, 0.0415],
        [0.0694, 0.0205, 0.0987,  ..., 0.0846, 0.0230, 0.0414],
        [0.0694, 0.0205, 0.0987,  ..., 0.0846, 0.0230, 0.0415]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644554.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8671.2354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.4008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3639.7505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1048.1932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7457],
        [-4.0168],
        [-4.2347],
        ...,
        [-4.3853],
        [-4.3795],
        [-4.3805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-369141.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0294],
        [1.0380],
        ...,
        [0.9986],
        [0.9976],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370136.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0295],
        [1.0380],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370139.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [-0.0014,  0.0184,  0.0055,  ...,  0.0045,  0.0178,  0.0122],
        [-0.0021,  0.0270,  0.0085,  ...,  0.0058,  0.0264,  0.0193],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2023.4769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3103, device='cuda:0')



h[100].sum tensor(-0.0927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9447, device='cuda:0')



h[200].sum tensor(-22.6362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0539, 0.0151,  ..., 0.0152, 0.0516, 0.0320],
        [0.0000, 0.0450, 0.0133,  ..., 0.0138, 0.0428, 0.0296],
        [0.0000, 0.0436, 0.0127,  ..., 0.0136, 0.0414, 0.0284],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56629.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0436, 0.0090, 0.0050,  ..., 0.2021, 0.1017, 0.1234],
        [0.0410, 0.0084, 0.0040,  ..., 0.2225, 0.1194, 0.1366],
        [0.0380, 0.0072, 0.0000,  ..., 0.2415, 0.1325, 0.1494],
        ...,
        [0.0696, 0.0208, 0.0990,  ..., 0.0846, 0.0227, 0.0411],
        [0.0696, 0.0208, 0.0990,  ..., 0.0846, 0.0227, 0.0410],
        [0.0696, 0.0208, 0.0990,  ..., 0.0846, 0.0227, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(636754.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8750.2158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.1006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3692.1440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1029.0723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0475],
        [ 0.2523],
        [ 0.3808],
        ...,
        [-4.4081],
        [-4.4018],
        [-4.4022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-392366.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0295],
        [1.0380],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370139.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0295],
        [1.0380],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370142.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2253.3579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1061, device='cuda:0')



h[100].sum tensor(-0.1118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3532, device='cuda:0')



h[200].sum tensor(-22.6131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0014, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0014, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62575.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0664, 0.0193, 0.0920,  ..., 0.0860, 0.0242, 0.0421],
        [0.0646, 0.0184, 0.0825,  ..., 0.0961, 0.0297, 0.0493],
        [0.0638, 0.0181, 0.0777,  ..., 0.1018, 0.0326, 0.0533],
        ...,
        [0.0697, 0.0209, 0.0991,  ..., 0.0847, 0.0224, 0.0407],
        [0.0696, 0.0209, 0.0991,  ..., 0.0847, 0.0224, 0.0407],
        [0.0697, 0.0209, 0.0991,  ..., 0.0847, 0.0224, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670549.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8640.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.4124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3598.5073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1080.9852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0368],
        [-1.9957],
        [-1.8071],
        ...,
        [-4.4183],
        [-4.4119],
        [-4.4123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-381336.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0295],
        [1.0380],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370142.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 190.0 event: 950 loss: tensor(475.3245, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0296],
        [1.0381],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370144.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0253,  0.0079,  ...,  0.0056,  0.0246,  0.0179],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2290.6602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3840, device='cuda:0')



h[100].sum tensor(-0.1128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3938, device='cuda:0')



h[200].sum tensor(-22.6106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0814, 0.0259,  ..., 0.0195, 0.0789, 0.0598],
        [0.0000, 0.0490, 0.0147,  ..., 0.0145, 0.0467, 0.0329],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62095.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0314, 0.0078, 0.0000,  ..., 0.2997, 0.1913, 0.1839],
        [0.0418, 0.0100, 0.0028,  ..., 0.2356, 0.1382, 0.1417],
        [0.0546, 0.0148, 0.0141,  ..., 0.1612, 0.0802, 0.0922],
        ...,
        [0.0697, 0.0210, 0.0991,  ..., 0.0848, 0.0222, 0.0406],
        [0.0696, 0.0210, 0.0990,  ..., 0.0848, 0.0222, 0.0406],
        [0.0696, 0.0210, 0.0990,  ..., 0.0848, 0.0222, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664551.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8589.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6045, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.6772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3493.5662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1079.4940, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0477],
        [-0.0598],
        [-0.1618],
        ...,
        [-4.4243],
        [-4.4180],
        [-4.4184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-367154.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0296],
        [1.0381],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370144.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0296],
        [1.0381],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370147.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0286,  0.0091,  ...,  0.0061,  0.0279,  0.0206],
        [-0.0010,  0.0133,  0.0038,  ...,  0.0037,  0.0127,  0.0079],
        [-0.0013,  0.0168,  0.0050,  ...,  0.0043,  0.0162,  0.0109],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0003, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2654.4761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2160, device='cuda:0')



h[100].sum tensor(-0.1431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0998, device='cuda:0')



h[200].sum tensor(-22.5734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0854, 0.0261,  ..., 0.0201, 0.0829, 0.0581],
        [0.0000, 0.0859, 0.0263,  ..., 0.0202, 0.0833, 0.0585],
        [0.0000, 0.0298, 0.0080,  ..., 0.0116, 0.0276, 0.0169],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68125.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0141e-02, 0.0000e+00, 0.0000e+00,  ..., 4.1834e-01, 2.7253e-01,
         2.6520e-01],
        [1.9873e-02, 1.4381e-03, 0.0000e+00,  ..., 3.5612e-01, 2.2117e-01,
         2.2420e-01],
        [3.5861e-02, 5.6238e-03, 1.7136e-04,  ..., 2.6250e-01, 1.4812e-01,
         1.6180e-01],
        ...,
        [6.8736e-02, 2.0449e-02, 9.5330e-02,  ..., 8.8425e-02, 2.3738e-02,
         4.3411e-02],
        [6.6934e-02, 1.9547e-02, 8.8317e-02,  ..., 9.5510e-02, 2.7061e-02,
         4.8622e-02],
        [6.6059e-02, 1.9108e-02, 8.4832e-02,  ..., 9.9111e-02, 2.8740e-02,
         5.1263e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(685620.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8418.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.6916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3312.2778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1134.0038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2938],
        [ 0.3026],
        [ 0.3082],
        ...,
        [-4.0641],
        [-3.8340],
        [-3.6895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-341374.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0296],
        [1.0381],
        ...,
        [0.9986],
        [0.9975],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370147.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0297],
        [1.0381],
        ...,
        [0.9985],
        [0.9975],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370150.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2343.8782, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0425, device='cuda:0')



h[100].sum tensor(-0.1134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4900, device='cuda:0')



h[200].sum tensor(-22.6069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0014, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61495.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0579, 0.0157, 0.0457,  ..., 0.1305, 0.0543, 0.0728],
        [0.0643, 0.0183, 0.0810,  ..., 0.0971, 0.0300, 0.0501],
        [0.0670, 0.0197, 0.0931,  ..., 0.0862, 0.0232, 0.0424],
        ...,
        [0.0695, 0.0209, 0.0984,  ..., 0.0849, 0.0220, 0.0411],
        [0.0694, 0.0208, 0.0984,  ..., 0.0849, 0.0220, 0.0411],
        [0.0694, 0.0209, 0.0984,  ..., 0.0849, 0.0220, 0.0411]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654722.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8661.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3516.2827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1071.4933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7049],
        [-1.6067],
        [-2.2350],
        ...,
        [-4.4145],
        [-4.4083],
        [-4.4088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-373990.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0297],
        [1.0381],
        ...,
        [0.9985],
        [0.9975],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370150.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0297],
        [1.0381],
        ...,
        [0.9985],
        [0.9974],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370154.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0155,  0.0045,  ...,  0.0041,  0.0149,  0.0097],
        [-0.0008,  0.0115,  0.0031,  ...,  0.0035,  0.0110,  0.0064],
        [-0.0006,  0.0083,  0.0020,  ...,  0.0030,  0.0078,  0.0037],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2196.9131, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9283, device='cuda:0')



h[100].sum tensor(-0.0979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1811, device='cuda:0')



h[200].sum tensor(-22.6242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0557, 0.0158,  ..., 0.0156, 0.0534, 0.0332],
        [0.0000, 0.0415, 0.0109,  ..., 0.0134, 0.0393, 0.0215],
        [0.0000, 0.0341, 0.0083,  ..., 0.0123, 0.0319, 0.0153],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58767.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0398, 0.0076, 0.0000,  ..., 0.2180, 0.1089, 0.1348],
        [0.0385, 0.0068, 0.0000,  ..., 0.2201, 0.1060, 0.1372],
        [0.0401, 0.0075, 0.0000,  ..., 0.2106, 0.0969, 0.1312],
        ...,
        [0.0690, 0.0209, 0.0976,  ..., 0.0849, 0.0218, 0.0416],
        [0.0690, 0.0209, 0.0976,  ..., 0.0848, 0.0218, 0.0416],
        [0.0690, 0.0209, 0.0976,  ..., 0.0849, 0.0218, 0.0416]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642096.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8534.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.0764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3352.6272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1050.9635, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1925],
        [ 0.3610],
        [ 0.3483],
        ...,
        [-4.3906],
        [-4.3845],
        [-4.3851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345203.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0297],
        [1.0381],
        ...,
        [0.9985],
        [0.9974],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370154.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0298],
        [1.0381],
        ...,
        [0.9985],
        [0.9974],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370158.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [-0.0016,  0.0219,  0.0068,  ...,  0.0051,  0.0213,  0.0150],
        ...,
        [ 0.0000,  0.0009, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0009, -0.0005,  ...,  0.0019,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2861.6885, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1201, device='cuda:0')



h[100].sum tensor(-0.1524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3781, device='cuda:0')



h[200].sum tensor(-22.5565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0252, 0.0070,  ..., 0.0110, 0.0231, 0.0155],
        [0.0000, 0.0756, 0.0239,  ..., 0.0188, 0.0732, 0.0547],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72137.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0603, 0.0176, 0.0574,  ..., 0.1164, 0.0461, 0.0638],
        [0.0457, 0.0133, 0.0318,  ..., 0.2081, 0.1189, 0.1244],
        [0.0234, 0.0075, 0.0006,  ..., 0.3629, 0.2422, 0.2271],
        ...,
        [0.0689, 0.0211, 0.0973,  ..., 0.0849, 0.0215, 0.0418],
        [0.0688, 0.0210, 0.0972,  ..., 0.0849, 0.0215, 0.0418],
        [0.0689, 0.0211, 0.0972,  ..., 0.0849, 0.0215, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708993.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8312.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3210.6462, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1166.2185, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8248],
        [-0.2678],
        [ 0.1423],
        ...,
        [-4.3847],
        [-4.3787],
        [-4.3794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325327.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0298],
        [1.0381],
        ...,
        [0.9985],
        [0.9974],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370158.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0297],
        [1.0382],
        ...,
        [0.9984],
        [0.9974],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370161.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2390.0046, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2333, device='cuda:0')



h[100].sum tensor(-0.1094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5179, device='cuda:0')



h[200].sum tensor(-22.6074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0079, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62262.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0666, 0.0203, 0.0946,  ..., 0.0817, 0.0204, 0.0400],
        [0.0666, 0.0203, 0.0943,  ..., 0.0828, 0.0207, 0.0408],
        [0.0663, 0.0202, 0.0929,  ..., 0.0851, 0.0213, 0.0428],
        ...,
        [0.0688, 0.0214, 0.0971,  ..., 0.0849, 0.0211, 0.0419],
        [0.0687, 0.0214, 0.0970,  ..., 0.0849, 0.0211, 0.0419],
        [0.0688, 0.0214, 0.0971,  ..., 0.0849, 0.0211, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657777.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8280.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6170, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.1619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3126.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1086.3717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9349],
        [-3.6325],
        [-3.1583],
        ...,
        [-4.3780],
        [-4.3695],
        [-4.3679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301071.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0297],
        [1.0382],
        ...,
        [0.9984],
        [0.9974],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370161.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0298],
        [1.0382],
        ...,
        [0.9984],
        [0.9973],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370164.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0019,  0.0258,  0.0082,  ...,  0.0057,  0.0252,  0.0183],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2021.4999, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0658, device='cuda:0')



h[100].sum tensor(-0.0772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7629, device='cuda:0')



h[200].sum tensor(-22.6461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0756, 0.0235,  ..., 0.0188, 0.0732, 0.0522],
        [0.0000, 0.0501, 0.0151,  ..., 0.0149, 0.0479, 0.0337],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55910.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0289, 0.0083, 0.0000,  ..., 0.3024, 0.1820, 0.1883],
        [0.0426, 0.0127, 0.0149,  ..., 0.2256, 0.1256, 0.1365],
        [0.0595, 0.0182, 0.0487,  ..., 0.1286, 0.0534, 0.0715],
        ...,
        [0.0690, 0.0217, 0.0973,  ..., 0.0849, 0.0207, 0.0418],
        [0.0690, 0.0217, 0.0973,  ..., 0.0849, 0.0207, 0.0418],
        [0.0690, 0.0217, 0.0973,  ..., 0.0849, 0.0207, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631373.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8634.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.6507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3483.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1025.8660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1460],
        [-0.3688],
        [-1.2920],
        ...,
        [-4.4033],
        [-4.3974],
        [-4.3983]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-357662.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0298],
        [1.0382],
        ...,
        [0.9984],
        [0.9973],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370164.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0298],
        [1.0383],
        ...,
        [0.9984],
        [0.9973],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370167.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [-0.0006,  0.0093,  0.0024,  ...,  0.0032,  0.0087,  0.0046],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3823.6313, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.9330, device='cuda:0')



h[100].sum tensor(-0.2256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2501, device='cuda:0')



h[200].sum tensor(-22.4584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0599, 0.0175,  ..., 0.0164, 0.0576, 0.0368],
        [0.0000, 0.0105, 0.0019,  ..., 0.0088, 0.0085, 0.0034],
        [0.0000, 0.0582, 0.0174,  ..., 0.0162, 0.0559, 0.0378],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87668.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0293, 0.0076, 0.0000,  ..., 0.2936, 0.1672, 0.1838],
        [0.0342, 0.0094, 0.0000,  ..., 0.2707, 0.1522, 0.1679],
        [0.0222, 0.0064, 0.0000,  ..., 0.3498, 0.2141, 0.2202],
        ...,
        [0.0694, 0.0219, 0.0977,  ..., 0.0851, 0.0203, 0.0417],
        [0.0694, 0.0219, 0.0976,  ..., 0.0850, 0.0203, 0.0417],
        [0.0694, 0.0219, 0.0977,  ..., 0.0851, 0.0203, 0.0417]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(773174.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8111.4038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.0979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.7573, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3035.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1303.7571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1523],
        [ 0.1367],
        [ 0.1211],
        ...,
        [-4.4203],
        [-4.4153],
        [-4.4175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312144.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0298],
        [1.0383],
        ...,
        [0.9984],
        [0.9973],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370167.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0298],
        [1.0383],
        ...,
        [0.9983],
        [0.9973],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370170., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024,  0.0331,  0.0107,  ...,  0.0068,  0.0324,  0.0243],
        [-0.0027,  0.0357,  0.0116,  ...,  0.0072,  0.0351,  0.0265],
        [-0.0025,  0.0334,  0.0108,  ...,  0.0069,  0.0327,  0.0245],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2670.9187, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8519, device='cuda:0')



h[100].sum tensor(-0.1298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0466, device='cuda:0')



h[200].sum tensor(-22.5771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1286, 0.0413,  ..., 0.0269, 0.1259, 0.0938],
        [0.0000, 0.1482, 0.0481,  ..., 0.0299, 0.1454, 0.1099],
        [0.0000, 0.1544, 0.0503,  ..., 0.0309, 0.1516, 0.1151],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68403.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.6798e-05, 0.0000e+00,  ..., 5.4247e-01, 3.7152e-01,
         3.4819e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5472e-01, 3.8159e-01,
         3.5617e-01],
        [1.6762e-03, 1.8679e-04, 0.0000e+00,  ..., 5.3480e-01, 3.6424e-01,
         3.4321e-01],
        ...,
        [6.9768e-02, 2.1532e-02, 9.7687e-02,  ..., 8.5211e-02, 2.0123e-02,
         4.1957e-02],
        [6.9735e-02, 2.1523e-02, 9.7639e-02,  ..., 8.5171e-02, 2.0114e-02,
         4.1939e-02],
        [6.9758e-02, 2.1537e-02, 9.7662e-02,  ..., 8.5207e-02, 2.0121e-02,
         4.1960e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686148.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8378.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.9798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3008.9600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1141.6421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1488],
        [ 0.1541],
        [ 0.1657],
        ...,
        [-4.4409],
        [-4.4349],
        [-4.4358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310204.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0298],
        [1.0383],
        ...,
        [0.9983],
        [0.9973],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370170., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0298],
        [1.0384],
        ...,
        [0.9983],
        [0.9973],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370172.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0028,  0.0377,  0.0123,  ...,  0.0075,  0.0370,  0.0281],
        [-0.0019,  0.0266,  0.0084,  ...,  0.0058,  0.0260,  0.0189],
        [-0.0007,  0.0094,  0.0025,  ...,  0.0032,  0.0089,  0.0047],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2492.7021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7636, device='cuda:0')



h[100].sum tensor(-0.1153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7415, device='cuda:0')



h[200].sum tensor(-22.5940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1406, 0.0454,  ..., 0.0286, 0.1379, 0.1037],
        [0.0000, 0.0802, 0.0245,  ..., 0.0194, 0.0779, 0.0537],
        [0.0000, 0.0694, 0.0207,  ..., 0.0177, 0.0672, 0.0447],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64320.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.0010, 0.0000,  ..., 0.4589, 0.3008, 0.2956],
        [0.0164, 0.0028, 0.0000,  ..., 0.3732, 0.2269, 0.2399],
        [0.0244, 0.0040, 0.0000,  ..., 0.3209, 0.1814, 0.2058],
        ...,
        [0.0702, 0.0211, 0.0979,  ..., 0.0854, 0.0200, 0.0423],
        [0.0701, 0.0211, 0.0978,  ..., 0.0853, 0.0199, 0.0423],
        [0.0702, 0.0211, 0.0979,  ..., 0.0854, 0.0200, 0.0423]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664941.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8557.0479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.0459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2991.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1107.6481, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2994],
        [ 0.3258],
        [ 0.3457],
        ...,
        [-4.4591],
        [-4.4531],
        [-4.4541]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322919.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0298],
        [1.0384],
        ...,
        [0.9983],
        [0.9973],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370172.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 200.0 event: 1000 loss: tensor(488.0343, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0299],
        [1.0385],
        ...,
        [0.9983],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370174.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [-0.0008,  0.0108,  0.0029,  ...,  0.0033,  0.0103,  0.0058],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2317.2588, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6887, device='cuda:0')



h[100].sum tensor(-0.1016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4384, device='cuda:0')



h[200].sum tensor(-22.6102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0016, 0.0000],
        [0.0000, 0.0136, 0.0030,  ..., 0.0090, 0.0118, 0.0060],
        [0.0000, 0.0242, 0.0061,  ..., 0.0107, 0.0223, 0.0123],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63170.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0628, 0.0172, 0.0712,  ..., 0.1067, 0.0322, 0.0586],
        [0.0586, 0.0154, 0.0483,  ..., 0.1296, 0.0463, 0.0749],
        [0.0518, 0.0125, 0.0254,  ..., 0.1643, 0.0667, 0.0999],
        ...,
        [0.0707, 0.0208, 0.0982,  ..., 0.0855, 0.0198, 0.0425],
        [0.0707, 0.0207, 0.0982,  ..., 0.0855, 0.0197, 0.0425],
        [0.0707, 0.0208, 0.0982,  ..., 0.0855, 0.0198, 0.0425]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666175.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8733.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.8022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2991.4888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1097.5780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6062],
        [-0.8964],
        [-0.2346],
        ...,
        [-4.4845],
        [-4.4781],
        [-4.4790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344966.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0299],
        [1.0385],
        ...,
        [0.9983],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370174.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0299],
        [1.0385],
        ...,
        [0.9983],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370177.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0021,  0.0286,  0.0091,  ...,  0.0061,  0.0280,  0.0206],
        [-0.0019,  0.0266,  0.0084,  ...,  0.0058,  0.0260,  0.0190],
        [-0.0005,  0.0073,  0.0017,  ...,  0.0028,  0.0069,  0.0030],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2594.7581, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3555, device='cuda:0')



h[100].sum tensor(-0.1232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9741, device='cuda:0')



h[200].sum tensor(-22.5803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1321, 0.0425,  ..., 0.0271, 0.1296, 0.0968],
        [0.0000, 0.0982, 0.0307,  ..., 0.0219, 0.0959, 0.0687],
        [0.0000, 0.1240, 0.0396,  ..., 0.0259, 0.1216, 0.0900],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65759.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1786e-03, 3.4235e-04, 0.0000e+00,  ..., 4.8244e-01, 3.1867e-01,
         3.1257e-01],
        [5.6145e-03, 2.7807e-04, 0.0000e+00,  ..., 4.5767e-01, 2.9693e-01,
         2.9644e-01],
        [5.2455e-03, 4.1202e-04, 0.0000e+00,  ..., 4.6201e-01, 2.9959e-01,
         2.9939e-01],
        ...,
        [7.1051e-02, 2.0645e-02, 9.8511e-02,  ..., 8.5662e-02, 1.9572e-02,
         4.2638e-02],
        [7.1017e-02, 2.0637e-02, 9.8461e-02,  ..., 8.5622e-02, 1.9563e-02,
         4.2619e-02],
        [7.1044e-02, 2.0652e-02, 9.8488e-02,  ..., 8.5662e-02, 1.9571e-02,
         4.2644e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670342.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8785.8975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.6727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2945.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1121.2023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2011],
        [ 0.1953],
        [ 0.1935],
        ...,
        [-4.4938],
        [-4.4871],
        [-4.4878]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351961.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0299],
        [1.0385],
        ...,
        [0.9983],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370177.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0386],
        ...,
        [0.9983],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370180.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [-0.0008,  0.0113,  0.0031,  ...,  0.0034,  0.0108,  0.0063],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2438.3306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5898, device='cuda:0')



h[100].sum tensor(-0.1105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7161, device='cuda:0')



h[200].sum tensor(-22.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0140, 0.0032,  ..., 0.0089, 0.0122, 0.0064],
        [0.0000, 0.0121, 0.0025,  ..., 0.0087, 0.0104, 0.0048],
        [0.0000, 0.0486, 0.0135,  ..., 0.0143, 0.0466, 0.0276],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65367.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0158, 0.0556,  ..., 0.1222, 0.0418, 0.0699],
        [0.0561, 0.0140, 0.0368,  ..., 0.1440, 0.0537, 0.0856],
        [0.0479, 0.0107, 0.0040,  ..., 0.1870, 0.0793, 0.1163],
        ...,
        [0.0714, 0.0207, 0.0988,  ..., 0.0857, 0.0195, 0.0426],
        [0.0713, 0.0207, 0.0987,  ..., 0.0856, 0.0195, 0.0425],
        [0.0714, 0.0207, 0.0987,  ..., 0.0857, 0.0195, 0.0426]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679104.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8930.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.5387, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3018.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1116.0223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4941],
        [-0.9053],
        [-0.5258],
        ...,
        [-4.5287],
        [-4.5224],
        [-4.5235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-373132.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0386],
        ...,
        [0.9983],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370180.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0387],
        ...,
        [0.9982],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370184.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2571.6599, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1663, device='cuda:0')



h[100].sum tensor(-0.1192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9465, device='cuda:0')



h[200].sum tensor(-22.5822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0073, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64127.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0686, 0.0194, 0.0942,  ..., 0.0840, 0.0198, 0.0421],
        [0.0690, 0.0196, 0.0955,  ..., 0.0835, 0.0192, 0.0417],
        [0.0687, 0.0195, 0.0939,  ..., 0.0862, 0.0199, 0.0439],
        ...,
        [0.0713, 0.0207, 0.0985,  ..., 0.0855, 0.0195, 0.0427],
        [0.0713, 0.0207, 0.0985,  ..., 0.0855, 0.0195, 0.0427],
        [0.0713, 0.0207, 0.0985,  ..., 0.0855, 0.0195, 0.0427]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661178.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8958.0059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.9352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3075.8062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1106.3567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2671],
        [-3.5600],
        [-3.6336],
        ...,
        [-4.5225],
        [-4.5177],
        [-4.5209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-376882.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0387],
        ...,
        [0.9982],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370184.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0387],
        ...,
        [0.9982],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370187.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [-0.0006,  0.0090,  0.0023,  ...,  0.0031,  0.0086,  0.0044],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2279.8701, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2202, device='cuda:0')



h[100].sum tensor(-0.0951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3699, device='cuda:0')



h[200].sum tensor(-22.6131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0117, 0.0024,  ..., 0.0088, 0.0100, 0.0045],
        [0.0000, 0.0318, 0.0088,  ..., 0.0119, 0.0299, 0.0186],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61670.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0144, 0.0238,  ..., 0.1707, 0.0793, 0.1021],
        [0.0494, 0.0127, 0.0145,  ..., 0.1898, 0.0891, 0.1160],
        [0.0438, 0.0105, 0.0077,  ..., 0.2215, 0.1084, 0.1384],
        ...,
        [0.0711, 0.0210, 0.0984,  ..., 0.0852, 0.0195, 0.0427],
        [0.0688, 0.0201, 0.0868,  ..., 0.0965, 0.0263, 0.0507],
        [0.0641, 0.0182, 0.0608,  ..., 0.1218, 0.0422, 0.0685]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658715., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8871.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.6554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3083.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1087.3976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1250],
        [ 0.1591],
        [ 0.1878],
        ...,
        [-3.9112],
        [-3.1401],
        [-2.1536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-352298.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0387],
        ...,
        [0.9982],
        [0.9972],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370187.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0388],
        ...,
        [0.9982],
        [0.9972],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370191.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0095,  0.0025,  ...,  0.0032,  0.0090,  0.0048],
        [-0.0006,  0.0090,  0.0023,  ...,  0.0031,  0.0085,  0.0044],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2376.6865, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3312, device='cuda:0')



h[100].sum tensor(-0.1011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5322, device='cuda:0')



h[200].sum tensor(-22.6035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0501, 0.0141,  ..., 0.0147, 0.0481, 0.0289],
        [0.0000, 0.0264, 0.0070,  ..., 0.0111, 0.0246, 0.0142],
        [0.0000, 0.0117, 0.0024,  ..., 0.0089, 0.0099, 0.0045],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62872.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0463, 0.0112, 0.0037,  ..., 0.1951, 0.0863, 0.1214],
        [0.0526, 0.0137, 0.0245,  ..., 0.1640, 0.0670, 0.0996],
        [0.0602, 0.0167, 0.0525,  ..., 0.1262, 0.0435, 0.0730],
        ...,
        [0.0710, 0.0213, 0.0982,  ..., 0.0849, 0.0197, 0.0426],
        [0.0710, 0.0213, 0.0982,  ..., 0.0849, 0.0197, 0.0426],
        [0.0710, 0.0213, 0.0982,  ..., 0.0849, 0.0197, 0.0426]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666909.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8923.4863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6567, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.2637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3279.7319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1093.6962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4604],
        [-0.7768],
        [-1.3956],
        ...,
        [-4.5219],
        [-4.5170],
        [-4.5199]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-383713.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0300],
        [1.0388],
        ...,
        [0.9982],
        [0.9972],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370191.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0301],
        [1.0388],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370195.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0018,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2294.8364, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2684, device='cuda:0')



h[100].sum tensor(-0.0934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3769, device='cuda:0')



h[200].sum tensor(-22.6126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0078, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0078, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0078, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61327.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0641, 0.0186, 0.0803,  ..., 0.0976, 0.0249, 0.0532],
        [0.0650, 0.0189, 0.0826,  ..., 0.0960, 0.0242, 0.0518],
        [0.0653, 0.0192, 0.0812,  ..., 0.0981, 0.0259, 0.0532],
        ...,
        [0.0707, 0.0216, 0.0982,  ..., 0.0849, 0.0197, 0.0427],
        [0.0707, 0.0216, 0.0982,  ..., 0.0848, 0.0197, 0.0427],
        [0.0707, 0.0216, 0.0982,  ..., 0.0849, 0.0197, 0.0427]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657946.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8826.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.8151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3253.0432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1083.9106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7072],
        [-1.8174],
        [-1.7631],
        ...,
        [-4.5335],
        [-4.5273],
        [-4.5288]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-367286.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0301],
        [1.0388],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370195.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0301],
        [1.0389],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370198.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0024]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3099.8652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1303, device='cuda:0')



h[100].sum tensor(-0.1507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9640, device='cuda:0')



h[200].sum tensor(-22.5308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0384, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76342.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0681, 0.0204, 0.0953,  ..., 0.0816, 0.0190, 0.0413],
        [0.0684, 0.0206, 0.0956,  ..., 0.0820, 0.0191, 0.0415],
        [0.0688, 0.0209, 0.0960,  ..., 0.0827, 0.0192, 0.0419],
        ...,
        [0.0703, 0.0216, 0.0978,  ..., 0.0848, 0.0197, 0.0432],
        [0.0703, 0.0216, 0.0977,  ..., 0.0848, 0.0197, 0.0432],
        [0.0703, 0.0216, 0.0978,  ..., 0.0848, 0.0197, 0.0432]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(732972.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8592.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.9743, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3161.2046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1212.0182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0081],
        [-4.2856],
        [-4.4778],
        ...,
        [-4.5144],
        [-4.5108],
        [-4.5143]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-355923.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0301],
        [1.0389],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370198.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0302],
        [1.0389],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370202.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0106,  0.0029,  ...,  0.0034,  0.0101,  0.0056],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2244.4814, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4854, device='cuda:0')



h[100].sum tensor(-0.0867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2626, device='cuda:0')



h[200].sum tensor(-22.6192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0244, 0.0062,  ..., 0.0109, 0.0225, 0.0124],
        [0.0000, 0.0134, 0.0029,  ..., 0.0092, 0.0116, 0.0058],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59938.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0519, 0.0140, 0.0247,  ..., 0.1661, 0.0711, 0.1014],
        [0.0597, 0.0171, 0.0498,  ..., 0.1263, 0.0465, 0.0733],
        [0.0661, 0.0197, 0.0827,  ..., 0.0953, 0.0271, 0.0513],
        ...,
        [0.0700, 0.0214, 0.0975,  ..., 0.0848, 0.0198, 0.0437],
        [0.0700, 0.0214, 0.0974,  ..., 0.0847, 0.0198, 0.0437],
        [0.0700, 0.0214, 0.0975,  ..., 0.0848, 0.0198, 0.0437]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648905., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8721.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.3166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3184.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1073.3136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0524],
        [-2.0082],
        [-2.9698],
        ...,
        [-4.5114],
        [-4.5054],
        [-4.5065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351279.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0302],
        [1.0389],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370202.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0302],
        [1.0390],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370205.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [-0.0013,  0.0187,  0.0057,  ...,  0.0046,  0.0182,  0.0123],
        [-0.0011,  0.0163,  0.0048,  ...,  0.0043,  0.0158,  0.0104],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2407.0913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5189, device='cuda:0')



h[100].sum tensor(-0.0970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5597, device='cuda:0')



h[200].sum tensor(-22.6031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0217, 0.0058,  ..., 0.0105, 0.0198, 0.0126],
        [0.0000, 0.0344, 0.0097,  ..., 0.0125, 0.0325, 0.0206],
        [0.0000, 0.1010, 0.0316,  ..., 0.0228, 0.0986, 0.0706],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61702.2852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0567, 0.0164, 0.0401,  ..., 0.1473, 0.0648, 0.0868],
        [0.0483, 0.0137, 0.0171,  ..., 0.2032, 0.1048, 0.1246],
        [0.0333, 0.0100, 0.0000,  ..., 0.3073, 0.1838, 0.1941],
        ...,
        [0.0641, 0.0190, 0.0631,  ..., 0.1178, 0.0415, 0.0668],
        [0.0655, 0.0196, 0.0715,  ..., 0.1097, 0.0362, 0.0611],
        [0.0684, 0.0207, 0.0887,  ..., 0.0931, 0.0253, 0.0497]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652110.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8664.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.8040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3110.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1088.6389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6564],
        [-0.1609],
        [ 0.1617],
        ...,
        [-2.6002],
        [-2.9898],
        [-3.5716]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340517., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0302],
        [1.0390],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370205.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 210.0 event: 1050 loss: tensor(472.0046, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0303],
        [1.0390],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370208.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1965.9880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.7775, device='cuda:0')



h[100].sum tensor(-0.0644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7208, device='cuda:0')



h[200].sum tensor(-22.6488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56125.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0663, 0.0195, 0.0896,  ..., 0.0869, 0.0219, 0.0459],
        [0.0667, 0.0198, 0.0892,  ..., 0.0878, 0.0231, 0.0463],
        [0.0662, 0.0198, 0.0823,  ..., 0.0952, 0.0282, 0.0512],
        ...,
        [0.0697, 0.0212, 0.0974,  ..., 0.0848, 0.0199, 0.0440],
        [0.0697, 0.0212, 0.0973,  ..., 0.0848, 0.0198, 0.0440],
        [0.0697, 0.0212, 0.0973,  ..., 0.0848, 0.0199, 0.0440]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634180.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8692.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.0008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.5022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3132.3357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1043.3602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0307],
        [-2.1425],
        [-1.9712],
        ...,
        [-4.5082],
        [-4.5022],
        [-4.5033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340451.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0303],
        [1.0390],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370208.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0303],
        [1.0391],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370210.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [-0.0006,  0.0092,  0.0024,  ...,  0.0032,  0.0087,  0.0045],
        [-0.0006,  0.0094,  0.0024,  ...,  0.0032,  0.0089,  0.0046],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2467.8262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5002, device='cuda:0')



h[100].sum tensor(-0.0999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7030, device='cuda:0')



h[200].sum tensor(-22.5957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0024,  ..., 0.0090, 0.0101, 0.0046],
        [0.0000, 0.0320, 0.0089,  ..., 0.0121, 0.0301, 0.0187],
        [0.0000, 0.0720, 0.0216,  ..., 0.0183, 0.0698, 0.0466],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64833.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0570, 0.0157, 0.0423,  ..., 0.1375, 0.0525, 0.0817],
        [0.0482, 0.0123, 0.0173,  ..., 0.1933, 0.0898, 0.1198],
        [0.0388, 0.0090, 0.0000,  ..., 0.2536, 0.1304, 0.1609],
        ...,
        [0.0702, 0.0213, 0.0980,  ..., 0.0848, 0.0197, 0.0436],
        [0.0701, 0.0213, 0.0979,  ..., 0.0847, 0.0197, 0.0436],
        [0.0702, 0.0213, 0.0979,  ..., 0.0848, 0.0197, 0.0436]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671166.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8690.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.2056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3087.6626, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1118.1511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6464],
        [-0.1117],
        [ 0.1926],
        ...,
        [-4.5400],
        [-4.5338],
        [-4.5349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344948., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0303],
        [1.0391],
        ...,
        [0.9982],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370210.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0287],
        [1.0304],
        [1.0391],
        ...,
        [0.9981],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370213.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2504.1826, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3961, device='cuda:0')



h[100].sum tensor(-0.1028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8339, device='cuda:0')



h[200].sum tensor(-22.5899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0077, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66243.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0681, 0.0199, 0.0942,  ..., 0.0833, 0.0200, 0.0427],
        [0.0688, 0.0202, 0.0965,  ..., 0.0819, 0.0190, 0.0416],
        [0.0692, 0.0205, 0.0968,  ..., 0.0825, 0.0192, 0.0421],
        ...,
        [0.0708, 0.0213, 0.0987,  ..., 0.0846, 0.0196, 0.0433],
        [0.0707, 0.0212, 0.0986,  ..., 0.0846, 0.0196, 0.0433],
        [0.0708, 0.0213, 0.0986,  ..., 0.0846, 0.0196, 0.0433]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688177.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8816.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.8124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3194.6851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1129.5120, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4237],
        [-3.9748],
        [-4.3258],
        ...,
        [-4.5753],
        [-4.5690],
        [-4.5700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-376647.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0287],
        [1.0304],
        [1.0391],
        ...,
        [0.9981],
        [0.9971],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370213.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0287],
        [1.0304],
        [1.0392],
        ...,
        [0.9981],
        [0.9971],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370215.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [-0.0012,  0.0173,  0.0052,  ...,  0.0044,  0.0168,  0.0112],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2339.7583, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2468, device='cuda:0')



h[100].sum tensor(-0.0912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5199, device='cuda:0')



h[200].sum tensor(-22.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0202, 0.0053,  ..., 0.0103, 0.0183, 0.0115],
        [0.0000, 0.0306, 0.0090,  ..., 0.0119, 0.0286, 0.0201],
        [0.0000, 0.1336, 0.0431,  ..., 0.0278, 0.1309, 0.0977],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61908.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0459, 0.0115, 0.0110,  ..., 0.2321, 0.1258, 0.1428],
        [0.0357, 0.0086, 0.0000,  ..., 0.3103, 0.1865, 0.1942],
        [0.0164, 0.0031, 0.0000,  ..., 0.4516, 0.2941, 0.2877],
        ...,
        [0.0711, 0.0212, 0.0991,  ..., 0.0846, 0.0196, 0.0431],
        [0.0711, 0.0212, 0.0991,  ..., 0.0846, 0.0196, 0.0431],
        [0.0711, 0.0212, 0.0991,  ..., 0.0846, 0.0196, 0.0431]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(659123.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8915.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.7301, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3193.6726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1096.4471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2280],
        [ 0.2129],
        [ 0.1998],
        ...,
        [-4.5981],
        [-4.5915],
        [-4.5925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-373776.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0287],
        [1.0304],
        [1.0392],
        ...,
        [0.9981],
        [0.9971],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370215.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0305],
        [1.0392],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370218.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0057,  0.0799,  0.0269,  ...,  0.0140,  0.0789,  0.0630],
        [-0.0022,  0.0322,  0.0104,  ...,  0.0067,  0.0315,  0.0235],
        [-0.0012,  0.0172,  0.0052,  ...,  0.0044,  0.0167,  0.0112],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2458.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6107, device='cuda:0')



h[100].sum tensor(-0.0982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7192, device='cuda:0')



h[200].sum tensor(-22.5934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1400, 0.0453,  ..., 0.0287, 0.1373, 0.1030],
        [0.0000, 0.1780, 0.0585,  ..., 0.0346, 0.1751, 0.1345],
        [0.0000, 0.1051, 0.0331,  ..., 0.0234, 0.1027, 0.0741],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65359.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0000,  ..., 0.5627, 0.3840, 0.3616],
        [0.0043, 0.0000, 0.0000,  ..., 0.5740, 0.3913, 0.3693],
        [0.0132, 0.0010, 0.0000,  ..., 0.4596, 0.2942, 0.2957],
        ...,
        [0.0710, 0.0210, 0.0990,  ..., 0.0846, 0.0195, 0.0435],
        [0.0710, 0.0209, 0.0989,  ..., 0.0845, 0.0195, 0.0435],
        [0.0710, 0.0210, 0.0989,  ..., 0.0846, 0.0195, 0.0435]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676339.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8772.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.3551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3061.3188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1127.6168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2041],
        [ 0.2097],
        [ 0.2316],
        ...,
        [-4.5943],
        [-4.5878],
        [-4.5888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350517., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0305],
        [1.0392],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370218.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0306],
        [1.0393],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370221.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032,  0.0453,  0.0149,  ...,  0.0087,  0.0446,  0.0344],
        [-0.0023,  0.0328,  0.0105,  ...,  0.0068,  0.0321,  0.0240],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [-0.0012,  0.0174,  0.0052,  ...,  0.0044,  0.0168,  0.0113]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3150.4785, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7258, device='cuda:0')



h[100].sum tensor(-0.1430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0510, device='cuda:0')



h[200].sum tensor(-22.5232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1763, 0.0577,  ..., 0.0342, 0.1734, 0.1330],
        [0.0000, 0.0872, 0.0274,  ..., 0.0205, 0.0849, 0.0618],
        [0.0000, 0.0490, 0.0147,  ..., 0.0147, 0.0469, 0.0327],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0209, 0.0055,  ..., 0.0105, 0.0189, 0.0118],
        [0.0000, 0.0350, 0.0098,  ..., 0.0127, 0.0330, 0.0210]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78753.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0083, 0.0006, 0.0000,  ..., 0.5682, 0.3944, 0.3667],
        [0.0250, 0.0055, 0.0000,  ..., 0.3829, 0.2464, 0.2445],
        [0.0449, 0.0106, 0.0173,  ..., 0.2424, 0.1370, 0.1511],
        ...,
        [0.0665, 0.0183, 0.0724,  ..., 0.1095, 0.0360, 0.0617],
        [0.0601, 0.0157, 0.0426,  ..., 0.1505, 0.0653, 0.0896],
        [0.0516, 0.0122, 0.0189,  ..., 0.2049, 0.1039, 0.1267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744743.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8454.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.1909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.2598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2859.9326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1243.2399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2040],
        [-0.0244],
        [-0.7671],
        ...,
        [-2.4511],
        [-1.6755],
        [-0.7699]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320122.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0306],
        [1.0393],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370221.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0306],
        [1.0393],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370224.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2847.4851, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7230, device='cuda:0')



h[100].sum tensor(-0.1213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4661, device='cuda:0')



h[200].sum tensor(-22.5544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69637.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0631, 0.0162, 0.0701,  ..., 0.1061, 0.0329, 0.0612],
        [0.0633, 0.0163, 0.0701,  ..., 0.1068, 0.0332, 0.0617],
        [0.0639, 0.0166, 0.0707,  ..., 0.1072, 0.0333, 0.0619],
        ...,
        [0.0706, 0.0198, 0.0980,  ..., 0.0844, 0.0194, 0.0449],
        [0.0706, 0.0198, 0.0980,  ..., 0.0843, 0.0194, 0.0449],
        [0.0706, 0.0198, 0.0980,  ..., 0.0843, 0.0194, 0.0449]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684257.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8737.2139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.6370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3039.5911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1158.2941, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3754],
        [-1.1857],
        [-0.9840],
        ...,
        [-4.5680],
        [-4.5618],
        [-4.5628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342014.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0306],
        [1.0393],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370224.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0307],
        [1.0394],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370227.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [-0.0016,  0.0228,  0.0071,  ...,  0.0052,  0.0223,  0.0157],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3044.6028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4951, device='cuda:0')



h[100].sum tensor(-0.1328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7250, device='cuda:0')



h[200].sum tensor(-22.5343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0855, 0.0261,  ..., 0.0202, 0.0832, 0.0578],
        [0.0000, 0.0220, 0.0058,  ..., 0.0104, 0.0201, 0.0128],
        [0.0000, 0.0261, 0.0073,  ..., 0.0111, 0.0242, 0.0162],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76788.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0474, 0.0094, 0.0000,  ..., 0.2088, 0.1095, 0.1315],
        [0.0563, 0.0132, 0.0266,  ..., 0.1563, 0.0718, 0.0953],
        [0.0594, 0.0147, 0.0438,  ..., 0.1407, 0.0607, 0.0845],
        ...,
        [0.0706, 0.0194, 0.0979,  ..., 0.0842, 0.0193, 0.0454],
        [0.0705, 0.0194, 0.0978,  ..., 0.0841, 0.0193, 0.0453],
        [0.0705, 0.0194, 0.0978,  ..., 0.0842, 0.0193, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(730119., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8490.7861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.0040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.7783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2803.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1223.6760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6175],
        [-1.9553],
        [-2.4503],
        ...,
        [-4.5613],
        [-4.5546],
        [-4.5552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293963., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0307],
        [1.0394],
        ...,
        [0.9981],
        [0.9970],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370227.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0307],
        [1.0394],
        ...,
        [0.9980],
        [0.9970],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370230.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0103,  0.0027,  ...,  0.0033,  0.0098,  0.0054],
        [-0.0013,  0.0195,  0.0059,  ...,  0.0047,  0.0190,  0.0130],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2078.5540, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0097, device='cuda:0')



h[100].sum tensor(-0.0702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0469, device='cuda:0')



h[200].sum tensor(-22.6310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0524, 0.0153,  ..., 0.0150, 0.0503, 0.0330],
        [0.0000, 0.0289, 0.0077,  ..., 0.0115, 0.0270, 0.0160],
        [0.0000, 0.0737, 0.0220,  ..., 0.0184, 0.0714, 0.0480],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58363.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0348, 0.0041, 0.0000,  ..., 0.2804, 0.1575, 0.1824],
        [0.0456, 0.0082, 0.0046,  ..., 0.2159, 0.1102, 0.1380],
        [0.0471, 0.0087, 0.0000,  ..., 0.2121, 0.1077, 0.1350],
        ...,
        [0.0710, 0.0195, 0.0984,  ..., 0.0839, 0.0191, 0.0451],
        [0.0710, 0.0194, 0.0984,  ..., 0.0839, 0.0191, 0.0451],
        [0.0710, 0.0195, 0.0984,  ..., 0.0839, 0.0191, 0.0451]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643778.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8977.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.2190, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3185.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1058.6970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3143],
        [ 0.2289],
        [ 0.0109],
        ...,
        [-4.5963],
        [-4.5899],
        [-4.5908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-375304.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0307],
        [1.0394],
        ...,
        [0.9980],
        [0.9970],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370230.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0395],
        ...,
        [0.9980],
        [0.9970],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370233.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2590.3027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3624, device='cuda:0')



h[100].sum tensor(-0.1026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9751, device='cuda:0')



h[200].sum tensor(-22.5780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0076, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0078, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0078, 0.0015, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0078, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68620.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0690, 0.0188, 0.0965,  ..., 0.0806, 0.0183, 0.0427],
        [0.0693, 0.0189, 0.0968,  ..., 0.0809, 0.0184, 0.0429],
        [0.0698, 0.0192, 0.0972,  ..., 0.0816, 0.0185, 0.0434],
        ...,
        [0.0710, 0.0197, 0.0974,  ..., 0.0853, 0.0198, 0.0459],
        [0.0713, 0.0199, 0.0990,  ..., 0.0836, 0.0189, 0.0446],
        [0.0713, 0.0199, 0.0990,  ..., 0.0837, 0.0189, 0.0447]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689617.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8573.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.3632, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2758.3191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1159.0973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6719],
        [-4.6546],
        [-4.5844],
        ...,
        [-4.4592],
        [-4.5598],
        [-4.6042]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299345.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0395],
        ...,
        [0.9980],
        [0.9970],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370233.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 220.0 event: 1100 loss: tensor(488.2199, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0395],
        ...,
        [0.9980],
        [0.9969],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370236.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0187,  0.0057,  ...,  0.0046,  0.0181,  0.0123],
        [-0.0013,  0.0191,  0.0058,  ...,  0.0047,  0.0185,  0.0127],
        [-0.0017,  0.0250,  0.0079,  ...,  0.0056,  0.0245,  0.0176],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2420.4302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6172, device='cuda:0')



h[100].sum tensor(-0.0909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7201, device='cuda:0')



h[200].sum tensor(-22.5950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0865, 0.0268,  ..., 0.0204, 0.0841, 0.0587],
        [0.0000, 0.0825, 0.0254,  ..., 0.0198, 0.0802, 0.0554],
        [0.0000, 0.0883, 0.0279,  ..., 0.0208, 0.0860, 0.0627],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0078, 0.0014, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0078, 0.0014, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0078, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63667.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0359, 0.0062, 0.0000,  ..., 0.2912, 0.1681, 0.1871],
        [0.0304, 0.0044, 0.0000,  ..., 0.3301, 0.1970, 0.2135],
        [0.0289, 0.0048, 0.0000,  ..., 0.3484, 0.2138, 0.2251],
        ...,
        [0.0717, 0.0205, 0.0998,  ..., 0.0834, 0.0187, 0.0441],
        [0.0716, 0.0205, 0.0997,  ..., 0.0834, 0.0187, 0.0441],
        [0.0716, 0.0205, 0.0997,  ..., 0.0834, 0.0187, 0.0441]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668327.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9078.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.0348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3273.8708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1101.3610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2669],
        [ 0.3235],
        [ 0.3230],
        ...,
        [-4.6505],
        [-4.6438],
        [-4.6447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-405636.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0395],
        ...,
        [0.9980],
        [0.9969],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370236.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0396],
        ...,
        [0.9980],
        [0.9969],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370239.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0020,  0.0302,  0.0097,  ...,  0.0064,  0.0296,  0.0219],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025],
        ...,
        [-0.0009,  0.0133,  0.0038,  ...,  0.0038,  0.0128,  0.0079],
        [-0.0019,  0.0284,  0.0091,  ...,  0.0061,  0.0278,  0.0204],
        [-0.0014,  0.0204,  0.0063,  ...,  0.0049,  0.0198,  0.0137]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2582.1758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4860, device='cuda:0')



h[100].sum tensor(-0.1000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9932, device='cuda:0')



h[200].sum tensor(-22.5786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1151, 0.0373,  ..., 0.0249, 0.1126, 0.0850],
        [0.0000, 0.0757, 0.0236,  ..., 0.0188, 0.0734, 0.0523],
        [0.0000, 0.0361, 0.0104,  ..., 0.0128, 0.0340, 0.0221],
        ...,
        [0.0000, 0.0567, 0.0175,  ..., 0.0161, 0.0545, 0.0390],
        [0.0000, 0.0971, 0.0305,  ..., 0.0223, 0.0947, 0.0672],
        [0.0000, 0.1137, 0.0362,  ..., 0.0249, 0.1111, 0.0809]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66062.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0095, 0.0017, 0.0000,  ..., 0.5094, 0.3453, 0.3314],
        [0.0200, 0.0037, 0.0000,  ..., 0.4112, 0.2645, 0.2666],
        [0.0287, 0.0058, 0.0000,  ..., 0.3522, 0.2173, 0.2272],
        ...,
        [0.0441, 0.0106, 0.0071,  ..., 0.2550, 0.1400, 0.1622],
        [0.0330, 0.0065, 0.0000,  ..., 0.3244, 0.1895, 0.2099],
        [0.0329, 0.0068, 0.0000,  ..., 0.3299, 0.1958, 0.2130]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679750.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9028.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.2046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3331.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1123.0260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0578],
        [ 0.0702],
        [ 0.0772],
        ...,
        [-0.4864],
        [ 0.1745],
        [ 0.1095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-419136.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0396],
        ...,
        [0.9980],
        [0.9969],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370239.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0397],
        ...,
        [0.9979],
        [0.9969],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370242.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0401,  0.0132,  ...,  0.0079,  0.0394,  0.0300],
        [-0.0029,  0.0433,  0.0142,  ...,  0.0084,  0.0426,  0.0326],
        [-0.0027,  0.0398,  0.0130,  ...,  0.0079,  0.0391,  0.0298],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0003, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2323.2290, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1044, device='cuda:0')



h[100].sum tensor(-0.0820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4991, device='cuda:0')



h[200].sum tensor(-22.6065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1459, 0.0475,  ..., 0.0297, 0.1433, 0.1078],
        [0.0000, 0.1501, 0.0489,  ..., 0.0304, 0.1475, 0.1112],
        [0.0000, 0.1392, 0.0451,  ..., 0.0287, 0.1366, 0.1022],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0014, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0014, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0079, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62090.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0118, 0.0028, 0.0000,  ..., 0.4613, 0.3089, 0.3006],
        [0.0145, 0.0036, 0.0000,  ..., 0.4484, 0.2993, 0.2914],
        [0.0208, 0.0053, 0.0000,  ..., 0.4077, 0.2665, 0.2641],
        ...,
        [0.0713, 0.0207, 0.0997,  ..., 0.0833, 0.0186, 0.0443],
        [0.0712, 0.0207, 0.0997,  ..., 0.0833, 0.0186, 0.0443],
        [0.0712, 0.0207, 0.0997,  ..., 0.0833, 0.0186, 0.0443]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660006.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8869.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.3166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3135.4243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1094.0337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2428],
        [ 0.2332],
        [ 0.1538],
        ...,
        [-4.6444],
        [-4.6379],
        [-4.6389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-366505.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0308],
        [1.0397],
        ...,
        [0.9979],
        [0.9969],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370242.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0397],
        ...,
        [0.9979],
        [0.9969],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370245.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0110,  0.0030,  ...,  0.0034,  0.0105,  0.0059],
        [-0.0011,  0.0167,  0.0050,  ...,  0.0043,  0.0162,  0.0106],
        [-0.0023,  0.0345,  0.0112,  ...,  0.0071,  0.0339,  0.0253],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2483.1338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9895, device='cuda:0')



h[100].sum tensor(-0.0900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7745, device='cuda:0')



h[200].sum tensor(-22.5915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0408, 0.0114,  ..., 0.0134, 0.0388, 0.0233],
        [0.0000, 0.0896, 0.0277,  ..., 0.0210, 0.0873, 0.0610],
        [0.0000, 0.1130, 0.0359,  ..., 0.0246, 0.1106, 0.0803],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0078, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65271.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0385, 0.0077, 0.0076,  ..., 0.2404, 0.1240, 0.1571],
        [0.0240, 0.0040, 0.0000,  ..., 0.3412, 0.2024, 0.2251],
        [0.0146, 0.0016, 0.0000,  ..., 0.4125, 0.2592, 0.2727],
        ...,
        [0.0708, 0.0203, 0.0991,  ..., 0.0833, 0.0187, 0.0449],
        [0.0707, 0.0203, 0.0991,  ..., 0.0833, 0.0187, 0.0449],
        [0.0707, 0.0203, 0.0991,  ..., 0.0833, 0.0187, 0.0449]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672952.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8684.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.4022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3033.8706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1121.5001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1660],
        [ 0.2893],
        [ 0.2823],
        ...,
        [-4.6201],
        [-4.6135],
        [-4.6137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347952.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0397],
        ...,
        [0.9979],
        [0.9969],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370245.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0398],
        ...,
        [0.9979],
        [0.9968],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370248.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0161,  0.0048,  ...,  0.0042,  0.0156,  0.0101],
        [-0.0020,  0.0294,  0.0094,  ...,  0.0063,  0.0288,  0.0211],
        [-0.0005,  0.0079,  0.0019,  ...,  0.0029,  0.0075,  0.0034],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2351.7700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0921, device='cuda:0')



h[100].sum tensor(-0.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4973, device='cuda:0')



h[200].sum tensor(-22.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1230, 0.0392,  ..., 0.0260, 0.1205, 0.0885],
        [0.0000, 0.0687, 0.0204,  ..., 0.0177, 0.0666, 0.0436],
        [0.0000, 0.0649, 0.0190,  ..., 0.0171, 0.0628, 0.0404],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63102.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0204, 0.0023, 0.0000,  ..., 0.3555, 0.2163, 0.2360],
        [0.0253, 0.0030, 0.0000,  ..., 0.3140, 0.1788, 0.2094],
        [0.0300, 0.0045, 0.0000,  ..., 0.2801, 0.1498, 0.1872],
        ...,
        [0.0703, 0.0198, 0.0986,  ..., 0.0834, 0.0189, 0.0455],
        [0.0703, 0.0198, 0.0985,  ..., 0.0834, 0.0189, 0.0455],
        [0.0703, 0.0198, 0.0986,  ..., 0.0834, 0.0189, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664673.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8674.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.0731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3037.1646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1099.9253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3261],
        [ 0.3285],
        [ 0.3266],
        ...,
        [-4.6028],
        [-4.5967],
        [-4.5978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347306.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0398],
        ...,
        [0.9979],
        [0.9968],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370248.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0398],
        ...,
        [0.9979],
        [0.9968],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370251.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0026,  0.0385,  0.0125,  ...,  0.0077,  0.0379,  0.0286],
        [-0.0027,  0.0399,  0.0130,  ...,  0.0079,  0.0393,  0.0298],
        [-0.0012,  0.0184,  0.0055,  ...,  0.0046,  0.0179,  0.0120],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2468.2620, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6731, device='cuda:0')



h[100].sum tensor(-0.0862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7283, device='cuda:0')



h[200].sum tensor(-22.5947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1221, 0.0388,  ..., 0.0259, 0.1196, 0.0877],
        [0.0000, 0.1228, 0.0391,  ..., 0.0260, 0.1204, 0.0883],
        [0.0000, 0.1178, 0.0373,  ..., 0.0253, 0.1154, 0.0840],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65142.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.0038, 0.0000,  ..., 0.3616, 0.2303, 0.2387],
        [0.0208, 0.0037, 0.0000,  ..., 0.3649, 0.2324, 0.2413],
        [0.0262, 0.0053, 0.0000,  ..., 0.3310, 0.2046, 0.2187],
        ...,
        [0.0701, 0.0195, 0.0983,  ..., 0.0834, 0.0191, 0.0459],
        [0.0700, 0.0195, 0.0982,  ..., 0.0834, 0.0191, 0.0459],
        [0.0701, 0.0195, 0.0982,  ..., 0.0834, 0.0191, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670003.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8460.6279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8758, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.0167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2753.7224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1121.7509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3359],
        [ 0.3456],
        [ 0.3313],
        ...,
        [-4.5889],
        [-4.5695],
        [-4.5196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294522.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0398],
        ...,
        [0.9979],
        [0.9968],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370251.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0399],
        ...,
        [0.9978],
        [0.9968],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370254.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0095,  0.0024,  ...,  0.0032,  0.0090,  0.0046],
        [-0.0006,  0.0089,  0.0022,  ...,  0.0031,  0.0084,  0.0041],
        [-0.0011,  0.0175,  0.0052,  ...,  0.0044,  0.0170,  0.0113],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2063.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4325, device='cuda:0')



h[100].sum tensor(-0.0613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9626, device='cuda:0')



h[200].sum tensor(-22.6362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0318, 0.0087,  ..., 0.0120, 0.0298, 0.0182],
        [0.0000, 0.0630, 0.0183,  ..., 0.0168, 0.0609, 0.0388],
        [0.0000, 0.0321, 0.0082,  ..., 0.0121, 0.0301, 0.0157],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0018, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58401.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0471, 0.0108, 0.0175,  ..., 0.1824, 0.0838, 0.1178],
        [0.0397, 0.0080, 0.0000,  ..., 0.2241, 0.1119, 0.1473],
        [0.0455, 0.0102, 0.0099,  ..., 0.1942, 0.0901, 0.1263],
        ...,
        [0.0701, 0.0198, 0.0985,  ..., 0.0836, 0.0190, 0.0456],
        [0.0701, 0.0198, 0.0985,  ..., 0.0835, 0.0190, 0.0456],
        [0.0701, 0.0198, 0.0985,  ..., 0.0836, 0.0190, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645400.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8778.3154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.8783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3132.1423, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1056.0157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0497],
        [ 0.1157],
        [-0.1291],
        ...,
        [-4.6021],
        [-4.5959],
        [-4.5965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-364299.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0399],
        ...,
        [0.9978],
        [0.9968],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370254.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0400],
        ...,
        [0.9978],
        [0.9968],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370257.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2675.0601, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0278, device='cuda:0')



h[100].sum tensor(-0.0954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0723, device='cuda:0')



h[200].sum tensor(-22.5752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0079, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68377.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0665, 0.0188, 0.0901,  ..., 0.0868, 0.0217, 0.0479],
        [0.0663, 0.0187, 0.0885,  ..., 0.0891, 0.0227, 0.0495],
        [0.0663, 0.0189, 0.0873,  ..., 0.0913, 0.0237, 0.0512],
        ...,
        [0.0678, 0.0195, 0.0852,  ..., 0.0970, 0.0277, 0.0546],
        [0.0696, 0.0203, 0.0957,  ..., 0.0867, 0.0210, 0.0473],
        [0.0702, 0.0205, 0.0989,  ..., 0.0837, 0.0189, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686633.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8476.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2882.9473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1149.1726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0814],
        [-2.8372],
        [-2.6091],
        ...,
        [-3.8821],
        [-4.2833],
        [-4.4987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315985.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0308],
        [1.0400],
        ...,
        [0.9978],
        [0.9968],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370257.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0309],
        [1.0401],
        ...,
        [0.9978],
        [0.9967],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370259.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0182,  0.0055,  ...,  0.0046,  0.0177,  0.0118],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2207.0786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3254, device='cuda:0')



h[100].sum tensor(-0.0679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2392, device='cuda:0')



h[200].sum tensor(-22.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0311, 0.0085,  ..., 0.0120, 0.0292, 0.0177],
        [0.0000, 0.0213, 0.0057,  ..., 0.0105, 0.0194, 0.0121],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61562.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0426, 0.0120, 0.0137,  ..., 0.2345, 0.1303, 0.1487],
        [0.0508, 0.0148, 0.0209,  ..., 0.1900, 0.0991, 0.1176],
        [0.0575, 0.0171, 0.0352,  ..., 0.1537, 0.0727, 0.0925],
        ...,
        [0.0705, 0.0208, 0.0995,  ..., 0.0839, 0.0189, 0.0447],
        [0.0705, 0.0208, 0.0995,  ..., 0.0838, 0.0189, 0.0447],
        [0.0705, 0.0208, 0.0995,  ..., 0.0838, 0.0189, 0.0447]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664014.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8813.9238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.0445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3201.3459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1083.3690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1231],
        [ 0.1064],
        [ 0.0925],
        ...,
        [-4.6441],
        [-4.6379],
        [-4.6387]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-379313.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0309],
        [1.0401],
        ...,
        [0.9978],
        [0.9967],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370259.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0309],
        [1.0402],
        ...,
        [0.9978],
        [0.9967],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370262.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0109,  0.0030,  ...,  0.0034,  0.0104,  0.0058],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2479.6777, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7617, device='cuda:0')



h[100].sum tensor(-0.0829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7412, device='cuda:0')



h[200].sum tensor(-22.5938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0199, 0.0046,  ..., 0.0102, 0.0180, 0.0084],
        [0.0000, 0.0137, 0.0030,  ..., 0.0093, 0.0118, 0.0059],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67719.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0563, 0.0153, 0.0388,  ..., 0.1382, 0.0523, 0.0840],
        [0.0611, 0.0171, 0.0596,  ..., 0.1184, 0.0410, 0.0694],
        [0.0651, 0.0187, 0.0789,  ..., 0.1007, 0.0294, 0.0570],
        ...,
        [0.0708, 0.0210, 0.1000,  ..., 0.0840, 0.0189, 0.0443],
        [0.0708, 0.0210, 0.0999,  ..., 0.0839, 0.0189, 0.0443],
        [0.0708, 0.0210, 0.1000,  ..., 0.0840, 0.0189, 0.0443]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702095.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8610.3184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.0713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2997.5103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1142.5674, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5512],
        [-2.6781],
        [-2.5916],
        ...,
        [-4.6671],
        [-4.6607],
        [-4.6614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347942.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0309],
        [1.0402],
        ...,
        [0.9978],
        [0.9967],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370262.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 230.0 event: 1150 loss: tensor(462.6546, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0310],
        [1.0403],
        ...,
        [0.9977],
        [0.9966],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370264.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0173,  0.0052,  ...,  0.0044,  0.0168,  0.0111],
        [-0.0019,  0.0295,  0.0094,  ...,  0.0063,  0.0289,  0.0212],
        [-0.0006,  0.0101,  0.0027,  ...,  0.0033,  0.0096,  0.0051],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2468.9155, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5971, device='cuda:0')



h[100].sum tensor(-0.0820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7172, device='cuda:0')



h[200].sum tensor(-22.5938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1372, 0.0443,  ..., 0.0283, 0.1347, 0.1002],
        [0.0000, 0.0963, 0.0301,  ..., 0.0220, 0.0940, 0.0664],
        [0.0000, 0.0538, 0.0164,  ..., 0.0155, 0.0518, 0.0365],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0078, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66441.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0143, 0.0043, 0.0000,  ..., 0.4147, 0.2665, 0.2708],
        [0.0221, 0.0060, 0.0000,  ..., 0.3615, 0.2233, 0.2354],
        [0.0375, 0.0105, 0.0089,  ..., 0.2697, 0.1543, 0.1728],
        ...,
        [0.0711, 0.0207, 0.1002,  ..., 0.0842, 0.0190, 0.0442],
        [0.0711, 0.0207, 0.1002,  ..., 0.0841, 0.0190, 0.0442],
        [0.0711, 0.0207, 0.1002,  ..., 0.0842, 0.0190, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686445.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8564.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.3060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2839.9956, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1134.8644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2291],
        [ 0.2146],
        [ 0.0692],
        ...,
        [-4.6876],
        [-4.6811],
        [-4.6818]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322120.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0310],
        [1.0403],
        ...,
        [0.9977],
        [0.9966],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370264.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0310],
        [1.0404],
        ...,
        [0.9977],
        [0.9966],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370266.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [-0.0010,  0.0165,  0.0049,  ...,  0.0042,  0.0159,  0.0104],
        [-0.0020,  0.0313,  0.0100,  ...,  0.0065,  0.0307,  0.0227],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2663.5442, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3844, device='cuda:0')



h[100].sum tensor(-0.0927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1244, device='cuda:0')



h[200].sum tensor(-22.5726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0325, 0.0090,  ..., 0.0120, 0.0305, 0.0189],
        [0.0000, 0.0604, 0.0181,  ..., 0.0164, 0.0583, 0.0394],
        [0.0000, 0.0893, 0.0276,  ..., 0.0209, 0.0871, 0.0606],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0077, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69489.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0499, 0.0122, 0.0257,  ..., 0.1892, 0.0933, 0.1176],
        [0.0357, 0.0082, 0.0048,  ..., 0.2785, 0.1601, 0.1787],
        [0.0227, 0.0047, 0.0000,  ..., 0.3625, 0.2234, 0.2361],
        ...,
        [0.0693, 0.0193, 0.0892,  ..., 0.0956, 0.0260, 0.0522],
        [0.0716, 0.0203, 0.1006,  ..., 0.0843, 0.0191, 0.0440],
        [0.0716, 0.0203, 0.1006,  ..., 0.0843, 0.0191, 0.0440]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699770.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8700.7764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.8941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2885.5327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1156.4913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1362],
        [ 0.2145],
        [ 0.2476],
        ...,
        [-3.7579],
        [-4.2831],
        [-4.5626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335437.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0310],
        [1.0404],
        ...,
        [0.9977],
        [0.9966],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370266.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0311],
        [1.0404],
        ...,
        [0.9976],
        [0.9966],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370267.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0097,  0.0025,  ...,  0.0032,  0.0093,  0.0049],
        [-0.0015,  0.0238,  0.0074,  ...,  0.0054,  0.0232,  0.0165],
        [-0.0019,  0.0289,  0.0092,  ...,  0.0061,  0.0283,  0.0207],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2769.0291, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8800, device='cuda:0')



h[100].sum tensor(-0.0984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3430, device='cuda:0')



h[200].sum tensor(-22.5601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0512, 0.0149,  ..., 0.0148, 0.0492, 0.0319],
        [0.0000, 0.0920, 0.0284,  ..., 0.0211, 0.0897, 0.0630],
        [0.0000, 0.1333, 0.0428,  ..., 0.0275, 0.1308, 0.0971],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70823.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0325, 0.0060, 0.0000,  ..., 0.2948, 0.1719, 0.1902],
        [0.0240, 0.0037, 0.0000,  ..., 0.3499, 0.2140, 0.2279],
        [0.0185, 0.0025, 0.0000,  ..., 0.3915, 0.2470, 0.2557],
        ...,
        [0.0721, 0.0198, 0.1011,  ..., 0.0845, 0.0192, 0.0438],
        [0.0721, 0.0198, 0.1011,  ..., 0.0845, 0.0192, 0.0438],
        [0.0721, 0.0198, 0.1011,  ..., 0.0845, 0.0192, 0.0438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(703906.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8895.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.0843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2992.7275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1162.8303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0833],
        [ 0.0936],
        [ 0.1048],
        ...,
        [-4.7484],
        [-4.7417],
        [-4.7423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-377701.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0311],
        [1.0404],
        ...,
        [0.9976],
        [0.9966],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370267.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0311],
        [1.0404],
        ...,
        [0.9976],
        [0.9966],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370267.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2127.7896, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9651, device='cuda:0')



h[100].sum tensor(-0.0636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1865, device='cuda:0')



h[200].sum tensor(-22.6245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0015, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0076, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59616.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0698, 0.0187, 0.0985,  ..., 0.0813, 0.0186, 0.0419],
        [0.0701, 0.0188, 0.0988,  ..., 0.0817, 0.0187, 0.0421],
        [0.0705, 0.0191, 0.0992,  ..., 0.0824, 0.0188, 0.0426],
        ...,
        [0.0721, 0.0198, 0.1011,  ..., 0.0845, 0.0192, 0.0438],
        [0.0721, 0.0198, 0.1011,  ..., 0.0845, 0.0192, 0.0438],
        [0.0721, 0.0198, 0.1011,  ..., 0.0845, 0.0192, 0.0438]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655268.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9068.2939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.6514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3067.2256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1067.7533, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7733],
        [-4.8033],
        [-4.8113],
        ...,
        [-4.7281],
        [-4.7278],
        [-4.7292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-375705.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0311],
        [1.0404],
        ...,
        [0.9976],
        [0.9966],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370267.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0311],
        [1.0405],
        ...,
        [0.9976],
        [0.9965],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370270.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2089.7354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3499, device='cuda:0')



h[100].sum tensor(-0.0611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0966, device='cuda:0')



h[200].sum tensor(-22.6278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0073, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0074, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0016, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0075, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58807.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0699, 0.0183, 0.0986,  ..., 0.0815, 0.0188, 0.0419],
        [0.0702, 0.0185, 0.0989,  ..., 0.0819, 0.0189, 0.0421],
        [0.0706, 0.0187, 0.0993,  ..., 0.0826, 0.0190, 0.0426],
        ...,
        [0.0723, 0.0194, 0.1012,  ..., 0.0847, 0.0195, 0.0439],
        [0.0722, 0.0194, 0.1012,  ..., 0.0847, 0.0195, 0.0439],
        [0.0722, 0.0194, 0.1012,  ..., 0.0847, 0.0195, 0.0439]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(651725.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9203.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.0632, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3178.8843, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1056.1080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3707],
        [-4.5776],
        [-4.7413],
        ...,
        [-4.7627],
        [-4.7559],
        [-4.7566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-407262.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0311],
        [1.0405],
        ...,
        [0.9976],
        [0.9965],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370270.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0312],
        [1.0406],
        ...,
        [0.9976],
        [0.9965],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370273., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [-0.0006,  0.0103,  0.0027,  ...,  0.0032,  0.0098,  0.0053],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2308.8252, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0398, device='cuda:0')



h[100].sum tensor(-0.0716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4897, device='cuda:0')



h[200].sum tensor(-22.6066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0178, 0.0038,  ..., 0.0095, 0.0160, 0.0068],
        [0.0000, 0.0131, 0.0028,  ..., 0.0088, 0.0113, 0.0055],
        [0.0000, 0.0114, 0.0022,  ..., 0.0086, 0.0096, 0.0041],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62375.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0545, 0.0114, 0.0332,  ..., 0.1463, 0.0555, 0.0903],
        [0.0593, 0.0135, 0.0523,  ..., 0.1281, 0.0454, 0.0766],
        [0.0614, 0.0145, 0.0597,  ..., 0.1219, 0.0415, 0.0719],
        ...,
        [0.0720, 0.0191, 0.1011,  ..., 0.0849, 0.0197, 0.0442],
        [0.0720, 0.0191, 0.1010,  ..., 0.0849, 0.0197, 0.0441],
        [0.0720, 0.0191, 0.1010,  ..., 0.0849, 0.0197, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665950.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9046.7822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3064.2017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1088.8722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8609],
        [-1.4904],
        [-2.1462],
        ...,
        [-4.7603],
        [-4.7535],
        [-4.7542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-389926.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0312],
        [1.0406],
        ...,
        [0.9976],
        [0.9965],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370273., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0312],
        [1.0406],
        ...,
        [0.9976],
        [0.9965],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370276.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0081,  0.0019,  ...,  0.0029,  0.0076,  0.0035],
        [-0.0005,  0.0081,  0.0019,  ...,  0.0029,  0.0076,  0.0035],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2751.2678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6241, device='cuda:0')



h[100].sum tensor(-0.0926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3056, device='cuda:0')



h[200].sum tensor(-22.5648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0304, 0.0075,  ..., 0.0116, 0.0285, 0.0146],
        [0.0000, 0.0238, 0.0052,  ..., 0.0105, 0.0219, 0.0091],
        [0.0000, 0.0171, 0.0035,  ..., 0.0095, 0.0152, 0.0061],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0076, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0076, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0076, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72262.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0478, 0.0087, 0.0131,  ..., 0.1768, 0.0751, 0.1122],
        [0.0497, 0.0096, 0.0177,  ..., 0.1664, 0.0672, 0.1051],
        [0.0555, 0.0122, 0.0373,  ..., 0.1439, 0.0540, 0.0886],
        ...,
        [0.0714, 0.0191, 0.1007,  ..., 0.0852, 0.0200, 0.0445],
        [0.0713, 0.0191, 0.1006,  ..., 0.0852, 0.0200, 0.0445],
        [0.0714, 0.0191, 0.1007,  ..., 0.0852, 0.0200, 0.0445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723672.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8777.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.8724, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2938.5947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1172.8076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5368],
        [-0.6909],
        [-1.2724],
        ...,
        [-4.7416],
        [-4.7352],
        [-4.7360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-369103.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0312],
        [1.0406],
        ...,
        [0.9976],
        [0.9965],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370276.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0312],
        [1.0407],
        ...,
        [0.9975],
        [0.9965],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370279.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0018,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2125.9880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9399, device='cuda:0')



h[100].sum tensor(-0.0575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0367, device='cuda:0')



h[200].sum tensor(-22.6307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0075, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0076, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0077, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60211.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0676, 0.0180, 0.0958,  ..., 0.0843, 0.0201, 0.0445],
        [0.0684, 0.0184, 0.0974,  ..., 0.0834, 0.0198, 0.0435],
        [0.0691, 0.0188, 0.0985,  ..., 0.0835, 0.0198, 0.0434],
        ...,
        [0.0707, 0.0195, 0.1003,  ..., 0.0856, 0.0202, 0.0446],
        [0.0706, 0.0195, 0.1003,  ..., 0.0855, 0.0202, 0.0446],
        [0.0707, 0.0195, 0.1003,  ..., 0.0856, 0.0202, 0.0446]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657073.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8736.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.3156, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2925.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1075.5148, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4210],
        [-3.8992],
        [-4.2584],
        ...,
        [-4.7020],
        [-4.6952],
        [-4.6954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346885.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0312],
        [1.0407],
        ...,
        [0.9975],
        [0.9965],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370279.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0312],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370283.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0025],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0025]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2339.4946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2880, device='cuda:0')



h[100].sum tensor(-0.0660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3798, device='cuda:0')



h[200].sum tensor(-22.6129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0231, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0077, 0.0017, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0078, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0079, 0.0017, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0079, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62819.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0672, 0.0190, 0.0962,  ..., 0.0842, 0.0201, 0.0437],
        [0.0666, 0.0187, 0.0933,  ..., 0.0879, 0.0217, 0.0466],
        [0.0648, 0.0180, 0.0845,  ..., 0.0978, 0.0268, 0.0539],
        ...,
        [0.0701, 0.0203, 0.1002,  ..., 0.0860, 0.0204, 0.0444],
        [0.0700, 0.0203, 0.1001,  ..., 0.0860, 0.0204, 0.0443],
        [0.0700, 0.0203, 0.1002,  ..., 0.0860, 0.0204, 0.0444]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665710.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8502.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.6924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2845.5303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1102.2252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4976],
        [-2.9284],
        [-2.1846],
        ...,
        [-4.7007],
        [-4.6947],
        [-4.6956]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316221.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0312],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370283.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0313],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370286.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3667.9756, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.6075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0778, device='cuda:0')



h[100].sum tensor(-0.1309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6868, device='cuda:0')



h[200].sum tensor(-22.4828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0079, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0080, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0081, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0081, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0081, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88950.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0656, 0.0195, 0.0875,  ..., 0.0932, 0.0264, 0.0487],
        [0.0674, 0.0202, 0.0961,  ..., 0.0855, 0.0210, 0.0434],
        [0.0668, 0.0200, 0.0915,  ..., 0.0911, 0.0242, 0.0473],
        ...,
        [0.0697, 0.0213, 0.1004,  ..., 0.0864, 0.0204, 0.0438],
        [0.0697, 0.0213, 0.1003,  ..., 0.0863, 0.0204, 0.0437],
        [0.0697, 0.0213, 0.1003,  ..., 0.0864, 0.0204, 0.0437]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(806097.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8102.4951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.2213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.2166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2745.2610, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1328.6724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0083],
        [-2.8327],
        [-3.2102],
        ...,
        [-4.6971],
        [-4.6912],
        [-4.6921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298824.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0313],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370286.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 240.0 event: 1200 loss: tensor(457.0026, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0313],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370288.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0129,  0.0036,  ...,  0.0038,  0.0123,  0.0074],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2825.2007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8284, device='cuda:0')



h[100].sum tensor(-0.0874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1893, device='cuda:0')



h[200].sum tensor(-22.5672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0348, 0.0097,  ..., 0.0128, 0.0326, 0.0206],
        [0.0000, 0.0159, 0.0037,  ..., 0.0099, 0.0138, 0.0076],
        [0.0000, 0.0035, 0.0000,  ..., 0.0081, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0082, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0082, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0082, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72363.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0439, 0.0120, 0.0181,  ..., 0.2198, 0.1145, 0.1336],
        [0.0570, 0.0166, 0.0455,  ..., 0.1451, 0.0612, 0.0831],
        [0.0652, 0.0199, 0.0816,  ..., 0.1016, 0.0314, 0.0535],
        ...,
        [0.0699, 0.0219, 0.1009,  ..., 0.0867, 0.0204, 0.0431],
        [0.0698, 0.0219, 0.1009,  ..., 0.0866, 0.0204, 0.0431],
        [0.0698, 0.0219, 0.1009,  ..., 0.0866, 0.0204, 0.0431]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(721685.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8489.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.4925, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3105.9998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1180.6306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4135],
        [-1.2662],
        [-2.1555],
        ...,
        [-4.7187],
        [-4.7127],
        [-4.7136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-365364.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0313],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370288.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0291],
        [1.0314],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370290.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [-0.0007,  0.0116,  0.0032,  ...,  0.0036,  0.0110,  0.0063],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0003, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3188.9788, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7274, device='cuda:0')



h[100].sum tensor(-0.1043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9051, device='cuda:0')



h[200].sum tensor(-22.5308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0080, 0.0014, 0.0000],
        [0.0000, 0.0145, 0.0033,  ..., 0.0098, 0.0124, 0.0065],
        [0.0000, 0.0126, 0.0026,  ..., 0.0095, 0.0105, 0.0048],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76935.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0658, 0.0201, 0.0892,  ..., 0.0931, 0.0255, 0.0473],
        [0.0616, 0.0184, 0.0679,  ..., 0.1154, 0.0384, 0.0627],
        [0.0581, 0.0169, 0.0519,  ..., 0.1329, 0.0469, 0.0752],
        ...,
        [0.0699, 0.0221, 0.1013,  ..., 0.0869, 0.0205, 0.0426],
        [0.0699, 0.0221, 0.1013,  ..., 0.0868, 0.0205, 0.0426],
        [0.0699, 0.0221, 0.1013,  ..., 0.0869, 0.0205, 0.0426]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(737442.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8217.1191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.0492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.7879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2841.7104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1227.2496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4644],
        [-2.6504],
        [-1.7460],
        ...,
        [-4.7204],
        [-4.7143],
        [-4.7152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319639.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0291],
        [1.0314],
        [1.0407],
        ...,
        [0.9975],
        [0.9964],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370290.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0291],
        [1.0314],
        [1.0408],
        ...,
        [0.9974],
        [0.9964],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370292.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0160,  0.0047,  ...,  0.0043,  0.0154,  0.0099],
        [-0.0017,  0.0277,  0.0087,  ...,  0.0061,  0.0270,  0.0196],
        [-0.0031,  0.0493,  0.0162,  ...,  0.0095,  0.0485,  0.0374],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0003, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2515.8452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2405, device='cuda:0')



h[100].sum tensor(-0.0705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6651, device='cuda:0')



h[200].sum tensor(-22.5977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0542, 0.0164,  ..., 0.0159, 0.0517, 0.0365],
        [0.0000, 0.1231, 0.0391,  ..., 0.0266, 0.1202, 0.0879],
        [0.0000, 0.1512, 0.0489,  ..., 0.0310, 0.1481, 0.1111],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0014, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65393.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.3607e-02, 8.3201e-03, 6.2850e-03,  ..., 2.8137e-01, 1.5922e-01,
         1.7378e-01],
        [1.4322e-02, 3.1603e-03, 0.0000e+00,  ..., 4.1808e-01, 2.6435e-01,
         2.6415e-01],
        [4.5722e-03, 2.5529e-04, 0.0000e+00,  ..., 5.2023e-01, 3.4569e-01,
         3.3133e-01],
        ...,
        [6.6507e-02, 2.0619e-02, 8.3231e-02,  ..., 1.0516e-01, 3.2421e-02,
         5.4745e-02],
        [6.9719e-02, 2.2044e-02, 1.0136e-01,  ..., 8.7055e-02, 2.0676e-02,
         4.2478e-02],
        [6.9738e-02, 2.2057e-02, 1.0138e-01,  ..., 8.7085e-02, 2.0682e-02,
         4.2497e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680300.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8455.3721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.6459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3065.5413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1126.6517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2081],
        [ 0.2843],
        [ 0.2765],
        ...,
        [-2.7814],
        [-3.7897],
        [-4.3783]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-354267.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0291],
        [1.0314],
        [1.0408],
        ...,
        [0.9974],
        [0.9964],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370292.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0315],
        [1.0408],
        ...,
        [0.9974],
        [0.9963],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370293.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0143,  0.0041,  ...,  0.0040,  0.0137,  0.0085],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [-0.0009,  0.0143,  0.0041,  ...,  0.0040,  0.0137,  0.0085],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0003, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0003, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2720.5598, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0506, device='cuda:0')



h[100].sum tensor(-0.0799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0757, device='cuda:0')



h[200].sum tensor(-22.5765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0148, 0.0033,  ..., 0.0098, 0.0126, 0.0066],
        [0.0000, 0.0538, 0.0149,  ..., 0.0158, 0.0514, 0.0308],
        [0.0000, 0.0150, 0.0033,  ..., 0.0099, 0.0127, 0.0067],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0083, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0083, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0083, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70065.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0582, 0.0162, 0.0503,  ..., 0.1327, 0.0506, 0.0740],
        [0.0498, 0.0122, 0.0082,  ..., 0.1776, 0.0777, 0.1048],
        [0.0521, 0.0135, 0.0225,  ..., 0.1646, 0.0676, 0.0964],
        ...,
        [0.0697, 0.0217, 0.1015,  ..., 0.0873, 0.0209, 0.0426],
        [0.0696, 0.0217, 0.1015,  ..., 0.0873, 0.0209, 0.0426],
        [0.0696, 0.0217, 0.1015,  ..., 0.0873, 0.0209, 0.0426]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(707470.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8279.0840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.9152, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2867.5620, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1169.8149, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6782],
        [-0.7749],
        [-0.1530],
        ...,
        [-4.7477],
        [-4.7417],
        [-4.7426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315184., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0315],
        [1.0408],
        ...,
        [0.9974],
        [0.9963],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370293.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0315],
        [1.0408],
        ...,
        [0.9974],
        [0.9963],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370294.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [-0.0005,  0.0091,  0.0022,  ...,  0.0032,  0.0086,  0.0042],
        [-0.0005,  0.0087,  0.0021,  ...,  0.0032,  0.0082,  0.0039],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0019,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2311.0957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8847, device='cuda:0')



h[100].sum tensor(-0.0602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3209, device='cuda:0')



h[200].sum tensor(-22.6158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0121, 0.0023,  ..., 0.0093, 0.0099, 0.0043],
        [0.0000, 0.0315, 0.0083,  ..., 0.0123, 0.0291, 0.0176],
        [0.0000, 0.0618, 0.0175,  ..., 0.0170, 0.0592, 0.0372],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62403.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0559, 0.0141, 0.0454,  ..., 0.1392, 0.0531, 0.0797],
        [0.0471, 0.0098, 0.0197,  ..., 0.1885, 0.0850, 0.1132],
        [0.0398, 0.0062, 0.0000,  ..., 0.2317, 0.1129, 0.1425],
        ...,
        [0.0696, 0.0209, 0.1016,  ..., 0.0875, 0.0213, 0.0430],
        [0.0695, 0.0209, 0.1015,  ..., 0.0875, 0.0213, 0.0430],
        [0.0695, 0.0209, 0.1015,  ..., 0.0875, 0.0213, 0.0430]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665555.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8431.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.7468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2930.8687, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1102.3839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0359],
        [-0.2985],
        [ 0.0577],
        ...,
        [-4.7539],
        [-4.7479],
        [-4.7488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335790.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0315],
        [1.0408],
        ...,
        [0.9974],
        [0.9963],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370294.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0316],
        [1.0407],
        ...,
        [0.9974],
        [0.9963],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370296.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2747.7925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6522, device='cuda:0')



h[100].sum tensor(-0.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1636, device='cuda:0')



h[200].sum tensor(-22.5709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0080, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0080, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0080, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68227.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0193, 0.0990,  ..., 0.0845, 0.0209, 0.0412],
        [0.0677, 0.0194, 0.0994,  ..., 0.0849, 0.0210, 0.0414],
        [0.0681, 0.0197, 0.0998,  ..., 0.0856, 0.0211, 0.0418],
        ...,
        [0.0696, 0.0204, 0.1017,  ..., 0.0877, 0.0216, 0.0431],
        [0.0696, 0.0204, 0.1016,  ..., 0.0877, 0.0216, 0.0431],
        [0.0696, 0.0204, 0.1016,  ..., 0.0877, 0.0216, 0.0431]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(685214.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8374.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.9313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2826.1409, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1151.7589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9410],
        [-4.3376],
        [-4.6079],
        ...,
        [-4.7685],
        [-4.7624],
        [-4.7632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324532.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0316],
        [1.0407],
        ...,
        [0.9974],
        [0.9963],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370296.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0316],
        [1.0407],
        ...,
        [0.9973],
        [0.9963],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370297.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3108.7915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.8209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0958, device='cuda:0')



h[100].sum tensor(-0.0972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8128, device='cuda:0')



h[200].sum tensor(-22.5336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0080, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0080, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0080, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0016, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77462.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0628, 0.0165, 0.0750,  ..., 0.1090, 0.0360, 0.0578],
        [0.0640, 0.0172, 0.0799,  ..., 0.1046, 0.0335, 0.0548],
        [0.0572, 0.0143, 0.0446,  ..., 0.1507, 0.0684, 0.0854],
        ...,
        [0.0698, 0.0202, 0.1018,  ..., 0.0879, 0.0218, 0.0429],
        [0.0698, 0.0201, 0.1018,  ..., 0.0878, 0.0218, 0.0429],
        [0.0698, 0.0202, 0.1018,  ..., 0.0878, 0.0218, 0.0429]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744810.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8144.7993, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.0946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.4979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2620.5837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1233.5350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4997],
        [-0.3045],
        [-0.0104],
        ...,
        [-4.7889],
        [-4.7824],
        [-4.7831]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307633.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0316],
        [1.0407],
        ...,
        [0.9973],
        [0.9963],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370297.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0317],
        [1.0408],
        ...,
        [0.9973],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370299.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0208,  0.0062,  ...,  0.0050,  0.0202,  0.0138],
        [-0.0018,  0.0301,  0.0094,  ...,  0.0064,  0.0294,  0.0215],
        [-0.0013,  0.0208,  0.0062,  ...,  0.0050,  0.0202,  0.0138],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0019,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3008.6680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1185, device='cuda:0')



h[100].sum tensor(-0.0918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6700, device='cuda:0')



h[200].sum tensor(-22.5428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1134, 0.0353,  ..., 0.0249, 0.1104, 0.0798],
        [0.0000, 0.1215, 0.0381,  ..., 0.0261, 0.1185, 0.0865],
        [0.0000, 0.2061, 0.0673,  ..., 0.0392, 0.2024, 0.1561],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0015, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0082, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74184.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0097, 0.0000, 0.0000,  ..., 0.4467, 0.2867, 0.2815],
        [0.0049, 0.0000, 0.0000,  ..., 0.5096, 0.3376, 0.3226],
        [0.0000, 0.0000, 0.0000,  ..., 0.6362, 0.4407, 0.4051],
        ...,
        [0.0700, 0.0206, 0.1022,  ..., 0.0881, 0.0219, 0.0423],
        [0.0700, 0.0206, 0.1021,  ..., 0.0880, 0.0219, 0.0423],
        [0.0689, 0.0200, 0.0965,  ..., 0.0938, 0.0252, 0.0463]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717553.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8327.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(109.8559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2749.8738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1204.0907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4141],
        [ 0.4068],
        [ 0.3968],
        ...,
        [-4.7019],
        [-4.4992],
        [-4.1641]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340787.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0317],
        [1.0408],
        ...,
        [0.9973],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370299.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0317],
        [1.0408],
        ...,
        [0.9973],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370301.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0009, -0.0007,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0007,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2680.8357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7382, device='cuda:0')



h[100].sum tensor(-0.0757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0300, device='cuda:0')



h[200].sum tensor(-22.5760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0081, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0081, 0.0015, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0082, 0.0015, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0015, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0083, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68966.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0192, 0.0904,  ..., 0.0942, 0.0266, 0.0464],
        [0.0676, 0.0201, 0.0984,  ..., 0.0869, 0.0220, 0.0414],
        [0.0685, 0.0206, 0.1004,  ..., 0.0859, 0.0214, 0.0407],
        ...,
        [0.0700, 0.0213, 0.1023,  ..., 0.0881, 0.0219, 0.0419],
        [0.0700, 0.0213, 0.1022,  ..., 0.0881, 0.0219, 0.0418],
        [0.0700, 0.0213, 0.1022,  ..., 0.0881, 0.0219, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(696963.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8347.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2664, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(106.5569, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2716.3433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1161.4152, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6139],
        [-3.4221],
        [-3.9786],
        ...,
        [-4.8106],
        [-4.8033],
        [-4.8035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336859.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0317],
        [1.0408],
        ...,
        [0.9973],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370301.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0317],
        [1.0407],
        ...,
        [0.9973],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370304.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0445,  0.0145,  ...,  0.0087,  0.0437,  0.0333],
        [-0.0030,  0.0488,  0.0160,  ...,  0.0094,  0.0480,  0.0369],
        [-0.0019,  0.0314,  0.0099,  ...,  0.0067,  0.0307,  0.0225],
        ...,
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0009, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2150.2671, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2328, device='cuda:0')



h[100].sum tensor(-0.0506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0795, device='cuda:0')



h[200].sum tensor(-22.6297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.2113, 0.0693,  ..., 0.0402, 0.2077, 0.1603],
        [0.0000, 0.1777, 0.0576,  ..., 0.0351, 0.1743, 0.1325],
        [0.0000, 0.1364, 0.0433,  ..., 0.0287, 0.1334, 0.0985],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0084, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0084, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0084, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59276.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3461e-01, 4.3957e-01,
         4.0147e-01],
        [2.8197e-04, 0.0000e+00, 0.0000e+00,  ..., 5.8890e-01, 4.0094e-01,
         3.7192e-01],
        [5.9444e-03, 0.0000e+00, 0.0000e+00,  ..., 5.0984e-01, 3.3534e-01,
         3.2069e-01],
        ...,
        [6.9783e-02, 2.1781e-02, 1.0205e-01,  ..., 8.8083e-02, 2.1938e-02,
         4.1913e-02],
        [6.9726e-02, 2.1761e-02, 1.0196e-01,  ..., 8.8008e-02, 2.1920e-02,
         4.1877e-02],
        [6.9743e-02, 2.1772e-02, 1.0198e-01,  ..., 8.8035e-02, 2.1926e-02,
         4.1893e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655102., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8587.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.7908, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2866.5686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1073.2063, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3466],
        [ 0.3471],
        [ 0.3545],
        ...,
        [-4.8264],
        [-4.8193],
        [-4.8197]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370191.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0317],
        [1.0407],
        ...,
        [0.9973],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370304.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 250.0 event: 1250 loss: tensor(484.7149, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0318],
        [1.0408],
        ...,
        [0.9972],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370307.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0074,  0.0016,  ...,  0.0030,  0.0069,  0.0027],
        [-0.0004,  0.0074,  0.0016,  ...,  0.0030,  0.0069,  0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2357.3511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7576, device='cuda:0')



h[100].sum tensor(-0.0587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4484, device='cuda:0')



h[200].sum tensor(-22.6102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0157, 0.0029,  ..., 0.0102, 0.0137, 0.0046],
        [0.0000, 0.0158, 0.0029,  ..., 0.0102, 0.0138, 0.0046],
        [0.0000, 0.0158, 0.0030,  ..., 0.0102, 0.0138, 0.0046],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0085, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0085, 0.0016, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0085, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61858.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0553, 0.0154, 0.0535,  ..., 0.1326, 0.0422, 0.0748],
        [0.0544, 0.0150, 0.0494,  ..., 0.1378, 0.0441, 0.0784],
        [0.0557, 0.0157, 0.0534,  ..., 0.1348, 0.0424, 0.0761],
        ...,
        [0.0696, 0.0222, 0.1018,  ..., 0.0880, 0.0219, 0.0420],
        [0.0696, 0.0222, 0.1017,  ..., 0.0879, 0.0219, 0.0420],
        [0.0696, 0.0222, 0.1018,  ..., 0.0879, 0.0219, 0.0420]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663362.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8387.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.6474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2601.9927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1100.4446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7063],
        [-1.4742],
        [-1.5191],
        ...,
        [-4.8252],
        [-4.8181],
        [-4.8184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317143.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0318],
        [1.0408],
        ...,
        [0.9972],
        [0.9962],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370307.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0318],
        [1.0407],
        ...,
        [0.9972],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370309.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [-0.0010,  0.0163,  0.0047,  ...,  0.0044,  0.0157,  0.0100],
        [-0.0018,  0.0304,  0.0096,  ...,  0.0066,  0.0297,  0.0216],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2109.4910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.6368, device='cuda:0')



h[100].sum tensor(-0.0471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9925, device='cuda:0')



h[200].sum tensor(-22.6351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0193, 0.0048,  ..., 0.0108, 0.0173, 0.0103],
        [0.0000, 0.0590, 0.0180,  ..., 0.0169, 0.0567, 0.0402],
        [0.0000, 0.1409, 0.0451,  ..., 0.0296, 0.1381, 0.1021],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0016, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58506.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0530, 0.0160, 0.0388,  ..., 0.1722, 0.0838, 0.0981],
        [0.0349, 0.0101, 0.0078,  ..., 0.2894, 0.1715, 0.1753],
        [0.0138, 0.0043, 0.0000,  ..., 0.4558, 0.2982, 0.2844],
        ...,
        [0.0697, 0.0227, 0.1018,  ..., 0.0878, 0.0218, 0.0418],
        [0.0696, 0.0226, 0.1017,  ..., 0.0877, 0.0218, 0.0418],
        [0.0696, 0.0226, 0.1018,  ..., 0.0878, 0.0218, 0.0418]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655094.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8594.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(101.2183, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2757.9231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1065.2574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2496],
        [-0.3438],
        [ 0.1870],
        ...,
        [-4.8380],
        [-4.8307],
        [-4.8309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-364046.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0318],
        [1.0407],
        ...,
        [0.9972],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370309.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0318],
        [1.0407],
        ...,
        [0.9972],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370312.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0113,  0.0030,  ...,  0.0037,  0.0108,  0.0059],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [-0.0011,  0.0180,  0.0053,  ...,  0.0047,  0.0174,  0.0114],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2557.5669, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2123, device='cuda:0')



h[100].sum tensor(-0.0659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8071, device='cuda:0')



h[200].sum tensor(-22.5906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0591, 0.0174,  ..., 0.0170, 0.0569, 0.0375],
        [0.0000, 0.0552, 0.0154,  ..., 0.0164, 0.0531, 0.0315],
        [0.0000, 0.0396, 0.0113,  ..., 0.0140, 0.0376, 0.0242],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0086, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0086, 0.0017, 0.0000],
        [0.0000, 0.0268, 0.0074,  ..., 0.0122, 0.0248, 0.0163]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64869.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0281, 0.0071, 0.0000,  ..., 0.3155, 0.1795, 0.1940],
        [0.0350, 0.0090, 0.0000,  ..., 0.2691, 0.1414, 0.1638],
        [0.0386, 0.0108, 0.0000,  ..., 0.2539, 0.1330, 0.1533],
        ...,
        [0.0691, 0.0227, 0.0973,  ..., 0.0922, 0.0248, 0.0446],
        [0.0658, 0.0214, 0.0773,  ..., 0.1119, 0.0385, 0.0578],
        [0.0563, 0.0181, 0.0380,  ..., 0.1701, 0.0804, 0.0964]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681884.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8462.3564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(102.4280, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2626.5532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1122.9750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3296],
        [ 0.3445],
        [ 0.3470],
        ...,
        [-4.2536],
        [-3.4752],
        [-2.3782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358651.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0318],
        [1.0407],
        ...,
        [0.9972],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370312.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0319],
        [1.0407],
        ...,
        [0.9971],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370314.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0193,  0.0058,  ...,  0.0049,  0.0188,  0.0125],
        [-0.0010,  0.0169,  0.0050,  ...,  0.0045,  0.0164,  0.0105],
        [-0.0010,  0.0166,  0.0049,  ...,  0.0045,  0.0161,  0.0103],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2532.9663, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6155, device='cuda:0')



h[100].sum tensor(-0.0641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7199, device='cuda:0')



h[200].sum tensor(-22.5930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0719, 0.0212,  ..., 0.0189, 0.0698, 0.0453],
        [0.0000, 0.0941, 0.0289,  ..., 0.0224, 0.0918, 0.0635],
        [0.0000, 0.0903, 0.0276,  ..., 0.0219, 0.0880, 0.0603],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0086, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65536.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0243, 0.0060, 0.0000,  ..., 0.3318, 0.1887, 0.2063],
        [0.0151, 0.0032, 0.0000,  ..., 0.3941, 0.2359, 0.2474],
        [0.0140, 0.0030, 0.0000,  ..., 0.4051, 0.2439, 0.2546],
        ...,
        [0.0699, 0.0230, 0.1018,  ..., 0.0875, 0.0216, 0.0419],
        [0.0698, 0.0230, 0.1017,  ..., 0.0874, 0.0216, 0.0419],
        [0.0698, 0.0230, 0.1017,  ..., 0.0874, 0.0216, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687449.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8429.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(103.7542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2507.4163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1129.9479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2539],
        [ 0.3115],
        [ 0.3149],
        ...,
        [-4.6388],
        [-4.4781],
        [-4.3755]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344205.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0319],
        [1.0407],
        ...,
        [0.9971],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370314.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0319],
        [1.0408],
        ...,
        [0.9971],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370317.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0088,  0.0022,  ...,  0.0033,  0.0084,  0.0039],
        [-0.0005,  0.0090,  0.0022,  ...,  0.0033,  0.0086,  0.0040],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2281.7571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7023, device='cuda:0')



h[100].sum tensor(-0.0528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2942, device='cuda:0')



h[200].sum tensor(-22.6179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0608, 0.0174,  ..., 0.0172, 0.0588, 0.0361],
        [0.0000, 0.0313, 0.0084,  ..., 0.0127, 0.0295, 0.0174],
        [0.0000, 0.0118, 0.0023,  ..., 0.0097, 0.0102, 0.0041],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60593.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0395, 0.0111, 0.0000,  ..., 0.2289, 0.1104, 0.1396],
        [0.0469, 0.0139, 0.0183,  ..., 0.1901, 0.0851, 0.1134],
        [0.0553, 0.0171, 0.0396,  ..., 0.1470, 0.0569, 0.0845],
        ...,
        [0.0699, 0.0229, 0.1017,  ..., 0.0873, 0.0216, 0.0422],
        [0.0698, 0.0229, 0.1016,  ..., 0.0872, 0.0216, 0.0422],
        [0.0699, 0.0229, 0.1016,  ..., 0.0873, 0.0216, 0.0422]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663192.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8638.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(104.9537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2591.2983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1082.2573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1522],
        [ 0.0962],
        [-0.0330],
        ...,
        [-4.8711],
        [-4.8629],
        [-4.8611]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-379601.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0319],
        [1.0408],
        ...,
        [0.9971],
        [0.9961],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370317.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0320],
        [1.0408],
        ...,
        [0.9971],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370319.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2739.3989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1989, device='cuda:0')



h[100].sum tensor(-0.0709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0973, device='cuda:0')



h[200].sum tensor(-22.5733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0146, 0.0027,  ..., 0.0101, 0.0130, 0.0037],
        [0.0000, 0.0090, 0.0013,  ..., 0.0092, 0.0074, 0.0018],
        [0.0000, 0.0034, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0085, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68801.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0579, 0.0180, 0.0642,  ..., 0.1207, 0.0364, 0.0675],
        [0.0611, 0.0192, 0.0746,  ..., 0.1103, 0.0320, 0.0598],
        [0.0648, 0.0208, 0.0870,  ..., 0.0984, 0.0269, 0.0510],
        ...,
        [0.0699, 0.0229, 0.1015,  ..., 0.0872, 0.0216, 0.0425],
        [0.0698, 0.0229, 0.1014,  ..., 0.0871, 0.0216, 0.0425],
        [0.0698, 0.0229, 0.1014,  ..., 0.0872, 0.0216, 0.0425]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704121.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8388.9043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(107.2014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2404.3657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1155.9474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2290],
        [-3.5470],
        [-3.9077],
        ...,
        [-4.8747],
        [-4.8670],
        [-4.8669]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-351592.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0320],
        [1.0408],
        ...,
        [0.9971],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370319.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0295],
        [1.0320],
        [1.0408],
        ...,
        [0.9971],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370322.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2319.0005, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8825, device='cuda:0')



h[100].sum tensor(-0.0528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3206, device='cuda:0')



h[200].sum tensor(-22.6151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63041.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0675, 0.0217, 0.0987,  ..., 0.0839, 0.0207, 0.0408],
        [0.0678, 0.0218, 0.0991,  ..., 0.0843, 0.0208, 0.0410],
        [0.0682, 0.0221, 0.0995,  ..., 0.0850, 0.0210, 0.0415],
        ...,
        [0.0698, 0.0229, 0.1014,  ..., 0.0872, 0.0215, 0.0427],
        [0.0697, 0.0229, 0.1013,  ..., 0.0871, 0.0214, 0.0427],
        [0.0698, 0.0229, 0.1013,  ..., 0.0872, 0.0214, 0.0427]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679831., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8400.8252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.2927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2422.1606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1108.3217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6734],
        [-4.8044],
        [-4.9088],
        ...,
        [-4.8590],
        [-4.8560],
        [-4.8585]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350064.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0295],
        [1.0320],
        [1.0408],
        ...,
        [0.9971],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370322.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0295],
        [1.0320],
        [1.0408],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370324.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2228.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8267, device='cuda:0')



h[100].sum tensor(-0.0483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1663, device='cuda:0')



h[200].sum tensor(-22.6246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0210, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0024,  ..., 0.0096, 0.0103, 0.0043],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60413.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0556, 0.0179, 0.0440,  ..., 0.1439, 0.0585, 0.0827],
        [0.0629, 0.0201, 0.0767,  ..., 0.1069, 0.0339, 0.0572],
        [0.0646, 0.0208, 0.0835,  ..., 0.1012, 0.0299, 0.0533],
        ...,
        [0.0698, 0.0229, 0.1013,  ..., 0.0873, 0.0212, 0.0427],
        [0.0698, 0.0229, 0.1012,  ..., 0.0872, 0.0212, 0.0426],
        [0.0698, 0.0229, 0.1013,  ..., 0.0872, 0.0212, 0.0426]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(667332., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8609.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(108.7370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2607.5178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1078.9376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0045],
        [-1.7746],
        [-2.1888],
        ...,
        [-4.8975],
        [-4.8897],
        [-4.8894]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-395115.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0295],
        [1.0320],
        [1.0408],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370324.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0321],
        [1.0409],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370326.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2551.1638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0067, device='cuda:0')



h[100].sum tensor(-0.0607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7770, device='cuda:0')



h[200].sum tensor(-22.5926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64131.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.0216, 0.0989,  ..., 0.0840, 0.0203, 0.0406],
        [0.0681, 0.0217, 0.0992,  ..., 0.0844, 0.0204, 0.0408],
        [0.0685, 0.0220, 0.0996,  ..., 0.0851, 0.0205, 0.0413],
        ...,
        [0.0701, 0.0228, 0.1015,  ..., 0.0873, 0.0210, 0.0425],
        [0.0700, 0.0228, 0.1014,  ..., 0.0872, 0.0210, 0.0425],
        [0.0700, 0.0228, 0.1014,  ..., 0.0872, 0.0210, 0.0425]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676258.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8490.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7750, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.6304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2455.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1115.3685, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8510],
        [-4.9254],
        [-4.9740],
        ...,
        [-4.9147],
        [-4.9052],
        [-4.9041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-357742., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0321],
        [1.0409],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370326.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0321],
        [1.0409],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370329., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [-0.0016,  0.0266,  0.0084,  ...,  0.0060,  0.0262,  0.0186],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2420.6069, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1876, device='cuda:0')



h[100].sum tensor(-0.0548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5113, device='cuda:0')



h[200].sum tensor(-22.6056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0018, 0.0000],
        [0.0000, 0.0517, 0.0156,  ..., 0.0158, 0.0500, 0.0343],
        [0.0000, 0.1007, 0.0320,  ..., 0.0235, 0.0988, 0.0718],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63190.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0552, 0.0180, 0.0394,  ..., 0.1546, 0.0695, 0.0892],
        [0.0371, 0.0139, 0.0122,  ..., 0.2614, 0.1469, 0.1616],
        [0.0189, 0.0104, 0.0000,  ..., 0.3725, 0.2291, 0.2367],
        ...,
        [0.0701, 0.0226, 0.1014,  ..., 0.0874, 0.0208, 0.0428],
        [0.0701, 0.0225, 0.1013,  ..., 0.0873, 0.0208, 0.0428],
        [0.0701, 0.0226, 0.1014,  ..., 0.0873, 0.0208, 0.0428]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677736.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8567.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.5660, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2559.8181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1103.9781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2351],
        [ 0.0588],
        [ 0.2113],
        ...,
        [-4.9330],
        [-4.9249],
        [-4.9245]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-383612.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0321],
        [1.0409],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370329., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 260.0 event: 1300 loss: tensor(465.9316, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0297],
        [1.0322],
        [1.0409],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370331.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0081,  0.0020,  ...,  0.0031,  0.0077,  0.0033],
        [-0.0005,  0.0097,  0.0025,  ...,  0.0034,  0.0093,  0.0046],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2859.0693, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6118, device='cuda:0')



h[100].sum tensor(-0.0709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3038, device='cuda:0')



h[200].sum tensor(-22.5634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0635, 0.0185,  ..., 0.0176, 0.0618, 0.0384],
        [0.0000, 0.0311, 0.0085,  ..., 0.0126, 0.0295, 0.0173],
        [0.0000, 0.0215, 0.0051,  ..., 0.0111, 0.0200, 0.0094],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73266.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0368, 0.0118, 0.0000,  ..., 0.2350, 0.1145, 0.1478],
        [0.0442, 0.0140, 0.0118,  ..., 0.1976, 0.0893, 0.1219],
        [0.0498, 0.0159, 0.0177,  ..., 0.1713, 0.0718, 0.1035],
        ...,
        [0.0699, 0.0223, 0.1011,  ..., 0.0875, 0.0207, 0.0434],
        [0.0699, 0.0223, 0.1010,  ..., 0.0874, 0.0207, 0.0433],
        [0.0699, 0.0223, 0.1011,  ..., 0.0874, 0.0207, 0.0433]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(742391.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8362.7598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.2118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2514.7688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1188.5934, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1723],
        [ 0.0762],
        [-0.0944],
        ...,
        [-4.9294],
        [-4.9213],
        [-4.9209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-381033.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0297],
        [1.0322],
        [1.0409],
        ...,
        [0.9970],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370331.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0323],
        [1.0410],
        ...,
        [0.9970],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370334.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2988.9502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7009, device='cuda:0')



h[100].sum tensor(-0.0746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4629, device='cuda:0')



h[200].sum tensor(-22.5517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74171.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0673, 0.0208, 0.0980,  ..., 0.0843, 0.0200, 0.0422],
        [0.0672, 0.0208, 0.0970,  ..., 0.0861, 0.0207, 0.0435],
        [0.0666, 0.0208, 0.0938,  ..., 0.0905, 0.0226, 0.0468],
        ...,
        [0.0695, 0.0220, 0.1006,  ..., 0.0876, 0.0207, 0.0442],
        [0.0687, 0.0218, 0.0964,  ..., 0.0915, 0.0235, 0.0469],
        [0.0661, 0.0211, 0.0825,  ..., 0.1052, 0.0328, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(733520.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8294.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7713, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.1584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2540.3975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1196.1705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9169],
        [-3.7224],
        [-3.4204],
        ...,
        [-4.7567],
        [-4.4440],
        [-3.8317]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-381618.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0323],
        [1.0410],
        ...,
        [0.9970],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370334.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0323],
        [1.0410],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370336.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [-0.0006,  0.0102,  0.0027,  ...,  0.0035,  0.0098,  0.0050],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2384.2236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4998, device='cuda:0')



h[100].sum tensor(-0.0507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4108, device='cuda:0')



h[200].sum tensor(-22.6113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0082, 0.0019, 0.0000],
        [0.0000, 0.0130, 0.0027,  ..., 0.0098, 0.0115, 0.0052],
        [0.0000, 0.0113, 0.0021,  ..., 0.0095, 0.0098, 0.0037],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0084, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0084, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64341.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0647, 0.0198, 0.0879,  ..., 0.0944, 0.0255, 0.0500],
        [0.0611, 0.0186, 0.0716,  ..., 0.1117, 0.0352, 0.0626],
        [0.0595, 0.0183, 0.0644,  ..., 0.1202, 0.0394, 0.0689],
        ...,
        [0.0693, 0.0217, 0.1003,  ..., 0.0877, 0.0206, 0.0447],
        [0.0693, 0.0217, 0.1002,  ..., 0.0876, 0.0206, 0.0446],
        [0.0693, 0.0217, 0.1002,  ..., 0.0877, 0.0206, 0.0446]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684978.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8273.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.7138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2409.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1116.3080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8924],
        [-3.3029],
        [-3.5050],
        ...,
        [-4.9098],
        [-4.9018],
        [-4.9013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-341524.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0323],
        [1.0410],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370336.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0323],
        [1.0411],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370339.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3052.4426, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6572, device='cuda:0')



h[100].sum tensor(-0.0749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6026, device='cuda:0')



h[200].sum tensor(-22.5466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0349, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0082, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74115.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0671, 0.0205, 0.0977,  ..., 0.0846, 0.0199, 0.0427],
        [0.0674, 0.0206, 0.0981,  ..., 0.0850, 0.0199, 0.0430],
        [0.0678, 0.0209, 0.0985,  ..., 0.0857, 0.0201, 0.0435],
        ...,
        [0.0622, 0.0194, 0.0636,  ..., 0.1243, 0.0444, 0.0705],
        [0.0625, 0.0195, 0.0654,  ..., 0.1224, 0.0432, 0.0691],
        [0.0642, 0.0200, 0.0741,  ..., 0.1137, 0.0375, 0.0630]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(730910.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8111.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.0718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2377.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1200.5680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6164],
        [-4.7348],
        [-4.7932],
        ...,
        [-1.8821],
        [-1.9193],
        [-2.0658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331665.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0323],
        [1.0411],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370339.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0324],
        [1.0411],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370341.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [-0.0007,  0.0126,  0.0035,  ...,  0.0038,  0.0122,  0.0070]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2540.3296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3898, device='cuda:0')



h[100].sum tensor(-0.0552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6869, device='cuda:0')



h[200].sum tensor(-22.5962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0157, 0.0037,  ..., 0.0104, 0.0142, 0.0074],
        [0.0000, 0.0135, 0.0029,  ..., 0.0101, 0.0120, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65880.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0644, 0.0195, 0.0845,  ..., 0.0982, 0.0278, 0.0523],
        [0.0660, 0.0200, 0.0917,  ..., 0.0919, 0.0234, 0.0478],
        [0.0648, 0.0197, 0.0854,  ..., 0.0995, 0.0273, 0.0534],
        ...,
        [0.0673, 0.0208, 0.0897,  ..., 0.0990, 0.0270, 0.0524],
        [0.0621, 0.0191, 0.0659,  ..., 0.1227, 0.0415, 0.0695],
        [0.0598, 0.0183, 0.0559,  ..., 0.1328, 0.0473, 0.0769]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687105.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8358.7178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.9831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2555.1616, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1130.6155, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5479],
        [-1.9291],
        [-2.0588],
        ...,
        [-4.3647],
        [-3.9220],
        [-3.5595]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-366126.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0324],
        [1.0411],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370341.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0300],
        [1.0324],
        [1.0412],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370343.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2389.6074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4833, device='cuda:0')



h[100].sum tensor(-0.0491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4084, device='cuda:0')



h[200].sum tensor(-22.6109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63083.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0629, 0.0187, 0.0788,  ..., 0.1047, 0.0304, 0.0570],
        [0.0656, 0.0197, 0.0901,  ..., 0.0940, 0.0239, 0.0492],
        [0.0659, 0.0200, 0.0898,  ..., 0.0954, 0.0244, 0.0502],
        ...,
        [0.0698, 0.0215, 0.1007,  ..., 0.0883, 0.0203, 0.0444],
        [0.0698, 0.0215, 0.1006,  ..., 0.0882, 0.0202, 0.0443],
        [0.0698, 0.0215, 0.1006,  ..., 0.0882, 0.0202, 0.0443]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673883.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8350.9043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.6046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2508.7458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1112.6085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2481],
        [-1.7247],
        [-1.9880],
        ...,
        [-4.9564],
        [-4.9480],
        [-4.9474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350394.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0300],
        [1.0324],
        [1.0412],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370343.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0300],
        [1.0324],
        [1.0412],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370345.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2156.6763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.6648, device='cuda:0')



h[100].sum tensor(-0.0403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9965, device='cuda:0')



h[200].sum tensor(-22.6331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0194, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0083, 0.0017, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0083, 0.0017, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0083, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59402.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0679, 0.0202, 0.0984,  ..., 0.0851, 0.0195, 0.0422],
        [0.0682, 0.0204, 0.0987,  ..., 0.0855, 0.0196, 0.0424],
        [0.0686, 0.0207, 0.0991,  ..., 0.0862, 0.0197, 0.0429],
        ...,
        [0.0702, 0.0214, 0.1010,  ..., 0.0884, 0.0201, 0.0442],
        [0.0701, 0.0214, 0.1009,  ..., 0.0883, 0.0201, 0.0441],
        [0.0701, 0.0214, 0.1009,  ..., 0.0883, 0.0201, 0.0441]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660916.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8540.7598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.0694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2672.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1079.1547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5612],
        [-4.3359],
        [-3.9653],
        ...,
        [-4.9777],
        [-4.9690],
        [-4.9683]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-384837.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0300],
        [1.0324],
        [1.0412],
        ...,
        [0.9970],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370345.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0325],
        [1.0412],
        ...,
        [0.9969],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370347.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0028,  0.0485,  0.0160,  ...,  0.0094,  0.0479,  0.0366],
        [-0.0022,  0.0383,  0.0124,  ...,  0.0078,  0.0378,  0.0282],
        [-0.0019,  0.0327,  0.0105,  ...,  0.0070,  0.0322,  0.0236],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2752.0474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1725, device='cuda:0')



h[100].sum tensor(-0.0609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0935, device='cuda:0')



h[200].sum tensor(-22.5753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1476, 0.0477,  ..., 0.0307, 0.1454, 0.1080],
        [0.0000, 0.1847, 0.0606,  ..., 0.0365, 0.1823, 0.1384],
        [0.0000, 0.1953, 0.0643,  ..., 0.0382, 0.1929, 0.1471],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0085, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66904.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7338e-03, 6.8756e-04, 0.0000e+00,  ..., 5.7207e-01, 3.8680e-01,
         3.7737e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.6588e-01, 4.6351e-01,
         4.4032e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.0782e-01, 4.9774e-01,
         4.6844e-01],
        ...,
        [7.0387e-02, 2.1385e-02, 1.0118e-01,  ..., 8.8606e-02, 2.0115e-02,
         4.4155e-02],
        [7.0316e-02, 2.1357e-02, 1.0108e-01,  ..., 8.8510e-02, 2.0095e-02,
         4.4103e-02],
        [7.0326e-02, 2.1362e-02, 1.0110e-01,  ..., 8.8525e-02, 2.0097e-02,
         4.4111e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686743.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8612.4131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.5683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2802.8748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1139.5602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0932],
        [ 0.0503],
        [ 0.0154],
        ...,
        [-4.9925],
        [-4.9837],
        [-4.9829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-423137.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0325],
        [1.0412],
        ...,
        [0.9969],
        [0.9959],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370347.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0325],
        [1.0412],
        ...,
        [0.9969],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370350.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0393,  0.0128,  ...,  0.0080,  0.0388,  0.0291],
        [-0.0015,  0.0267,  0.0084,  ...,  0.0060,  0.0262,  0.0187],
        [-0.0030,  0.0533,  0.0176,  ...,  0.0102,  0.0526,  0.0406],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3031.4731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4448, device='cuda:0')



h[100].sum tensor(-0.0696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5716, device='cuda:0')



h[200].sum tensor(-22.5495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1171, 0.0371,  ..., 0.0260, 0.1150, 0.0828],
        [0.0000, 0.1725, 0.0564,  ..., 0.0346, 0.1702, 0.1284],
        [0.0000, 0.0961, 0.0298,  ..., 0.0228, 0.0942, 0.0655],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72846.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.6186e-03, 2.9401e-03, 0.0000e+00,  ..., 4.2866e-01, 2.6951e-01,
         2.8154e-01],
        [4.8883e-04, 1.1727e-03, 0.0000e+00,  ..., 4.9852e-01, 3.2460e-01,
         3.2907e-01],
        [1.0353e-02, 3.6034e-03, 0.0000e+00,  ..., 4.1801e-01, 2.5968e-01,
         2.7452e-01],
        ...,
        [7.0276e-02, 2.1381e-02, 1.0105e-01,  ..., 8.8836e-02, 2.0004e-02,
         4.4443e-02],
        [7.0204e-02, 2.1353e-02, 1.0095e-01,  ..., 8.8739e-02, 1.9984e-02,
         4.4390e-02],
        [7.0214e-02, 2.1357e-02, 1.0096e-01,  ..., 8.8753e-02, 1.9987e-02,
         4.4399e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715168.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8238.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.9611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2500.1294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1201.0625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3002],
        [ 0.3031],
        [ 0.2900],
        ...,
        [-4.9638],
        [-4.9540],
        [-4.9516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339673.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0325],
        [1.0412],
        ...,
        [0.9969],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370350.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0325],
        [1.0413],
        ...,
        [0.9969],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370352.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0272,  0.0086,  ...,  0.0061,  0.0267,  0.0191],
        [-0.0024,  0.0422,  0.0138,  ...,  0.0084,  0.0416,  0.0314],
        [-0.0027,  0.0478,  0.0157,  ...,  0.0093,  0.0472,  0.0360],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2492.1460, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6941, device='cuda:0')



h[100].sum tensor(-0.0497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5853, device='cuda:0')



h[200].sum tensor(-22.6028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1326, 0.0424,  ..., 0.0283, 0.1304, 0.0955],
        [0.0000, 0.1375, 0.0441,  ..., 0.0291, 0.1353, 0.0995],
        [0.0000, 0.1555, 0.0503,  ..., 0.0319, 0.1532, 0.1143],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0084, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0084, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0084, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63091.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.6954e-04, 0.0000e+00,  ..., 5.2014e-01, 3.4590e-01,
         3.4508e-01],
        [0.0000e+00, 3.5226e-04, 0.0000e+00,  ..., 5.1170e-01, 3.3837e-01,
         3.3942e-01],
        [2.0958e-03, 9.7455e-04, 0.0000e+00,  ..., 4.9270e-01, 3.2478e-01,
         3.2593e-01],
        ...,
        [7.0106e-02, 2.0961e-02, 1.0081e-01,  ..., 8.8971e-02, 1.9997e-02,
         4.5141e-02],
        [7.0035e-02, 2.0934e-02, 1.0071e-01,  ..., 8.8875e-02, 1.9977e-02,
         4.5087e-02],
        [7.0044e-02, 2.0938e-02, 1.0072e-01,  ..., 8.8888e-02, 1.9979e-02,
         4.5096e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668797.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8514.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.9627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2701.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1112.2806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2365],
        [ 0.2544],
        [ 0.2616],
        ...,
        [-4.9679],
        [-4.9595],
        [-4.9600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-378433.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0325],
        [1.0413],
        ...,
        [0.9969],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370352.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 270.0 event: 1350 loss: tensor(494.5858, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0325],
        [1.0413],
        ...,
        [0.9969],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370354.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2413.5190, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3618, device='cuda:0')



h[100].sum tensor(-0.0459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3906, device='cuda:0')



h[200].sum tensor(-22.6120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0018, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62864.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0670, 0.0193, 0.0962,  ..., 0.0876, 0.0200, 0.0454],
        [0.0635, 0.0179, 0.0780,  ..., 0.1063, 0.0319, 0.0589],
        [0.0589, 0.0163, 0.0521,  ..., 0.1330, 0.0496, 0.0779],
        ...,
        [0.0699, 0.0207, 0.1006,  ..., 0.0891, 0.0200, 0.0458],
        [0.0698, 0.0207, 0.1005,  ..., 0.0890, 0.0200, 0.0457],
        [0.0698, 0.0207, 0.1005,  ..., 0.0890, 0.0200, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668417.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8496.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6856, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.3942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2740.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1110.3610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8087],
        [-1.9046],
        [-0.9492],
        ...,
        [-4.9546],
        [-4.9452],
        [-4.9359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-382350.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0325],
        [1.0413],
        ...,
        [0.9969],
        [0.9960],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370354.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0326],
        [1.0413],
        ...,
        [0.9969],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370356.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2241.2021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0063, device='cuda:0')



h[100].sum tensor(-0.0390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0464, device='cuda:0')



h[200].sum tensor(-22.6303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0024,  ..., 0.0095, 0.0104, 0.0044],
        [0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0018, 0.0000],
        [0.0000, 0.0125, 0.0026,  ..., 0.0097, 0.0110, 0.0049],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60173.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0612, 0.0172, 0.0723,  ..., 0.1120, 0.0336, 0.0636],
        [0.0640, 0.0184, 0.0828,  ..., 0.1020, 0.0280, 0.0560],
        [0.0616, 0.0175, 0.0711,  ..., 0.1149, 0.0353, 0.0655],
        ...,
        [0.0697, 0.0209, 0.1005,  ..., 0.0893, 0.0200, 0.0460],
        [0.0696, 0.0208, 0.1004,  ..., 0.0892, 0.0199, 0.0460],
        [0.0696, 0.0209, 0.1004,  ..., 0.0893, 0.0199, 0.0460]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660204.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8510.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.9887, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2798.6687, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1087.4305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6170],
        [-3.8980],
        [-3.9370],
        ...,
        [-4.9649],
        [-4.9562],
        [-4.9554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-382699.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0326],
        [1.0413],
        ...,
        [0.9969],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370356.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0414],
        ...,
        [0.9969],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370358.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2795.1492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0407, device='cuda:0')



h[100].sum tensor(-0.0569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0742, device='cuda:0')



h[200].sum tensor(-22.5770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0082, 0.0017, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68084.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0670, 0.0197, 0.0968,  ..., 0.0876, 0.0197, 0.0452],
        [0.0668, 0.0196, 0.0957,  ..., 0.0894, 0.0202, 0.0468],
        [0.0655, 0.0191, 0.0897,  ..., 0.0965, 0.0235, 0.0524],
        ...,
        [0.0698, 0.0211, 0.1008,  ..., 0.0895, 0.0200, 0.0459],
        [0.0697, 0.0210, 0.1007,  ..., 0.0894, 0.0200, 0.0458],
        [0.0697, 0.0210, 0.1007,  ..., 0.0894, 0.0200, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688050.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8277.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.2472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2629.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1161.5089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8098],
        [-3.4358],
        [-2.9496],
        ...,
        [-4.9714],
        [-4.9624],
        [-4.9614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342733.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0414],
        ...,
        [0.9969],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370358.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0414],
        ...,
        [0.9968],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370360.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0102,  0.0027,  ...,  0.0035,  0.0097,  0.0051],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [-0.0005,  0.0102,  0.0027,  ...,  0.0035,  0.0097,  0.0051],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3335.5969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5505, device='cuda:0')



h[100].sum tensor(-0.0738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0254, device='cuda:0')



h[200].sum tensor(-22.5254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0390, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0111, 0.0021,  ..., 0.0095, 0.0095, 0.0037],
        [0.0000, 0.0384, 0.0097,  ..., 0.0137, 0.0366, 0.0180],
        [0.0000, 0.0112, 0.0021,  ..., 0.0095, 0.0096, 0.0038],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0017, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78110.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0564, 0.0155, 0.0492,  ..., 0.1359, 0.0480, 0.0804],
        [0.0520, 0.0137, 0.0314,  ..., 0.1547, 0.0573, 0.0948],
        [0.0557, 0.0154, 0.0482,  ..., 0.1389, 0.0468, 0.0836],
        ...,
        [0.0699, 0.0215, 0.1011,  ..., 0.0895, 0.0199, 0.0456],
        [0.0698, 0.0214, 0.1010,  ..., 0.0894, 0.0199, 0.0455],
        [0.0698, 0.0214, 0.1011,  ..., 0.0894, 0.0199, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(738753.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8028.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.1764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2560.1812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1250.2664, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9981],
        [-1.0738],
        [-0.9539],
        ...,
        [-4.9892],
        [-4.9803],
        [-4.9795]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332265.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0414],
        ...,
        [0.9968],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370360.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0415],
        ...,
        [0.9968],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370362.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [-0.0006,  0.0106,  0.0028,  ...,  0.0036,  0.0102,  0.0055],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2450.4292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4036, device='cuda:0')



h[100].sum tensor(-0.0438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3967, device='cuda:0')



h[200].sum tensor(-22.6120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0334, 0.0087,  ..., 0.0130, 0.0316, 0.0167],
        [0.0000, 0.0298, 0.0080,  ..., 0.0125, 0.0281, 0.0164],
        [0.0000, 0.0459, 0.0124,  ..., 0.0150, 0.0440, 0.0241],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63039.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0115, 0.0133,  ..., 0.1835, 0.0749, 0.1149],
        [0.0458, 0.0113, 0.0135,  ..., 0.1886, 0.0789, 0.1183],
        [0.0454, 0.0113, 0.0073,  ..., 0.1919, 0.0800, 0.1210],
        ...,
        [0.0698, 0.0220, 0.1015,  ..., 0.0896, 0.0198, 0.0453],
        [0.0698, 0.0220, 0.1014,  ..., 0.0895, 0.0198, 0.0453],
        [0.0698, 0.0220, 0.1014,  ..., 0.0895, 0.0198, 0.0453]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(669822.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8411.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.8919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2824.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1118.3624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7109],
        [-0.4381],
        [-0.5813],
        ...,
        [-4.9926],
        [-4.9836],
        [-4.9827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-364918.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0415],
        ...,
        [0.9968],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370362.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0415],
        ...,
        [0.9967],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370364.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0312,  0.0100,  ...,  0.0068,  0.0306,  0.0224],
        [-0.0011,  0.0209,  0.0064,  ...,  0.0051,  0.0203,  0.0139],
        [-0.0013,  0.0243,  0.0076,  ...,  0.0057,  0.0237,  0.0167],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2515.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1868, device='cuda:0')



h[100].sum tensor(-0.0451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5111, device='cuda:0')



h[200].sum tensor(-22.6064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0904, 0.0278,  ..., 0.0219, 0.0881, 0.0608],
        [0.0000, 0.1035, 0.0324,  ..., 0.0240, 0.1012, 0.0716],
        [0.0000, 0.0945, 0.0292,  ..., 0.0226, 0.0922, 0.0641],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65822.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0161, 0.0012, 0.0000,  ..., 0.3749, 0.2236, 0.2451],
        [0.0164, 0.0014, 0.0000,  ..., 0.3751, 0.2244, 0.2452],
        [0.0202, 0.0019, 0.0000,  ..., 0.3545, 0.2076, 0.2312],
        ...,
        [0.0698, 0.0224, 0.1018,  ..., 0.0898, 0.0197, 0.0451],
        [0.0697, 0.0224, 0.1017,  ..., 0.0897, 0.0197, 0.0451],
        [0.0697, 0.0224, 0.1017,  ..., 0.0897, 0.0197, 0.0451]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690233.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8327.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9987, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.0586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2843.5249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1142.2507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3850],
        [ 0.3785],
        [ 0.3720],
        ...,
        [-4.9993],
        [-4.9903],
        [-4.9894]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-360702.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0415],
        ...,
        [0.9967],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370364.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0415],
        ...,
        [0.9967],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370366.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2366.7822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1432, device='cuda:0')



h[100].sum tensor(-0.0396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2125, device='cuda:0')



h[200].sum tensor(-22.6216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0018,  ..., 0.0095, 0.0085, 0.0030],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0016, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61598.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0552, 0.0157, 0.0418,  ..., 0.1442, 0.0531, 0.0852],
        [0.0613, 0.0185, 0.0695,  ..., 0.1171, 0.0370, 0.0654],
        [0.0659, 0.0207, 0.0893,  ..., 0.0982, 0.0257, 0.0516],
        ...,
        [0.0697, 0.0224, 0.1017,  ..., 0.0899, 0.0198, 0.0452],
        [0.0696, 0.0224, 0.1016,  ..., 0.0898, 0.0198, 0.0451],
        [0.0696, 0.0224, 0.1017,  ..., 0.0898, 0.0198, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664806.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8433.4912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5936, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.9692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2942.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1105.0255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6048],
        [-1.4691],
        [-2.3287],
        ...,
        [-4.9988],
        [-4.9898],
        [-4.9889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-374257.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0415],
        ...,
        [0.9967],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370366.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0416],
        ...,
        [0.9967],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370369., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3655.2988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2757, device='cuda:0')



h[100].sum tensor(-0.0790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5696, device='cuda:0')



h[200].sum tensor(-22.4987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0443, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0473, 0.0141,  ..., 0.0152, 0.0454, 0.0308],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0016, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0017, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81610.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0342, 0.0090, 0.0145,  ..., 0.2790, 0.1631, 0.1766],
        [0.0571, 0.0169, 0.0481,  ..., 0.1465, 0.0629, 0.0850],
        [0.0650, 0.0203, 0.0833,  ..., 0.1038, 0.0307, 0.0554],
        ...,
        [0.0694, 0.0223, 0.1015,  ..., 0.0900, 0.0199, 0.0455],
        [0.0694, 0.0223, 0.1014,  ..., 0.0899, 0.0198, 0.0454],
        [0.0694, 0.0223, 0.1014,  ..., 0.0899, 0.0198, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(749007.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8090.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.5488, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.9878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2714.4971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1277.8713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1072],
        [-0.2472],
        [-0.7268],
        ...,
        [-4.9895],
        [-4.9806],
        [-4.9798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331290.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0416],
        ...,
        [0.9967],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370369., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0416],
        ...,
        [0.9966],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370371.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0104,  0.0028,  ...,  0.0035,  0.0100,  0.0053],
        [-0.0012,  0.0230,  0.0071,  ...,  0.0055,  0.0224,  0.0156],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2665.0337, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7753, device='cuda:0')



h[100].sum tensor(-0.0471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7432, device='cuda:0')



h[200].sum tensor(-22.5952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1110, 0.0349,  ..., 0.0250, 0.1086, 0.0776],
        [0.0000, 0.0592, 0.0175,  ..., 0.0170, 0.0572, 0.0377],
        [0.0000, 0.0617, 0.0184,  ..., 0.0175, 0.0597, 0.0397],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65094.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1269e-02, 8.9271e-05, 0.0000e+00,  ..., 3.9665e-01, 2.4251e-01,
         2.6081e-01],
        [2.5056e-02, 2.7690e-03, 0.0000e+00,  ..., 3.1468e-01, 1.7782e-01,
         2.0447e-01],
        [3.3831e-02, 6.6754e-03, 3.9521e-03,  ..., 2.6757e-01, 1.4303e-01,
         1.7162e-01],
        ...,
        [6.9231e-02, 2.1994e-02, 1.0111e-01,  ..., 9.0169e-02, 1.9982e-02,
         4.5774e-02],
        [6.9160e-02, 2.1965e-02, 1.0102e-01,  ..., 9.0070e-02, 1.9962e-02,
         4.5718e-02],
        [6.9168e-02, 2.1969e-02, 1.0102e-01,  ..., 9.0082e-02, 1.9964e-02,
         4.5725e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672529.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8255.0361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.9865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2738.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1137.1517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4292],
        [ 0.3931],
        [ 0.0643],
        ...,
        [-4.9810],
        [-4.9722],
        [-4.9714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330025.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0327],
        [1.0416],
        ...,
        [0.9966],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370371.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0416],
        ...,
        [0.9966],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370373.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2313.6321, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3453, device='cuda:0')



h[100].sum tensor(-0.0360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0960, device='cuda:0')



h[200].sum tensor(-22.6286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0294, 0.0078,  ..., 0.0124, 0.0276, 0.0159],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0018, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61303.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0656, 0.0198, 0.0913,  ..., 0.0942, 0.0240, 0.0490],
        [0.0588, 0.0169, 0.0556,  ..., 0.1340, 0.0527, 0.0768],
        [0.0437, 0.0104, 0.0258,  ..., 0.2181, 0.1115, 0.1362],
        ...,
        [0.0693, 0.0217, 0.1011,  ..., 0.0903, 0.0202, 0.0458],
        [0.0693, 0.0216, 0.1010,  ..., 0.0902, 0.0201, 0.0457],
        [0.0693, 0.0216, 0.1010,  ..., 0.0903, 0.0201, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663329.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8324.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.5255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2738.9185, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1103.0764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1858],
        [-0.6036],
        [-0.0622],
        ...,
        [-4.9923],
        [-4.9835],
        [-4.9828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338296.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0416],
        ...,
        [0.9966],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370373.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 280.0 event: 1400 loss: tensor(478.9352, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0416],
        ...,
        [0.9965],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370375.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2525.6150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7627, device='cuda:0')



h[100].sum tensor(-0.0418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4492, device='cuda:0')



h[200].sum tensor(-22.6087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0291, 0.0077,  ..., 0.0124, 0.0274, 0.0157],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64680.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0639, 0.0191, 0.0811,  ..., 0.1046, 0.0313, 0.0559],
        [0.0561, 0.0157, 0.0416,  ..., 0.1493, 0.0623, 0.0872],
        [0.0430, 0.0099, 0.0151,  ..., 0.2218, 0.1110, 0.1384],
        ...,
        [0.0694, 0.0218, 0.1012,  ..., 0.0905, 0.0203, 0.0455],
        [0.0694, 0.0218, 0.1011,  ..., 0.0904, 0.0202, 0.0455],
        [0.0694, 0.0218, 0.1011,  ..., 0.0904, 0.0202, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679334., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8397.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9012, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.2974, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2807.1548, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1126.6967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9329],
        [-0.2222],
        [ 0.1704],
        ...,
        [-4.8926],
        [-4.9261],
        [-4.9519]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-363278.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0416],
        ...,
        [0.9965],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370375.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0417],
        ...,
        [0.9965],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370378.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2638.6396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0042, device='cuda:0')



h[100].sum tensor(-0.0444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6306, device='cuda:0')



h[200].sum tensor(-22.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0084, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65912.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0590, 0.0172, 0.0535,  ..., 0.1323, 0.0502, 0.0749],
        [0.0579, 0.0167, 0.0469,  ..., 0.1396, 0.0547, 0.0801],
        [0.0584, 0.0170, 0.0472,  ..., 0.1404, 0.0549, 0.0806],
        ...,
        [0.0695, 0.0221, 0.1013,  ..., 0.0906, 0.0203, 0.0453],
        [0.0694, 0.0220, 0.1012,  ..., 0.0905, 0.0203, 0.0453],
        [0.0694, 0.0220, 0.1012,  ..., 0.0905, 0.0203, 0.0453]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683211., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8389.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0220, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.6700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2799.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1136.4821, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3479],
        [-0.0487],
        [ 0.1302],
        ...,
        [-5.0155],
        [-5.0068],
        [-5.0062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-364937.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0417],
        ...,
        [0.9965],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370378.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0417],
        ...,
        [0.9965],
        [0.9957],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370380.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0102,  0.0027,  ...,  0.0035,  0.0098,  0.0051],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026],
        [-0.0005,  0.0102,  0.0027,  ...,  0.0035,  0.0098,  0.0051],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2604.1816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5231, device='cuda:0')



h[100].sum tensor(-0.0425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5603, device='cuda:0')



h[200].sum tensor(-22.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0181, 0.0046,  ..., 0.0107, 0.0166, 0.0095],
        [0.0000, 0.0526, 0.0148,  ..., 0.0161, 0.0509, 0.0296],
        [0.0000, 0.0183, 0.0046,  ..., 0.0108, 0.0168, 0.0096],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65205.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0533, 0.0148, 0.0358,  ..., 0.1576, 0.0619, 0.0936],
        [0.0461, 0.0115, 0.0047,  ..., 0.1967, 0.0859, 0.1214],
        [0.0531, 0.0148, 0.0332,  ..., 0.1626, 0.0642, 0.0972],
        ...,
        [0.0695, 0.0222, 0.1013,  ..., 0.0907, 0.0203, 0.0452],
        [0.0694, 0.0222, 0.1012,  ..., 0.0906, 0.0202, 0.0452],
        [0.0694, 0.0222, 0.1012,  ..., 0.0906, 0.0202, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680162.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8309.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9455, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.8912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2638.4866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1134.0875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1580],
        [-0.6701],
        [-0.7689],
        ...,
        [-5.0125],
        [-5.0032],
        [-4.9944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330145.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0327],
        [1.0417],
        ...,
        [0.9965],
        [0.9957],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370380.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0328],
        [1.0418],
        ...,
        [0.9964],
        [0.9957],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370383.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0218,  0.0067,  ...,  0.0053,  0.0213,  0.0146],
        [-0.0005,  0.0104,  0.0028,  ...,  0.0035,  0.0100,  0.0053],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0004, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2373.9397, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7065, device='cuda:0')



h[100].sum tensor(-0.0353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1488, device='cuda:0')



h[200].sum tensor(-22.6252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0308, 0.0085,  ..., 0.0127, 0.0292, 0.0172],
        [0.0000, 0.0330, 0.0092,  ..., 0.0131, 0.0314, 0.0190],
        [0.0000, 0.0524, 0.0148,  ..., 0.0161, 0.0507, 0.0294],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60896.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0436, 0.0113, 0.0014,  ..., 0.2205, 0.1096, 0.1368],
        [0.0442, 0.0115, 0.0014,  ..., 0.2138, 0.1008, 0.1325],
        [0.0405, 0.0097, 0.0000,  ..., 0.2336, 0.1096, 0.1469],
        ...,
        [0.0697, 0.0225, 0.1016,  ..., 0.0907, 0.0202, 0.0449],
        [0.0697, 0.0224, 0.1015,  ..., 0.0906, 0.0202, 0.0449],
        [0.0697, 0.0225, 0.1016,  ..., 0.0906, 0.0202, 0.0449]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663174.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8482.3428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5273, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.4267, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2752.7954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1093.0437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4366],
        [ 0.4399],
        [ 0.4387],
        ...,
        [-5.0482],
        [-5.0395],
        [-5.0390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-363062.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0328],
        [1.0418],
        ...,
        [0.9964],
        [0.9957],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370383.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0328],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370385.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0144,  0.0042,  ...,  0.0042,  0.0140,  0.0085],
        [-0.0010,  0.0191,  0.0058,  ...,  0.0049,  0.0187,  0.0124],
        [-0.0010,  0.0188,  0.0057,  ...,  0.0048,  0.0184,  0.0122],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3373.6475, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9089, device='cuda:0')



h[100].sum tensor(-0.0636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.9316, device='cuda:0')



h[200].sum tensor(-22.5290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0381, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0335, 0.0094,  ..., 0.0131, 0.0320, 0.0194],
        [0.0000, 0.0513, 0.0150,  ..., 0.0159, 0.0497, 0.0314],
        [0.0000, 0.1088, 0.0344,  ..., 0.0249, 0.1068, 0.0758],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0019, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78301.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0402, 0.0095, 0.0036,  ..., 0.2390, 0.1187, 0.1498],
        [0.0315, 0.0066, 0.0000,  ..., 0.2983, 0.1653, 0.1902],
        [0.0188, 0.0040, 0.0000,  ..., 0.3899, 0.2407, 0.2520],
        ...,
        [0.0700, 0.0223, 0.1018,  ..., 0.0908, 0.0203, 0.0449],
        [0.0699, 0.0223, 0.1017,  ..., 0.0907, 0.0202, 0.0448],
        [0.0699, 0.0223, 0.1017,  ..., 0.0907, 0.0202, 0.0448]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(746962.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8358.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.2279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(113.9531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2686.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1236.7806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3234],
        [ 0.3584],
        [ 0.3319],
        ...,
        [-5.0688],
        [-5.0601],
        [-5.0596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-375075.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0328],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370385.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370387.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [-0.0009,  0.0168,  0.0050,  ...,  0.0045,  0.0164,  0.0105],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2401.5215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2566, device='cuda:0')



h[100].sum tensor(-0.0354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2291, device='cuda:0')



h[200].sum tensor(-22.6221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0020, 0.0000],
        [0.0000, 0.0198, 0.0052,  ..., 0.0109, 0.0184, 0.0109],
        [0.0000, 0.0169, 0.0042,  ..., 0.0105, 0.0155, 0.0084],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0020, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0020, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61456.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0646, 0.0194, 0.0817,  ..., 0.1047, 0.0313, 0.0554],
        [0.0587, 0.0171, 0.0528,  ..., 0.1411, 0.0568, 0.0807],
        [0.0517, 0.0145, 0.0305,  ..., 0.1859, 0.0886, 0.1120],
        ...,
        [0.0699, 0.0220, 0.1016,  ..., 0.0908, 0.0204, 0.0452],
        [0.0699, 0.0219, 0.1015,  ..., 0.0907, 0.0203, 0.0452],
        [0.0699, 0.0219, 0.1015,  ..., 0.0907, 0.0203, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668952.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8583.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5825, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.3643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2715.9053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1090.9340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5511],
        [-2.1576],
        [-1.3044],
        ...,
        [-5.0633],
        [-5.0542],
        [-5.0393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-384672.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370387.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0420],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370389.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0250,  0.0078,  ...,  0.0058,  0.0245,  0.0172],
        [-0.0030,  0.0567,  0.0188,  ...,  0.0107,  0.0561,  0.0433],
        [-0.0012,  0.0236,  0.0074,  ...,  0.0056,  0.0232,  0.0161],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2714.0483, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7816, device='cuda:0')



h[100].sum tensor(-0.0436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7442, device='cuda:0')



h[200].sum tensor(-22.5925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1242, 0.0397,  ..., 0.0272, 0.1223, 0.0885],
        [0.0000, 0.0989, 0.0309,  ..., 0.0232, 0.0971, 0.0677],
        [0.0000, 0.1009, 0.0322,  ..., 0.0236, 0.0991, 0.0720],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0021, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0021, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66342.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0038, 0.0000,  ..., 0.3918, 0.2443, 0.2550],
        [0.0179, 0.0027, 0.0000,  ..., 0.3875, 0.2404, 0.2521],
        [0.0242, 0.0066, 0.0000,  ..., 0.3561, 0.2192, 0.2298],
        ...,
        [0.0699, 0.0218, 0.1014,  ..., 0.0909, 0.0204, 0.0454],
        [0.0698, 0.0218, 0.1013,  ..., 0.0908, 0.0204, 0.0454],
        [0.0698, 0.0218, 0.1013,  ..., 0.0908, 0.0204, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686071.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8395.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.6119, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2473.0735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1137.9078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3459],
        [ 0.2991],
        [ 0.0284],
        ...,
        [-5.0714],
        [-5.0627],
        [-5.0624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345161.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0420],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370389.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0420],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370391.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0184,  0.0056,  ...,  0.0048,  0.0180,  0.0118],
        [-0.0005,  0.0110,  0.0030,  ...,  0.0036,  0.0106,  0.0057],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2499.6294, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2863, device='cuda:0')



h[100].sum tensor(-0.0371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3796, device='cuda:0')



h[200].sum tensor(-22.6135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0231, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0638, 0.0188,  ..., 0.0177, 0.0623, 0.0388],
        [0.0000, 0.0363, 0.0103,  ..., 0.0135, 0.0349, 0.0217],
        [0.0000, 0.0138, 0.0031,  ..., 0.0100, 0.0126, 0.0059],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63107.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0390, 0.0090, 0.0000,  ..., 0.2419, 0.1188, 0.1532],
        [0.0476, 0.0126, 0.0201,  ..., 0.1975, 0.0903, 0.1215],
        [0.0576, 0.0168, 0.0451,  ..., 0.1451, 0.0552, 0.0844],
        ...,
        [0.0699, 0.0218, 0.1015,  ..., 0.0910, 0.0204, 0.0453],
        [0.0699, 0.0218, 0.1014,  ..., 0.0909, 0.0203, 0.0452],
        [0.0699, 0.0218, 0.1014,  ..., 0.0909, 0.0203, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675398.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8526.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7379, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.9449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2494.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1106.7006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1677],
        [-0.2204],
        [-0.8886],
        ...,
        [-5.0827],
        [-5.0741],
        [-5.0738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-354662.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0420],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370391.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370394.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2398.9812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8861, device='cuda:0')



h[100].sum tensor(-0.0338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1750, device='cuda:0')



h[200].sum tensor(-22.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0021, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60910.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.0208, 0.0992,  ..., 0.0878, 0.0197, 0.0430],
        [0.0681, 0.0209, 0.0995,  ..., 0.0882, 0.0197, 0.0433],
        [0.0685, 0.0212, 0.0999,  ..., 0.0889, 0.0199, 0.0437],
        ...,
        [0.0701, 0.0220, 0.1018,  ..., 0.0911, 0.0203, 0.0450],
        [0.0700, 0.0219, 0.1017,  ..., 0.0910, 0.0203, 0.0450],
        [0.0700, 0.0219, 0.1017,  ..., 0.0910, 0.0203, 0.0450]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665287.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8592.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.8832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2484.5439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1087.9359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8254],
        [-3.6142],
        [-4.1561],
        ...,
        [-5.0988],
        [-5.0901],
        [-5.0898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-363091., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370394.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370396.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2656.5752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2478, device='cuda:0')



h[100].sum tensor(-0.0402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6662, device='cuda:0')



h[200].sum tensor(-22.5991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0083, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66059.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0680, 0.0209, 0.0996,  ..., 0.0879, 0.0196, 0.0427],
        [0.0681, 0.0210, 0.0991,  ..., 0.0891, 0.0199, 0.0436],
        [0.0679, 0.0210, 0.0975,  ..., 0.0919, 0.0207, 0.0459],
        ...,
        [0.0694, 0.0218, 0.0982,  ..., 0.0953, 0.0223, 0.0477],
        [0.0674, 0.0210, 0.0904,  ..., 0.1033, 0.0262, 0.0537],
        [0.0665, 0.0207, 0.0865,  ..., 0.1074, 0.0282, 0.0567]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691958.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8464.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.2180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2334.0518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1135.9042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5445],
        [-4.2252],
        [-3.7514],
        ...,
        [-4.6506],
        [-4.3529],
        [-4.1663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335986.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0329],
        [1.0419],
        ...,
        [0.9964],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370396.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 290.0 event: 1450 loss: tensor(470.2497, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370398.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [-0.0007,  0.0134,  0.0039,  ...,  0.0040,  0.0130,  0.0077],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2183.1333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.3093, device='cuda:0')



h[100].sum tensor(-0.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7985, device='cuda:0')



h[200].sum tensor(-22.6442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0083, 0.0021, 0.0000],
        [0.0000, 0.0161, 0.0040,  ..., 0.0103, 0.0150, 0.0080],
        [0.0000, 0.0467, 0.0141,  ..., 0.0152, 0.0454, 0.0303],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58660.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0615, 0.0184, 0.0650,  ..., 0.1234, 0.0410, 0.0680],
        [0.0555, 0.0165, 0.0400,  ..., 0.1635, 0.0701, 0.0955],
        [0.0413, 0.0125, 0.0142,  ..., 0.2605, 0.1428, 0.1620],
        ...,
        [0.0705, 0.0221, 0.1025,  ..., 0.0913, 0.0202, 0.0444],
        [0.0704, 0.0221, 0.1024,  ..., 0.0912, 0.0202, 0.0443],
        [0.0704, 0.0221, 0.1025,  ..., 0.0912, 0.0202, 0.0444]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665141.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8750.9541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.4768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2548.7273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1066.3861, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8783],
        [-0.5064],
        [-0.1052],
        ...,
        [-5.1421],
        [-5.1333],
        [-5.1330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-398271.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370398.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370400.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0020,  0.0389,  0.0127,  ...,  0.0080,  0.0384,  0.0287],
        [-0.0018,  0.0356,  0.0116,  ...,  0.0075,  0.0352,  0.0260],
        [-0.0017,  0.0335,  0.0109,  ...,  0.0071,  0.0330,  0.0243],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2788.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5524, device='cuda:0')



h[100].sum tensor(-0.0425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8568, device='cuda:0')



h[200].sum tensor(-22.5872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0850, 0.0269,  ..., 0.0211, 0.0835, 0.0592],
        [0.0000, 0.1193, 0.0383,  ..., 0.0266, 0.1177, 0.0846],
        [0.0000, 0.1294, 0.0418,  ..., 0.0282, 0.1277, 0.0929],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0021, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67360.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0167, 0.0049, 0.0000,  ..., 0.4172, 0.2591, 0.2693],
        [0.0106, 0.0030, 0.0000,  ..., 0.4586, 0.2893, 0.2977],
        [0.0086, 0.0023, 0.0000,  ..., 0.4745, 0.2997, 0.3087],
        ...,
        [0.0706, 0.0222, 0.1028,  ..., 0.0915, 0.0201, 0.0443],
        [0.0705, 0.0222, 0.1027,  ..., 0.0914, 0.0201, 0.0442],
        [0.0705, 0.0222, 0.1027,  ..., 0.0914, 0.0201, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(700595.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8559.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(117.9394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2398.3096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1144.1749, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1620],
        [ 0.1613],
        [ 0.1628],
        ...,
        [-5.1540],
        [-5.1452],
        [-5.1450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-372386.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370400.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370402.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0199,  0.0061,  ...,  0.0050,  0.0196,  0.0131],
        [-0.0004,  0.0091,  0.0024,  ...,  0.0033,  0.0088,  0.0042],
        [-0.0004,  0.0089,  0.0023,  ...,  0.0033,  0.0086,  0.0040],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2466.4497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6314, device='cuda:0')



h[100].sum tensor(-0.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2839, device='cuda:0')



h[200].sum tensor(-22.6182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0418, 0.0113,  ..., 0.0143, 0.0405, 0.0209],
        [0.0000, 0.0565, 0.0164,  ..., 0.0167, 0.0552, 0.0330],
        [0.0000, 0.0472, 0.0132,  ..., 0.0152, 0.0460, 0.0253],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0085, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62527.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0366, 0.0079, 0.0000,  ..., 0.2579, 0.1220, 0.1639],
        [0.0356, 0.0076, 0.0000,  ..., 0.2678, 0.1296, 0.1704],
        [0.0391, 0.0091, 0.0000,  ..., 0.2499, 0.1169, 0.1579],
        ...,
        [0.0705, 0.0219, 0.1027,  ..., 0.0916, 0.0201, 0.0446],
        [0.0705, 0.0218, 0.1026,  ..., 0.0915, 0.0201, 0.0445],
        [0.0705, 0.0219, 0.1026,  ..., 0.0915, 0.0201, 0.0445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678206.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8662.8447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.4113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2400.7087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1102.4989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3694],
        [ 0.3725],
        [ 0.3776],
        ...,
        [-5.1534],
        [-5.1446],
        [-5.1444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-375543.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370402.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370405.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0278,  0.0089,  ...,  0.0063,  0.0275,  0.0196],
        [-0.0006,  0.0130,  0.0037,  ...,  0.0039,  0.0127,  0.0074],
        [-0.0007,  0.0142,  0.0042,  ...,  0.0041,  0.0139,  0.0084],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2549.7439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5278, device='cuda:0')



h[100].sum tensor(-0.0351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4149, device='cuda:0')



h[200].sum tensor(-22.6110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0234, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0716, 0.0217,  ..., 0.0190, 0.0703, 0.0454],
        [0.0000, 0.0781, 0.0239,  ..., 0.0200, 0.0766, 0.0507],
        [0.0000, 0.0476, 0.0138,  ..., 0.0153, 0.0463, 0.0283],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63498.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0383, 0.0088, 0.0000,  ..., 0.2637, 0.1375, 0.1663],
        [0.0387, 0.0090, 0.0000,  ..., 0.2651, 0.1395, 0.1670],
        [0.0438, 0.0109, 0.0000,  ..., 0.2365, 0.1184, 0.1471],
        ...,
        [0.0704, 0.0216, 0.1024,  ..., 0.0917, 0.0200, 0.0450],
        [0.0703, 0.0216, 0.1023,  ..., 0.0916, 0.0200, 0.0449],
        [0.0703, 0.0216, 0.1023,  ..., 0.0916, 0.0200, 0.0449]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679507.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8545.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.0231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2269.2036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1115.7809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3751],
        [ 0.2724],
        [-0.0311],
        ...,
        [-5.1443],
        [-5.1357],
        [-5.1354]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346922.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370405.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370407.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0150,  0.0044,  ...,  0.0042,  0.0147,  0.0091],
        [-0.0005,  0.0100,  0.0027,  ...,  0.0035,  0.0097,  0.0050],
        [-0.0004,  0.0079,  0.0020,  ...,  0.0031,  0.0077,  0.0033],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3039.0381, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6352, device='cuda:0')



h[100].sum tensor(-0.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3072, device='cuda:0')



h[200].sum tensor(-22.5647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0534, 0.0158,  ..., 0.0161, 0.0521, 0.0331],
        [0.0000, 0.0461, 0.0133,  ..., 0.0150, 0.0448, 0.0271],
        [0.0000, 0.0790, 0.0242,  ..., 0.0202, 0.0776, 0.0513],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71204.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0251, 0.0041, 0.0000,  ..., 0.3486, 0.2026, 0.2254],
        [0.0316, 0.0056, 0.0000,  ..., 0.3031, 0.1641, 0.1946],
        [0.0284, 0.0044, 0.0000,  ..., 0.3253, 0.1789, 0.2102],
        ...,
        [0.0704, 0.0213, 0.1024,  ..., 0.0918, 0.0200, 0.0452],
        [0.0703, 0.0213, 0.1023,  ..., 0.0917, 0.0200, 0.0452],
        [0.0703, 0.0213, 0.1023,  ..., 0.0917, 0.0200, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712667.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8519.2402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.5929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2295.8745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1178.4879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1282],
        [ 0.1690],
        [ 0.1907],
        ...,
        [-5.1452],
        [-5.1368],
        [-5.1366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-367272.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0329],
        [1.0419],
        ...,
        [0.9963],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370407.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0330],
        [1.0419],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370409.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2303.9302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4189, device='cuda:0')



h[100].sum tensor(-0.0279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9606, device='cuda:0')



h[200].sum tensor(-22.6354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0552, 0.0170,  ..., 0.0164, 0.0539, 0.0373],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59950.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0420, 0.0097, 0.0123,  ..., 0.2456, 0.1308, 0.1539],
        [0.0584, 0.0159, 0.0434,  ..., 0.1460, 0.0580, 0.0843],
        [0.0644, 0.0185, 0.0771,  ..., 0.1130, 0.0344, 0.0611],
        ...,
        [0.0702, 0.0210, 0.1021,  ..., 0.0919, 0.0199, 0.0456],
        [0.0702, 0.0210, 0.1021,  ..., 0.0918, 0.0199, 0.0456],
        [0.0702, 0.0210, 0.1021,  ..., 0.0919, 0.0199, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663778., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8658.2549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.9672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2312.9412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1083.8624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0749],
        [-0.7750],
        [-1.7879],
        ...,
        [-5.1299],
        [-5.1189],
        [-5.1140]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-370886.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0330],
        [1.0419],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370409.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0306],
        [1.0330],
        [1.0419],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370411.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3018.6909, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8927, device='cuda:0')



h[100].sum tensor(-0.0448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1987, device='cuda:0')



h[200].sum tensor(-22.5687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0083, 0.0023, 0.0000],
        [0.0000, 0.0296, 0.0081,  ..., 0.0125, 0.0285, 0.0162],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71916.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0646, 0.0182, 0.0820,  ..., 0.1062, 0.0303, 0.0565],
        [0.0619, 0.0170, 0.0655,  ..., 0.1234, 0.0415, 0.0688],
        [0.0506, 0.0122, 0.0296,  ..., 0.1893, 0.0843, 0.1161],
        ...,
        [0.0701, 0.0207, 0.1019,  ..., 0.0920, 0.0198, 0.0459],
        [0.0700, 0.0207, 0.1018,  ..., 0.0919, 0.0198, 0.0459],
        [0.0700, 0.0207, 0.1018,  ..., 0.0919, 0.0198, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716229.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8234.2949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.3160, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1971.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1195.1498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4518],
        [-2.0075],
        [-1.0834],
        ...,
        [-5.1294],
        [-5.1213],
        [-5.1212]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290829.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0306],
        [1.0330],
        [1.0419],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370411.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0306],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370414.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0089,  0.0023,  ...,  0.0033,  0.0086,  0.0041],
        [-0.0004,  0.0093,  0.0025,  ...,  0.0034,  0.0090,  0.0044],
        [-0.0009,  0.0175,  0.0053,  ...,  0.0047,  0.0171,  0.0111],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0020,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3228.2725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1583, device='cuda:0')



h[100].sum tensor(-0.0489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.5297, device='cuda:0')



h[200].sum tensor(-22.5504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0317, 0.0088,  ..., 0.0128, 0.0305, 0.0180],
        [0.0000, 0.0613, 0.0181,  ..., 0.0175, 0.0599, 0.0368],
        [0.0000, 0.0503, 0.0143,  ..., 0.0158, 0.0490, 0.0277],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75598.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0408, 0.0080, 0.0000,  ..., 0.2422, 0.1194, 0.1532],
        [0.0381, 0.0061, 0.0000,  ..., 0.2553, 0.1239, 0.1631],
        [0.0372, 0.0060, 0.0000,  ..., 0.2640, 0.1300, 0.1692],
        ...,
        [0.0701, 0.0210, 0.1021,  ..., 0.0921, 0.0194, 0.0457],
        [0.0700, 0.0209, 0.1021,  ..., 0.0921, 0.0194, 0.0457],
        [0.0701, 0.0210, 0.1021,  ..., 0.0921, 0.0194, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(740141.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8291.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.9555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.5139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2150.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1222.4298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3841],
        [ 0.4164],
        [ 0.4257],
        ...,
        [-5.1282],
        [-5.1178],
        [-5.1155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331512.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0306],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370414.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370416.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [-0.0008,  0.0168,  0.0051,  ...,  0.0046,  0.0164,  0.0105],
        [-0.0018,  0.0354,  0.0115,  ...,  0.0075,  0.0349,  0.0258],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2749.1318, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2960, device='cuda:0')



h[100].sum tensor(-0.0364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6732, device='cuda:0')



h[200].sum tensor(-22.5977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0448, 0.0134,  ..., 0.0150, 0.0434, 0.0287],
        [0.0000, 0.0759, 0.0237,  ..., 0.0199, 0.0744, 0.0515],
        [0.0000, 0.0610, 0.0185,  ..., 0.0176, 0.0595, 0.0392],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66154.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0384, 0.0067, 0.0018,  ..., 0.2679, 0.1425, 0.1698],
        [0.0309, 0.0043, 0.0000,  ..., 0.3181, 0.1798, 0.2047],
        [0.0330, 0.0047, 0.0000,  ..., 0.3057, 0.1687, 0.1964],
        ...,
        [0.0700, 0.0210, 0.1021,  ..., 0.0922, 0.0192, 0.0458],
        [0.0700, 0.0210, 0.1021,  ..., 0.0922, 0.0192, 0.0458],
        [0.0700, 0.0210, 0.1021,  ..., 0.0922, 0.0192, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687475.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8418.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.0856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2206.3882, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1143.3145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3037],
        [ 0.3093],
        [ 0.3107],
        ...,
        [-5.1324],
        [-5.1245],
        [-5.1243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323556.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370416.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370418.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [-0.0008,  0.0165,  0.0050,  ...,  0.0045,  0.0161,  0.0103],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3195.1982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5806, device='cuda:0')



h[100].sum tensor(-0.0463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4453, device='cuda:0')



h[200].sum tensor(-22.5562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0621, 0.0184,  ..., 0.0177, 0.0606, 0.0375],
        [0.0000, 0.0166, 0.0041,  ..., 0.0106, 0.0153, 0.0082],
        [0.0000, 0.0196, 0.0051,  ..., 0.0111, 0.0183, 0.0106],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72600.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0469, 0.0092, 0.0017,  ..., 0.2092, 0.0954, 0.1296],
        [0.0550, 0.0132, 0.0304,  ..., 0.1636, 0.0652, 0.0972],
        [0.0520, 0.0121, 0.0108,  ..., 0.1862, 0.0815, 0.1129],
        ...,
        [0.0701, 0.0209, 0.1022,  ..., 0.0923, 0.0190, 0.0459],
        [0.0701, 0.0209, 0.1021,  ..., 0.0923, 0.0190, 0.0458],
        [0.0701, 0.0209, 0.1021,  ..., 0.0923, 0.0190, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714868.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8377.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.4260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2237.5806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1198.9088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2414],
        [ 0.2512],
        [ 0.2636],
        ...,
        [-5.1367],
        [-5.1288],
        [-5.1286]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332846.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370418.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 300.0 event: 1500 loss: tensor(480.3930, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370420.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2868.8887, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4593, device='cuda:0')



h[100].sum tensor(-0.0379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8432, device='cuda:0')



h[200].sum tensor(-22.5882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0085, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0086, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68468.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0679, 0.0193, 0.0994,  ..., 0.0890, 0.0184, 0.0440],
        [0.0682, 0.0194, 0.0997,  ..., 0.0894, 0.0185, 0.0442],
        [0.0686, 0.0197, 0.1001,  ..., 0.0901, 0.0186, 0.0447],
        ...,
        [0.0701, 0.0204, 0.1020,  ..., 0.0923, 0.0190, 0.0460],
        [0.0697, 0.0202, 0.1002,  ..., 0.0940, 0.0199, 0.0472],
        [0.0685, 0.0196, 0.0947,  ..., 0.0998, 0.0229, 0.0516]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698342.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8419.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2554, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.9809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2214.6213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1164.3257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3591],
        [-4.6463],
        [-4.8728],
        ...,
        [-5.0737],
        [-4.9458],
        [-4.7242]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325281.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9956],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370420.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370422.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014,  0.0291,  0.0093,  ...,  0.0065,  0.0286,  0.0206],
        [-0.0009,  0.0177,  0.0054,  ...,  0.0047,  0.0174,  0.0113],
        [-0.0004,  0.0091,  0.0024,  ...,  0.0034,  0.0088,  0.0042],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2458.0098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4335, device='cuda:0')



h[100].sum tensor(-0.0278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1089, device='cuda:0')



h[200].sum tensor(-22.6276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0873, 0.0271,  ..., 0.0216, 0.0856, 0.0582],
        [0.0000, 0.0851, 0.0263,  ..., 0.0213, 0.0835, 0.0564],
        [0.0000, 0.0757, 0.0230,  ..., 0.0199, 0.0741, 0.0487],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61238.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0155, 0.0000, 0.0000,  ..., 0.4146, 0.2490, 0.2727],
        [0.0194, 0.0000, 0.0000,  ..., 0.3880, 0.2264, 0.2546],
        [0.0225, 0.0000, 0.0000,  ..., 0.3682, 0.2087, 0.2412],
        ...,
        [0.0702, 0.0201, 0.1020,  ..., 0.0925, 0.0190, 0.0460],
        [0.0702, 0.0201, 0.1020,  ..., 0.0924, 0.0190, 0.0459],
        [0.0702, 0.0201, 0.1020,  ..., 0.0925, 0.0190, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665726.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8616.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5522, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.9487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2300.5212, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1102.0469, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2759],
        [ 0.3086],
        [ 0.3299],
        ...,
        [-5.1471],
        [-5.1392],
        [-5.1390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343093.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0331],
        [1.0420],
        ...,
        [0.9962],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370422.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370425.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0236,  0.0074,  ...,  0.0056,  0.0232,  0.0162],
        [-0.0012,  0.0248,  0.0078,  ...,  0.0058,  0.0244,  0.0171],
        [-0.0016,  0.0334,  0.0108,  ...,  0.0072,  0.0330,  0.0242],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3132.4653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4040, device='cuda:0')



h[100].sum tensor(-0.0428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2734, device='cuda:0')



h[200].sum tensor(-22.5637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1328, 0.0429,  ..., 0.0288, 0.1309, 0.0957],
        [0.0000, 0.1000, 0.0320,  ..., 0.0236, 0.0982, 0.0714],
        [0.0000, 0.0559, 0.0172,  ..., 0.0168, 0.0544, 0.0378],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0142, 0.0032,  ..., 0.0104, 0.0129, 0.0061]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72205.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0059, 0.0000, 0.0000,  ..., 0.5252, 0.3465, 0.3459],
        [0.0171, 0.0010, 0.0000,  ..., 0.4239, 0.2662, 0.2763],
        [0.0335, 0.0036, 0.0000,  ..., 0.3159, 0.1817, 0.2019],
        ...,
        [0.0701, 0.0199, 0.1003,  ..., 0.0950, 0.0203, 0.0473],
        [0.0680, 0.0187, 0.0889,  ..., 0.1065, 0.0274, 0.0556],
        [0.0612, 0.0149, 0.0516,  ..., 0.1454, 0.0522, 0.0833]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713149.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8376.4424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.7380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2087.6619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1203.5502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2740],
        [ 0.3189],
        [ 0.3635],
        ...,
        [-4.5773],
        [-3.7229],
        [-2.4442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297968.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370425.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370425.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012,  0.0249,  0.0079,  ...,  0.0058,  0.0245,  0.0172],
        [-0.0007,  0.0148,  0.0044,  ...,  0.0042,  0.0144,  0.0089],
        [-0.0004,  0.0091,  0.0024,  ...,  0.0034,  0.0088,  0.0042],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2623.8379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4149, device='cuda:0')



h[100].sum tensor(-0.0313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3984, device='cuda:0')



h[200].sum tensor(-22.6117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0759, 0.0231,  ..., 0.0198, 0.0743, 0.0490],
        [0.0000, 0.0626, 0.0185,  ..., 0.0178, 0.0611, 0.0380],
        [0.0000, 0.0378, 0.0099,  ..., 0.0139, 0.0364, 0.0175],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63993.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0372, 0.0022, 0.0000,  ..., 0.2716, 0.1396, 0.1734],
        [0.0391, 0.0036, 0.0000,  ..., 0.2562, 0.1252, 0.1633],
        [0.0425, 0.0047, 0.0000,  ..., 0.2314, 0.1023, 0.1467],
        ...,
        [0.0705, 0.0201, 0.1024,  ..., 0.0928, 0.0190, 0.0457],
        [0.0705, 0.0201, 0.1024,  ..., 0.0927, 0.0190, 0.0457],
        [0.0705, 0.0201, 0.1024,  ..., 0.0928, 0.0190, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677251.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8625.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.5194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2294.4915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1127.1533, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4693],
        [ 0.4831],
        [ 0.4335],
        ...,
        [-5.1607],
        [-5.1530],
        [-5.1531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343066.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370425.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370427.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [-0.0011,  0.0236,  0.0074,  ...,  0.0056,  0.0232,  0.0161],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2750.9287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1191, device='cuda:0')



h[100].sum tensor(-0.0339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6474, device='cuda:0')



h[200].sum tensor(-22.5993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0085, 0.0021, 0.0000],
        [0.0000, 0.0360, 0.0103,  ..., 0.0136, 0.0346, 0.0215],
        [0.0000, 0.0675, 0.0207,  ..., 0.0186, 0.0659, 0.0446],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65315.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0625, 0.0158, 0.0642,  ..., 0.1260, 0.0428, 0.0691],
        [0.0492, 0.0097, 0.0289,  ..., 0.2111, 0.1028, 0.1286],
        [0.0348, 0.0042, 0.0039,  ..., 0.3095, 0.1749, 0.1968],
        ...,
        [0.0708, 0.0202, 0.1029,  ..., 0.0930, 0.0190, 0.0454],
        [0.0707, 0.0202, 0.1029,  ..., 0.0930, 0.0190, 0.0453],
        [0.0707, 0.0202, 0.1029,  ..., 0.0930, 0.0190, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682707.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8599.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.4671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2274.8816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1142.5975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5501],
        [-1.4264],
        [-0.4612],
        ...,
        [-5.1795],
        [-5.1711],
        [-5.1705]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334964.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370427.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370429.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0200,  0.0062,  ...,  0.0051,  0.0196,  0.0132],
        [-0.0009,  0.0180,  0.0055,  ...,  0.0047,  0.0176,  0.0115],
        [-0.0009,  0.0182,  0.0055,  ...,  0.0048,  0.0178,  0.0117],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3007.9280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2259, device='cuda:0')



h[100].sum tensor(-0.0393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1013, device='cuda:0')



h[200].sum tensor(-22.5744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0811, 0.0249,  ..., 0.0206, 0.0795, 0.0533],
        [0.0000, 0.0556, 0.0160,  ..., 0.0167, 0.0541, 0.0322],
        [0.0000, 0.0576, 0.0167,  ..., 0.0170, 0.0561, 0.0338],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71685.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0307, 0.0014, 0.0000,  ..., 0.3281, 0.1888, 0.2100],
        [0.0389, 0.0032, 0.0000,  ..., 0.2698, 0.1396, 0.1705],
        [0.0429, 0.0044, 0.0000,  ..., 0.2463, 0.1211, 0.1543],
        ...,
        [0.0709, 0.0199, 0.1031,  ..., 0.0931, 0.0192, 0.0453],
        [0.0709, 0.0199, 0.1030,  ..., 0.0930, 0.0192, 0.0452],
        [0.0709, 0.0199, 0.1030,  ..., 0.0931, 0.0192, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718798.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8587.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5713, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.5233, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2254.1597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1195.2330, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.7562e-01],
        [-2.7776e-03],
        [-2.6922e-01],
        ...,
        [-5.1939e+00],
        [-5.1858e+00],
        [-5.1855e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336997.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370429.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370431.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0108,  0.0030,  ...,  0.0036,  0.0105,  0.0057],
        [-0.0003,  0.0074,  0.0018,  ...,  0.0031,  0.0071,  0.0029],
        [-0.0008,  0.0175,  0.0053,  ...,  0.0047,  0.0171,  0.0111],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2437.4255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4416, device='cuda:0')



h[100].sum tensor(-0.0265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1100, device='cuda:0')



h[200].sum tensor(-22.6277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0317, 0.0088,  ..., 0.0129, 0.0304, 0.0180],
        [0.0000, 0.0757, 0.0230,  ..., 0.0198, 0.0741, 0.0488],
        [0.0000, 0.0461, 0.0132,  ..., 0.0152, 0.0447, 0.0271],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61302.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0478, 0.0061, 0.0171,  ..., 0.2122, 0.0988, 0.1302],
        [0.0379, 0.0009, 0.0000,  ..., 0.2754, 0.1420, 0.1746],
        [0.0418, 0.0027, 0.0045,  ..., 0.2535, 0.1253, 0.1593],
        ...,
        [0.0711, 0.0197, 0.1032,  ..., 0.0932, 0.0194, 0.0452],
        [0.0710, 0.0197, 0.1032,  ..., 0.0931, 0.0194, 0.0452],
        [0.0711, 0.0197, 0.1032,  ..., 0.0931, 0.0194, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(670360.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8851.8379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.1628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2464.5349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1103.4984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2109],
        [ 0.2741],
        [ 0.3221],
        ...,
        [-5.2059],
        [-5.1978],
        [-5.1975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-385447., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370431.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0309],
        [1.0329],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370434.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0207,  0.0064,  ...,  0.0052,  0.0203,  0.0137],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3237.8420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6546, device='cuda:0')



h[100].sum tensor(-0.0431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4561, device='cuda:0')



h[200].sum tensor(-22.5534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0201, 0.0053,  ..., 0.0111, 0.0188, 0.0111],
        [0.0000, 0.0238, 0.0066,  ..., 0.0118, 0.0225, 0.0142],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0088, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74389.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0505, 0.0081, 0.0237,  ..., 0.2027, 0.0972, 0.1222],
        [0.0567, 0.0115, 0.0344,  ..., 0.1657, 0.0703, 0.0964],
        [0.0612, 0.0140, 0.0530,  ..., 0.1401, 0.0512, 0.0786],
        ...,
        [0.0710, 0.0198, 0.1033,  ..., 0.0931, 0.0196, 0.0451],
        [0.0710, 0.0198, 0.1033,  ..., 0.0931, 0.0196, 0.0451],
        [0.0710, 0.0198, 0.1033,  ..., 0.0931, 0.0196, 0.0451]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731315.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8574.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.2072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2328.3542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1217.3823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2813],
        [ 0.1879],
        [ 0.0848],
        ...,
        [-5.2106],
        [-5.2026],
        [-5.2023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349209.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0309],
        [1.0329],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370434.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370437.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2792.4976, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1923, device='cuda:0')



h[100].sum tensor(-0.0328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6581, device='cuda:0')



h[200].sum tensor(-22.5966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0021, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0086, 0.0021, 0.0000],
        [0.0000, 0.0116, 0.0024,  ..., 0.0100, 0.0104, 0.0041],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0088, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0088, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0088, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67101.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0682, 0.0185, 0.0990,  ..., 0.0913, 0.0201, 0.0445],
        [0.0667, 0.0175, 0.0895,  ..., 0.1019, 0.0263, 0.0520],
        [0.0623, 0.0149, 0.0646,  ..., 0.1285, 0.0415, 0.0712],
        ...,
        [0.0708, 0.0198, 0.1032,  ..., 0.0931, 0.0198, 0.0453],
        [0.0708, 0.0198, 0.1032,  ..., 0.0930, 0.0198, 0.0453],
        [0.0708, 0.0198, 0.1032,  ..., 0.0931, 0.0198, 0.0453]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692004., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8618.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.1681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2322.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1156.9646, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3828],
        [-3.6567],
        [-2.6197],
        ...,
        [-5.2058],
        [-5.1982],
        [-5.1979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333178., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9961],
        [0.9955],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370437.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9954],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370440.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [-0.0008,  0.0175,  0.0053,  ...,  0.0047,  0.0172,  0.0111],
        [-0.0005,  0.0106,  0.0029,  ...,  0.0036,  0.0103,  0.0054],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3046.0139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3596, device='cuda:0')



h[100].sum tensor(-0.0374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1208, device='cuda:0')



h[200].sum tensor(-22.5743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0438, 0.0119,  ..., 0.0150, 0.0424, 0.0224],
        [0.0000, 0.0275, 0.0068,  ..., 0.0125, 0.0263, 0.0118],
        [0.0000, 0.0671, 0.0200,  ..., 0.0187, 0.0656, 0.0415],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0089, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0089, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0089, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71711.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0487, 0.0072, 0.0050,  ..., 0.1966, 0.0827, 0.1204],
        [0.0471, 0.0066, 0.0102,  ..., 0.2084, 0.0910, 0.1287],
        [0.0357, 0.0021, 0.0000,  ..., 0.2881, 0.1497, 0.1835],
        ...,
        [0.0706, 0.0200, 0.1031,  ..., 0.0931, 0.0200, 0.0455],
        [0.0705, 0.0200, 0.1030,  ..., 0.0930, 0.0199, 0.0455],
        [0.0706, 0.0200, 0.1030,  ..., 0.0930, 0.0199, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(715782.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8462.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.8858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2273.9822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1197.5382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4218],
        [ 0.4232],
        [ 0.4423],
        ...,
        [-5.2002],
        [-5.1924],
        [-5.1921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311371.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9954],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370440.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 310.0 event: 1550 loss: tensor(435.0755, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9954],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370443.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2479.2981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.2951, device='cuda:0')



h[100].sum tensor(-0.0250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0886, device='cuda:0')



h[200].sum tensor(-22.6287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0088, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0089, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0089, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0089, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61363.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0183, 0.0971,  ..., 0.0929, 0.0206, 0.0464],
        [0.0683, 0.0188, 0.1001,  ..., 0.0905, 0.0197, 0.0444],
        [0.0688, 0.0191, 0.1005,  ..., 0.0912, 0.0199, 0.0449],
        ...,
        [0.0704, 0.0199, 0.1027,  ..., 0.0931, 0.0202, 0.0459],
        [0.0704, 0.0199, 0.1026,  ..., 0.0930, 0.0202, 0.0459],
        [0.0704, 0.0199, 0.1027,  ..., 0.0931, 0.0202, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(667782.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8677.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.1868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2505.8472, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1106.3452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1806],
        [-3.1176],
        [-2.8644],
        ...,
        [-5.1901],
        [-5.1825],
        [-5.1821]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345924., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9954],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370443.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9954],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370446.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3963.3479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3556, device='cuda:0')



h[100].sum tensor(-0.0549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7274, device='cuda:0')



h[200].sum tensor(-22.4899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0088, 0.0022, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84392.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0682, 0.0187, 0.1000,  ..., 0.0896, 0.0197, 0.0440],
        [0.0685, 0.0189, 0.1003,  ..., 0.0901, 0.0198, 0.0442],
        [0.0689, 0.0192, 0.1007,  ..., 0.0908, 0.0199, 0.0447],
        ...,
        [0.0704, 0.0198, 0.1026,  ..., 0.0930, 0.0204, 0.0460],
        [0.0704, 0.0198, 0.1025,  ..., 0.0930, 0.0204, 0.0460],
        [0.0704, 0.0198, 0.1026,  ..., 0.0930, 0.0204, 0.0460]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(773704.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8458.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.8218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.3758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2423.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1298.5021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6283],
        [-4.7277],
        [-4.7100],
        ...,
        [-5.1948],
        [-5.1873],
        [-5.1871]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332706.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9954],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370446.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9953],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370448.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [-0.0004,  0.0093,  0.0024,  ...,  0.0035,  0.0090,  0.0043],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2314.5918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.4480, device='cuda:0')



h[100].sum tensor(-0.0210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8188, device='cuda:0')



h[200].sum tensor(-22.6444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0087, 0.0022, 0.0000],
        [0.0000, 0.0120, 0.0025,  ..., 0.0101, 0.0110, 0.0045],
        [0.0000, 0.0307, 0.0084,  ..., 0.0131, 0.0295, 0.0170],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59054.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0647, 0.0169, 0.0795,  ..., 0.1107, 0.0335, 0.0587],
        [0.0573, 0.0127, 0.0420,  ..., 0.1547, 0.0613, 0.0898],
        [0.0478, 0.0075, 0.0162,  ..., 0.2140, 0.1002, 0.1315],
        ...,
        [0.0706, 0.0201, 0.1028,  ..., 0.0929, 0.0205, 0.0458],
        [0.0706, 0.0200, 0.1028,  ..., 0.0928, 0.0205, 0.0458],
        [0.0706, 0.0200, 0.1028,  ..., 0.0929, 0.0205, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661305.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8816.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.1220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2613.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1083.1986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0169],
        [-0.3316],
        [ 0.1024],
        ...,
        [-5.2137],
        [-5.2061],
        [-5.2059]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-361535.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0420],
        ...,
        [0.9960],
        [0.9953],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370448.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0421],
        ...,
        [0.9959],
        [0.9953],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370451.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0018,  0.0373,  0.0121,  ...,  0.0079,  0.0368,  0.0273],
        [-0.0015,  0.0318,  0.0102,  ...,  0.0070,  0.0314,  0.0228],
        [-0.0014,  0.0305,  0.0098,  ...,  0.0068,  0.0301,  0.0217],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2675.4827, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5950, device='cuda:0')



h[100].sum tensor(-0.0279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4247, device='cuda:0')



h[200].sum tensor(-22.6105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1025, 0.0323,  ..., 0.0243, 0.1009, 0.0706],
        [0.0000, 0.1412, 0.0457,  ..., 0.0304, 0.1393, 0.1023],
        [0.0000, 0.0997, 0.0313,  ..., 0.0240, 0.0981, 0.0682],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0090, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64577.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0248, 0.0007, 0.0000,  ..., 0.3614, 0.2143, 0.2323],
        [0.0187, 0.0000, 0.0000,  ..., 0.4073, 0.2515, 0.2631],
        [0.0258, 0.0010, 0.0000,  ..., 0.3633, 0.2159, 0.2331],
        ...,
        [0.0707, 0.0204, 0.1031,  ..., 0.0927, 0.0204, 0.0456],
        [0.0707, 0.0204, 0.1030,  ..., 0.0927, 0.0204, 0.0456],
        [0.0707, 0.0204, 0.1030,  ..., 0.0927, 0.0204, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683400.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8714.1846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(120.2675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2540.1089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1132.5944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4226],
        [ 0.4030],
        [ 0.3909],
        ...,
        [-5.2292],
        [-5.2215],
        [-5.2212]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349286.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0329],
        [1.0421],
        ...,
        [0.9959],
        [0.9953],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370451.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9953],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370453.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3149.9319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1120, device='cuda:0')



h[100].sum tensor(-0.0370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2307, device='cuda:0')



h[200].sum tensor(-22.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0022, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0022, 0.0000],
        [0.0000, 0.0242, 0.0062,  ..., 0.0122, 0.0231, 0.0117],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73290.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0651, 0.0175, 0.0836,  ..., 0.1069, 0.0293, 0.0564],
        [0.0607, 0.0150, 0.0570,  ..., 0.1350, 0.0474, 0.0760],
        [0.0521, 0.0103, 0.0251,  ..., 0.1887, 0.0819, 0.1137],
        ...,
        [0.0709, 0.0205, 0.1033,  ..., 0.0925, 0.0204, 0.0455],
        [0.0708, 0.0205, 0.1032,  ..., 0.0925, 0.0204, 0.0455],
        [0.0709, 0.0205, 0.1032,  ..., 0.0925, 0.0204, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(724849.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8485.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.9153, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2241.6909, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1213.3618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7536],
        [-0.8470],
        [-0.1432],
        ...,
        [-5.2433],
        [-5.2351],
        [-5.2339]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287448.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9953],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370453.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9953],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370455.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0277,  0.0088,  ...,  0.0063,  0.0273,  0.0194],
        [-0.0007,  0.0161,  0.0048,  ...,  0.0045,  0.0158,  0.0100],
        [-0.0010,  0.0227,  0.0071,  ...,  0.0056,  0.0223,  0.0153],
        ...,
        [ 0.0000,  0.0008, -0.0005,  ...,  0.0021,  0.0005, -0.0027],
        [-0.0006,  0.0141,  0.0041,  ...,  0.0042,  0.0138,  0.0083],
        [-0.0006,  0.0141,  0.0041,  ...,  0.0042,  0.0138,  0.0083]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2694.8501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9687, device='cuda:0')



h[100].sum tensor(-0.0280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4793, device='cuda:0')



h[200].sum tensor(-22.6069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0415, 0.0116,  ..., 0.0148, 0.0404, 0.0233],
        [0.0000, 0.0860, 0.0265,  ..., 0.0218, 0.0846, 0.0571],
        [0.0000, 0.0507, 0.0148,  ..., 0.0163, 0.0495, 0.0307],
        ...,
        [0.0000, 0.0288, 0.0077,  ..., 0.0130, 0.0277, 0.0154],
        [0.0000, 0.0287, 0.0077,  ..., 0.0130, 0.0277, 0.0154],
        [0.0000, 0.0288, 0.0077,  ..., 0.0130, 0.0277, 0.0154]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64393.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.2793e-02, 4.9880e-03, 0.0000e+00,  ..., 2.4475e-01, 1.2396e-01,
         1.5237e-01],
        [3.8238e-02, 2.9323e-03, 0.0000e+00,  ..., 2.7772e-01, 1.4969e-01,
         1.7483e-01],
        [4.3633e-02, 5.6015e-03, 1.1371e-04,  ..., 2.4585e-01, 1.2509e-01,
         1.5299e-01],
        ...,
        [5.9029e-02, 1.3462e-02, 3.5392e-02,  ..., 1.6238e-01, 6.4957e-02,
         9.4802e-02],
        [5.6443e-02, 1.2020e-02, 2.1045e-02,  ..., 1.7704e-01, 7.4309e-02,
         1.0513e-01],
        [5.6457e-02, 1.2026e-02, 2.1039e-02,  ..., 1.7710e-01, 7.4332e-02,
         1.0517e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683032.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8869.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(122.2641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2579.0017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1129.0732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3188],
        [ 0.3463],
        [ 0.3410],
        ...,
        [-2.9634],
        [-2.4292],
        [-2.4284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-362453.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9953],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370455.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370458.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2459.4546, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3680, device='cuda:0')



h[100].sum tensor(-0.0233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.0993, device='cuda:0')



h[200].sum tensor(-22.6279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61220.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0691, 0.0189, 0.1007,  ..., 0.0890, 0.0200, 0.0437],
        [0.0692, 0.0190, 0.1005,  ..., 0.0900, 0.0202, 0.0445],
        [0.0680, 0.0183, 0.0935,  ..., 0.0985, 0.0243, 0.0510],
        ...,
        [0.0714, 0.0200, 0.1033,  ..., 0.0924, 0.0206, 0.0457],
        [0.0714, 0.0200, 0.1033,  ..., 0.0924, 0.0206, 0.0457],
        [0.0714, 0.0200, 0.1033,  ..., 0.0924, 0.0206, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672216.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8983.3213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5412, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.3876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2614.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1101.2377, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4749],
        [-3.9921],
        [-3.2686],
        ...,
        [-5.2756],
        [-5.2678],
        [-5.2675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-376874.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370458.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370458.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        ...,
        [-0.0003,  0.0075,  0.0018,  ...,  0.0032,  0.0073,  0.0029],
        [-0.0005,  0.0119,  0.0033,  ...,  0.0039,  0.0116,  0.0065],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2682.1431, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0439, device='cuda:0')



h[100].sum tensor(-0.0276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4903, device='cuda:0')



h[200].sum tensor(-22.6071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0674, 0.0199,  ..., 0.0191, 0.0662, 0.0415],
        [0.0000, 0.0258, 0.0067,  ..., 0.0125, 0.0248, 0.0129],
        [0.0000, 0.0149, 0.0035,  ..., 0.0108, 0.0140, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66580., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0642, 0.0163, 0.0738,  ..., 0.1167, 0.0376, 0.0632],
        [0.0683, 0.0185, 0.0958,  ..., 0.0950, 0.0231, 0.0480],
        [0.0698, 0.0193, 0.1015,  ..., 0.0902, 0.0202, 0.0444],
        ...,
        [0.0425, 0.0040, 0.0000,  ..., 0.2642, 0.1353, 0.1661],
        [0.0525, 0.0096, 0.0235,  ..., 0.2033, 0.0938, 0.1236],
        [0.0618, 0.0147, 0.0502,  ..., 0.1488, 0.0577, 0.0853]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701545.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8823.7881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.9112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2482.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1149.2970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7013],
        [-2.8647],
        [-3.8200],
        ...,
        [-0.0072],
        [-0.8763],
        [-2.2546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350233.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370458.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0309],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370460.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0021,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2322.9102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.9137, device='cuda:0')



h[100].sum tensor(-0.0207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8868, device='cuda:0')



h[200].sum tensor(-22.6399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0090, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59663.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0693, 0.0188, 0.1007,  ..., 0.0889, 0.0199, 0.0438],
        [0.0696, 0.0190, 0.1010,  ..., 0.0893, 0.0200, 0.0441],
        [0.0700, 0.0192, 0.1014,  ..., 0.0901, 0.0201, 0.0445],
        ...,
        [0.0683, 0.0181, 0.0862,  ..., 0.1100, 0.0313, 0.0585],
        [0.0704, 0.0193, 0.0976,  ..., 0.0981, 0.0241, 0.0500],
        [0.0715, 0.0199, 0.1033,  ..., 0.0923, 0.0206, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665630.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9005.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.6934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2512.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1089.9910, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2749],
        [-3.8828],
        [-3.2479],
        ...,
        [-3.8489],
        [-4.4432],
        [-4.8958]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-360393.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0309],
        [1.0330],
        [1.0421],
        ...,
        [0.9959],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370460.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0309],
        [1.0331],
        [1.0422],
        ...,
        [0.9958],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370463.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0027],
        [-0.0009,  0.0205,  0.0063,  ...,  0.0052,  0.0202,  0.0135]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3764.5037, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9985, device='cuda:0')



h[100].sum tensor(-0.0474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.3830, device='cuda:0')



h[200].sum tensor(-22.5047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0425, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0088, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0089, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0090, 0.0024, 0.0000],
        [0.0000, 0.0239, 0.0066,  ..., 0.0123, 0.0230, 0.0143],
        [0.0000, 0.0411, 0.0120,  ..., 0.0150, 0.0401, 0.0256]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83690.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0695, 0.0191, 0.1010,  ..., 0.0888, 0.0198, 0.0435],
        [0.0677, 0.0181, 0.0900,  ..., 0.1010, 0.0271, 0.0521],
        [0.0619, 0.0153, 0.0559,  ..., 0.1399, 0.0540, 0.0790],
        ...,
        [0.0683, 0.0184, 0.0824,  ..., 0.1138, 0.0356, 0.0604],
        [0.0597, 0.0141, 0.0446,  ..., 0.1689, 0.0759, 0.0983],
        [0.0493, 0.0092, 0.0176,  ..., 0.2362, 0.1253, 0.1445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(779012., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8546.0537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.7117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.4434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2234.4019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1299.2214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6868],
        [-2.7238],
        [-1.4083],
        ...,
        [-3.8413],
        [-2.6067],
        [-1.3015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320108.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0309],
        [1.0331],
        [1.0422],
        ...,
        [0.9958],
        [0.9952],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370463.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 320.0 event: 1600 loss: tensor(433.3325, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0310],
        [1.0331],
        [1.0422],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370465.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2258.6245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.3859, device='cuda:0')



h[100].sum tensor(-0.0194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8097, device='cuda:0')



h[200].sum tensor(-22.6438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0133, 0.0023,  ..., 0.0104, 0.0125, 0.0029],
        [0.0000, 0.0082, 0.0012,  ..., 0.0096, 0.0074, 0.0015],
        [0.0000, 0.0486, 0.0146,  ..., 0.0160, 0.0476, 0.0319],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0090, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0090, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0090, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58486.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0601, 0.0140, 0.0578,  ..., 0.1344, 0.0424, 0.0770],
        [0.0577, 0.0126, 0.0369,  ..., 0.1560, 0.0601, 0.0913],
        [0.0459, 0.0074, 0.0125,  ..., 0.2392, 0.1233, 0.1479],
        ...,
        [0.0720, 0.0200, 0.1038,  ..., 0.0922, 0.0205, 0.0454],
        [0.0720, 0.0200, 0.1037,  ..., 0.0921, 0.0205, 0.0454],
        [0.0720, 0.0200, 0.1038,  ..., 0.0922, 0.0205, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664219.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9216.3135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.4209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2625.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1076.1830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0033],
        [-0.9005],
        [-0.0955],
        ...,
        [-5.3247],
        [-5.3165],
        [-5.3159]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-410392.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0310],
        [1.0331],
        [1.0422],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370465.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0310],
        [1.0332],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370467.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0021,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3543.7981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6746, device='cuda:0')



h[100].sum tensor(-0.0426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0435, device='cuda:0')



h[200].sum tensor(-22.5232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0307, 0.0084,  ..., 0.0131, 0.0298, 0.0172],
        [0.0000, 0.0317, 0.0087,  ..., 0.0133, 0.0308, 0.0181],
        [0.0000, 0.0130, 0.0028,  ..., 0.0104, 0.0122, 0.0054],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0090, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0090, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0090, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78170.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0099, 0.0204,  ..., 0.1813, 0.0772, 0.1089],
        [0.0534, 0.0100, 0.0201,  ..., 0.1824, 0.0782, 0.1095],
        [0.0597, 0.0135, 0.0446,  ..., 0.1495, 0.0565, 0.0864],
        ...,
        [0.0722, 0.0201, 0.1041,  ..., 0.0921, 0.0205, 0.0452],
        [0.0721, 0.0201, 0.1040,  ..., 0.0920, 0.0204, 0.0452],
        [0.0722, 0.0201, 0.1041,  ..., 0.0920, 0.0205, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(751582.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8695.4551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.1641, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.9805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2111.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1255.1110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4146],
        [-1.5509],
        [-2.1984],
        ...,
        [-5.3415],
        [-5.3332],
        [-5.3320]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303780.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0310],
        [1.0332],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370467.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0310],
        [1.0332],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370470.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0094,  0.0024,  ...,  0.0035,  0.0091,  0.0044],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3446.1211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2339, device='cuda:0')



h[100].sum tensor(-0.0403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8330, device='cuda:0')



h[200].sum tensor(-22.5325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0323, 0.0089,  ..., 0.0135, 0.0314, 0.0186],
        [0.0000, 0.0119, 0.0025,  ..., 0.0103, 0.0111, 0.0046],
        [0.0000, 0.0031, 0.0000,  ..., 0.0089, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80748.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0508, 0.0086, 0.0199,  ..., 0.1977, 0.0896, 0.1203],
        [0.0578, 0.0123, 0.0351,  ..., 0.1592, 0.0641, 0.0932],
        [0.0613, 0.0143, 0.0487,  ..., 0.1447, 0.0560, 0.0825],
        ...,
        [0.0722, 0.0202, 0.1041,  ..., 0.0920, 0.0203, 0.0453],
        [0.0722, 0.0202, 0.1041,  ..., 0.0919, 0.0203, 0.0453],
        [0.0722, 0.0202, 0.1041,  ..., 0.0919, 0.0203, 0.0453]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(780144.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8854.9287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.4330, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.1070, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2380.0300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1268.5565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8589e-03],
        [-3.4077e-01],
        [-6.3476e-01],
        ...,
        [-5.3272e+00],
        [-5.3184e+00],
        [-5.3173e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-364251.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0310],
        [1.0332],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370470.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370472.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0167,  0.0049,  ...,  0.0046,  0.0164,  0.0104],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2678.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3185, device='cuda:0')



h[100].sum tensor(-0.0263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5304, device='cuda:0')



h[200].sum tensor(-22.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0784, 0.0242,  ..., 0.0207, 0.0771, 0.0536],
        [0.0000, 0.0307, 0.0083,  ..., 0.0132, 0.0298, 0.0172],
        [0.0000, 0.0031, 0.0000,  ..., 0.0089, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66533.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0259, 0.0006, 0.0000,  ..., 0.3673, 0.2240, 0.2350],
        [0.0387, 0.0039, 0.0053,  ..., 0.2855, 0.1608, 0.1793],
        [0.0482, 0.0074, 0.0055,  ..., 0.2287, 0.1193, 0.1403],
        ...,
        [0.0723, 0.0198, 0.1039,  ..., 0.0920, 0.0203, 0.0456],
        [0.0722, 0.0198, 0.1039,  ..., 0.0919, 0.0203, 0.0456],
        [0.0722, 0.0198, 0.1039,  ..., 0.0919, 0.0203, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698134., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8918.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.3094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2205.9561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1156.9720, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3016],
        [ 0.3024],
        [ 0.3053],
        ...,
        [-5.3400],
        [-5.3316],
        [-5.3308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319731.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370472.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370475.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3469.8967, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6731, device='cuda:0')



h[100].sum tensor(-0.0398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.8972, device='cuda:0')



h[200].sum tensor(-22.5298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0378, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0089, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0089, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0089, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82039.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0695, 0.0184, 0.0997,  ..., 0.0905, 0.0201, 0.0453],
        [0.0702, 0.0187, 0.1012,  ..., 0.0897, 0.0198, 0.0444],
        [0.0708, 0.0191, 0.1022,  ..., 0.0898, 0.0198, 0.0443],
        ...,
        [0.0724, 0.0198, 0.1041,  ..., 0.0920, 0.0202, 0.0456],
        [0.0724, 0.0198, 0.1040,  ..., 0.0919, 0.0202, 0.0455],
        [0.0724, 0.0198, 0.1040,  ..., 0.0920, 0.0202, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(794014.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8819.5830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.5506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.7178, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2308.3296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1282.3668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0139],
        [-3.6512],
        [-4.2075],
        ...,
        [-5.3490],
        [-5.3404],
        [-5.3395]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358555.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0423],
        ...,
        [0.9958],
        [0.9952],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370475.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0424],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370478.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0077,  0.0018,  ...,  0.0033,  0.0075,  0.0031],
        [-0.0009,  0.0202,  0.0061,  ...,  0.0052,  0.0199,  0.0133],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2681.3162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6800, device='cuda:0')



h[100].sum tensor(-0.0258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5832, device='cuda:0')



h[200].sum tensor(-22.6029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1099, 0.0344,  ..., 0.0256, 0.1084, 0.0767],
        [0.0000, 0.0326, 0.0089,  ..., 0.0135, 0.0317, 0.0188],
        [0.0000, 0.0232, 0.0063,  ..., 0.0121, 0.0223, 0.0137],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65227.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0317, 0.0009, 0.0000,  ..., 0.3217, 0.1860, 0.2052],
        [0.0474, 0.0067, 0.0168,  ..., 0.2248, 0.1141, 0.1389],
        [0.0576, 0.0119, 0.0404,  ..., 0.1671, 0.0732, 0.0990],
        ...,
        [0.0725, 0.0197, 0.1041,  ..., 0.0920, 0.0201, 0.0457],
        [0.0724, 0.0197, 0.1040,  ..., 0.0919, 0.0201, 0.0457],
        [0.0724, 0.0197, 0.1040,  ..., 0.0919, 0.0201, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690319.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9040.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9013, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.5146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2291.7339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1144.4913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3314],
        [-0.1737],
        [-1.0614],
        ...,
        [-5.3222],
        [-5.3358],
        [-5.3394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342755.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0424],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370478.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0424],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370481.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [-0.0005,  0.0115,  0.0031,  ...,  0.0039,  0.0112,  0.0062],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3092.4512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7202, device='cuda:0')



h[100].sum tensor(-0.0325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3196, device='cuda:0')



h[200].sum tensor(-22.5645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0433, 0.0113,  ..., 0.0153, 0.0423, 0.0221],
        [0.0000, 0.0122, 0.0025,  ..., 0.0104, 0.0113, 0.0047],
        [0.0000, 0.0405, 0.0117,  ..., 0.0149, 0.0395, 0.0252],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70076.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0570, 0.0116, 0.0336,  ..., 0.1595, 0.0626, 0.0941],
        [0.0573, 0.0122, 0.0318,  ..., 0.1656, 0.0713, 0.0973],
        [0.0474, 0.0081, 0.0113,  ..., 0.2339, 0.1232, 0.1438],
        ...,
        [0.0725, 0.0201, 0.1045,  ..., 0.0919, 0.0199, 0.0455],
        [0.0725, 0.0201, 0.1044,  ..., 0.0918, 0.0199, 0.0455],
        [0.0725, 0.0201, 0.1044,  ..., 0.0918, 0.0199, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706521.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9030.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.5215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2319.5251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1183.6571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2881],
        [-1.3748],
        [-0.4310],
        ...,
        [-5.3638],
        [-5.3551],
        [-5.3540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345738.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0333],
        [1.0424],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370481.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0334],
        [1.0424],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370484.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0103,  0.0027,  ...,  0.0037,  0.0100,  0.0052],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2562.9839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1996, device='cuda:0')



h[100].sum tensor(-0.0232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3669, device='cuda:0')



h[200].sum tensor(-22.6138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0334, 0.0092,  ..., 0.0137, 0.0324, 0.0194],
        [0.0000, 0.0130, 0.0027,  ..., 0.0106, 0.0121, 0.0053],
        [0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64679.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0050, 0.0128,  ..., 0.2601, 0.1401, 0.1627],
        [0.0542, 0.0103, 0.0264,  ..., 0.1851, 0.0855, 0.1110],
        [0.0619, 0.0146, 0.0491,  ..., 0.1440, 0.0578, 0.0821],
        ...,
        [0.0725, 0.0201, 0.1044,  ..., 0.0918, 0.0198, 0.0458],
        [0.0724, 0.0201, 0.1043,  ..., 0.0917, 0.0198, 0.0457],
        [0.0725, 0.0201, 0.1043,  ..., 0.0918, 0.0198, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691392.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9070.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.1137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2372.2805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1139.1207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2131],
        [ 0.1829],
        [ 0.1560],
        ...,
        [-5.3383],
        [-5.3290],
        [-5.3275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347098.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0334],
        [1.0424],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370484.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0334],
        [1.0425],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370487.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2857.1392, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4977, device='cuda:0')



h[100].sum tensor(-0.0277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8488, device='cuda:0')



h[200].sum tensor(-22.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0203, 0.0046,  ..., 0.0117, 0.0193, 0.0086],
        [0.0000, 0.0083, 0.0011,  ..., 0.0099, 0.0074, 0.0015],
        [0.0000, 0.0125, 0.0026,  ..., 0.0106, 0.0116, 0.0050],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69904.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0556, 0.0110, 0.0342,  ..., 0.1609, 0.0593, 0.0965],
        [0.0580, 0.0123, 0.0433,  ..., 0.1507, 0.0533, 0.0891],
        [0.0574, 0.0119, 0.0374,  ..., 0.1578, 0.0582, 0.0940],
        ...,
        [0.0725, 0.0201, 0.1044,  ..., 0.0918, 0.0197, 0.0460],
        [0.0724, 0.0201, 0.1044,  ..., 0.0917, 0.0196, 0.0459],
        [0.0724, 0.0201, 0.1044,  ..., 0.0917, 0.0196, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(717688.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8933.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.4511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2311.3350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1186.2003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5714],
        [-0.2591],
        [-0.0640],
        ...,
        [-5.3589],
        [-5.3502],
        [-5.3491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331327.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0334],
        [1.0425],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370487.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0334],
        [1.0425],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370490.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0005, -0.0027],
        [-0.0003,  0.0081,  0.0019,  ...,  0.0033,  0.0078,  0.0033],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3014.3584, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1939, device='cuda:0')



h[100].sum tensor(-0.0298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0966, device='cuda:0')



h[200].sum tensor(-22.5729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0022, 0.0000],
        [0.0000, 0.0236, 0.0057,  ..., 0.0123, 0.0226, 0.0113],
        [0.0000, 0.0304, 0.0074,  ..., 0.0134, 0.0294, 0.0141],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70990.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0558, 0.0112, 0.0268,  ..., 0.1665, 0.0682, 0.0992],
        [0.0526, 0.0091, 0.0212,  ..., 0.1811, 0.0732, 0.1104],
        [0.0482, 0.0067, 0.0119,  ..., 0.2070, 0.0884, 0.1289],
        ...,
        [0.0725, 0.0202, 0.1047,  ..., 0.0918, 0.0196, 0.0459],
        [0.0724, 0.0202, 0.1046,  ..., 0.0917, 0.0196, 0.0459],
        [0.0724, 0.0202, 0.1046,  ..., 0.0918, 0.0196, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718323.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8950.7207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.3990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2401.1697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1193.8607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3728],
        [ 0.3770],
        [ 0.3854],
        ...,
        [-5.3653],
        [-5.3565],
        [-5.3553]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349612.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0334],
        [1.0425],
        ...,
        [0.9958],
        [0.9951],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370490.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 330.0 event: 1650 loss: tensor(486.2178, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0334],
        [1.0425],
        ...,
        [0.9957],
        [0.9951],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370493.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0382,  0.0123,  ...,  0.0081,  0.0377,  0.0280],
        [-0.0011,  0.0260,  0.0081,  ...,  0.0062,  0.0256,  0.0180],
        [-0.0006,  0.0141,  0.0040,  ...,  0.0043,  0.0138,  0.0083],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2815.3413, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8308, device='cuda:0')



h[100].sum tensor(-0.0261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7513, device='cuda:0')



h[200].sum tensor(-22.5922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1201, 0.0378,  ..., 0.0274, 0.1184, 0.0849],
        [0.0000, 0.1184, 0.0372,  ..., 0.0272, 0.1168, 0.0835],
        [0.0000, 0.0601, 0.0183,  ..., 0.0181, 0.0588, 0.0411],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68270.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0152, 0.0000, 0.0000,  ..., 0.4239, 0.2660, 0.2749],
        [0.0185, 0.0000, 0.0000,  ..., 0.4048, 0.2504, 0.2619],
        [0.0329, 0.0029, 0.0008,  ..., 0.3175, 0.1835, 0.2024],
        ...,
        [0.0723, 0.0199, 0.1043,  ..., 0.0920, 0.0197, 0.0465],
        [0.0723, 0.0198, 0.1043,  ..., 0.0919, 0.0196, 0.0465],
        [0.0723, 0.0198, 0.1043,  ..., 0.0919, 0.0196, 0.0465]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706214.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8935.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.4031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2419.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1174.6522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3267],
        [ 0.3252],
        [ 0.3113],
        ...,
        [-5.3471],
        [-5.3384],
        [-5.3372]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335149.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0334],
        [1.0425],
        ...,
        [0.9957],
        [0.9951],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370493.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0313],
        [1.0334],
        [1.0425],
        ...,
        [0.9957],
        [0.9950],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370497.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2997.0479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1257, device='cuda:0')



h[100].sum tensor(-0.0285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0867, device='cuda:0')



h[200].sum tensor(-22.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0093, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70591., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0699, 0.0182, 0.1013,  ..., 0.0889, 0.0192, 0.0450],
        [0.0701, 0.0183, 0.1016,  ..., 0.0894, 0.0193, 0.0452],
        [0.0706, 0.0186, 0.1020,  ..., 0.0901, 0.0194, 0.0457],
        ...,
        [0.0722, 0.0193, 0.1039,  ..., 0.0923, 0.0198, 0.0471],
        [0.0721, 0.0192, 0.1038,  ..., 0.0922, 0.0198, 0.0470],
        [0.0721, 0.0192, 0.1038,  ..., 0.0923, 0.0198, 0.0470]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(714337.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8901.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.2618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2378.0122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1193.4740, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0656],
        [-4.8811],
        [-4.5517],
        ...,
        [-5.3279],
        [-5.3194],
        [-5.3182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322973.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0313],
        [1.0334],
        [1.0425],
        ...,
        [0.9957],
        [0.9950],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370497.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0313],
        [1.0335],
        [1.0426],
        ...,
        [0.9957],
        [0.9950],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370500.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2580.9551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8319, device='cuda:0')



h[100].sum tensor(-0.0215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3132, device='cuda:0')



h[200].sum tensor(-22.6153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0093, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65197.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0663, 0.0161, 0.0837,  ..., 0.1071, 0.0310, 0.0579],
        [0.0697, 0.0178, 0.0998,  ..., 0.0914, 0.0203, 0.0467],
        [0.0705, 0.0183, 0.1018,  ..., 0.0903, 0.0195, 0.0459],
        ...,
        [0.0721, 0.0190, 0.1037,  ..., 0.0926, 0.0199, 0.0473],
        [0.0720, 0.0190, 0.1036,  ..., 0.0925, 0.0199, 0.0472],
        [0.0720, 0.0190, 0.1036,  ..., 0.0925, 0.0199, 0.0472]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691247.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8975.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2437.2402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1147.6888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6535],
        [-3.4771],
        [-3.9295],
        ...,
        [-5.3174],
        [-5.3083],
        [-5.3067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327084.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0313],
        [1.0335],
        [1.0426],
        ...,
        [0.9957],
        [0.9950],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370500.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0314],
        [1.0335],
        [1.0426],
        ...,
        [0.9957],
        [0.9950],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370503.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0137,  0.0038,  ...,  0.0042,  0.0134,  0.0079],
        [-0.0007,  0.0159,  0.0045,  ...,  0.0046,  0.0156,  0.0097],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3114.1519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0613, device='cuda:0')



h[100].sum tensor(-0.0294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2233, device='cuda:0')



h[200].sum tensor(-22.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1367, 0.0434,  ..., 0.0301, 0.1349, 0.0984],
        [0.0000, 0.0498, 0.0147,  ..., 0.0165, 0.0487, 0.0327],
        [0.0000, 0.0188, 0.0047,  ..., 0.0117, 0.0179, 0.0100],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72515.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0073, 0.0000, 0.0000,  ..., 0.5535, 0.3765, 0.3602],
        [0.0257, 0.0031, 0.0071,  ..., 0.3607, 0.2238, 0.2301],
        [0.0469, 0.0074, 0.0207,  ..., 0.2301, 0.1212, 0.1417],
        ...,
        [0.0721, 0.0191, 0.1038,  ..., 0.0928, 0.0198, 0.0470],
        [0.0721, 0.0191, 0.1037,  ..., 0.0927, 0.0198, 0.0470],
        [0.0721, 0.0191, 0.1037,  ..., 0.0927, 0.0198, 0.0470]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(721408., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8810.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6158, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.0626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2333.8140, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1214.3640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0929],
        [ 0.1183],
        [ 0.1183],
        ...,
        [-5.3004],
        [-5.2921],
        [-5.2923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313082.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0314],
        [1.0335],
        [1.0426],
        ...,
        [0.9957],
        [0.9950],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370503.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0314],
        [1.0335],
        [1.0426],
        ...,
        [0.9956],
        [0.9950],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370506.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2769.9958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0902, device='cuda:0')



h[100].sum tensor(-0.0238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6431, device='cuda:0')



h[200].sum tensor(-22.5979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0167, 0.0033,  ..., 0.0114, 0.0158, 0.0056],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68818.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0563, 0.0113, 0.0347,  ..., 0.1656, 0.0714, 0.0975],
        [0.0512, 0.0086, 0.0222,  ..., 0.1972, 0.0934, 0.1193],
        [0.0441, 0.0048, 0.0131,  ..., 0.2374, 0.1187, 0.1476],
        ...,
        [0.0722, 0.0195, 0.1041,  ..., 0.0929, 0.0198, 0.0466],
        [0.0721, 0.0195, 0.1040,  ..., 0.0928, 0.0198, 0.0466],
        [0.0721, 0.0195, 0.1040,  ..., 0.0928, 0.0198, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708355., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8855.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.7513, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2297.8049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1185.2672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1132],
        [ 0.1650],
        [ 0.1904],
        ...,
        [-5.3484],
        [-5.3393],
        [-5.3375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309431.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0314],
        [1.0335],
        [1.0426],
        ...,
        [0.9956],
        [0.9950],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370506.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0315],
        [1.0335],
        [1.0427],
        ...,
        [0.9956],
        [0.9949],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370508.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [-0.0011,  0.0266,  0.0083,  ...,  0.0063,  0.0262,  0.0185],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2564.2307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8969, device='cuda:0')



h[100].sum tensor(-0.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3227, device='cuda:0')



h[200].sum tensor(-22.6165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0298, 0.0085,  ..., 0.0134, 0.0288, 0.0191],
        [0.0000, 0.0251, 0.0069,  ..., 0.0127, 0.0241, 0.0152],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63166.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0665, 0.0166, 0.0807,  ..., 0.1111, 0.0349, 0.0590],
        [0.0583, 0.0125, 0.0459,  ..., 0.1637, 0.0736, 0.0947],
        [0.0537, 0.0102, 0.0241,  ..., 0.1951, 0.0963, 0.1161],
        ...,
        [0.0724, 0.0196, 0.1044,  ..., 0.0930, 0.0199, 0.0463],
        [0.0723, 0.0196, 0.1043,  ..., 0.0929, 0.0198, 0.0463],
        [0.0723, 0.0196, 0.1043,  ..., 0.0929, 0.0198, 0.0463]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678667.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9098.6104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(128.6623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2418.1082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1135.7656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8709],
        [-2.9013],
        [-1.9088],
        ...,
        [-5.3668],
        [-5.3583],
        [-5.3574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338613.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0315],
        [1.0335],
        [1.0427],
        ...,
        [0.9956],
        [0.9949],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370508.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0316],
        [1.0335],
        [1.0427],
        ...,
        [0.9956],
        [0.9949],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370511.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0222,  0.0068,  ...,  0.0056,  0.0219,  0.0149],
        [-0.0007,  0.0176,  0.0052,  ...,  0.0049,  0.0173,  0.0111],
        [-0.0020,  0.0473,  0.0155,  ...,  0.0096,  0.0468,  0.0355],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2933.5010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9297, device='cuda:0')



h[100].sum tensor(-0.0257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9119, device='cuda:0')



h[200].sum tensor(-22.5826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0525, 0.0158,  ..., 0.0170, 0.0514, 0.0350],
        [0.0000, 0.1372, 0.0438,  ..., 0.0304, 0.1356, 0.0989],
        [0.0000, 0.1295, 0.0411,  ..., 0.0292, 0.1279, 0.0925],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69542.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0375, 0.0052, 0.0086,  ..., 0.2906, 0.1688, 0.1803],
        [0.0151, 0.0000, 0.0000,  ..., 0.4352, 0.2780, 0.2781],
        [0.0068, 0.0000, 0.0000,  ..., 0.4942, 0.3200, 0.3184],
        ...,
        [0.0726, 0.0195, 0.1046,  ..., 0.0931, 0.0200, 0.0462],
        [0.0725, 0.0194, 0.1045,  ..., 0.0930, 0.0199, 0.0461],
        [0.0725, 0.0194, 0.1045,  ..., 0.0930, 0.0199, 0.0461]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711439.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9090.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.3708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2437.6577, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1187.4209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1697],
        [ 0.2630],
        [ 0.3054],
        ...,
        [-5.3841],
        [-5.3752],
        [-5.3738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-356839.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0316],
        [1.0335],
        [1.0427],
        ...,
        [0.9956],
        [0.9949],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370511.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0316],
        [1.0335],
        [1.0428],
        ...,
        [0.9956],
        [0.9949],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370514.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0134,  0.0037,  ...,  0.0042,  0.0132,  0.0077],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [-0.0003,  0.0074,  0.0016,  ...,  0.0033,  0.0072,  0.0028],
        ...,
        [-0.0010,  0.0228,  0.0070,  ...,  0.0057,  0.0225,  0.0154],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3443.0537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6099, device='cuda:0')



h[100].sum tensor(-0.0327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7418, device='cuda:0')



h[200].sum tensor(-22.5363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0363, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0507, 0.0145,  ..., 0.0167, 0.0497, 0.0308],
        [0.0000, 0.0394, 0.0098,  ..., 0.0149, 0.0384, 0.0187],
        [0.0000, 0.0224, 0.0046,  ..., 0.0123, 0.0216, 0.0075],
        ...,
        [0.0000, 0.0223, 0.0059,  ..., 0.0124, 0.0214, 0.0128],
        [0.0000, 0.0265, 0.0074,  ..., 0.0131, 0.0256, 0.0163],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76365.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0326, 0.0010, 0.0000,  ..., 0.2993, 0.1614, 0.1895],
        [0.0393, 0.0021, 0.0000,  ..., 0.2526, 0.1216, 0.1591],
        [0.0463, 0.0048, 0.0016,  ..., 0.2108, 0.0893, 0.1309],
        ...,
        [0.0587, 0.0115, 0.0338,  ..., 0.1742, 0.0769, 0.1018],
        [0.0619, 0.0133, 0.0505,  ..., 0.1559, 0.0645, 0.0892],
        [0.0692, 0.0173, 0.0853,  ..., 0.1128, 0.0339, 0.0597]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(739466., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8974.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.9894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.4346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2350.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1248.8823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3679],
        [ 0.4035],
        [ 0.4241],
        ...,
        [-2.6210],
        [-3.3294],
        [-4.1837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337786.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0316],
        [1.0335],
        [1.0428],
        ...,
        [0.9956],
        [0.9949],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370514.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0336],
        [1.0428],
        ...,
        [0.9955],
        [0.9949],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370516.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2974.6802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5000, device='cuda:0')



h[100].sum tensor(-0.0255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9952, device='cuda:0')



h[200].sum tensor(-22.5797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71888.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0704, 0.0178, 0.1019,  ..., 0.0901, 0.0194, 0.0443],
        [0.0707, 0.0179, 0.1023,  ..., 0.0905, 0.0195, 0.0445],
        [0.0712, 0.0182, 0.1026,  ..., 0.0912, 0.0196, 0.0450],
        ...,
        [0.0728, 0.0189, 0.1046,  ..., 0.0935, 0.0200, 0.0463],
        [0.0727, 0.0188, 0.1045,  ..., 0.0934, 0.0200, 0.0463],
        [0.0727, 0.0188, 0.1045,  ..., 0.0934, 0.0200, 0.0463]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727544.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8978.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.2173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2334.8062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1213.2321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2635],
        [-5.2812],
        [-5.2638],
        ...,
        [-5.3891],
        [-5.3803],
        [-5.3790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339565.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0336],
        [1.0428],
        ...,
        [0.9955],
        [0.9949],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370516.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0318],
        [1.0336],
        [1.0429],
        ...,
        [0.9955],
        [0.9948],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370518.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3447.1631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.7064, device='cuda:0')



h[100].sum tensor(-0.0319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7559, device='cuda:0')



h[200].sum tensor(-22.5366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77790.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0705, 0.0178, 0.1023,  ..., 0.0901, 0.0194, 0.0439],
        [0.0707, 0.0179, 0.1023,  ..., 0.0909, 0.0196, 0.0444],
        [0.0708, 0.0180, 0.1016,  ..., 0.0928, 0.0200, 0.0459],
        ...,
        [0.0729, 0.0189, 0.1050,  ..., 0.0936, 0.0200, 0.0459],
        [0.0728, 0.0189, 0.1049,  ..., 0.0935, 0.0200, 0.0458],
        [0.0728, 0.0189, 0.1049,  ..., 0.0935, 0.0200, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(749681.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8947.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.1249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.1114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2278.4053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1264.8322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9479],
        [-4.6866],
        [-4.2975],
        ...,
        [-5.4064],
        [-5.3972],
        [-5.3957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326234.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0318],
        [1.0336],
        [1.0429],
        ...,
        [0.9955],
        [0.9948],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370518.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 340.0 event: 1700 loss: tensor(488.1768, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0318],
        [1.0336],
        [1.0429],
        ...,
        [0.9955],
        [0.9948],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370520.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        ...,
        [-0.0005,  0.0122,  0.0033,  ...,  0.0040,  0.0119,  0.0067],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3208.8071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1803, device='cuda:0')



h[100].sum tensor(-0.0282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.3868, device='cuda:0')



h[200].sum tensor(-22.5581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0091, 0.0022, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0022, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0092, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0400, 0.0113,  ..., 0.0152, 0.0389, 0.0246],
        [0.0000, 0.0153, 0.0035,  ..., 0.0113, 0.0143, 0.0071],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74903.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0692, 0.0169, 0.0941,  ..., 0.0992, 0.0256, 0.0495],
        [0.0648, 0.0142, 0.0708,  ..., 0.1244, 0.0415, 0.0670],
        [0.0592, 0.0109, 0.0411,  ..., 0.1567, 0.0615, 0.0895],
        ...,
        [0.0436, 0.0047, 0.0110,  ..., 0.2651, 0.1409, 0.1618],
        [0.0577, 0.0104, 0.0397,  ..., 0.1815, 0.0810, 0.1053],
        [0.0683, 0.0161, 0.0793,  ..., 0.1206, 0.0386, 0.0638]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(744538.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8941.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2265.2244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1241.5856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8280],
        [-0.8919],
        [-0.1710],
        ...,
        [-0.2740],
        [-1.4935],
        [-3.1000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332576.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0318],
        [1.0336],
        [1.0429],
        ...,
        [0.9955],
        [0.9948],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370520.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0337],
        [1.0430],
        ...,
        [0.9954],
        [0.9947],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370522., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2555.6008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8655, device='cuda:0')



h[100].sum tensor(-0.0188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3181, device='cuda:0')



h[200].sum tensor(-22.6176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0123, 0.0025,  ..., 0.0105, 0.0114, 0.0048],
        [0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0022, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0091, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0093, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62745.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0621, 0.0118, 0.0610,  ..., 0.1347, 0.0461, 0.0749],
        [0.0687, 0.0159, 0.0912,  ..., 0.1037, 0.0273, 0.0528],
        [0.0710, 0.0173, 0.1000,  ..., 0.0954, 0.0221, 0.0468],
        ...,
        [0.0734, 0.0185, 0.1058,  ..., 0.0937, 0.0202, 0.0453],
        [0.0733, 0.0184, 0.1057,  ..., 0.0936, 0.0201, 0.0453],
        [0.0733, 0.0184, 0.1057,  ..., 0.0936, 0.0202, 0.0453]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678610.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9351.9395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.5826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2474.2075, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1133.0513, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8292],
        [-2.7327],
        [-3.1938],
        ...,
        [-5.4493],
        [-5.4398],
        [-5.4382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-372293.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0337],
        [1.0430],
        ...,
        [0.9954],
        [0.9947],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370522., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0338],
        [1.0430],
        ...,
        [0.9954],
        [0.9947],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370524.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0128,  0.0035,  ...,  0.0041,  0.0125,  0.0072],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [-0.0015,  0.0351,  0.0112,  ...,  0.0075,  0.0346,  0.0255],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2971.7251, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4869, device='cuda:0')



h[100].sum tensor(-0.0242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9933, device='cuda:0')



h[200].sum tensor(-22.5799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0028,  ..., 0.0105, 0.0123, 0.0056],
        [0.0000, 0.0714, 0.0207,  ..., 0.0197, 0.0701, 0.0450],
        [0.0000, 0.0373, 0.0104,  ..., 0.0144, 0.0363, 0.0225],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70750.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0452, 0.0016, 0.0032,  ..., 0.2348, 0.1194, 0.1420],
        [0.0360, 0.0000, 0.0000,  ..., 0.2877, 0.1559, 0.1780],
        [0.0372, 0.0008, 0.0032,  ..., 0.2855, 0.1552, 0.1762],
        ...,
        [0.0735, 0.0180, 0.1056,  ..., 0.0939, 0.0203, 0.0456],
        [0.0734, 0.0180, 0.1055,  ..., 0.0938, 0.0203, 0.0456],
        [0.0734, 0.0180, 0.1055,  ..., 0.0938, 0.0203, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(713556., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9099.3721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.4057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2224.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1208.9159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3923],
        [ 0.3991],
        [ 0.4016],
        ...,
        [-5.4488],
        [-5.4394],
        [-5.4378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317041.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0338],
        [1.0430],
        ...,
        [0.9954],
        [0.9947],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370524.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0338],
        [1.0431],
        ...,
        [0.9953],
        [0.9947],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370526.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0170,  0.0049,  ...,  0.0047,  0.0167,  0.0106],
        [-0.0004,  0.0110,  0.0028,  ...,  0.0038,  0.0107,  0.0057],
        [-0.0003,  0.0087,  0.0021,  ...,  0.0034,  0.0085,  0.0039],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2520.9368, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.9869, device='cuda:0')



h[100].sum tensor(-0.0175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1897, device='cuda:0')



h[200].sum tensor(-22.6231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0551, 0.0151,  ..., 0.0171, 0.0538, 0.0317],
        [0.0000, 0.0503, 0.0134,  ..., 0.0164, 0.0491, 0.0277],
        [0.0000, 0.0205, 0.0046,  ..., 0.0118, 0.0195, 0.0087],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0092, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62535.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.3677e-02, 8.4433e-04, 1.5979e-04,  ..., 2.3160e-01, 1.0858e-01,
         1.4181e-01],
        [4.8333e-02, 3.9231e-03, 1.0723e-02,  ..., 2.0810e-01, 9.2895e-02,
         1.2567e-01],
        [5.7804e-02, 8.8901e-03, 3.9028e-02,  ..., 1.5990e-01, 6.0831e-02,
         9.2564e-02],
        ...,
        [7.3269e-02, 1.8178e-02, 1.0538e-01,  ..., 9.3992e-02, 2.0417e-02,
         4.5694e-02],
        [7.3198e-02, 1.8155e-02, 1.0528e-01,  ..., 9.3897e-02, 2.0398e-02,
         4.5644e-02],
        [7.3201e-02, 1.8155e-02, 1.0529e-01,  ..., 9.3900e-02, 2.0398e-02,
         4.5645e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678674.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9314.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.2049, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2445.9214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1133.8854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2200],
        [-0.4529],
        [-1.5873],
        ...,
        [-5.4396],
        [-5.4314],
        [-5.4318]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-360829.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0338],
        [1.0431],
        ...,
        [0.9953],
        [0.9947],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370526.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0338],
        [1.0431],
        ...,
        [0.9953],
        [0.9946],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370529.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2963.3120, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6856, device='cuda:0')



h[100].sum tensor(-0.0228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8762, device='cuda:0')



h[200].sum tensor(-22.5851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0023, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0091, 0.0023, 0.0000],
        [0.0000, 0.0394, 0.0111,  ..., 0.0148, 0.0382, 0.0241],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69998.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0686, 0.0162, 0.0913,  ..., 0.1021, 0.0277, 0.0516],
        [0.0631, 0.0128, 0.0606,  ..., 0.1349, 0.0501, 0.0736],
        [0.0463, 0.0057, 0.0250,  ..., 0.2350, 0.1214, 0.1405],
        ...,
        [0.0730, 0.0185, 0.1050,  ..., 0.0940, 0.0204, 0.0458],
        [0.0729, 0.0185, 0.1049,  ..., 0.0939, 0.0204, 0.0458],
        [0.0729, 0.0185, 0.1049,  ..., 0.0939, 0.0204, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710638.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9084.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(130.1365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2334.6394, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1200.4062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5263],
        [-1.2256],
        [-0.2457],
        ...,
        [-5.4404],
        [-5.4312],
        [-5.4295]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329349.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0338],
        [1.0431],
        ...,
        [0.9953],
        [0.9946],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370529.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0339],
        [1.0431],
        ...,
        [0.9953],
        [0.9946],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370532.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0093,  0.0023,  ...,  0.0036,  0.0090,  0.0043],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0022,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2603.3564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3589, device='cuda:0')



h[100].sum tensor(-0.0175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2441, device='cuda:0')



h[200].sum tensor(-22.6206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0318, 0.0078,  ..., 0.0137, 0.0307, 0.0152],
        [0.0000, 0.0255, 0.0063,  ..., 0.0127, 0.0245, 0.0128],
        [0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65178.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0060, 0.0149,  ..., 0.1925, 0.0801, 0.1145],
        [0.0548, 0.0088, 0.0332,  ..., 0.1704, 0.0669, 0.0991],
        [0.0641, 0.0144, 0.0718,  ..., 0.1249, 0.0386, 0.0679],
        ...,
        [0.0727, 0.0191, 0.1048,  ..., 0.0940, 0.0204, 0.0458],
        [0.0726, 0.0190, 0.1047,  ..., 0.0939, 0.0204, 0.0458],
        [0.0726, 0.0190, 0.1047,  ..., 0.0939, 0.0204, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697868.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9160.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.7884, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2475.8518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1154.1515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2922],
        [-0.7474],
        [-1.5045],
        ...,
        [-5.4389],
        [-5.4299],
        [-5.4280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358233.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0339],
        [1.0431],
        ...,
        [0.9953],
        [0.9946],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370532.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0339],
        [1.0431],
        ...,
        [0.9953],
        [0.9946],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370535.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3062.4863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3068, device='cuda:0')



h[100].sum tensor(-0.0228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9670, device='cuda:0')



h[200].sum tensor(-22.5813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0288, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0139, 0.0024,  ..., 0.0110, 0.0129, 0.0032],
        [0.0000, 0.0203, 0.0046,  ..., 0.0121, 0.0193, 0.0085],
        [0.0000, 0.0268, 0.0068,  ..., 0.0131, 0.0257, 0.0138],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0023, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70943.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0559, 0.0109, 0.0374,  ..., 0.1597, 0.0577, 0.0919],
        [0.0556, 0.0105, 0.0319,  ..., 0.1660, 0.0633, 0.0958],
        [0.0550, 0.0100, 0.0248,  ..., 0.1742, 0.0700, 0.1010],
        ...,
        [0.0724, 0.0198, 0.1044,  ..., 0.0939, 0.0202, 0.0458],
        [0.0723, 0.0197, 0.1043,  ..., 0.0938, 0.0202, 0.0458],
        [0.0723, 0.0197, 0.1043,  ..., 0.0938, 0.0202, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(719684.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9015.3896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(126.2689, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2408.1750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1205.2124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0431],
        [-0.1709],
        [-0.4784],
        ...,
        [-5.4343],
        [-5.4255],
        [-5.4235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337567.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0339],
        [1.0431],
        ...,
        [0.9953],
        [0.9946],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370535.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0339],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370539.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0107,  0.0028,  ...,  0.0039,  0.0104,  0.0055],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [-0.0013,  0.0314,  0.0100,  ...,  0.0071,  0.0310,  0.0224],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2780.6626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6058, device='cuda:0')



h[100].sum tensor(-0.0185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4262, device='cuda:0')



h[200].sum tensor(-22.6098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0116, 0.0023,  ..., 0.0108, 0.0106, 0.0041],
        [0.0000, 0.0451, 0.0132,  ..., 0.0161, 0.0439, 0.0287],
        [0.0000, 0.0293, 0.0084,  ..., 0.0136, 0.0282, 0.0185],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0097, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0097, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0097, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65333.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0540, 0.0108, 0.0235,  ..., 0.1726, 0.0703, 0.1004],
        [0.0490, 0.0084, 0.0068,  ..., 0.2104, 0.1024, 0.1244],
        [0.0506, 0.0094, 0.0095,  ..., 0.2083, 0.1040, 0.1223],
        ...,
        [0.0720, 0.0202, 0.1037,  ..., 0.0939, 0.0202, 0.0462],
        [0.0719, 0.0202, 0.1036,  ..., 0.0938, 0.0201, 0.0462],
        [0.0719, 0.0202, 0.1036,  ..., 0.0937, 0.0201, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689107.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9005.3574, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.2394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2421.9331, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1160.0510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 8.8208e-02],
        [ 1.9019e-01],
        [-2.6054e-03],
        ...,
        [-5.4128e+00],
        [-5.4041e+00],
        [-5.4022e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325227.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0339],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370539.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0324],
        [1.0339],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370542.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0103,  0.0027,  ...,  0.0038,  0.0100,  0.0051],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2804.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6627, device='cuda:0')



h[100].sum tensor(-0.0183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4346, device='cuda:0')



h[200].sum tensor(-22.6097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0236, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0241, 0.0059,  ..., 0.0128, 0.0231, 0.0115],
        [0.0000, 0.0131, 0.0028,  ..., 0.0111, 0.0121, 0.0053],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0024, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0024, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66513.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0086, 0.0272,  ..., 0.2071, 0.0969, 0.1229],
        [0.0594, 0.0141, 0.0517,  ..., 0.1446, 0.0531, 0.0815],
        [0.0664, 0.0177, 0.0850,  ..., 0.1087, 0.0290, 0.0575],
        ...,
        [0.0717, 0.0203, 0.1029,  ..., 0.0939, 0.0202, 0.0466],
        [0.0716, 0.0203, 0.1028,  ..., 0.0939, 0.0202, 0.0466],
        [0.0716, 0.0203, 0.1028,  ..., 0.0938, 0.0202, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695872.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8886.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2322.8267, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1173.1727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2771],
        [-1.1448],
        [-2.1889],
        ...,
        [-5.3938],
        [-5.3853],
        [-5.3833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301936.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0324],
        [1.0339],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370542.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0324],
        [1.0340],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370544.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0096,  0.0024,  ...,  0.0037,  0.0093,  0.0045],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2677.1289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1163, device='cuda:0')



h[100].sum tensor(-0.0164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2086, device='cuda:0')



h[200].sum tensor(-22.6217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0253, 0.0063,  ..., 0.0130, 0.0243, 0.0124],
        [0.0000, 0.0124, 0.0025,  ..., 0.0110, 0.0114, 0.0046],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64745.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0099, 0.0257,  ..., 0.1833, 0.0773, 0.1081],
        [0.0588, 0.0137, 0.0475,  ..., 0.1467, 0.0542, 0.0833],
        [0.0635, 0.0162, 0.0687,  ..., 0.1253, 0.0410, 0.0687],
        ...,
        [0.0716, 0.0201, 0.1025,  ..., 0.0939, 0.0203, 0.0470],
        [0.0716, 0.0200, 0.1024,  ..., 0.0938, 0.0202, 0.0469],
        [0.0716, 0.0200, 0.1024,  ..., 0.0938, 0.0202, 0.0469]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689013.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8843.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(124.0348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2236.3530, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1161.8729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0734],
        [-0.4195],
        [-0.6374],
        ...,
        [-5.3920],
        [-5.3844],
        [-5.3832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285109.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0324],
        [1.0340],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370544.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 350.0 event: 1750 loss: tensor(485.9052, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0325],
        [1.0340],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370546.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0362,  0.0116,  ...,  0.0079,  0.0357,  0.0263],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [-0.0009,  0.0228,  0.0070,  ...,  0.0058,  0.0224,  0.0153],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3291.7434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4016, device='cuda:0')



h[100].sum tensor(-0.0239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2731, device='cuda:0')



h[200].sum tensor(-22.5649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1031, 0.0327,  ..., 0.0253, 0.1016, 0.0734],
        [0.0000, 0.1218, 0.0385,  ..., 0.0282, 0.1202, 0.0859],
        [0.0000, 0.0309, 0.0083,  ..., 0.0139, 0.0299, 0.0170],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0098, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73373.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.4693e-03, 0.0000e+00, 0.0000e+00,  ..., 5.0267e-01, 3.3361e-01,
         3.1461e-01],
        [1.4386e-02, 3.1099e-04, 0.0000e+00,  ..., 4.2310e-01, 2.6730e-01,
         2.6356e-01],
        [3.5952e-02, 3.5808e-03, 0.0000e+00,  ..., 2.8548e-01, 1.5565e-01,
         1.7475e-01],
        ...,
        [7.1829e-02, 2.0092e-02, 1.0264e-01,  ..., 9.3806e-02, 2.0232e-02,
         4.6821e-02],
        [7.1762e-02, 2.0067e-02, 1.0256e-01,  ..., 9.3713e-02, 2.0214e-02,
         4.6770e-02],
        [7.1747e-02, 2.0059e-02, 1.0254e-01,  ..., 9.3690e-02, 2.0209e-02,
         4.6755e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727413.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8816.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(127.2445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2262.6106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1230.0516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1595],
        [ 0.2125],
        [ 0.2688],
        ...,
        [-5.4286],
        [-5.4199],
        [-5.4176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310175.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0325],
        [1.0340],
        [1.0431],
        ...,
        [0.9952],
        [0.9945],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370546.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0325],
        [1.0341],
        [1.0431],
        ...,
        [0.9951],
        [0.9944],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370548.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [-0.0008,  0.0211,  0.0064,  ...,  0.0055,  0.0208,  0.0139],
        [-0.0008,  0.0210,  0.0064,  ...,  0.0055,  0.0207,  0.0139],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2930.7319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4542, device='cuda:0')



h[100].sum tensor(-0.0193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6963, device='cuda:0')



h[200].sum tensor(-22.5966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0241, 0.0066,  ..., 0.0127, 0.0231, 0.0143],
        [0.0000, 0.0414, 0.0119,  ..., 0.0155, 0.0403, 0.0256],
        [0.0000, 0.1212, 0.0383,  ..., 0.0281, 0.1196, 0.0853],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0097, 0.0025, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0097, 0.0025, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0097, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66994.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0563, 0.0124, 0.0413,  ..., 0.1681, 0.0762, 0.0964],
        [0.0451, 0.0081, 0.0174,  ..., 0.2362, 0.1268, 0.1413],
        [0.0264, 0.0020, 0.0000,  ..., 0.3526, 0.2163, 0.2175],
        ...,
        [0.0721, 0.0198, 0.1026,  ..., 0.0938, 0.0203, 0.0468],
        [0.0720, 0.0198, 0.1025,  ..., 0.0937, 0.0203, 0.0468],
        [0.0720, 0.0198, 0.1025,  ..., 0.0936, 0.0203, 0.0468]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699142.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8934.7256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(129.6712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2182.9006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1178.0519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2962],
        [-0.5358],
        [ 0.0957],
        ...,
        [-5.4538],
        [-5.4450],
        [-5.4424]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300642.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0325],
        [1.0341],
        [1.0431],
        ...,
        [0.9951],
        [0.9944],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370548.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0326],
        [1.0341],
        [1.0431],
        ...,
        [0.9951],
        [0.9944],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370550.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0138,  0.0039,  ...,  0.0043,  0.0136,  0.0080],
        [-0.0018,  0.0439,  0.0143,  ...,  0.0091,  0.0435,  0.0327],
        [-0.0008,  0.0209,  0.0063,  ...,  0.0055,  0.0206,  0.0138],
        ...,
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2562.0364, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4492, device='cuda:0')



h[100].sum tensor(-0.0150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1111, device='cuda:0')



h[200].sum tensor(-22.6281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1356, 0.0433,  ..., 0.0303, 0.1340, 0.0973],
        [0.0000, 0.1021, 0.0317,  ..., 0.0250, 0.1008, 0.0699],
        [0.0000, 0.1043, 0.0324,  ..., 0.0254, 0.1029, 0.0716],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63146.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.0923e-02, 5.6423e-04, 0.0000e+00,  ..., 3.7645e-01, 2.3179e-01,
         2.3425e-01],
        [1.8444e-02, 0.0000e+00, 0.0000e+00,  ..., 3.9087e-01, 2.3977e-01,
         2.4425e-01],
        [1.9064e-02, 2.0039e-04, 0.0000e+00,  ..., 3.9026e-01, 2.3866e-01,
         2.4390e-01],
        ...,
        [7.2539e-02, 1.9652e-02, 1.0303e-01,  ..., 9.3683e-02, 2.0334e-02,
         4.6518e-02],
        [7.2471e-02, 1.9628e-02, 1.0294e-01,  ..., 9.3590e-02, 2.0316e-02,
         4.6468e-02],
        [7.2452e-02, 1.9618e-02, 1.0292e-01,  ..., 9.3563e-02, 2.0311e-02,
         4.6450e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(688701.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9159.5361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.8218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2297.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1139.5841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2550],
        [ 0.2486],
        [ 0.2315],
        ...,
        [-5.4991],
        [-5.4901],
        [-5.4876]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342169.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0326],
        [1.0341],
        [1.0431],
        ...,
        [0.9951],
        [0.9944],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370550.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0326],
        [1.0341],
        [1.0430],
        ...,
        [0.9951],
        [0.9944],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370552.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3441.1506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7409, device='cuda:0')



h[100].sum tensor(-0.0253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6149, device='cuda:0')



h[200].sum tensor(-22.5467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0026,  ..., 0.0108, 0.0118, 0.0050],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0399, 0.0114,  ..., 0.0152, 0.0390, 0.0246],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76525.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0547, 0.0106, 0.0242,  ..., 0.1723, 0.0724, 0.1008],
        [0.0526, 0.0099, 0.0229,  ..., 0.1915, 0.0906, 0.1125],
        [0.0334, 0.0051, 0.0112,  ..., 0.3113, 0.1838, 0.1910],
        ...,
        [0.0729, 0.0196, 0.1035,  ..., 0.0935, 0.0203, 0.0462],
        [0.0728, 0.0196, 0.1034,  ..., 0.0935, 0.0203, 0.0462],
        [0.0728, 0.0195, 0.1033,  ..., 0.0934, 0.0203, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(754777.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9010.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.9854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.8747, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2212.6514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1252.9324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2785],
        [ 0.2147],
        [ 0.1487],
        ...,
        [-5.5396],
        [-5.5304],
        [-5.5276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346786.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0326],
        [1.0341],
        [1.0430],
        ...,
        [0.9951],
        [0.9944],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370552.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0326],
        [1.0341],
        [1.0430],
        ...,
        [0.9951],
        [0.9944],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370552.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0161,  0.0047,  ...,  0.0047,  0.0158,  0.0099],
        [-0.0003,  0.0082,  0.0019,  ...,  0.0034,  0.0080,  0.0034],
        [-0.0010,  0.0250,  0.0078,  ...,  0.0061,  0.0247,  0.0172],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2998.6821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7138, device='cuda:0')



h[100].sum tensor(-0.0201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.8804, device='cuda:0')



h[200].sum tensor(-22.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0548, 0.0153,  ..., 0.0175, 0.0538, 0.0313],
        [0.0000, 0.0699, 0.0205,  ..., 0.0199, 0.0688, 0.0437],
        [0.0000, 0.0517, 0.0142,  ..., 0.0171, 0.0507, 0.0287],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0096, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68463.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0331, 0.0014, 0.0000,  ..., 0.2869, 0.1486, 0.1791],
        [0.0318, 0.0012, 0.0000,  ..., 0.2984, 0.1579, 0.1863],
        [0.0332, 0.0005, 0.0000,  ..., 0.2918, 0.1513, 0.1822],
        ...,
        [0.0729, 0.0196, 0.1035,  ..., 0.0935, 0.0203, 0.0462],
        [0.0728, 0.0196, 0.1034,  ..., 0.0935, 0.0203, 0.0462],
        [0.0728, 0.0195, 0.1033,  ..., 0.0934, 0.0203, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706419.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9188.0830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.3763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2261.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1184.2833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3714],
        [ 0.3251],
        [ 0.2670],
        ...,
        [-5.5382],
        [-5.5302],
        [-5.5277]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-353584.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0326],
        [1.0341],
        [1.0430],
        ...,
        [0.9951],
        [0.9944],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370552.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0341],
        [1.0430],
        ...,
        [0.9950],
        [0.9943],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370554.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0224,  0.0069,  ...,  0.0057,  0.0221,  0.0151],
        [-0.0012,  0.0316,  0.0100,  ...,  0.0071,  0.0312,  0.0226],
        [-0.0011,  0.0278,  0.0087,  ...,  0.0065,  0.0275,  0.0195],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3124.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3276, device='cuda:0')



h[100].sum tensor(-0.0215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1161, device='cuda:0')



h[200].sum tensor(-22.5742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0913, 0.0279,  ..., 0.0232, 0.0901, 0.0613],
        [0.0000, 0.1098, 0.0343,  ..., 0.0262, 0.1085, 0.0764],
        [0.0000, 0.1149, 0.0360,  ..., 0.0270, 0.1136, 0.0805],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0025, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0025, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69343.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0261, 0.0006, 0.0000,  ..., 0.3385, 0.1962, 0.2120],
        [0.0182, 0.0000, 0.0000,  ..., 0.3885, 0.2348, 0.2448],
        [0.0162, 0.0000, 0.0000,  ..., 0.4052, 0.2483, 0.2557],
        ...,
        [0.0722, 0.0188, 0.0994,  ..., 0.0980, 0.0228, 0.0495],
        [0.0717, 0.0185, 0.0972,  ..., 0.1001, 0.0241, 0.0511],
        [0.0722, 0.0187, 0.0993,  ..., 0.0978, 0.0228, 0.0495]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708871.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9240.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.1174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2283.8499, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1190.4987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2880],
        [ 0.2877],
        [ 0.2699],
        ...,
        [-4.9409],
        [-4.8283],
        [-4.8248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-367658.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0341],
        [1.0430],
        ...,
        [0.9950],
        [0.9943],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370554.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0341],
        [1.0430],
        ...,
        [0.9950],
        [0.9943],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370556.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0018,  0.0451,  0.0147,  ...,  0.0092,  0.0447,  0.0337],
        [-0.0007,  0.0179,  0.0053,  ...,  0.0049,  0.0177,  0.0114],
        [-0.0010,  0.0245,  0.0076,  ...,  0.0060,  0.0242,  0.0168],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2848.6848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1256, device='cuda:0')



h[100].sum tensor(-0.0182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6483, device='cuda:0')



h[200].sum tensor(-22.5982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0909, 0.0277,  ..., 0.0231, 0.0898, 0.0610],
        [0.0000, 0.1111, 0.0347,  ..., 0.0263, 0.1098, 0.0775],
        [0.0000, 0.1015, 0.0313,  ..., 0.0248, 0.1003, 0.0696],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0025, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0025, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66828.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0193, 0.0000, 0.0000,  ..., 0.3826, 0.2400, 0.2412],
        [0.0197, 0.0000, 0.0000,  ..., 0.3801, 0.2359, 0.2400],
        [0.0230, 0.0000, 0.0000,  ..., 0.3590, 0.2142, 0.2273],
        ...,
        [0.0734, 0.0186, 0.1034,  ..., 0.0934, 0.0205, 0.0469],
        [0.0734, 0.0185, 0.1033,  ..., 0.0933, 0.0204, 0.0469],
        [0.0733, 0.0185, 0.1033,  ..., 0.0932, 0.0204, 0.0468]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701563.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9319.8506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.9206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2212.0659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1170.0839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3521],
        [ 0.3672],
        [ 0.3824],
        ...,
        [-5.5646],
        [-5.5552],
        [-5.5527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-356128.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0341],
        [1.0430],
        ...,
        [0.9950],
        [0.9943],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370556.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9950],
        [0.9943],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370559.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0250,  0.0077,  ...,  0.0061,  0.0247,  0.0172],
        [-0.0008,  0.0207,  0.0062,  ...,  0.0054,  0.0205,  0.0137],
        [-0.0010,  0.0247,  0.0076,  ...,  0.0060,  0.0244,  0.0170],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2624.9214, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7790, device='cuda:0')



h[100].sum tensor(-0.0155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3055, device='cuda:0')



h[200].sum tensor(-22.6176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0773, 0.0229,  ..., 0.0209, 0.0763, 0.0499],
        [0.0000, 0.0998, 0.0307,  ..., 0.0245, 0.0987, 0.0683],
        [0.0000, 0.1120, 0.0349,  ..., 0.0265, 0.1108, 0.0782],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0025, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0025, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63204.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0120, 0.0000, 0.0000,  ..., 0.4147, 0.2578, 0.2656],
        [0.0050, 0.0000, 0.0000,  ..., 0.4750, 0.3071, 0.3053],
        [0.0069, 0.0000, 0.0000,  ..., 0.4879, 0.3193, 0.3134],
        ...,
        [0.0736, 0.0182, 0.1035,  ..., 0.0932, 0.0204, 0.0471],
        [0.0736, 0.0182, 0.1034,  ..., 0.0932, 0.0204, 0.0470],
        [0.0735, 0.0182, 0.1034,  ..., 0.0931, 0.0204, 0.0470]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684586., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9380.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.6017, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2187.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1141.8236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0919],
        [ 0.0714],
        [ 0.0680],
        ...,
        [-5.5852],
        [-5.5757],
        [-5.5728]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349708.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9950],
        [0.9943],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370559.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9949],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370561.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0096,  0.0024,  ...,  0.0036,  0.0094,  0.0046],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3715.1416, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6452, device='cuda:0')



h[100].sum tensor(-0.0276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.1853, device='cuda:0')



h[200].sum tensor(-22.5180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0232, 0.0056,  ..., 0.0124, 0.0225, 0.0111],
        [0.0000, 0.0121, 0.0025,  ..., 0.0106, 0.0115, 0.0048],
        [0.0000, 0.0030, 0.0000,  ..., 0.0092, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0025, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0094, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84355.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0538, 0.0081, 0.0302,  ..., 0.1730, 0.0705, 0.1042],
        [0.0619, 0.0123, 0.0593,  ..., 0.1351, 0.0462, 0.0774],
        [0.0681, 0.0155, 0.0855,  ..., 0.1085, 0.0296, 0.0584],
        ...,
        [0.0739, 0.0183, 0.1040,  ..., 0.0931, 0.0203, 0.0467],
        [0.0730, 0.0178, 0.0992,  ..., 0.0979, 0.0239, 0.0499],
        [0.0699, 0.0163, 0.0833,  ..., 0.1142, 0.0358, 0.0610]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(802917., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9129.6416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.7340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(148.8755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2092.5088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1320.1630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6019],
        [-1.6406],
        [-2.7008],
        ...,
        [-5.3993],
        [-5.0079],
        [-4.3014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347561.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9949],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370561.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9949],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370563.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2635.8042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0045, device='cuda:0')



h[100].sum tensor(-0.0154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3384, device='cuda:0')



h[200].sum tensor(-22.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0092, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0094, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63460.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0716, 0.0175, 0.1020,  ..., 0.0896, 0.0194, 0.0441],
        [0.0719, 0.0176, 0.1023,  ..., 0.0899, 0.0195, 0.0443],
        [0.0723, 0.0179, 0.1026,  ..., 0.0906, 0.0196, 0.0448],
        ...,
        [0.0741, 0.0186, 0.1046,  ..., 0.0930, 0.0200, 0.0462],
        [0.0740, 0.0186, 0.1045,  ..., 0.0929, 0.0200, 0.0461],
        [0.0740, 0.0185, 0.1045,  ..., 0.0929, 0.0200, 0.0461]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689791.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9485.7510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.7028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2288.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1141.5338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5086],
        [-5.5602],
        [-5.5818],
        ...,
        [-5.6380],
        [-5.6287],
        [-5.6259]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-385446.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9949],
        [0.9942],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370563.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 360.0 event: 1800 loss: tensor(422.6984, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9948],
        [0.9942],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370565.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [-0.0003,  0.0077,  0.0017,  ...,  0.0033,  0.0075,  0.0031],
        [-0.0003,  0.0077,  0.0017,  ...,  0.0033,  0.0075,  0.0031],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0022,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2624.9509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8469, device='cuda:0')



h[100].sum tensor(-0.0149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3154, device='cuda:0')



h[200].sum tensor(-22.6173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0159, 0.0031,  ..., 0.0113, 0.0152, 0.0052],
        [0.0000, 0.0160, 0.0031,  ..., 0.0113, 0.0152, 0.0053],
        [0.0000, 0.0160, 0.0031,  ..., 0.0114, 0.0153, 0.0053],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0095, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0095, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0095, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63924.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0599, 0.0115, 0.0532,  ..., 0.1423, 0.0487, 0.0826],
        [0.0612, 0.0122, 0.0605,  ..., 0.1357, 0.0424, 0.0784],
        [0.0620, 0.0127, 0.0629,  ..., 0.1342, 0.0411, 0.0774],
        ...,
        [0.0741, 0.0187, 0.1049,  ..., 0.0929, 0.0199, 0.0462],
        [0.0740, 0.0187, 0.1048,  ..., 0.0928, 0.0198, 0.0462],
        [0.0740, 0.0187, 0.1047,  ..., 0.0927, 0.0198, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697045., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9508.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(150.4094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2397.0669, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1143.5065, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0425],
        [-2.8823],
        [-3.5979],
        ...,
        [-5.6475],
        [-5.6373],
        [-5.6338]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-409593.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0340],
        [1.0430],
        ...,
        [0.9948],
        [0.9942],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370565.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0340],
        [1.0430],
        ...,
        [0.9948],
        [0.9941],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370568.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3059.8191, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3705, device='cuda:0')



h[100].sum tensor(-0.0192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9763, device='cuda:0')



h[200].sum tensor(-22.5800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0095, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71552.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0620, 0.0125, 0.0546,  ..., 0.1375, 0.0531, 0.0785],
        [0.0632, 0.0131, 0.0592,  ..., 0.1334, 0.0502, 0.0755],
        [0.0653, 0.0142, 0.0671,  ..., 0.1262, 0.0453, 0.0704],
        ...,
        [0.0737, 0.0182, 0.1036,  ..., 0.0930, 0.0197, 0.0473],
        [0.0736, 0.0182, 0.1035,  ..., 0.0929, 0.0197, 0.0473],
        [0.0736, 0.0182, 0.1035,  ..., 0.0929, 0.0197, 0.0473]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731035.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9230.4229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2229.4390, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1213.1721, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2587],
        [-0.3450],
        [-0.5287],
        ...,
        [-4.9761],
        [-4.9271],
        [-4.7345]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-359925.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0340],
        [1.0430],
        ...,
        [0.9948],
        [0.9941],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370568.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0340],
        [1.0430],
        ...,
        [0.9947],
        [0.9940],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370571.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0074,  0.0016,  ...,  0.0033,  0.0072,  0.0028],
        [-0.0004,  0.0106,  0.0027,  ...,  0.0038,  0.0104,  0.0054],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3414.9995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9517, device='cuda:0')



h[100].sum tensor(-0.0224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4996, device='cuda:0')



h[200].sum tensor(-22.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0592, 0.0164,  ..., 0.0182, 0.0582, 0.0348],
        [0.0000, 0.0239, 0.0057,  ..., 0.0126, 0.0231, 0.0115],
        [0.0000, 0.0134, 0.0028,  ..., 0.0110, 0.0126, 0.0056],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0026, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0026, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77214.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0396, 0.0010, 0.0015,  ..., 0.2410, 0.1205, 0.1539],
        [0.0506, 0.0061, 0.0262,  ..., 0.1878, 0.0839, 0.1164],
        [0.0613, 0.0115, 0.0537,  ..., 0.1394, 0.0513, 0.0819],
        ...,
        [0.0721, 0.0170, 0.0981,  ..., 0.0976, 0.0222, 0.0520],
        [0.0716, 0.0167, 0.0959,  ..., 0.0997, 0.0235, 0.0536],
        [0.0721, 0.0170, 0.0979,  ..., 0.0975, 0.0222, 0.0519]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(751210., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8979.9814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.0494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(149.0041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2073.4990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1270.9971, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2107],
        [-0.3541],
        [-1.3511],
        ...,
        [-4.8957],
        [-4.7995],
        [-4.8828]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313785.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0340],
        [1.0430],
        ...,
        [0.9947],
        [0.9940],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370571.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0340],
        [1.0430],
        ...,
        [0.9946],
        [0.9939],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370574.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [-0.0005,  0.0130,  0.0035,  ...,  0.0042,  0.0127,  0.0073],
        [-0.0008,  0.0222,  0.0067,  ...,  0.0057,  0.0219,  0.0148],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2884.0374, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9499, device='cuda:0')



h[100].sum tensor(-0.0164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6226, device='cuda:0')



h[200].sum tensor(-22.6000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0260, 0.0064,  ..., 0.0130, 0.0251, 0.0130],
        [0.0000, 0.0434, 0.0116,  ..., 0.0158, 0.0424, 0.0245],
        [0.0000, 0.0730, 0.0211,  ..., 0.0205, 0.0718, 0.0458],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0097, 0.0027, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0096, 0.0026, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0096, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67136.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0509, 0.0074, 0.0326,  ..., 0.1840, 0.0841, 0.1144],
        [0.0367, 0.0028, 0.0104,  ..., 0.2568, 0.1364, 0.1654],
        [0.0224, 0.0000, 0.0000,  ..., 0.3332, 0.1929, 0.2185],
        ...,
        [0.0728, 0.0175, 0.1013,  ..., 0.0931, 0.0195, 0.0497],
        [0.0727, 0.0175, 0.1012,  ..., 0.0931, 0.0195, 0.0496],
        [0.0727, 0.0175, 0.1012,  ..., 0.0930, 0.0195, 0.0496]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697731.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9215.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0823, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(146.2534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2257.6799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1183.7524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9051],
        [-0.0672],
        [ 0.3531],
        ...,
        [-5.4845],
        [-5.4757],
        [-5.4729]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328746.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0340],
        [1.0430],
        ...,
        [0.9946],
        [0.9939],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370574.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0340],
        [1.0431],
        ...,
        [0.9945],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370576.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0182,  0.0053,  ...,  0.0051,  0.0179,  0.0115],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [-0.0005,  0.0144,  0.0040,  ...,  0.0044,  0.0141,  0.0084],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2487.3784, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.0970, device='cuda:0')



h[100].sum tensor(-0.0119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9136, device='cuda:0')



h[200].sum tensor(-22.6375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0186, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0317, 0.0076,  ..., 0.0140, 0.0307, 0.0148],
        [0.0000, 0.0649, 0.0183,  ..., 0.0192, 0.0636, 0.0392],
        [0.0000, 0.0902, 0.0278,  ..., 0.0233, 0.0888, 0.0626],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0098, 0.0026, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0098, 0.0026, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0098, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62078.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0318, 0.0000, 0.0000,  ..., 0.2720, 0.1425, 0.1782],
        [0.0155, 0.0000, 0.0000,  ..., 0.3670, 0.2204, 0.2419],
        [0.0077, 0.0000, 0.0000,  ..., 0.4756, 0.3142, 0.3133],
        ...,
        [0.0725, 0.0180, 0.1010,  ..., 0.0930, 0.0192, 0.0502],
        [0.0724, 0.0180, 0.1009,  ..., 0.0929, 0.0191, 0.0502],
        [0.0724, 0.0179, 0.1008,  ..., 0.0929, 0.0191, 0.0501]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677691.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9192.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.8169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2282.5591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1144.4701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2914],
        [ 0.2315],
        [ 0.1631],
        ...,
        [-5.4596],
        [-5.4511],
        [-5.4484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314876.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0340],
        [1.0431],
        ...,
        [0.9945],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370576.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0340],
        [1.0431],
        ...,
        [0.9945],
        [0.9938],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370579.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [-0.0005,  0.0133,  0.0036,  ...,  0.0043,  0.0130,  0.0076],
        [-0.0003,  0.0093,  0.0022,  ...,  0.0037,  0.0090,  0.0042],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2685.1687, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2775, device='cuda:0')



h[100].sum tensor(-0.0136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2322, device='cuda:0')



h[200].sum tensor(-22.6213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0163, 0.0037,  ..., 0.0118, 0.0152, 0.0078],
        [0.0000, 0.0228, 0.0052,  ..., 0.0128, 0.0217, 0.0103],
        [0.0000, 0.0796, 0.0234,  ..., 0.0218, 0.0781, 0.0511],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0100, 0.0025, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0100, 0.0025, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0100, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63094.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0567, 0.0113, 0.0408,  ..., 0.1535, 0.0607, 0.0934],
        [0.0465, 0.0063, 0.0211,  ..., 0.2059, 0.0965, 0.1304],
        [0.0301, 0.0011, 0.0000,  ..., 0.2961, 0.1628, 0.1925],
        ...,
        [0.0722, 0.0193, 0.1014,  ..., 0.0929, 0.0187, 0.0503],
        [0.0721, 0.0192, 0.1013,  ..., 0.0928, 0.0187, 0.0502],
        [0.0721, 0.0192, 0.1012,  ..., 0.0927, 0.0187, 0.0502]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678004.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9110.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.4608, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2365.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1156.5135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0682],
        [ 0.1411],
        [ 0.2603],
        ...,
        [-5.4521],
        [-5.4433],
        [-5.4327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305435.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0340],
        [1.0431],
        ...,
        [0.9945],
        [0.9938],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370579.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0331],
        [1.0340],
        [1.0431],
        ...,
        [0.9944],
        [0.9937],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370581.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2666.8142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7506, device='cuda:0')



h[100].sum tensor(-0.0131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1552, device='cuda:0')



h[200].sum tensor(-22.6238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0209, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0101, 0.0024, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0101, 0.0024, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0101, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66054.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0698, 0.0192, 0.0995,  ..., 0.0894, 0.0178, 0.0478],
        [0.0695, 0.0191, 0.0974,  ..., 0.0922, 0.0195, 0.0498],
        [0.0668, 0.0178, 0.0812,  ..., 0.1098, 0.0317, 0.0620],
        ...,
        [0.0721, 0.0204, 0.1020,  ..., 0.0928, 0.0184, 0.0501],
        [0.0720, 0.0204, 0.1019,  ..., 0.0927, 0.0184, 0.0500],
        [0.0720, 0.0204, 0.1019,  ..., 0.0927, 0.0183, 0.0500]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701081.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8888.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9936, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.2775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2326.9458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1187.0371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3755],
        [-2.8197],
        [-1.8931],
        ...,
        [-5.4597],
        [-5.4525],
        [-5.4505]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291248.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0331],
        [1.0340],
        [1.0431],
        ...,
        [0.9944],
        [0.9937],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370581.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0331],
        [1.0340],
        [1.0431],
        ...,
        [0.9944],
        [0.9937],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370581.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2906.8838, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8817, device='cuda:0')



h[100].sum tensor(-0.0155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6127, device='cuda:0')



h[200].sum tensor(-22.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0253, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0101, 0.0024, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0101, 0.0024, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0101, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68392.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0698, 0.0192, 0.0995,  ..., 0.0894, 0.0178, 0.0478],
        [0.0700, 0.0194, 0.0998,  ..., 0.0898, 0.0179, 0.0481],
        [0.0704, 0.0196, 0.1001,  ..., 0.0904, 0.0180, 0.0486],
        ...,
        [0.0721, 0.0204, 0.1020,  ..., 0.0928, 0.0184, 0.0501],
        [0.0720, 0.0204, 0.1019,  ..., 0.0927, 0.0184, 0.0500],
        [0.0720, 0.0204, 0.1019,  ..., 0.0927, 0.0183, 0.0500]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(707583.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8917.8975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.1160, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2329.4512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1204.8698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0082],
        [-4.9347],
        [-4.7868],
        ...,
        [-5.4622],
        [-5.4535],
        [-5.4506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293075.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0331],
        [1.0340],
        [1.0431],
        ...,
        [0.9944],
        [0.9937],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370581.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0332],
        [1.0340],
        [1.0431],
        ...,
        [0.9943],
        [0.9937],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370582.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2705.8218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3220, device='cuda:0')



h[100].sum tensor(-0.0133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2387, device='cuda:0')



h[200].sum tensor(-22.6201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0102, 0.0023, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0102, 0.0023, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0102, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65175.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0698, 0.0200, 0.1001,  ..., 0.0894, 0.0177, 0.0476],
        [0.0700, 0.0201, 0.1003,  ..., 0.0898, 0.0177, 0.0478],
        [0.0705, 0.0204, 0.1007,  ..., 0.0905, 0.0178, 0.0483],
        ...,
        [0.0721, 0.0212, 0.1026,  ..., 0.0928, 0.0182, 0.0498],
        [0.0720, 0.0211, 0.1025,  ..., 0.0927, 0.0182, 0.0497],
        [0.0720, 0.0211, 0.1025,  ..., 0.0927, 0.0182, 0.0497]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692000., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8906.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.0109, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2337.3352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1180.8231, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7021],
        [-4.9295],
        [-5.1263],
        ...,
        [-5.4707],
        [-5.4613],
        [-5.4577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286900.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0332],
        [1.0340],
        [1.0431],
        ...,
        [0.9943],
        [0.9937],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370582.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0340],
        [1.0431],
        ...,
        [0.9943],
        [0.9936],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370583.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2780.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4093, device='cuda:0')



h[100].sum tensor(-0.0141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3975, device='cuda:0')



h[200].sum tensor(-22.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0022, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0102, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0102, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0102, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67255.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0700, 0.0199, 0.1002,  ..., 0.0897, 0.0178, 0.0472],
        [0.0702, 0.0200, 0.1005,  ..., 0.0900, 0.0178, 0.0474],
        [0.0707, 0.0203, 0.1008,  ..., 0.0907, 0.0179, 0.0479],
        ...,
        [0.0723, 0.0211, 0.1027,  ..., 0.0931, 0.0183, 0.0494],
        [0.0723, 0.0211, 0.1027,  ..., 0.0930, 0.0183, 0.0494],
        [0.0722, 0.0210, 0.1026,  ..., 0.0929, 0.0183, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(708011.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8969.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1194, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.9460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2431.1736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1195.5107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7969],
        [-5.0706],
        [-5.2366],
        ...,
        [-5.5010],
        [-5.4923],
        [-5.4895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304991.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0340],
        [1.0431],
        ...,
        [0.9943],
        [0.9936],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370583.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 370.0 event: 1850 loss: tensor(403.0221, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0340],
        [1.0431],
        ...,
        [0.9942],
        [0.9936],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370584.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [-0.0004,  0.0101,  0.0025,  ...,  0.0038,  0.0098,  0.0049],
        [-0.0003,  0.0077,  0.0017,  ...,  0.0035,  0.0074,  0.0030],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2494.9771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3909, device='cuda:0')



h[100].sum tensor(-0.0115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9565, device='cuda:0')



h[200].sum tensor(-22.6344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0026,  ..., 0.0113, 0.0118, 0.0051],
        [0.0000, 0.0243, 0.0058,  ..., 0.0131, 0.0231, 0.0116],
        [0.0000, 0.0754, 0.0220,  ..., 0.0212, 0.0739, 0.0478],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0023, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0100, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63758.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0591, 0.0128, 0.0485,  ..., 0.1452, 0.0529, 0.0855],
        [0.0485, 0.0074, 0.0252,  ..., 0.2031, 0.0927, 0.1252],
        [0.0316, 0.0018, 0.0000,  ..., 0.2998, 0.1638, 0.1906],
        ...,
        [0.0728, 0.0201, 0.1026,  ..., 0.0935, 0.0187, 0.0490],
        [0.0727, 0.0201, 0.1025,  ..., 0.0934, 0.0187, 0.0489],
        [0.0727, 0.0201, 0.1025,  ..., 0.0934, 0.0187, 0.0489]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692795.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9204.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.1671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2503.8455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1160.2877, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3350],
        [ 0.0457],
        [ 0.3132],
        ...,
        [-5.5292],
        [-5.5201],
        [-5.5171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339912.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0340],
        [1.0431],
        ...,
        [0.9942],
        [0.9936],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370584.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0340],
        [1.0430],
        ...,
        [0.9942],
        [0.9935],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370584.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0098,  0.0024,  ...,  0.0038,  0.0095,  0.0047],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3167.3369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6871, device='cuda:0')



h[100].sum tensor(-0.0179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.1687, device='cuda:0')



h[200].sum tensor(-22.5713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0313, 0.0082,  ..., 0.0140, 0.0301, 0.0174],
        [0.0000, 0.0536, 0.0151,  ..., 0.0176, 0.0523, 0.0329],
        [0.0000, 0.0452, 0.0130,  ..., 0.0163, 0.0439, 0.0287],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0024, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0024, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0099, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72192.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0401, 0.0016, 0.0000,  ..., 0.2458, 0.1232, 0.1537],
        [0.0339, 0.0000, 0.0000,  ..., 0.2866, 0.1569, 0.1804],
        [0.0336, 0.0009, 0.0000,  ..., 0.2948, 0.1655, 0.1852],
        ...,
        [0.0732, 0.0193, 0.1025,  ..., 0.0940, 0.0191, 0.0485],
        [0.0731, 0.0193, 0.1024,  ..., 0.0939, 0.0191, 0.0485],
        [0.0731, 0.0193, 0.1023,  ..., 0.0938, 0.0191, 0.0484]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(728102.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9075.6807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.5869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.5728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2248.3853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1236.8429, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3759],
        [ 0.3542],
        [ 0.3330],
        ...,
        [-5.5581],
        [-5.5490],
        [-5.5461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305446.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0340],
        [1.0430],
        ...,
        [0.9942],
        [0.9935],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370584.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0340],
        [1.0430],
        ...,
        [0.9941],
        [0.9935],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370585.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0271,  0.0083,  ...,  0.0064,  0.0267,  0.0188],
        [-0.0014,  0.0382,  0.0122,  ...,  0.0082,  0.0378,  0.0280],
        [-0.0010,  0.0277,  0.0086,  ...,  0.0065,  0.0273,  0.0194],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3181.6138, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9014, device='cuda:0')



h[100].sum tensor(-0.0181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2000, device='cuda:0')



h[200].sum tensor(-22.5673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0310, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1467, 0.0465,  ..., 0.0320, 0.1448, 0.1063],
        [0.0000, 0.1285, 0.0402,  ..., 0.0292, 0.1268, 0.0914],
        [0.0000, 0.1075, 0.0330,  ..., 0.0259, 0.1059, 0.0742],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0097, 0.0025, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0097, 0.0025, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0097, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73981.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0000, 0.0000,  ..., 0.5399, 0.3631, 0.3465],
        [0.0022, 0.0000, 0.0000,  ..., 0.5138, 0.3401, 0.3297],
        [0.0045, 0.0000, 0.0000,  ..., 0.4750, 0.3057, 0.3046],
        ...,
        [0.0737, 0.0184, 0.1023,  ..., 0.0945, 0.0197, 0.0480],
        [0.0736, 0.0184, 0.1022,  ..., 0.0944, 0.0197, 0.0480],
        [0.0736, 0.0184, 0.1022,  ..., 0.0943, 0.0197, 0.0479]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731646.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9091.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.7489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.3772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2134.2510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1254.3555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1784],
        [ 0.1756],
        [ 0.1763],
        ...,
        [-5.0947],
        [-4.9531],
        [-4.9491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301160.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0340],
        [1.0430],
        ...,
        [0.9941],
        [0.9935],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370585.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0340],
        [1.0430],
        ...,
        [0.9941],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370586.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0023,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3060.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8059, device='cuda:0')



h[100].sum tensor(-0.0170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.0399, device='cuda:0')



h[200].sum tensor(-22.5757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0093, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0024, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0094, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0025, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0096, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71363.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0595, 0.0101, 0.0409,  ..., 0.1532, 0.0606, 0.0877],
        [0.0591, 0.0098, 0.0380,  ..., 0.1575, 0.0640, 0.0905],
        [0.0581, 0.0092, 0.0320,  ..., 0.1659, 0.0698, 0.0961],
        ...,
        [0.0742, 0.0182, 0.1028,  ..., 0.0949, 0.0200, 0.0472],
        [0.0742, 0.0182, 0.1027,  ..., 0.0948, 0.0200, 0.0472],
        [0.0741, 0.0181, 0.1027,  ..., 0.0947, 0.0200, 0.0471]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(716202.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9173.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(143.2594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2000.5111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1238.9460, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1615],
        [ 0.1685],
        [ 0.1851],
        ...,
        [-5.6271],
        [-5.6176],
        [-5.6145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278840.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0340],
        [1.0430],
        ...,
        [0.9941],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370586.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0429],
        ...,
        [0.9941],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370587.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2318.6167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.8495, device='cuda:0')



h[100].sum tensor(-0.0103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8774, device='cuda:0')



h[200].sum tensor(-22.6399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0093, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0094, 0.0023, 0.0000],
        [0.0000, 0.0140, 0.0030,  ..., 0.0111, 0.0132, 0.0062],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0096, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0096, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0096, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59725.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0720, 0.0178, 0.0997,  ..., 0.0940, 0.0207, 0.0451],
        [0.0698, 0.0164, 0.0879,  ..., 0.1070, 0.0289, 0.0540],
        [0.0629, 0.0124, 0.0527,  ..., 0.1451, 0.0530, 0.0800],
        ...,
        [0.0749, 0.0191, 0.1044,  ..., 0.0953, 0.0200, 0.0456],
        [0.0748, 0.0191, 0.1043,  ..., 0.0952, 0.0200, 0.0456],
        [0.0748, 0.0191, 0.1043,  ..., 0.0951, 0.0200, 0.0456]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672746.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9821.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.3575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.6462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2525.4946, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1123.2693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1996],
        [-3.2512],
        [-2.0603],
        ...,
        [-5.6969],
        [-5.6869],
        [-5.6835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-409112.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0429],
        ...,
        [0.9941],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370587.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0429],
        ...,
        [0.9940],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370588.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [-0.0009,  0.0260,  0.0080,  ...,  0.0063,  0.0256,  0.0180],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2683.8276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2528, device='cuda:0')



h[100].sum tensor(-0.0136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5208, device='cuda:0')



h[200].sum tensor(-22.6061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0093, 0.0022, 0.0000],
        [0.0000, 0.0290, 0.0083,  ..., 0.0135, 0.0281, 0.0186],
        [0.0000, 0.0244, 0.0067,  ..., 0.0128, 0.0236, 0.0148],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0096, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0096, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0096, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65364.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0692, 0.0166, 0.0822,  ..., 0.1133, 0.0349, 0.0565],
        [0.0612, 0.0122, 0.0469,  ..., 0.1611, 0.0694, 0.0881],
        [0.0576, 0.0102, 0.0288,  ..., 0.1831, 0.0839, 0.1030],
        ...,
        [0.0752, 0.0197, 0.1052,  ..., 0.0957, 0.0201, 0.0447],
        [0.0752, 0.0196, 0.1051,  ..., 0.0956, 0.0200, 0.0446],
        [0.0751, 0.0196, 0.1051,  ..., 0.0955, 0.0200, 0.0446]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697870.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9677.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.9829, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2393.5442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1178.1292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8744],
        [-2.6780],
        [-1.5490],
        ...,
        [-5.7273],
        [-5.7173],
        [-5.7151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-391033.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0429],
        ...,
        [0.9940],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370588.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0428],
        ...,
        [0.9940],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370591.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0027],
        [-0.0003,  0.0093,  0.0023,  ...,  0.0037,  0.0091,  0.0044],
        [-0.0003,  0.0085,  0.0020,  ...,  0.0035,  0.0083,  0.0037],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2516.2014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2369, device='cuda:0')



h[100].sum tensor(-0.0118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2262, device='cuda:0')



h[200].sum tensor(-22.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0024,  ..., 0.0108, 0.0111, 0.0045],
        [0.0000, 0.0183, 0.0039,  ..., 0.0119, 0.0175, 0.0071],
        [0.0000, 0.0422, 0.0107,  ..., 0.0157, 0.0413, 0.0211],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0097, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0097, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0097, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63857.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0592, 0.0113, 0.0370,  ..., 0.1619, 0.0613, 0.0896],
        [0.0526, 0.0076, 0.0168,  ..., 0.1965, 0.0811, 0.1131],
        [0.0461, 0.0040, 0.0023,  ..., 0.2325, 0.1021, 0.1375],
        ...,
        [0.0752, 0.0201, 0.1053,  ..., 0.0960, 0.0201, 0.0445],
        [0.0751, 0.0201, 0.1052,  ..., 0.0959, 0.0201, 0.0444],
        [0.0751, 0.0200, 0.1052,  ..., 0.0958, 0.0201, 0.0444]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695169.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9720.7910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.4900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2472.6606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1164.7021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2247],
        [ 0.1351],
        [ 0.3310],
        ...,
        [-5.7432],
        [-5.7329],
        [-5.7294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-400180.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0428],
        ...,
        [0.9940],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370591.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0427],
        ...,
        [0.9940],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370593.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0013,  0.0359,  0.0115,  ...,  0.0079,  0.0355,  0.0261],
        [-0.0017,  0.0482,  0.0158,  ...,  0.0098,  0.0477,  0.0362],
        [-0.0013,  0.0373,  0.0120,  ...,  0.0081,  0.0369,  0.0272],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2960.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9168, device='cuda:0')



h[100].sum tensor(-0.0154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.9100, device='cuda:0')



h[200].sum tensor(-22.5839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1671, 0.0541,  ..., 0.0356, 0.1655, 0.1233],
        [0.0000, 0.1881, 0.0614,  ..., 0.0389, 0.1863, 0.1404],
        [0.0000, 0.1853, 0.0604,  ..., 0.0385, 0.1836, 0.1381],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0024, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0024, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70862.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.2419e-03, 0.0000e+00, 0.0000e+00,  ..., 5.8724e-01, 3.9723e-01,
         3.6154e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.4203e-01, 4.4351e-01,
         3.9587e-01],
        [1.3498e-04, 0.0000e+00, 0.0000e+00,  ..., 6.3673e-01, 4.3795e-01,
         3.9260e-01],
        ...,
        [7.4917e-02, 2.0028e-02, 1.0458e-01,  ..., 9.6333e-02, 2.0357e-02,
         4.4857e-02],
        [7.4846e-02, 2.0002e-02, 1.0450e-01,  ..., 9.6235e-02, 2.0338e-02,
         4.4806e-02],
        [7.4809e-02, 1.9986e-02, 1.0446e-01,  ..., 9.6184e-02, 2.0329e-02,
         4.4777e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(727639.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9483.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.4392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.2142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2326.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1229.3604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0936],
        [ 0.0799],
        [ 0.0644],
        ...,
        [-5.7197],
        [-5.7095],
        [-5.7057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-358301.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0341],
        [1.0427],
        ...,
        [0.9940],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370593.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0342],
        [1.0427],
        ...,
        [0.9939],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370596.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2625.7393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1012, device='cuda:0')



h[100].sum tensor(-0.0121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3525, device='cuda:0')



h[200].sum tensor(-22.6156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0155, 0.0030,  ..., 0.0116, 0.0148, 0.0047],
        [0.0000, 0.0092, 0.0015,  ..., 0.0106, 0.0087, 0.0023],
        [0.0000, 0.0121, 0.0025,  ..., 0.0111, 0.0115, 0.0047],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0099, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64947.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.0127, 0.0532,  ..., 0.1441, 0.0447, 0.0795],
        [0.0623, 0.0137, 0.0605,  ..., 0.1369, 0.0411, 0.0745],
        [0.0610, 0.0130, 0.0528,  ..., 0.1460, 0.0460, 0.0808],
        ...,
        [0.0745, 0.0198, 0.1036,  ..., 0.0967, 0.0206, 0.0455],
        [0.0745, 0.0198, 0.1035,  ..., 0.0966, 0.0206, 0.0454],
        [0.0744, 0.0197, 0.1035,  ..., 0.0965, 0.0206, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(695512.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9508.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.8764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2327.4912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1183.9926, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9409],
        [-1.6838],
        [-1.3665],
        ...,
        [-5.6819],
        [-5.6725],
        [-5.6695]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343029.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0342],
        [1.0427],
        ...,
        [0.9939],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370596.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0342],
        [1.0426],
        ...,
        [0.9939],
        [0.9932],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370599.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0160,  0.0046,  ...,  0.0048,  0.0157,  0.0098],
        [-0.0004,  0.0114,  0.0031,  ...,  0.0040,  0.0112,  0.0060],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2787.6135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5440, device='cuda:0')



h[100].sum tensor(-0.0132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.5633, device='cuda:0')



h[200].sum tensor(-22.6027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0650, 0.0188,  ..., 0.0195, 0.0641, 0.0395],
        [0.0000, 0.0278, 0.0072,  ..., 0.0136, 0.0271, 0.0147],
        [0.0000, 0.0141, 0.0032,  ..., 0.0115, 0.0135, 0.0062],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0099, 0.0026, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0099, 0.0026, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0099, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66559.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0415, 0.0036, 0.0000,  ..., 0.2605, 0.1327, 0.1550],
        [0.0502, 0.0072, 0.0132,  ..., 0.2096, 0.0948, 0.1219],
        [0.0576, 0.0111, 0.0318,  ..., 0.1700, 0.0663, 0.0959],
        ...,
        [0.0742, 0.0195, 0.1027,  ..., 0.0970, 0.0209, 0.0461],
        [0.0741, 0.0195, 0.1027,  ..., 0.0969, 0.0208, 0.0461],
        [0.0741, 0.0195, 0.1026,  ..., 0.0969, 0.0208, 0.0460]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698022.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9409.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.0208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.5941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2275.0845, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1200.3918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3167],
        [ 0.1366],
        [-0.2436],
        ...,
        [-5.6565],
        [-5.6471],
        [-5.6437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319238.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0342],
        [1.0426],
        ...,
        [0.9939],
        [0.9932],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370599.2812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 380.0 event: 1900 loss: tensor(468.3148, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9938],
        [0.9932],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370601.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2612.0610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3095, device='cuda:0')



h[100].sum tensor(-0.0114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2369, device='cuda:0')



h[200].sum tensor(-22.6196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0097, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0100, 0.0026, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0100, 0.0026, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0100, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64248.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0717, 0.0186, 0.1000,  ..., 0.0937, 0.0203, 0.0442],
        [0.0719, 0.0187, 0.1003,  ..., 0.0941, 0.0204, 0.0444],
        [0.0718, 0.0187, 0.0983,  ..., 0.0973, 0.0219, 0.0466],
        ...,
        [0.0741, 0.0197, 0.1025,  ..., 0.0973, 0.0209, 0.0462],
        [0.0740, 0.0197, 0.1024,  ..., 0.0972, 0.0209, 0.0462],
        [0.0740, 0.0196, 0.1024,  ..., 0.0971, 0.0209, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690313.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9482.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.8016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.7437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2403.5249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1179.1315, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1185],
        [-4.8111],
        [-4.2964],
        ...,
        [-5.0745],
        [-5.4568],
        [-5.5781]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333014.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9938],
        [0.9932],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370601.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9938],
        [0.9931],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370603.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3204.8633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1346, device='cuda:0')



h[100].sum tensor(-0.0161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.2341, device='cuda:0')



h[200].sum tensor(-22.5675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0314, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0099, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0101, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0101, 0.0025, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0101, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73327.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0634, 0.0149, 0.0620,  ..., 0.1340, 0.0428, 0.0716],
        [0.0668, 0.0165, 0.0795,  ..., 0.1164, 0.0310, 0.0600],
        [0.0686, 0.0174, 0.0862,  ..., 0.1104, 0.0274, 0.0560],
        ...,
        [0.0741, 0.0200, 0.1027,  ..., 0.0975, 0.0209, 0.0461],
        [0.0741, 0.0200, 0.1026,  ..., 0.0974, 0.0209, 0.0461],
        [0.0740, 0.0200, 0.1026,  ..., 0.0974, 0.0209, 0.0460]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(730664., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9204.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(134.1176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2244.8801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1263.4146, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8378],
        [-1.6557],
        [-2.2872],
        ...,
        [-5.6565],
        [-5.6276],
        [-5.5342]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295675.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9938],
        [0.9931],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370603.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9937],
        [0.9931],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0183,  0.0055,  ...,  0.0052,  0.0180,  0.0117],
        [-0.0004,  0.0112,  0.0030,  ...,  0.0040,  0.0110,  0.0059],
        [-0.0008,  0.0241,  0.0075,  ...,  0.0061,  0.0238,  0.0164],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3386.4099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8563, device='cuda:0')



h[100].sum tensor(-0.0174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4856, device='cuda:0')



h[200].sum tensor(-22.5505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0466, 0.0126,  ..., 0.0167, 0.0458, 0.0246],
        [0.0000, 0.0616, 0.0185,  ..., 0.0192, 0.0607, 0.0397],
        [0.0000, 0.0337, 0.0094,  ..., 0.0148, 0.0329, 0.0196],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0024, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0024, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75082.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0036, 0.0000,  ..., 0.2542, 0.1202, 0.1500],
        [0.0420, 0.0035, 0.0000,  ..., 0.2694, 0.1359, 0.1588],
        [0.0472, 0.0061, 0.0000,  ..., 0.2430, 0.1184, 0.1412],
        ...,
        [0.0746, 0.0205, 0.1037,  ..., 0.0977, 0.0208, 0.0455],
        [0.0745, 0.0205, 0.1037,  ..., 0.0976, 0.0208, 0.0454],
        [0.0745, 0.0205, 0.1036,  ..., 0.0976, 0.0208, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(737299.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9357.6377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.8606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.0543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2453.3767, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1273.2596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4065],
        [ 0.4676],
        [ 0.4649],
        ...,
        [-5.6444],
        [-5.6087],
        [-5.5936]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342314.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9937],
        [0.9931],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9937],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0090,  0.0023,  ...,  0.0037,  0.0088,  0.0041],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2510.7427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4686, device='cuda:0')



h[100].sum tensor(-0.0103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.1140, device='cuda:0')



h[200].sum tensor(-22.6271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0318, 0.0088,  ..., 0.0143, 0.0311, 0.0182],
        [0.0000, 0.0114, 0.0023,  ..., 0.0111, 0.0108, 0.0042],
        [0.0000, 0.0028, 0.0000,  ..., 0.0098, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63249.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0490, 0.0060, 0.0142,  ..., 0.2331, 0.1130, 0.1342],
        [0.0596, 0.0120, 0.0372,  ..., 0.1697, 0.0683, 0.0929],
        [0.0677, 0.0166, 0.0739,  ..., 0.1255, 0.0384, 0.0639],
        ...,
        [0.0750, 0.0206, 0.1044,  ..., 0.0979, 0.0208, 0.0451],
        [0.0750, 0.0205, 0.1044,  ..., 0.0978, 0.0208, 0.0450],
        [0.0749, 0.0205, 0.1043,  ..., 0.0978, 0.0208, 0.0450]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687722.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9611.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7045, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.8851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2493.3518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1173.1195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1373],
        [-0.2871],
        [-1.0650],
        ...,
        [-5.7446],
        [-5.7345],
        [-5.7305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334966.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9937],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9937],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        [-0.0006,  0.0169,  0.0050,  ...,  0.0049,  0.0166,  0.0105],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0024,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2408.8677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3566, device='cuda:0')



h[100].sum tensor(-0.0095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.9515, device='cuda:0')



h[200].sum tensor(-22.6361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0028, 0.0000,  ..., 0.0097, 0.0023, 0.0000],
        [0.0000, 0.0264, 0.0069,  ..., 0.0135, 0.0257, 0.0137],
        [0.0000, 0.0250, 0.0058,  ..., 0.0133, 0.0243, 0.0097],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0100, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61650.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0645, 0.0150, 0.0584,  ..., 0.1398, 0.0496, 0.0733],
        [0.0589, 0.0116, 0.0340,  ..., 0.1711, 0.0674, 0.0943],
        [0.0543, 0.0090, 0.0188,  ..., 0.1971, 0.0811, 0.1119],
        ...,
        [0.0750, 0.0206, 0.1044,  ..., 0.0979, 0.0208, 0.0451],
        [0.0750, 0.0205, 0.1044,  ..., 0.0978, 0.0208, 0.0450],
        [0.0749, 0.0205, 0.1043,  ..., 0.0978, 0.0208, 0.0450]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684330.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9785.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5581, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.9985, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2745.7388, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1150.0504, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5903],
        [-0.4242],
        [-0.0576],
        ...,
        [-5.6982],
        [-5.7165],
        [-5.7165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-396953.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0342],
        [1.0426],
        ...,
        [0.9937],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0343],
        [1.0427],
        ...,
        [0.9936],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0005, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0006,  ...,  0.0023,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2697.1973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8150, device='cuda:0')



h[100].sum tensor(-0.0117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4568, device='cuda:0')



h[200].sum tensor(-22.6098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0028, 0.0000,  ..., 0.0097, 0.0022, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0097, 0.0022, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0097, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0099, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65150.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0716, 0.0185, 0.0966,  ..., 0.1001, 0.0234, 0.0470],
        [0.0716, 0.0185, 0.0963,  ..., 0.1010, 0.0232, 0.0477],
        [0.0714, 0.0184, 0.0935,  ..., 0.1052, 0.0253, 0.0505],
        ...,
        [0.0746, 0.0199, 0.1011,  ..., 0.1016, 0.0234, 0.0475],
        [0.0751, 0.0202, 0.1044,  ..., 0.0980, 0.0209, 0.0451],
        [0.0751, 0.0202, 0.1043,  ..., 0.0979, 0.0209, 0.0451]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697954.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9775.2324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.5893, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2759.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1178.4320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1825],
        [-3.1274],
        [-2.7004],
        ...,
        [-5.2830],
        [-5.5794],
        [-5.7031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-395455.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0343],
        [1.0427],
        ...,
        [0.9936],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0344],
        [1.0427],
        ...,
        [0.9936],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2857.4565, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6200, device='cuda:0')



h[100].sum tensor(-0.0129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7205, device='cuda:0')



h[200].sum tensor(-22.5943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0096, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0096, 0.0023, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0096, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0098, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68254.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0720, 0.0179, 0.0982,  ..., 0.0982, 0.0220, 0.0462],
        [0.0728, 0.0183, 0.1007,  ..., 0.0962, 0.0211, 0.0446],
        [0.0736, 0.0187, 0.1022,  ..., 0.0958, 0.0207, 0.0442],
        ...,
        [0.0753, 0.0195, 0.1041,  ..., 0.0983, 0.0212, 0.0456],
        [0.0752, 0.0194, 0.1040,  ..., 0.0982, 0.0212, 0.0455],
        [0.0752, 0.0194, 0.1039,  ..., 0.0982, 0.0212, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(706599.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9692.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1990, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(138.0349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2625.1011, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1209.7664, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4063],
        [-4.1356],
        [-4.6222],
        ...,
        [-5.7536],
        [-5.7437],
        [-5.7401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-365500.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0344],
        [1.0427],
        ...,
        [0.9936],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0345],
        [1.0428],
        ...,
        [0.9936],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2590.1538, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8205, device='cuda:0')



h[100].sum tensor(-0.0108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3115, device='cuda:0')



h[200].sum tensor(-22.6171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0095, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0095, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0095, 0.0024, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0097, 0.0024, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0097, 0.0024, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0097, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64052.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0729, 0.0174, 0.1010,  ..., 0.0949, 0.0208, 0.0442],
        [0.0732, 0.0175, 0.1012,  ..., 0.0952, 0.0209, 0.0444],
        [0.0736, 0.0178, 0.1016,  ..., 0.0960, 0.0210, 0.0449],
        ...,
        [0.0754, 0.0185, 0.1035,  ..., 0.0985, 0.0215, 0.0462],
        [0.0753, 0.0184, 0.1034,  ..., 0.0984, 0.0214, 0.0462],
        [0.0753, 0.0184, 0.1034,  ..., 0.0983, 0.0214, 0.0462]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687711.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9742.8184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.7863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(139.1076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2603.1382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1174.8571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.4640],
        [-5.2898],
        [-4.9841],
        ...,
        [-5.7443],
        [-5.7346],
        [-5.7310]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-355822.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0345],
        [1.0428],
        ...,
        [0.9936],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0345],
        [1.0428],
        ...,
        [0.9935],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370604.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [-0.0006,  0.0178,  0.0052,  ...,  0.0050,  0.0175,  0.0113],
        [-0.0006,  0.0189,  0.0055,  ...,  0.0051,  0.0186,  0.0122],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2857.9517, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9939, device='cuda:0')



h[100].sum tensor(-0.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.7752, device='cuda:0')



h[200].sum tensor(-22.5921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0205, 0.0053,  ..., 0.0122, 0.0197, 0.0116],
        [0.0000, 0.0362, 0.0100,  ..., 0.0147, 0.0353, 0.0216],
        [0.0000, 0.1021, 0.0312,  ..., 0.0251, 0.1007, 0.0699],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0097, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0096, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0096, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68501.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0617, 0.0100, 0.0479,  ..., 0.1607, 0.0683, 0.0886],
        [0.0518, 0.0048, 0.0217,  ..., 0.2206, 0.1122, 0.1286],
        [0.0355, 0.0000, 0.0000,  ..., 0.3230, 0.1909, 0.1961],
        ...,
        [0.0756, 0.0179, 0.1034,  ..., 0.0986, 0.0216, 0.0466],
        [0.0755, 0.0179, 0.1033,  ..., 0.0985, 0.0216, 0.0466],
        [0.0754, 0.0179, 0.1032,  ..., 0.0984, 0.0216, 0.0465]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710522.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9686.0020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.2192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.2851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2506.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1212.8912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8591],
        [-0.9813],
        [-0.1345],
        ...,
        [-5.7482],
        [-5.7385],
        [-5.7350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336617.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0345],
        [1.0428],
        ...,
        [0.9935],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370604.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0346],
        [1.0428],
        ...,
        [0.9935],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370605.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0006, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3254.2424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5703, device='cuda:0')



h[100].sum tensor(-0.0156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.4438, device='cuda:0')



h[200].sum tensor(-22.5558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0094, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0094, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0095, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0097, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0097, 0.0024, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0097, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76118.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0733, 0.0170, 0.1011,  ..., 0.0948, 0.0209, 0.0445],
        [0.0731, 0.0168, 0.0992,  ..., 0.0973, 0.0221, 0.0463],
        [0.0715, 0.0157, 0.0907,  ..., 0.1074, 0.0271, 0.0536],
        ...,
        [0.0758, 0.0181, 0.1036,  ..., 0.0984, 0.0215, 0.0466],
        [0.0757, 0.0181, 0.1035,  ..., 0.0983, 0.0215, 0.0465],
        [0.0757, 0.0180, 0.1035,  ..., 0.0982, 0.0215, 0.0465]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(743538.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9510.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.9590, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.3369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2396.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1284.5712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5424],
        [-3.2357],
        [-2.5131],
        ...,
        [-5.7390],
        [-5.7421],
        [-5.7420]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308078.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0346],
        [1.0428],
        ...,
        [0.9935],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370605.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 390.0 event: 1950 loss: tensor(432.1939, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9935],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370606.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2745.9846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0013, device='cuda:0')



h[100].sum tensor(-0.0118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6301, device='cuda:0')



h[200].sum tensor(-22.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0255, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0028, 0.0000,  ..., 0.0096, 0.0021, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0096, 0.0021, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0096, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69481.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0737, 0.0182, 0.1022,  ..., 0.0945, 0.0206, 0.0439],
        [0.0739, 0.0183, 0.1024,  ..., 0.0948, 0.0206, 0.0440],
        [0.0744, 0.0186, 0.1028,  ..., 0.0956, 0.0207, 0.0445],
        ...,
        [0.0761, 0.0193, 0.1047,  ..., 0.0980, 0.0212, 0.0459],
        [0.0761, 0.0193, 0.1047,  ..., 0.0980, 0.0212, 0.0458],
        [0.0760, 0.0193, 0.1046,  ..., 0.0979, 0.0212, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(728777.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9929.1084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.3297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.7235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2835.1575, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1212.4193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5011],
        [-4.7611],
        [-4.9221],
        ...,
        [-5.8037],
        [-5.7935],
        [-5.7898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-401630.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9935],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370606.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9935],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370606.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2792.0771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3156, device='cuda:0')



h[100].sum tensor(-0.0121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.6761, device='cuda:0')



h[200].sum tensor(-22.5958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0028, 0.0000,  ..., 0.0096, 0.0021, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0096, 0.0021, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0096, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67398.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0732, 0.0180, 0.1007,  ..., 0.0960, 0.0210, 0.0452],
        [0.0718, 0.0170, 0.0939,  ..., 0.1036, 0.0247, 0.0509],
        [0.0689, 0.0149, 0.0768,  ..., 0.1226, 0.0359, 0.0641],
        ...,
        [0.0761, 0.0193, 0.1047,  ..., 0.0980, 0.0212, 0.0459],
        [0.0761, 0.0193, 0.1047,  ..., 0.0980, 0.0212, 0.0458],
        [0.0760, 0.0193, 0.1046,  ..., 0.0979, 0.0212, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701555.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9770.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.1105, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.9856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2538.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1208.3923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9897],
        [-3.2512],
        [-2.3314],
        ...,
        [-5.8037],
        [-5.7935],
        [-5.7898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338563.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9935],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370606.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9935],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370606.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0190,  0.0056,  ...,  0.0052,  0.0186,  0.0123],
        [-0.0012,  0.0342,  0.0109,  ...,  0.0076,  0.0338,  0.0248],
        [-0.0014,  0.0413,  0.0133,  ...,  0.0087,  0.0408,  0.0305],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2592.8677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2133, device='cuda:0')



h[100].sum tensor(-0.0106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.3689, device='cuda:0')



h[200].sum tensor(-22.6134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1239, 0.0389,  ..., 0.0287, 0.1222, 0.0881],
        [0.0000, 0.1638, 0.0527,  ..., 0.0350, 0.1617, 0.1206],
        [0.0000, 0.1637, 0.0526,  ..., 0.0350, 0.1616, 0.1204],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0098, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65442.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.6453e-03, 0.0000e+00, 0.0000e+00,  ..., 5.1515e-01, 3.3954e-01,
         3.2207e-01],
        [2.6565e-03, 0.0000e+00, 0.0000e+00,  ..., 5.7689e-01, 3.9256e-01,
         3.6154e-01],
        [2.7454e-04, 0.0000e+00, 0.0000e+00,  ..., 6.0083e-01, 4.1232e-01,
         3.7710e-01],
        ...,
        [7.6148e-02, 1.9337e-02, 1.0474e-01,  ..., 9.8050e-02, 2.1202e-02,
         4.5879e-02],
        [7.6090e-02, 1.9316e-02, 1.0467e-01,  ..., 9.7970e-02, 2.1186e-02,
         4.5837e-02],
        [7.6032e-02, 1.9295e-02, 1.0460e-01,  ..., 9.7891e-02, 2.1171e-02,
         4.5795e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699167.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9850.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(137.9703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2625.4644, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1186.9984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4180],
        [ 0.4084],
        [ 0.4082],
        ...,
        [-5.7997],
        [-5.7887],
        [-5.7841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-354651.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9935],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370606.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370607.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0165,  0.0048,  ...,  0.0049,  0.0162,  0.0103],
        [-0.0004,  0.0135,  0.0038,  ...,  0.0044,  0.0132,  0.0079],
        [-0.0010,  0.0299,  0.0094,  ...,  0.0070,  0.0295,  0.0213],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0023,  0.0005, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2087.9556, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.6226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(10.6566, device='cuda:0')



h[100].sum tensor(-0.0068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.5570, device='cuda:0')



h[200].sum tensor(-22.6582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0293, 0.0078,  ..., 0.0139, 0.0283, 0.0162],
        [0.0000, 0.0871, 0.0263,  ..., 0.0230, 0.0856, 0.0579],
        [0.0000, 0.0906, 0.0275,  ..., 0.0236, 0.0891, 0.0608],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0100, 0.0021, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0099, 0.0021, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0099, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57257.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0569, 0.0077, 0.0272,  ..., 0.1993, 0.0936, 0.1141],
        [0.0438, 0.0013, 0.0000,  ..., 0.2862, 0.1586, 0.1715],
        [0.0390, 0.0000, 0.0000,  ..., 0.3209, 0.1844, 0.1945],
        ...,
        [0.0763, 0.0203, 0.1053,  ..., 0.0977, 0.0209, 0.0456],
        [0.0763, 0.0202, 0.1053,  ..., 0.0976, 0.0209, 0.0456],
        [0.0762, 0.0202, 0.1052,  ..., 0.0976, 0.0209, 0.0455]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665576.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10215.0459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.7345, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3063.7554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1105.0955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5634],
        [-0.3790],
        [ 0.3124],
        ...,
        [-5.8243],
        [-5.8140],
        [-5.8102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-435789.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0347],
        [1.0428],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370607.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0342],
        [1.0348],
        [1.0428],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370609.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0089,  0.0022,  ...,  0.0037,  0.0087,  0.0041],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [-0.0003,  0.0089,  0.0022,  ...,  0.0037,  0.0087,  0.0041],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2498.7783, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1279, device='cuda:0')



h[100].sum tensor(-0.0096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.2103, device='cuda:0')



h[200].sum tensor(-22.6225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0017,  ..., 0.0109, 0.0089, 0.0030],
        [0.0000, 0.0339, 0.0080,  ..., 0.0147, 0.0329, 0.0144],
        [0.0000, 0.0098, 0.0017,  ..., 0.0110, 0.0090, 0.0030],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0101, 0.0021, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0101, 0.0021, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0101, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62812.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0677, 0.0152, 0.0727,  ..., 0.1251, 0.0367, 0.0659],
        [0.0637, 0.0123, 0.0512,  ..., 0.1480, 0.0491, 0.0820],
        [0.0684, 0.0156, 0.0729,  ..., 0.1266, 0.0371, 0.0668],
        ...,
        [0.0762, 0.0205, 0.1051,  ..., 0.0975, 0.0207, 0.0459],
        [0.0761, 0.0205, 0.1050,  ..., 0.0974, 0.0207, 0.0459],
        [0.0761, 0.0205, 0.1049,  ..., 0.0974, 0.0207, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(685234.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10055.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.8282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2945.1670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1156.7990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8174],
        [-3.9068],
        [-4.2724],
        ...,
        [-5.8138],
        [-5.8037],
        [-5.7999]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-403929.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0342],
        [1.0348],
        [1.0428],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370609.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0348],
        [1.0427],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370611.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [-0.0004,  0.0121,  0.0033,  ...,  0.0042,  0.0118,  0.0066],
        [-0.0015,  0.0431,  0.0140,  ...,  0.0091,  0.0426,  0.0320],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0005, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4487.4004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.1591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.4992, device='cuda:0')



h[100].sum tensor(-0.0233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4790, device='cuda:0')



h[200].sum tensor(-22.4483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0338, 0.0093,  ..., 0.0148, 0.0328, 0.0198],
        [0.0000, 0.0943, 0.0295,  ..., 0.0244, 0.0927, 0.0664],
        [0.0000, 0.1257, 0.0397,  ..., 0.0294, 0.1239, 0.0893],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0102, 0.0022, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0102, 0.0022, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0102, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95133.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0418, 0.0041, 0.0146,  ..., 0.2980, 0.1729, 0.1808],
        [0.0182, 0.0000, 0.0000,  ..., 0.4665, 0.3083, 0.2912],
        [0.0064, 0.0000, 0.0000,  ..., 0.6054, 0.4202, 0.3821],
        ...,
        [0.0757, 0.0200, 0.1038,  ..., 0.0973, 0.0206, 0.0468],
        [0.0757, 0.0200, 0.1037,  ..., 0.0972, 0.0206, 0.0468],
        [0.0756, 0.0200, 0.1036,  ..., 0.0971, 0.0206, 0.0468]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(842632.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9408.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.8434, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.7087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2604.2842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1437.1405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1955],
        [ 0.2345],
        [ 0.2214],
        ...,
        [-5.7680],
        [-5.7582],
        [-5.7547]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-341732.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0348],
        [1.0427],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370611.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0348],
        [1.0427],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370612.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0007, -0.0007,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2264.8088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.6989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0996, device='cuda:0')



h[100].sum tensor(-0.0075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.7678, device='cuda:0')



h[200].sum tensor(-22.6466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0099, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0100, 0.0023, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0102, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0102, 0.0023, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0102, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60099.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0729, 0.0178, 0.0996,  ..., 0.0937, 0.0200, 0.0459],
        [0.0731, 0.0179, 0.0999,  ..., 0.0940, 0.0201, 0.0461],
        [0.0730, 0.0178, 0.0968,  ..., 0.0982, 0.0227, 0.0489],
        ...,
        [0.0753, 0.0189, 0.1021,  ..., 0.0972, 0.0206, 0.0480],
        [0.0753, 0.0189, 0.1020,  ..., 0.0971, 0.0206, 0.0479],
        [0.0752, 0.0189, 0.1019,  ..., 0.0970, 0.0206, 0.0479]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673836.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10011.0098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.4313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(131.8172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2945.9907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1130.5353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1059],
        [-4.7337],
        [-4.0927],
        ...,
        [-5.7137],
        [-5.7044],
        [-5.7010]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-384823.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0348],
        [1.0427],
        ...,
        [0.9934],
        [0.9928],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370612.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0427],
        ...,
        [0.9934],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370614.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2707.1753, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9281, device='cuda:0')



h[100].sum tensor(-0.0103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(2.4733, device='cuda:0')



h[200].sum tensor(-22.6091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0099, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0100, 0.0024, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0100, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0102, 0.0025, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0102, 0.0025, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0102, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65989.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0725, 0.0169, 0.0981,  ..., 0.0935, 0.0200, 0.0469],
        [0.0727, 0.0170, 0.0983,  ..., 0.0938, 0.0200, 0.0471],
        [0.0732, 0.0172, 0.0986,  ..., 0.0945, 0.0201, 0.0476],
        ...,
        [0.0749, 0.0179, 0.1004,  ..., 0.0970, 0.0206, 0.0491],
        [0.0749, 0.0179, 0.1004,  ..., 0.0969, 0.0206, 0.0490],
        [0.0748, 0.0179, 0.1003,  ..., 0.0968, 0.0205, 0.0490]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(691681.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9657.2793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.9955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2517.1750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1193.5016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0318],
        [-5.1084],
        [-5.0853],
        ...,
        [-5.6611],
        [-5.6523],
        [-5.6492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293853.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0427],
        ...,
        [0.9934],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370614.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0426],
        ...,
        [0.9933],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370615.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015,  0.0451,  0.0146,  ...,  0.0094,  0.0446,  0.0335],
        [-0.0010,  0.0300,  0.0094,  ...,  0.0070,  0.0296,  0.0212],
        [-0.0005,  0.0160,  0.0046,  ...,  0.0048,  0.0157,  0.0098],
        ...,
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027],
        [ 0.0000,  0.0008, -0.0007,  ...,  0.0024,  0.0006, -0.0027]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3493.6743, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.4600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1494, device='cuda:0')



h[100].sum tensor(-0.0154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.6745, device='cuda:0')



h[200].sum tensor(-22.5407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1389, 0.0440,  ..., 0.0315, 0.1370, 0.0996],
        [0.0000, 0.1024, 0.0314,  ..., 0.0257, 0.1009, 0.0699],
        [0.0000, 0.0588, 0.0171,  ..., 0.0188, 0.0577, 0.0370],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0102, 0.0026, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0102, 0.0026, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0102, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78851.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0000, 0.0000,  ..., 0.4859, 0.3232, 0.3138],
        [0.0161, 0.0000, 0.0000,  ..., 0.4317, 0.2767, 0.2781],
        [0.0270, 0.0000, 0.0000,  ..., 0.3627, 0.2181, 0.2325],
        ...,
        [0.0747, 0.0174, 0.0995,  ..., 0.0968, 0.0205, 0.0497],
        [0.0746, 0.0173, 0.0994,  ..., 0.0967, 0.0205, 0.0497],
        [0.0746, 0.0173, 0.0994,  ..., 0.0966, 0.0205, 0.0496]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(752697.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9228.5527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.2411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(132.1978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2245.8586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1310.7311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3984],
        [ 0.4124],
        [ 0.4310],
        ...,
        [-5.6368],
        [-5.6284],
        [-5.6254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250301.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0426],
        ...,
        [0.9933],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370615.8125, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:01:01.906268
evaluation loss: 463.2786865234375
epoch: 1 mean loss: 456.55047607421875
=> saveing checkpoint at epoch 1
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



training loss:
 [495.6762085  456.55047607] 

\evaluation loss:
 [512.86236572 463.27868652]



eval_efficiency:
 [0.57395838 0.56699521 0.55931687 0.5509185  0.54215076 0.53440046
 0.52563676 0.51577855 0.50522769 0.49469636 0.48354683 0.47055109
 0.45903191 0.44655333 0.43285175 0.42252826 0.40700015 0.39497346
 0.38372747 0.36968979 0.35353535 0.33733497 0.31870833 0.30230588
 0.28588398 0.26822127 0.24956233 0.23257957 0.21271083 0.19406722
 0.17851794 0.16311578 0.15528169 0.15613062 0.14557603 0.14528903
 0.13345942 0.12383418 0.11191307 0.11497118 0.10621017 0.09656156
 0.0941918  0.08571425 0.08028852 0.07395388 0.06768681 0.06255441
 0.06103122 0.05386508 0.04922134 0.04601628 0.04163561 0.03566717
 0.03590578 0.03229309 0.03150014 0.03045226 0.03170688 0.03299892
 0.04110615 0.03721148 0.0414277  0.03666367 0.03085121 0.02740913
 0.0330288  0.0330288  0.03187938 0.02815148 0.03208329 0.03013915
 0.0256758  0.01943181 0.01748767 0.01446128 0.01337902 0.01143489
 0.0106686  0.0106686  0.0106686  0.00958635 0.0085041  0.00655996
 0.00655996 0.00655996 0.00655996 0.00655996 0.00869051 0.00869051
 0.00869051 0.00869051 0.00515695 0.00515695 0.00515695 0.00515695
 0.00515695 0.00339016 0.00339016 0.00339016] 


eval_purity:
 [0.75080595 0.75277409 0.7532521  0.75342893 0.75571687 0.7568493
 0.75813299 0.76000294 0.75995473 0.76024548 0.76272481 0.7636726
 0.76417481 0.76382887 0.76556018 0.76878555 0.77446324 0.78073377
 0.78749949 0.79435968 0.79968271 0.80793729 0.81461564 0.82032374
 0.82600757 0.83376901 0.83035436 0.84293514 0.84542791 0.84250681
 0.83201756 0.81815449 0.84310152        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0426],
        ...,
        [0.9933],
        [0.9927],
        [0.9912]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(74123.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-2.0305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(2.4823, device='cuda:0')



h[100].sum tensor(1.1425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0.3627, device='cuda:0')



h[200].sum tensor(-0.0321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0035, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3040.2251, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0001, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(14270.0176, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-0.2502, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-18.4110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(8.2057, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0939],
        [-0.1155],
        [-0.1680],
        ...,
        [-0.0289],
        [-0.0289],
        [-0.0229]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-2231.7488, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0426],
        ...,
        [0.9933],
        [0.9927],
        [0.9912]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(74123.4375, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network after training 
result1: tensor([[-0.0939],
        [-0.1155],
        [-0.1680],
        ...,
        [-0.0289],
        [-0.0289],
        [-0.0229]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0426],
        ...,
        [0.9933],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(148246.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(139.1846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(9.9090, device='cuda:0')



h[100].sum tensor(14.6255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.4478, device='cuda:0')



h[200].sum tensor(6.9811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.0140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(16004.0654, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0252, 0.0321,  ..., 0.0031, 0.0008, 0.0000],
        [0.0000, 0.0053, 0.0067,  ..., 0.0007, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(100959.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1.8019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2614.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.9127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2042.2639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(13.2731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2103],
        [-0.1283],
        [-0.0784],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-11835.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0348],
        [1.0426],
        ...,
        [0.9933],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(148246.8906, device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network after training 
result1: tensor([[-0.0939],
        [-0.1155],
        [-0.1680],
        ...,
        [-0.0289],
        [-0.0289],
        [-0.0229]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



total time: 0:01:06.926700 hpmesh elements: 44 to 45

real	1m33.433s
user	1m6.856s
sys	0m25.414s
