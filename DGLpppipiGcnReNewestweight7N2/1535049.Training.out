0: gpu023.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-244f0056-de69-7f0b-e74d-3fdee2e366c7)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Tue Aug  9 18:37:24 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |
| N/A   25C    P0    41W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2aeed4fac880> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m44.054s
user	0m3.223s
sys	0m2.457s
[18:38:10] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.1745],
        [-0.7360],
        [-0.9721],
        ...,
        [-0.8034],
        [-0.0988],
        [ 0.5523]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-74.1039, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1506, -0.1525, -0.0766,  0.1306, -0.1337,  0.0675, -0.0359, -0.0588,
          0.0367,  0.0532, -0.1157,  0.0501,  0.1409, -0.0609,  0.0634,  0.1208,
         -0.1465,  0.0108, -0.0201,  0.1479,  0.0350,  0.1192,  0.0021,  0.0399,
         -0.0918, -0.0445, -0.0698, -0.0235, -0.0654, -0.1181, -0.0772,  0.1194,
         -0.0447,  0.1146,  0.1366,  0.0454, -0.0638, -0.1024, -0.0347,  0.0828,
         -0.1290,  0.0351, -0.0575,  0.0548, -0.0400, -0.0736,  0.1507,  0.0793,
         -0.1196,  0.0444, -0.0985,  0.0817, -0.0857,  0.1438,  0.0428,  0.0491,
         -0.0880, -0.0875, -0.0016, -0.0484, -0.0335, -0.0515,  0.0172,  0.1354,
         -0.0610, -0.0275, -0.1027,  0.1218, -0.1500, -0.0214, -0.1234, -0.0158,
          0.0903, -0.0325, -0.1187,  0.0598,  0.0381,  0.1099,  0.1112,  0.0315,
          0.0939, -0.0432, -0.0814, -0.0796,  0.0775,  0.0186,  0.0144, -0.0528,
         -0.0822,  0.0934,  0.0137, -0.0754, -0.1489, -0.0209, -0.0665, -0.0892,
          0.0218, -0.1425, -0.1422, -0.1255, -0.1033,  0.0758, -0.0762,  0.0987,
         -0.0624, -0.0012, -0.1437,  0.1150, -0.0010,  0.0968, -0.1334,  0.0330,
         -0.1358, -0.0434,  0.0886, -0.0920,  0.1189, -0.0659,  0.0880,  0.1212,
          0.1089,  0.0706,  0.0106, -0.1429, -0.0962,  0.0727,  0.0986, -0.0379,
          0.0719, -0.0210,  0.0073, -0.1213,  0.0942, -0.1324, -0.0632,  0.0581,
         -0.0536,  0.0389,  0.0020, -0.0339, -0.0148,  0.1249,  0.0787,  0.0778,
         -0.1148, -0.1362, -0.1308,  0.1298,  0.0415,  0.0506,  0.1014,  0.1450,
          0.0241, -0.0212,  0.0907, -0.1009,  0.1288,  0.0939, -0.0616, -0.0177,
         -0.0904,  0.0405, -0.0624, -0.1464, -0.0130,  0.1510, -0.1251,  0.0130,
         -0.1472, -0.1317, -0.1517,  0.0159, -0.0633, -0.1482,  0.0410,  0.0088,
         -0.0821,  0.0082,  0.0331, -0.0977,  0.0438,  0.0155,  0.0676, -0.0701,
          0.1283, -0.0301, -0.0464, -0.0894,  0.1207,  0.1255,  0.0927,  0.0483,
          0.1435,  0.1348, -0.0795, -0.1124, -0.0713, -0.1527, -0.0916, -0.0386,
          0.0854, -0.0559, -0.1393,  0.0722,  0.1511,  0.0315,  0.0772,  0.0214,
         -0.0800,  0.0908,  0.0330,  0.0146,  0.0066,  0.0420,  0.1178,  0.0950,
          0.1319,  0.0587, -0.0961,  0.1507, -0.0940,  0.0511,  0.0190, -0.1058,
          0.0168, -0.1302, -0.1368,  0.0305,  0.1200,  0.0684,  0.0630,  0.0833,
         -0.1062,  0.0040, -0.0223,  0.0110,  0.0847,  0.0726, -0.1110,  0.0920,
          0.0789,  0.1154, -0.0999,  0.0557, -0.1000,  0.0036, -0.0655, -0.0691,
          0.0309,  0.0130, -0.0092, -0.0910,  0.1388, -0.0237,  0.1526, -0.1079]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1506, -0.1525, -0.0766,  0.1306, -0.1337,  0.0675, -0.0359, -0.0588,
          0.0367,  0.0532, -0.1157,  0.0501,  0.1409, -0.0609,  0.0634,  0.1208,
         -0.1465,  0.0108, -0.0201,  0.1479,  0.0350,  0.1192,  0.0021,  0.0399,
         -0.0918, -0.0445, -0.0698, -0.0235, -0.0654, -0.1181, -0.0772,  0.1194,
         -0.0447,  0.1146,  0.1366,  0.0454, -0.0638, -0.1024, -0.0347,  0.0828,
         -0.1290,  0.0351, -0.0575,  0.0548, -0.0400, -0.0736,  0.1507,  0.0793,
         -0.1196,  0.0444, -0.0985,  0.0817, -0.0857,  0.1438,  0.0428,  0.0491,
         -0.0880, -0.0875, -0.0016, -0.0484, -0.0335, -0.0515,  0.0172,  0.1354,
         -0.0610, -0.0275, -0.1027,  0.1218, -0.1500, -0.0214, -0.1234, -0.0158,
          0.0903, -0.0325, -0.1187,  0.0598,  0.0381,  0.1099,  0.1112,  0.0315,
          0.0939, -0.0432, -0.0814, -0.0796,  0.0775,  0.0186,  0.0144, -0.0528,
         -0.0822,  0.0934,  0.0137, -0.0754, -0.1489, -0.0209, -0.0665, -0.0892,
          0.0218, -0.1425, -0.1422, -0.1255, -0.1033,  0.0758, -0.0762,  0.0987,
         -0.0624, -0.0012, -0.1437,  0.1150, -0.0010,  0.0968, -0.1334,  0.0330,
         -0.1358, -0.0434,  0.0886, -0.0920,  0.1189, -0.0659,  0.0880,  0.1212,
          0.1089,  0.0706,  0.0106, -0.1429, -0.0962,  0.0727,  0.0986, -0.0379,
          0.0719, -0.0210,  0.0073, -0.1213,  0.0942, -0.1324, -0.0632,  0.0581,
         -0.0536,  0.0389,  0.0020, -0.0339, -0.0148,  0.1249,  0.0787,  0.0778,
         -0.1148, -0.1362, -0.1308,  0.1298,  0.0415,  0.0506,  0.1014,  0.1450,
          0.0241, -0.0212,  0.0907, -0.1009,  0.1288,  0.0939, -0.0616, -0.0177,
         -0.0904,  0.0405, -0.0624, -0.1464, -0.0130,  0.1510, -0.1251,  0.0130,
         -0.1472, -0.1317, -0.1517,  0.0159, -0.0633, -0.1482,  0.0410,  0.0088,
         -0.0821,  0.0082,  0.0331, -0.0977,  0.0438,  0.0155,  0.0676, -0.0701,
          0.1283, -0.0301, -0.0464, -0.0894,  0.1207,  0.1255,  0.0927,  0.0483,
          0.1435,  0.1348, -0.0795, -0.1124, -0.0713, -0.1527, -0.0916, -0.0386,
          0.0854, -0.0559, -0.1393,  0.0722,  0.1511,  0.0315,  0.0772,  0.0214,
         -0.0800,  0.0908,  0.0330,  0.0146,  0.0066,  0.0420,  0.1178,  0.0950,
          0.1319,  0.0587, -0.0961,  0.1507, -0.0940,  0.0511,  0.0190, -0.1058,
          0.0168, -0.1302, -0.1368,  0.0305,  0.1200,  0.0684,  0.0630,  0.0833,
         -0.1062,  0.0040, -0.0223,  0.0110,  0.0847,  0.0726, -0.1110,  0.0920,
          0.0789,  0.1154, -0.0999,  0.0557, -0.1000,  0.0036, -0.0655, -0.0691,
          0.0309,  0.0130, -0.0092, -0.0910,  0.1388, -0.0237,  0.1526, -0.1079]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0647, -0.0349, -0.1013,  ..., -0.0880,  0.1151,  0.0984],
        [ 0.0635, -0.0124,  0.0018,  ..., -0.0904, -0.0627,  0.0435],
        [-0.0062, -0.0083, -0.0892,  ..., -0.0420, -0.0373,  0.0100],
        ...,
        [ 0.1227,  0.1204, -0.0320,  ..., -0.0301, -0.0988,  0.0532],
        [ 0.0540, -0.0372,  0.1120,  ...,  0.0462,  0.0066,  0.0914],
        [-0.0327,  0.0465, -0.1006,  ..., -0.0072, -0.0179, -0.1164]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0647, -0.0349, -0.1013,  ..., -0.0880,  0.1151,  0.0984],
        [ 0.0635, -0.0124,  0.0018,  ..., -0.0904, -0.0627,  0.0435],
        [-0.0062, -0.0083, -0.0892,  ..., -0.0420, -0.0373,  0.0100],
        ...,
        [ 0.1227,  0.1204, -0.0320,  ..., -0.0301, -0.0988,  0.0532],
        [ 0.0540, -0.0372,  0.1120,  ...,  0.0462,  0.0066,  0.0914],
        [-0.0327,  0.0465, -0.1006,  ..., -0.0072, -0.0179, -0.1164]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0232, -0.1584, -0.0763,  ..., -0.0740, -0.0198,  0.0404],
        [-0.0222, -0.1140,  0.0397,  ..., -0.1487, -0.0875, -0.0389],
        [-0.0363,  0.1036, -0.1262,  ...,  0.1355,  0.1571, -0.0675],
        ...,
        [-0.0801,  0.1243, -0.0781,  ..., -0.1313, -0.1639,  0.0879],
        [ 0.0349,  0.0559,  0.1250,  ..., -0.0702,  0.0188,  0.0236],
        [ 0.0880, -0.0603, -0.0465,  ...,  0.1241, -0.0145, -0.1080]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0232, -0.1584, -0.0763,  ..., -0.0740, -0.0198,  0.0404],
        [-0.0222, -0.1140,  0.0397,  ..., -0.1487, -0.0875, -0.0389],
        [-0.0363,  0.1036, -0.1262,  ...,  0.1355,  0.1571, -0.0675],
        ...,
        [-0.0801,  0.1243, -0.0781,  ..., -0.1313, -0.1639,  0.0879],
        [ 0.0349,  0.0559,  0.1250,  ..., -0.0702,  0.0188,  0.0236],
        [ 0.0880, -0.0603, -0.0465,  ...,  0.1241, -0.0145, -0.1080]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0856,  0.0497,  0.2056,  ...,  0.1566, -0.0754, -0.0613],
        [-0.2499, -0.0951,  0.0072,  ..., -0.0763, -0.0652,  0.2206],
        [-0.1838, -0.0386, -0.0071,  ...,  0.2488, -0.0226, -0.0310],
        ...,
        [-0.0557,  0.2103,  0.0425,  ...,  0.1542,  0.1369, -0.1468],
        [-0.1101,  0.0119, -0.0502,  ..., -0.0278,  0.0096,  0.1148],
        [ 0.0954,  0.0441,  0.1434,  ...,  0.2230, -0.0515, -0.1224]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0856,  0.0497,  0.2056,  ...,  0.1566, -0.0754, -0.0613],
        [-0.2499, -0.0951,  0.0072,  ..., -0.0763, -0.0652,  0.2206],
        [-0.1838, -0.0386, -0.0071,  ...,  0.2488, -0.0226, -0.0310],
        ...,
        [-0.0557,  0.2103,  0.0425,  ...,  0.1542,  0.1369, -0.1468],
        [-0.1101,  0.0119, -0.0502,  ..., -0.0278,  0.0096,  0.1148],
        [ 0.0954,  0.0441,  0.1434,  ...,  0.2230, -0.0515, -0.1224]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.1984],
        [ 0.4166],
        [-0.2959],
        [-0.2674],
        [ 0.0988],
        [ 0.2183],
        [ 0.0559],
        [ 0.3847],
        [-0.0204],
        [-0.0220],
        [-0.4114],
        [ 0.1930],
        [-0.1860],
        [-0.3812],
        [ 0.1772],
        [ 0.3380],
        [ 0.2750],
        [ 0.2724],
        [-0.0664],
        [ 0.1076],
        [ 0.2163],
        [ 0.3378],
        [ 0.2552],
        [-0.4132],
        [-0.2206],
        [ 0.1814],
        [-0.3290],
        [ 0.0136],
        [ 0.1873],
        [-0.0494],
        [ 0.0638],
        [ 0.1178]], device='cuda:0') 
 Parameter containing:
tensor([[-0.1984],
        [ 0.4166],
        [-0.2959],
        [-0.2674],
        [ 0.0988],
        [ 0.2183],
        [ 0.0559],
        [ 0.3847],
        [-0.0204],
        [-0.0220],
        [-0.4114],
        [ 0.1930],
        [-0.1860],
        [-0.3812],
        [ 0.1772],
        [ 0.3380],
        [ 0.2750],
        [ 0.2724],
        [-0.0664],
        [ 0.1076],
        [ 0.2163],
        [ 0.3378],
        [ 0.2552],
        [-0.4132],
        [-0.2206],
        [ 0.1814],
        [-0.3290],
        [ 0.0136],
        [ 0.1873],
        [-0.0494],
        [ 0.0638],
        [ 0.1178]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0203, -0.1151, -0.0419, -0.0842, -0.0814, -0.1475, -0.0014, -0.0131,
         -0.0432, -0.1047,  0.0049, -0.1193, -0.0637, -0.0628, -0.1028,  0.0841,
         -0.0543,  0.1334, -0.0838, -0.1224,  0.0354,  0.0349,  0.1047,  0.0097,
         -0.0724,  0.0747, -0.0583,  0.0347,  0.0282, -0.0842,  0.1477,  0.0964,
         -0.0462, -0.1063,  0.0355,  0.0279,  0.1235,  0.0286, -0.1436,  0.0041,
         -0.1014, -0.0030, -0.1513,  0.1066,  0.0814, -0.0893, -0.0002,  0.0614,
          0.0022,  0.0690, -0.0647,  0.1073,  0.1009,  0.1126,  0.0504, -0.0577,
          0.0047, -0.0172, -0.1518,  0.0304, -0.0669,  0.0200, -0.0681,  0.1476,
         -0.0719,  0.0607,  0.0156,  0.0507, -0.0287,  0.1251,  0.0267, -0.0267,
         -0.0129,  0.0660, -0.0981,  0.1319,  0.1138, -0.1172,  0.0105,  0.0934,
         -0.0069,  0.0370,  0.0240, -0.0529, -0.1106, -0.1268, -0.0847,  0.0906,
         -0.0643,  0.1502, -0.1146,  0.0051, -0.1151, -0.1497, -0.0410,  0.1097,
          0.0692,  0.0088, -0.1132, -0.0390, -0.0774, -0.0751, -0.1233, -0.0186,
         -0.0995, -0.0566, -0.1284,  0.1228, -0.0237,  0.0881,  0.0734,  0.1092,
          0.0002,  0.0406,  0.0621, -0.0987, -0.0622, -0.1455,  0.1195, -0.0090,
         -0.0735, -0.0674, -0.0118,  0.1197, -0.0905,  0.0249,  0.0547,  0.0843,
          0.1502, -0.1107,  0.0992, -0.0321,  0.0297,  0.1380,  0.0779,  0.0885,
          0.0926, -0.1266,  0.1295, -0.1258,  0.1155,  0.0173, -0.0108, -0.0346,
         -0.0666, -0.1375,  0.0395, -0.0851, -0.0497, -0.0186,  0.1255, -0.0660,
          0.0742, -0.1333, -0.1397, -0.0227,  0.1340, -0.1268, -0.1413, -0.0895,
         -0.0066, -0.0027,  0.0194, -0.0270, -0.0712, -0.0546,  0.0415, -0.0937,
         -0.1436,  0.1375, -0.0028,  0.0108, -0.0843,  0.0608, -0.1253, -0.0284,
          0.0689, -0.0646, -0.0659, -0.0608, -0.1168, -0.0985,  0.1155, -0.0361,
          0.1514,  0.0572, -0.1221, -0.0632,  0.0096,  0.0640, -0.1019,  0.0034,
          0.0276, -0.0520, -0.0209, -0.0350, -0.1137,  0.0147, -0.0940,  0.0818,
         -0.1436,  0.0782, -0.0693,  0.0961,  0.1161, -0.0558,  0.0947, -0.0560,
          0.1518, -0.1190,  0.0362, -0.1509, -0.0534, -0.0746, -0.0188,  0.1069,
          0.1401,  0.1375,  0.0313, -0.1152, -0.0295,  0.0723,  0.0405, -0.1442,
          0.0927,  0.0870, -0.1341,  0.0255,  0.0781,  0.0500,  0.0479,  0.1084,
         -0.1013,  0.0563,  0.1345,  0.0034, -0.1210, -0.0888, -0.0188, -0.1230,
         -0.0344, -0.0562,  0.0290,  0.1216, -0.0444,  0.0943, -0.1040,  0.0807,
         -0.0598, -0.0944, -0.0283, -0.1374,  0.0076, -0.0771, -0.0745,  0.1306]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0203, -0.1151, -0.0419, -0.0842, -0.0814, -0.1475, -0.0014, -0.0131,
         -0.0432, -0.1047,  0.0049, -0.1193, -0.0637, -0.0628, -0.1028,  0.0841,
         -0.0543,  0.1334, -0.0838, -0.1224,  0.0354,  0.0349,  0.1047,  0.0097,
         -0.0724,  0.0747, -0.0583,  0.0347,  0.0282, -0.0842,  0.1477,  0.0964,
         -0.0462, -0.1063,  0.0355,  0.0279,  0.1235,  0.0286, -0.1436,  0.0041,
         -0.1014, -0.0030, -0.1513,  0.1066,  0.0814, -0.0893, -0.0002,  0.0614,
          0.0022,  0.0690, -0.0647,  0.1073,  0.1009,  0.1126,  0.0504, -0.0577,
          0.0047, -0.0172, -0.1518,  0.0304, -0.0669,  0.0200, -0.0681,  0.1476,
         -0.0719,  0.0607,  0.0156,  0.0507, -0.0287,  0.1251,  0.0267, -0.0267,
         -0.0129,  0.0660, -0.0981,  0.1319,  0.1138, -0.1172,  0.0105,  0.0934,
         -0.0069,  0.0370,  0.0240, -0.0529, -0.1106, -0.1268, -0.0847,  0.0906,
         -0.0643,  0.1502, -0.1146,  0.0051, -0.1151, -0.1497, -0.0410,  0.1097,
          0.0692,  0.0088, -0.1132, -0.0390, -0.0774, -0.0751, -0.1233, -0.0186,
         -0.0995, -0.0566, -0.1284,  0.1228, -0.0237,  0.0881,  0.0734,  0.1092,
          0.0002,  0.0406,  0.0621, -0.0987, -0.0622, -0.1455,  0.1195, -0.0090,
         -0.0735, -0.0674, -0.0118,  0.1197, -0.0905,  0.0249,  0.0547,  0.0843,
          0.1502, -0.1107,  0.0992, -0.0321,  0.0297,  0.1380,  0.0779,  0.0885,
          0.0926, -0.1266,  0.1295, -0.1258,  0.1155,  0.0173, -0.0108, -0.0346,
         -0.0666, -0.1375,  0.0395, -0.0851, -0.0497, -0.0186,  0.1255, -0.0660,
          0.0742, -0.1333, -0.1397, -0.0227,  0.1340, -0.1268, -0.1413, -0.0895,
         -0.0066, -0.0027,  0.0194, -0.0270, -0.0712, -0.0546,  0.0415, -0.0937,
         -0.1436,  0.1375, -0.0028,  0.0108, -0.0843,  0.0608, -0.1253, -0.0284,
          0.0689, -0.0646, -0.0659, -0.0608, -0.1168, -0.0985,  0.1155, -0.0361,
          0.1514,  0.0572, -0.1221, -0.0632,  0.0096,  0.0640, -0.1019,  0.0034,
          0.0276, -0.0520, -0.0209, -0.0350, -0.1137,  0.0147, -0.0940,  0.0818,
         -0.1436,  0.0782, -0.0693,  0.0961,  0.1161, -0.0558,  0.0947, -0.0560,
          0.1518, -0.1190,  0.0362, -0.1509, -0.0534, -0.0746, -0.0188,  0.1069,
          0.1401,  0.1375,  0.0313, -0.1152, -0.0295,  0.0723,  0.0405, -0.1442,
          0.0927,  0.0870, -0.1341,  0.0255,  0.0781,  0.0500,  0.0479,  0.1084,
         -0.1013,  0.0563,  0.1345,  0.0034, -0.1210, -0.0888, -0.0188, -0.1230,
         -0.0344, -0.0562,  0.0290,  0.1216, -0.0444,  0.0943, -0.1040,  0.0807,
         -0.0598, -0.0944, -0.0283, -0.1374,  0.0076, -0.0771, -0.0745,  0.1306]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0767, -0.0007, -0.0629,  ...,  0.1217,  0.0144, -0.0507],
        [ 0.0082, -0.0900, -0.0769,  ...,  0.0574,  0.0076, -0.0313],
        [-0.0480, -0.1055,  0.0989,  ..., -0.0178,  0.0435, -0.1015],
        ...,
        [-0.1116,  0.1013,  0.0647,  ...,  0.0366,  0.0991,  0.1086],
        [ 0.1171, -0.0159,  0.1030,  ..., -0.0892, -0.0406,  0.0055],
        [ 0.0846, -0.1211, -0.0260,  ..., -0.0841,  0.0073,  0.0867]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0767, -0.0007, -0.0629,  ...,  0.1217,  0.0144, -0.0507],
        [ 0.0082, -0.0900, -0.0769,  ...,  0.0574,  0.0076, -0.0313],
        [-0.0480, -0.1055,  0.0989,  ..., -0.0178,  0.0435, -0.1015],
        ...,
        [-0.1116,  0.1013,  0.0647,  ...,  0.0366,  0.0991,  0.1086],
        [ 0.1171, -0.0159,  0.1030,  ..., -0.0892, -0.0406,  0.0055],
        [ 0.0846, -0.1211, -0.0260,  ..., -0.0841,  0.0073,  0.0867]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1098,  0.1323, -0.0549,  ...,  0.1762, -0.1476, -0.1711],
        [ 0.1400, -0.1370, -0.0442,  ..., -0.1050, -0.1044, -0.0340],
        [-0.0348, -0.1346,  0.0821,  ...,  0.0662, -0.0550,  0.0649],
        ...,
        [ 0.0850, -0.1730,  0.0122,  ...,  0.0263,  0.0398,  0.0212],
        [-0.0677,  0.0464,  0.0365,  ...,  0.0684, -0.1346,  0.1286],
        [-0.1269, -0.1409,  0.0191,  ..., -0.1255, -0.0325, -0.0925]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1098,  0.1323, -0.0549,  ...,  0.1762, -0.1476, -0.1711],
        [ 0.1400, -0.1370, -0.0442,  ..., -0.1050, -0.1044, -0.0340],
        [-0.0348, -0.1346,  0.0821,  ...,  0.0662, -0.0550,  0.0649],
        ...,
        [ 0.0850, -0.1730,  0.0122,  ...,  0.0263,  0.0398,  0.0212],
        [-0.0677,  0.0464,  0.0365,  ...,  0.0684, -0.1346,  0.1286],
        [-0.1269, -0.1409,  0.0191,  ..., -0.1255, -0.0325, -0.0925]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1498, -0.1127, -0.1713,  ..., -0.2268,  0.0622,  0.1592],
        [ 0.1476, -0.0837, -0.0734,  ...,  0.0260, -0.1638,  0.0431],
        [ 0.1216,  0.1342,  0.2173,  ..., -0.2269, -0.2284, -0.1256],
        ...,
        [-0.1806,  0.1704, -0.2261,  ...,  0.0734, -0.0147,  0.0337],
        [-0.1076,  0.2419, -0.0187,  ...,  0.1587, -0.0496,  0.0534],
        [ 0.0882,  0.0923, -0.1475,  ...,  0.1142, -0.0371,  0.1805]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1498, -0.1127, -0.1713,  ..., -0.2268,  0.0622,  0.1592],
        [ 0.1476, -0.0837, -0.0734,  ...,  0.0260, -0.1638,  0.0431],
        [ 0.1216,  0.1342,  0.2173,  ..., -0.2269, -0.2284, -0.1256],
        ...,
        [-0.1806,  0.1704, -0.2261,  ...,  0.0734, -0.0147,  0.0337],
        [-0.1076,  0.2419, -0.0187,  ...,  0.1587, -0.0496,  0.0534],
        [ 0.0882,  0.0923, -0.1475,  ...,  0.1142, -0.0371,  0.1805]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2092],
        [ 0.1895],
        [-0.3717],
        [-0.3282],
        [-0.3622],
        [-0.2612],
        [-0.1939],
        [ 0.4152],
        [ 0.1237],
        [-0.2711],
        [ 0.1012],
        [-0.1905],
        [-0.1227],
        [-0.0537],
        [ 0.3455],
        [ 0.4066],
        [-0.2114],
        [-0.3437],
        [-0.1410],
        [-0.3360],
        [-0.0780],
        [ 0.0332],
        [ 0.1400],
        [-0.2946],
        [ 0.1398],
        [ 0.2305],
        [-0.1147],
        [-0.4177],
        [-0.2907],
        [-0.2826],
        [-0.0186],
        [ 0.4023]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2092],
        [ 0.1895],
        [-0.3717],
        [-0.3282],
        [-0.3622],
        [-0.2612],
        [-0.1939],
        [ 0.4152],
        [ 0.1237],
        [-0.2711],
        [ 0.1012],
        [-0.1905],
        [-0.1227],
        [-0.0537],
        [ 0.3455],
        [ 0.4066],
        [-0.2114],
        [-0.3437],
        [-0.1410],
        [-0.3360],
        [-0.0780],
        [ 0.0332],
        [ 0.1400],
        [-0.2946],
        [ 0.1398],
        [ 0.2305],
        [-0.1147],
        [-0.4177],
        [-0.2907],
        [-0.2826],
        [-0.0186],
        [ 0.4023]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-9.5913, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.8460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(3.9472, device='cuda:0')



h[100].sum tensor(-1.8930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-1.9428, device='cuda:0')



h[200].sum tensor(2.8046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(2.8784, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2858.4834, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.5045e-04,  ..., 6.0497e-04, 1.2513e-03,
         5.4145e-05],
        [0.0000e+00, 0.0000e+00, 1.3068e-03,  ..., 3.1566e-03, 6.5292e-03,
         2.8252e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(14493.6738, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-12.4149, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(610.9254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(48.8724, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(353.4954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.2787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0462],
        [0.0565],
        [0.0815],
        ...,
        [0.0130],
        [0.0131],
        [0.0103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(1165.2311, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0462],
        [0.0565],
        [0.0815],
        ...,
        [0.0130],
        [0.0131],
        [0.0103]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-107.9491, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0705, device='cuda:0')



h[100].sum tensor(9.6242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5462, device='cuda:0')



h[200].sum tensor(-8.3745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.3066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(14717.7334, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0144, 0.0101, 0.0540,  ..., 0.0111, 0.0000, 0.0000],
        [0.0030, 0.0021, 0.0113,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(81002.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1165.4819, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(81.9971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2069.5989, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(145.6059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-138.1169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1559],
        [-0.0956],
        [-0.0585],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-9154.7686, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0462],
        [0.0565],
        [0.0815],
        ...,
        [0.0130],
        [0.0131],
        [0.0103]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 45, in <module>
    hpmesh = np.array(hpmesh)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 46, in array
    return _core.array(obj, dtype, copy, order, subok, ndmin)
  File "cupy/_core/core.pyx", line 2357, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2381, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2506, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 1473, in cupy._core.core._ndarray_base.__array__
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.

real	1m41.940s
user	0m11.760s
sys	0m14.937s
