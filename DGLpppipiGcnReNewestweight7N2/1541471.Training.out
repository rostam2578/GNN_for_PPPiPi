0: gpu022.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-d5f273be-27af-8c76-0b01-74d509810fbb)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Fri Aug 12 21:49:24 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B5:00.0 Off |                    0 |
| N/A   36C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b67f19e68e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	2m0.668s
user	0m3.526s
sys	0m2.255s
[21:51:23] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-2.0666],
        [ 0.0084],
        [-0.4270],
        ...,
        [ 0.0296],
        [-2.1736],
        [-0.4661]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-64.2250, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-4.1796e-02,  1.5142e-01,  1.2038e-01, -4.0418e-05, -7.3508e-02,
          5.7006e-02, -9.8066e-02, -1.3950e-02,  2.0607e-02,  1.1779e-01,
          5.0766e-02, -6.1132e-02,  4.6518e-02, -1.4887e-01, -2.0085e-02,
          1.2462e-01, -6.3435e-02,  7.4177e-02, -4.8053e-02, -6.0676e-02,
         -5.2305e-02,  8.8703e-02,  9.5804e-02,  4.1764e-02,  1.2418e-01,
          7.2623e-02,  1.0409e-01,  1.2446e-01, -6.2752e-02, -1.5092e-03,
          3.8336e-02,  9.6912e-02,  1.2871e-01, -1.1052e-01, -7.2513e-02,
         -4.9172e-02, -1.4725e-01, -8.1762e-02,  1.1364e-01,  1.3353e-01,
         -1.0817e-01,  8.8822e-02, -1.4271e-02, -1.5683e-02, -1.4384e-01,
          6.4464e-02,  1.3154e-01, -1.0936e-01, -7.8529e-02,  1.3237e-01,
         -3.2592e-02, -7.1598e-02, -2.0388e-02,  8.8371e-02,  1.3304e-01,
          3.2235e-02,  6.3434e-03,  1.4507e-01,  1.3263e-01,  1.1994e-01,
          1.5259e-01, -9.7545e-03,  1.4492e-01, -6.2964e-03,  1.0218e-01,
          3.6770e-02, -5.1029e-02, -1.4476e-01,  2.9871e-02,  2.7697e-02,
          1.5182e-01, -5.5146e-02, -7.6703e-02,  1.5115e-02, -8.0874e-02,
         -7.8670e-02, -2.6466e-02, -1.3240e-02,  1.7977e-02, -1.3384e-01,
         -3.3684e-02, -8.3475e-02,  1.3687e-02, -4.7623e-02, -1.1703e-01,
          1.1338e-01,  6.6012e-02,  9.4212e-02, -1.2057e-01, -1.5102e-01,
          9.6296e-02, -1.3733e-01, -1.0998e-01,  9.8004e-02, -5.6945e-02,
          7.8933e-02,  8.8411e-02, -9.9157e-02, -7.4834e-02,  1.1192e-01,
         -1.0853e-01,  1.0951e-01,  2.3600e-02, -6.7896e-02,  1.2080e-01,
         -1.0762e-01,  7.7223e-02,  8.9829e-02, -1.8069e-02,  8.0552e-02,
         -1.2609e-02, -1.1928e-01, -1.2535e-01, -1.2057e-02,  1.0109e-01,
          1.2452e-02, -1.1318e-01, -1.0360e-01, -1.3707e-01,  1.0207e-01,
         -8.2136e-02,  3.1487e-02,  5.1808e-02,  4.1118e-02, -1.2366e-01,
         -1.2865e-01, -3.4460e-02, -1.4706e-01, -2.7595e-02, -5.8453e-02,
         -7.7004e-02, -4.5347e-02, -1.1018e-02, -1.4724e-01, -1.2681e-01,
         -8.3121e-02, -4.3147e-02,  1.2478e-01,  6.9965e-02, -1.1174e-01,
         -1.1223e-01, -9.1928e-03, -6.9622e-02, -9.6243e-02,  1.3840e-01,
          4.8629e-02, -3.6903e-02, -1.5204e-01, -6.9607e-02, -6.3357e-02,
         -4.8318e-02, -1.2802e-01,  1.5324e-03,  1.4270e-01,  5.2585e-03,
          2.5870e-02,  1.3494e-01,  6.9231e-02, -9.4378e-02,  6.0058e-02,
         -4.6905e-02, -1.2745e-02,  2.3558e-02, -4.8256e-02, -2.5080e-02,
          9.3120e-02,  3.6226e-02,  1.3605e-01,  4.2083e-02,  1.4461e-01,
         -8.5687e-02, -1.2772e-01, -4.2216e-02, -3.4172e-03, -5.5317e-03,
         -1.2065e-01, -7.9593e-02, -1.4278e-01, -1.0802e-01,  7.3371e-02,
         -1.1451e-01,  4.2079e-02,  7.9546e-02,  1.1754e-01,  4.3043e-02,
          1.2311e-01,  1.4611e-01, -9.2397e-02, -1.1730e-01,  6.9576e-02,
         -9.7659e-02, -8.1262e-02,  4.1053e-02, -5.0188e-02,  9.1589e-02,
          1.3723e-01,  1.3900e-01, -1.8425e-02, -2.3962e-03, -5.2309e-03,
          9.5496e-02,  1.1882e-01,  4.2574e-02, -6.6346e-02,  8.2995e-02,
          1.2403e-01, -1.4564e-01, -9.7809e-02, -6.4735e-02, -3.3605e-02,
          1.5082e-01,  1.1160e-01, -1.0519e-01,  1.2783e-01,  9.0052e-02,
         -1.1109e-01,  1.3556e-01, -8.0180e-02,  4.5314e-02,  8.6903e-02,
          1.2433e-01, -5.9943e-02, -1.4057e-01,  1.2759e-01,  1.2107e-01,
         -2.8098e-02,  8.9043e-02,  3.6240e-02,  1.4804e-01, -6.6503e-02,
          8.6714e-02, -1.3030e-01,  3.4122e-02,  1.0653e-01,  1.1789e-02,
         -1.3574e-01,  2.8725e-02,  9.1224e-02, -6.5606e-02, -1.1631e-01,
          3.7545e-02, -1.5158e-01, -5.0878e-02,  3.6286e-02,  4.6241e-02,
         -1.1243e-01,  1.1812e-01,  5.3313e-02,  5.1210e-02, -2.0745e-02,
         -6.0842e-04, -5.2078e-02, -1.3253e-01, -6.4273e-03, -8.5796e-02,
          1.0077e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-4.1796e-02,  1.5142e-01,  1.2038e-01, -4.0418e-05, -7.3508e-02,
          5.7006e-02, -9.8066e-02, -1.3950e-02,  2.0607e-02,  1.1779e-01,
          5.0766e-02, -6.1132e-02,  4.6518e-02, -1.4887e-01, -2.0085e-02,
          1.2462e-01, -6.3435e-02,  7.4177e-02, -4.8053e-02, -6.0676e-02,
         -5.2305e-02,  8.8703e-02,  9.5804e-02,  4.1764e-02,  1.2418e-01,
          7.2623e-02,  1.0409e-01,  1.2446e-01, -6.2752e-02, -1.5092e-03,
          3.8336e-02,  9.6912e-02,  1.2871e-01, -1.1052e-01, -7.2513e-02,
         -4.9172e-02, -1.4725e-01, -8.1762e-02,  1.1364e-01,  1.3353e-01,
         -1.0817e-01,  8.8822e-02, -1.4271e-02, -1.5683e-02, -1.4384e-01,
          6.4464e-02,  1.3154e-01, -1.0936e-01, -7.8529e-02,  1.3237e-01,
         -3.2592e-02, -7.1598e-02, -2.0388e-02,  8.8371e-02,  1.3304e-01,
          3.2235e-02,  6.3434e-03,  1.4507e-01,  1.3263e-01,  1.1994e-01,
          1.5259e-01, -9.7545e-03,  1.4492e-01, -6.2964e-03,  1.0218e-01,
          3.6770e-02, -5.1029e-02, -1.4476e-01,  2.9871e-02,  2.7697e-02,
          1.5182e-01, -5.5146e-02, -7.6703e-02,  1.5115e-02, -8.0874e-02,
         -7.8670e-02, -2.6466e-02, -1.3240e-02,  1.7977e-02, -1.3384e-01,
         -3.3684e-02, -8.3475e-02,  1.3687e-02, -4.7623e-02, -1.1703e-01,
          1.1338e-01,  6.6012e-02,  9.4212e-02, -1.2057e-01, -1.5102e-01,
          9.6296e-02, -1.3733e-01, -1.0998e-01,  9.8004e-02, -5.6945e-02,
          7.8933e-02,  8.8411e-02, -9.9157e-02, -7.4834e-02,  1.1192e-01,
         -1.0853e-01,  1.0951e-01,  2.3600e-02, -6.7896e-02,  1.2080e-01,
         -1.0762e-01,  7.7223e-02,  8.9829e-02, -1.8069e-02,  8.0552e-02,
         -1.2609e-02, -1.1928e-01, -1.2535e-01, -1.2057e-02,  1.0109e-01,
          1.2452e-02, -1.1318e-01, -1.0360e-01, -1.3707e-01,  1.0207e-01,
         -8.2136e-02,  3.1487e-02,  5.1808e-02,  4.1118e-02, -1.2366e-01,
         -1.2865e-01, -3.4460e-02, -1.4706e-01, -2.7595e-02, -5.8453e-02,
         -7.7004e-02, -4.5347e-02, -1.1018e-02, -1.4724e-01, -1.2681e-01,
         -8.3121e-02, -4.3147e-02,  1.2478e-01,  6.9965e-02, -1.1174e-01,
         -1.1223e-01, -9.1928e-03, -6.9622e-02, -9.6243e-02,  1.3840e-01,
          4.8629e-02, -3.6903e-02, -1.5204e-01, -6.9607e-02, -6.3357e-02,
         -4.8318e-02, -1.2802e-01,  1.5324e-03,  1.4270e-01,  5.2585e-03,
          2.5870e-02,  1.3494e-01,  6.9231e-02, -9.4378e-02,  6.0058e-02,
         -4.6905e-02, -1.2745e-02,  2.3558e-02, -4.8256e-02, -2.5080e-02,
          9.3120e-02,  3.6226e-02,  1.3605e-01,  4.2083e-02,  1.4461e-01,
         -8.5687e-02, -1.2772e-01, -4.2216e-02, -3.4172e-03, -5.5317e-03,
         -1.2065e-01, -7.9593e-02, -1.4278e-01, -1.0802e-01,  7.3371e-02,
         -1.1451e-01,  4.2079e-02,  7.9546e-02,  1.1754e-01,  4.3043e-02,
          1.2311e-01,  1.4611e-01, -9.2397e-02, -1.1730e-01,  6.9576e-02,
         -9.7659e-02, -8.1262e-02,  4.1053e-02, -5.0188e-02,  9.1589e-02,
          1.3723e-01,  1.3900e-01, -1.8425e-02, -2.3962e-03, -5.2309e-03,
          9.5496e-02,  1.1882e-01,  4.2574e-02, -6.6346e-02,  8.2995e-02,
          1.2403e-01, -1.4564e-01, -9.7809e-02, -6.4735e-02, -3.3605e-02,
          1.5082e-01,  1.1160e-01, -1.0519e-01,  1.2783e-01,  9.0052e-02,
         -1.1109e-01,  1.3556e-01, -8.0180e-02,  4.5314e-02,  8.6903e-02,
          1.2433e-01, -5.9943e-02, -1.4057e-01,  1.2759e-01,  1.2107e-01,
         -2.8098e-02,  8.9043e-02,  3.6240e-02,  1.4804e-01, -6.6503e-02,
          8.6714e-02, -1.3030e-01,  3.4122e-02,  1.0653e-01,  1.1789e-02,
         -1.3574e-01,  2.8725e-02,  9.1224e-02, -6.5606e-02, -1.1631e-01,
          3.7545e-02, -1.5158e-01, -5.0878e-02,  3.6286e-02,  4.6241e-02,
         -1.1243e-01,  1.1812e-01,  5.3313e-02,  5.1210e-02, -2.0745e-02,
         -6.0842e-04, -5.2078e-02, -1.3253e-01, -6.4273e-03, -8.5796e-02,
          1.0077e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0139, -0.0583, -0.0072,  ...,  0.0274, -0.0411, -0.0404],
        [-0.0688,  0.0241, -0.0016,  ...,  0.0541, -0.0987, -0.0993],
        [ 0.1095,  0.0032, -0.0183,  ..., -0.0652, -0.0861,  0.0140],
        ...,
        [ 0.0673, -0.0538, -0.0593,  ..., -0.0844,  0.0097, -0.0107],
        [ 0.1066,  0.1126,  0.0811,  ...,  0.0189, -0.0566, -0.0168],
        [-0.1112, -0.0652, -0.0243,  ...,  0.1233,  0.0145, -0.0241]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0139, -0.0583, -0.0072,  ...,  0.0274, -0.0411, -0.0404],
        [-0.0688,  0.0241, -0.0016,  ...,  0.0541, -0.0987, -0.0993],
        [ 0.1095,  0.0032, -0.0183,  ..., -0.0652, -0.0861,  0.0140],
        ...,
        [ 0.0673, -0.0538, -0.0593,  ..., -0.0844,  0.0097, -0.0107],
        [ 0.1066,  0.1126,  0.0811,  ...,  0.0189, -0.0566, -0.0168],
        [-0.1112, -0.0652, -0.0243,  ...,  0.1233,  0.0145, -0.0241]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1442, -0.1145, -0.1745,  ..., -0.1205, -0.0929, -0.0499],
        [-0.1264, -0.1621,  0.0484,  ...,  0.1580, -0.0599, -0.0661],
        [-0.1722,  0.1345,  0.0892,  ..., -0.0648, -0.0225,  0.0596],
        ...,
        [ 0.0032, -0.0907,  0.1106,  ..., -0.1000, -0.1036,  0.1170],
        [-0.1121,  0.0046, -0.0905,  ..., -0.1178, -0.0934, -0.1486],
        [ 0.0296,  0.1713, -0.0447,  ...,  0.1289, -0.0848,  0.0431]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1442, -0.1145, -0.1745,  ..., -0.1205, -0.0929, -0.0499],
        [-0.1264, -0.1621,  0.0484,  ...,  0.1580, -0.0599, -0.0661],
        [-0.1722,  0.1345,  0.0892,  ..., -0.0648, -0.0225,  0.0596],
        ...,
        [ 0.0032, -0.0907,  0.1106,  ..., -0.1000, -0.1036,  0.1170],
        [-0.1121,  0.0046, -0.0905,  ..., -0.1178, -0.0934, -0.1486],
        [ 0.0296,  0.1713, -0.0447,  ...,  0.1289, -0.0848,  0.0431]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.2407, -0.0727,  0.2183,  ...,  0.1218, -0.0984,  0.1158],
        [ 0.2273, -0.0775, -0.0986,  ...,  0.0558, -0.2367,  0.1452],
        [-0.1016, -0.1225,  0.2300,  ..., -0.1229, -0.2394, -0.0013],
        ...,
        [-0.0735, -0.0448,  0.1478,  ..., -0.1065,  0.0847,  0.0119],
        [ 0.0968,  0.2455,  0.2083,  ..., -0.1757, -0.0558,  0.2162],
        [ 0.1180,  0.1066, -0.0378,  ...,  0.0295, -0.0073,  0.1672]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.2407, -0.0727,  0.2183,  ...,  0.1218, -0.0984,  0.1158],
        [ 0.2273, -0.0775, -0.0986,  ...,  0.0558, -0.2367,  0.1452],
        [-0.1016, -0.1225,  0.2300,  ..., -0.1229, -0.2394, -0.0013],
        ...,
        [-0.0735, -0.0448,  0.1478,  ..., -0.1065,  0.0847,  0.0119],
        [ 0.0968,  0.2455,  0.2083,  ..., -0.1757, -0.0558,  0.2162],
        [ 0.1180,  0.1066, -0.0378,  ...,  0.0295, -0.0073,  0.1672]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2602],
        [ 0.2755],
        [ 0.2297],
        [ 0.1684],
        [ 0.0306],
        [ 0.2713],
        [-0.4060],
        [ 0.0424],
        [ 0.1362],
        [ 0.0699],
        [-0.2715],
        [ 0.1738],
        [-0.4162],
        [-0.0935],
        [-0.1184],
        [ 0.3895],
        [ 0.2230],
        [ 0.0962],
        [-0.1661],
        [ 0.1448],
        [ 0.3551],
        [-0.0306],
        [ 0.4132],
        [ 0.2120],
        [-0.2419],
        [ 0.2296],
        [ 0.3806],
        [ 0.1560],
        [ 0.0172],
        [-0.2732],
        [-0.1458],
        [-0.1091]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2602],
        [ 0.2755],
        [ 0.2297],
        [ 0.1684],
        [ 0.0306],
        [ 0.2713],
        [-0.4060],
        [ 0.0424],
        [ 0.1362],
        [ 0.0699],
        [-0.2715],
        [ 0.1738],
        [-0.4162],
        [-0.0935],
        [-0.1184],
        [ 0.3895],
        [ 0.2230],
        [ 0.0962],
        [-0.1661],
        [ 0.1448],
        [ 0.3551],
        [-0.0306],
        [ 0.4132],
        [ 0.2120],
        [-0.2419],
        [ 0.2296],
        [ 0.3806],
        [ 0.1560],
        [ 0.0172],
        [-0.2732],
        [-0.1458],
        [-0.1091]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0358, -0.1267,  0.1241,  0.1089,  0.0106, -0.0399,  0.0943,  0.0453,
         -0.0689, -0.1407,  0.1500, -0.0245,  0.1407,  0.0210,  0.0328, -0.1198,
          0.0762,  0.1003, -0.0938,  0.0926, -0.0203, -0.0030,  0.0836,  0.0966,
         -0.0115, -0.1213,  0.0787, -0.1055, -0.1379, -0.1018,  0.0571, -0.0400,
         -0.0812, -0.0734,  0.0764,  0.0757, -0.0730, -0.0240, -0.0397, -0.0756,
         -0.1204, -0.0716,  0.0677,  0.0130,  0.0497, -0.0885, -0.0097,  0.0901,
         -0.0320, -0.0927,  0.0464, -0.0743, -0.1210, -0.0012,  0.0885,  0.0810,
          0.0018, -0.0489, -0.0518,  0.1470, -0.0043, -0.0081, -0.0883,  0.0834,
         -0.0676, -0.0265,  0.0794, -0.0665, -0.0263, -0.1113,  0.0059, -0.0999,
          0.1361, -0.0767, -0.0501, -0.0715,  0.0498,  0.0910, -0.1083,  0.1149,
          0.0460,  0.0385, -0.0394, -0.0263,  0.0890, -0.1339, -0.0930,  0.0677,
          0.0335, -0.1271, -0.0329, -0.0337, -0.0337, -0.0546,  0.0435, -0.0302,
         -0.0200,  0.0081, -0.0891, -0.0859, -0.1217,  0.0328, -0.0833,  0.1511,
          0.1058, -0.0948,  0.0849,  0.0319,  0.0719,  0.0959,  0.1305, -0.0684,
          0.1163,  0.0366,  0.0953, -0.1435,  0.1305, -0.0597, -0.1298, -0.0998,
         -0.0934,  0.0643,  0.0391,  0.0517,  0.0991,  0.0589,  0.0148, -0.0404,
          0.0749, -0.1012,  0.0218,  0.0993,  0.0380,  0.1318,  0.1117, -0.1432,
         -0.1249,  0.0996,  0.0876, -0.0005, -0.1303, -0.0580, -0.0126,  0.1249,
          0.0973,  0.1326,  0.1150, -0.0750,  0.1456, -0.0791,  0.0437,  0.1180,
         -0.0062,  0.0285,  0.0737,  0.1385,  0.1079, -0.0556, -0.0763,  0.0490,
          0.0267, -0.0466, -0.0036,  0.0774,  0.1377,  0.0721, -0.1079,  0.1425,
         -0.1270, -0.0031, -0.0135, -0.1108,  0.1179, -0.0269,  0.1103, -0.0519,
          0.1337, -0.0942,  0.0202,  0.0297,  0.1249, -0.0936,  0.1360,  0.1001,
         -0.1240,  0.1457, -0.1368, -0.1217, -0.0316, -0.0211, -0.0044,  0.0595,
         -0.0368,  0.0037,  0.1383,  0.0342,  0.0640, -0.1255,  0.1433, -0.1400,
         -0.0128, -0.1383, -0.0185,  0.1125,  0.0069, -0.0275,  0.0945,  0.0757,
          0.1067,  0.1381,  0.0847, -0.0084, -0.0378, -0.1130, -0.1116,  0.0169,
         -0.0493, -0.0143, -0.0277,  0.0590, -0.1505, -0.1044,  0.0515,  0.1472,
         -0.0897,  0.0180,  0.1473, -0.0834,  0.0677, -0.0267,  0.0591,  0.0442,
         -0.0456, -0.1099,  0.0494, -0.1094,  0.0944,  0.1352,  0.0100, -0.0537,
          0.1512, -0.1452, -0.0352,  0.0768, -0.0970, -0.0950,  0.0280,  0.0470,
         -0.0434, -0.0325, -0.0275, -0.0056,  0.1387,  0.0308, -0.0628,  0.0998]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0358, -0.1267,  0.1241,  0.1089,  0.0106, -0.0399,  0.0943,  0.0453,
         -0.0689, -0.1407,  0.1500, -0.0245,  0.1407,  0.0210,  0.0328, -0.1198,
          0.0762,  0.1003, -0.0938,  0.0926, -0.0203, -0.0030,  0.0836,  0.0966,
         -0.0115, -0.1213,  0.0787, -0.1055, -0.1379, -0.1018,  0.0571, -0.0400,
         -0.0812, -0.0734,  0.0764,  0.0757, -0.0730, -0.0240, -0.0397, -0.0756,
         -0.1204, -0.0716,  0.0677,  0.0130,  0.0497, -0.0885, -0.0097,  0.0901,
         -0.0320, -0.0927,  0.0464, -0.0743, -0.1210, -0.0012,  0.0885,  0.0810,
          0.0018, -0.0489, -0.0518,  0.1470, -0.0043, -0.0081, -0.0883,  0.0834,
         -0.0676, -0.0265,  0.0794, -0.0665, -0.0263, -0.1113,  0.0059, -0.0999,
          0.1361, -0.0767, -0.0501, -0.0715,  0.0498,  0.0910, -0.1083,  0.1149,
          0.0460,  0.0385, -0.0394, -0.0263,  0.0890, -0.1339, -0.0930,  0.0677,
          0.0335, -0.1271, -0.0329, -0.0337, -0.0337, -0.0546,  0.0435, -0.0302,
         -0.0200,  0.0081, -0.0891, -0.0859, -0.1217,  0.0328, -0.0833,  0.1511,
          0.1058, -0.0948,  0.0849,  0.0319,  0.0719,  0.0959,  0.1305, -0.0684,
          0.1163,  0.0366,  0.0953, -0.1435,  0.1305, -0.0597, -0.1298, -0.0998,
         -0.0934,  0.0643,  0.0391,  0.0517,  0.0991,  0.0589,  0.0148, -0.0404,
          0.0749, -0.1012,  0.0218,  0.0993,  0.0380,  0.1318,  0.1117, -0.1432,
         -0.1249,  0.0996,  0.0876, -0.0005, -0.1303, -0.0580, -0.0126,  0.1249,
          0.0973,  0.1326,  0.1150, -0.0750,  0.1456, -0.0791,  0.0437,  0.1180,
         -0.0062,  0.0285,  0.0737,  0.1385,  0.1079, -0.0556, -0.0763,  0.0490,
          0.0267, -0.0466, -0.0036,  0.0774,  0.1377,  0.0721, -0.1079,  0.1425,
         -0.1270, -0.0031, -0.0135, -0.1108,  0.1179, -0.0269,  0.1103, -0.0519,
          0.1337, -0.0942,  0.0202,  0.0297,  0.1249, -0.0936,  0.1360,  0.1001,
         -0.1240,  0.1457, -0.1368, -0.1217, -0.0316, -0.0211, -0.0044,  0.0595,
         -0.0368,  0.0037,  0.1383,  0.0342,  0.0640, -0.1255,  0.1433, -0.1400,
         -0.0128, -0.1383, -0.0185,  0.1125,  0.0069, -0.0275,  0.0945,  0.0757,
          0.1067,  0.1381,  0.0847, -0.0084, -0.0378, -0.1130, -0.1116,  0.0169,
         -0.0493, -0.0143, -0.0277,  0.0590, -0.1505, -0.1044,  0.0515,  0.1472,
         -0.0897,  0.0180,  0.1473, -0.0834,  0.0677, -0.0267,  0.0591,  0.0442,
         -0.0456, -0.1099,  0.0494, -0.1094,  0.0944,  0.1352,  0.0100, -0.0537,
          0.1512, -0.1452, -0.0352,  0.0768, -0.0970, -0.0950,  0.0280,  0.0470,
         -0.0434, -0.0325, -0.0275, -0.0056,  0.1387,  0.0308, -0.0628,  0.0998]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0440,  0.0156, -0.0245,  ...,  0.1205, -0.0072,  0.0612],
        [-0.1161,  0.0804,  0.0425,  ..., -0.0681,  0.0097,  0.0672],
        [-0.0590,  0.1040,  0.0727,  ..., -0.0008, -0.0304,  0.1109],
        ...,
        [ 0.0412, -0.0752,  0.0930,  ...,  0.1172,  0.0230,  0.1047],
        [ 0.1016,  0.0475, -0.0716,  ..., -0.1223,  0.0240, -0.0917],
        [ 0.0687,  0.1118,  0.1055,  ..., -0.1206,  0.0296,  0.1096]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0440,  0.0156, -0.0245,  ...,  0.1205, -0.0072,  0.0612],
        [-0.1161,  0.0804,  0.0425,  ..., -0.0681,  0.0097,  0.0672],
        [-0.0590,  0.1040,  0.0727,  ..., -0.0008, -0.0304,  0.1109],
        ...,
        [ 0.0412, -0.0752,  0.0930,  ...,  0.1172,  0.0230,  0.1047],
        [ 0.1016,  0.0475, -0.0716,  ..., -0.1223,  0.0240, -0.0917],
        [ 0.0687,  0.1118,  0.1055,  ..., -0.1206,  0.0296,  0.1096]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.1639, -0.1087, -0.1264,  ...,  0.1119, -0.0309,  0.0522],
        [ 0.0167, -0.1268, -0.0967,  ..., -0.1166, -0.1276,  0.0174],
        [-0.0039, -0.0537,  0.0199,  ...,  0.1101,  0.1533,  0.0701],
        ...,
        [-0.0923,  0.0082,  0.0240,  ..., -0.0950, -0.0531, -0.0070],
        [ 0.0547,  0.1613,  0.0144,  ...,  0.1351,  0.0810, -0.1572],
        [-0.1175,  0.0545, -0.0857,  ..., -0.1662,  0.0757,  0.0326]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1639, -0.1087, -0.1264,  ...,  0.1119, -0.0309,  0.0522],
        [ 0.0167, -0.1268, -0.0967,  ..., -0.1166, -0.1276,  0.0174],
        [-0.0039, -0.0537,  0.0199,  ...,  0.1101,  0.1533,  0.0701],
        ...,
        [-0.0923,  0.0082,  0.0240,  ..., -0.0950, -0.0531, -0.0070],
        [ 0.0547,  0.1613,  0.0144,  ...,  0.1351,  0.0810, -0.1572],
        [-0.1175,  0.0545, -0.0857,  ..., -0.1662,  0.0757,  0.0326]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0400,  0.2353, -0.1610,  ..., -0.1469, -0.2394,  0.1294],
        [-0.0356, -0.2373,  0.0158,  ...,  0.0632, -0.1666,  0.1761],
        [-0.2488,  0.1292, -0.0991,  ..., -0.1844,  0.1964,  0.1071],
        ...,
        [-0.0288,  0.1190,  0.0273,  ..., -0.1623,  0.2474, -0.1621],
        [ 0.2325, -0.2263,  0.1955,  ...,  0.1250, -0.2232, -0.1338],
        [-0.1597, -0.1879,  0.2025,  ..., -0.0538,  0.0498, -0.2153]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0400,  0.2353, -0.1610,  ..., -0.1469, -0.2394,  0.1294],
        [-0.0356, -0.2373,  0.0158,  ...,  0.0632, -0.1666,  0.1761],
        [-0.2488,  0.1292, -0.0991,  ..., -0.1844,  0.1964,  0.1071],
        ...,
        [-0.0288,  0.1190,  0.0273,  ..., -0.1623,  0.2474, -0.1621],
        [ 0.2325, -0.2263,  0.1955,  ...,  0.1250, -0.2232, -0.1338],
        [-0.1597, -0.1879,  0.2025,  ..., -0.0538,  0.0498, -0.2153]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3934],
        [ 0.0991],
        [ 0.3892],
        [ 0.2665],
        [ 0.1608],
        [-0.0178],
        [-0.3316],
        [-0.1877],
        [-0.0511],
        [-0.1727],
        [ 0.2537],
        [-0.2835],
        [-0.2175],
        [-0.4253],
        [ 0.3883],
        [-0.3969],
        [ 0.3365],
        [-0.2452],
        [-0.3178],
        [-0.4067],
        [ 0.1447],
        [ 0.1855],
        [ 0.2973],
        [ 0.1476],
        [-0.1889],
        [-0.4091],
        [ 0.2949],
        [ 0.2281],
        [-0.1139],
        [-0.3851],
        [-0.2637],
        [-0.1359]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3934],
        [ 0.0991],
        [ 0.3892],
        [ 0.2665],
        [ 0.1608],
        [-0.0178],
        [-0.3316],
        [-0.1877],
        [-0.0511],
        [-0.1727],
        [ 0.2537],
        [-0.2835],
        [-0.2175],
        [-0.4253],
        [ 0.3883],
        [-0.3969],
        [ 0.3365],
        [-0.2452],
        [-0.3178],
        [-0.4067],
        [ 0.1447],
        [ 0.1855],
        [ 0.2973],
        [ 0.1476],
        [-0.1889],
        [-0.4091],
        [ 0.2949],
        [ 0.2281],
        [-0.1139],
        [-0.3851],
        [-0.2637],
        [-0.1359]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-27.5411, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.9233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.9476, device='cuda:0')



h[100].sum tensor(-3.7533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.8521, device='cuda:0')



h[200].sum tensor(-3.8586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-3.9601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2787.9597, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8514e-04, 0.0000e+00,  ..., 0.0000e+00, 9.6240e-05,
         4.3844e-04],
        [0.0000e+00, 2.0096e-03, 0.0000e+00,  ..., 0.0000e+00, 5.0216e-04,
         2.2877e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(15158.6855, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-33.6406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(206.8249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(16.5454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-26.7563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0102],
        [-0.0124],
        [-0.0179],
        ...,
        [-0.0029],
        [-0.0029],
        [-0.0023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-256.3112, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.0102],
        [-0.0124],
        [-0.0179],
        ...,
        [-0.0029],
        [-0.0029],
        [-0.0023]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-135.4377, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9886, device='cuda:0')



h[100].sum tensor(-9.4715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.3948, device='cuda:0')



h[200].sum tensor(-13.0839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-12.9779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(14016.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0143, 0.0179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(68520.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-90.1207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3064.0640, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(215.5711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1277.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(89.8885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1652],
        [-0.1013],
        [-0.0620],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-9701.9023, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0102],
        [-0.0124],
        [-0.0179],
        ...,
        [-0.0029],
        [-0.0029],
        [-0.0023]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



load_model False 
TraEvN 10001 
BatchSize 30 
EpochNum 5 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 
startmesh 284 
endmesh 285 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-2.6664e-03,  3.8644e-02, -1.3687e-01,  1.2266e-01, -1.0474e-01,
         -2.3392e-02, -1.2854e-01,  1.0943e-01,  1.1584e-01,  1.1390e-01,
         -7.5646e-02,  5.7121e-02, -1.2723e-01,  8.9903e-02, -4.5657e-02,
         -1.2451e-01, -5.1709e-02, -6.6333e-02,  5.4502e-02, -1.2104e-01,
         -5.7485e-03,  5.0183e-02, -1.1610e-01,  3.5915e-02, -1.0012e-01,
          1.1200e-01,  7.6753e-02,  1.2758e-01,  1.8560e-02, -7.8992e-02,
         -1.9463e-02,  1.3991e-01, -6.9995e-02, -1.4514e-01, -3.1268e-02,
          2.9435e-02, -1.0019e-01,  3.8387e-02,  1.3252e-01,  1.8935e-02,
          5.1353e-02, -6.7764e-02,  6.6239e-02, -1.3145e-01, -3.7837e-02,
         -1.2310e-01, -3.9064e-02, -6.8005e-02, -2.3911e-02,  4.6267e-02,
         -6.9021e-02, -9.0955e-03,  6.2248e-02, -1.4661e-01, -5.7869e-02,
         -5.5670e-02,  8.4208e-02, -5.4088e-02, -1.1409e-01, -1.4683e-01,
         -9.5577e-02,  5.9888e-04, -1.3819e-01,  3.6600e-03,  2.8469e-02,
         -1.5975e-02,  4.0155e-02,  8.8752e-02,  1.3935e-01, -6.6856e-02,
         -1.0714e-02,  8.0260e-03,  5.2531e-02, -1.0205e-01, -3.2338e-02,
         -1.1193e-01,  1.0012e-01, -1.1769e-01, -7.2511e-02,  1.2602e-01,
          1.9764e-03, -2.4048e-02, -6.8004e-02,  7.3689e-02,  8.6505e-02,
         -9.0740e-02, -1.4228e-01, -3.9721e-02, -2.9083e-02,  1.2967e-01,
          1.1133e-01,  1.0398e-01, -8.9955e-02,  9.6115e-02,  9.6327e-03,
         -1.9869e-02, -2.2137e-02, -2.9675e-02, -1.3726e-01, -1.1123e-01,
         -5.5337e-02,  1.1664e-01,  5.6381e-02,  1.7232e-02,  1.2685e-01,
         -9.9337e-02, -7.4938e-02, -4.9271e-02, -8.6240e-03,  9.2985e-02,
          1.2195e-01,  3.3033e-02, -1.3363e-02,  1.0329e-01, -1.0063e-01,
         -5.2309e-02, -1.1162e-01, -5.9206e-02, -1.5462e-02,  4.1061e-02,
         -3.3035e-02, -4.9318e-02,  1.0411e-01,  7.2325e-02, -1.2569e-01,
         -3.0470e-02, -3.5986e-02,  1.3837e-01, -1.2320e-01,  2.8994e-02,
         -2.8382e-02,  4.9812e-02,  1.5012e-01, -4.7169e-02, -8.0310e-02,
         -7.3380e-02, -5.1698e-02,  4.1914e-02, -1.1898e-01,  7.7789e-02,
          8.1318e-02,  1.2110e-01,  7.1195e-02, -1.4758e-01, -6.9766e-02,
          4.3803e-02, -3.9603e-02,  5.1089e-02,  5.0244e-02, -1.1972e-01,
         -6.7046e-02,  4.6834e-02, -5.4793e-02, -5.1428e-03, -7.1078e-02,
          1.0446e-01,  2.2417e-02,  3.6138e-03, -3.0435e-02,  6.7166e-02,
          1.0024e-01, -7.9814e-02, -8.2008e-02,  1.1524e-01, -5.4170e-02,
          3.1774e-03,  9.1141e-02,  1.8074e-02, -8.0375e-02, -1.2840e-01,
         -1.2116e-01,  1.5106e-01, -1.3893e-01, -1.1287e-01, -3.8848e-02,
         -5.5827e-02, -1.2982e-01, -6.0394e-02, -5.9008e-02, -1.4778e-02,
          3.2016e-02, -1.1472e-01, -2.6230e-02,  7.8540e-02, -8.7101e-02,
         -1.0386e-01,  2.0701e-02, -3.8103e-02, -6.0642e-02, -6.1872e-02,
          1.2203e-01, -2.0700e-03,  1.1672e-01,  6.1606e-02,  1.1799e-01,
          1.3191e-01, -4.7037e-02,  4.2403e-02,  1.1679e-02, -9.9731e-02,
          6.4178e-02, -1.0741e-01,  1.2028e-01, -3.2859e-05,  1.4680e-01,
         -5.6358e-02,  5.3886e-02,  1.0996e-01, -1.7636e-02, -6.9447e-02,
          6.8915e-02,  4.6264e-02, -5.3519e-02,  1.1040e-02, -1.3321e-02,
          1.6082e-02,  8.3448e-02,  9.5224e-02, -1.1884e-01,  2.1815e-02,
          1.3310e-01, -8.3617e-03,  1.0250e-01,  1.2405e-02, -1.2073e-01,
         -8.2887e-02, -9.1213e-02,  5.5650e-02,  1.2013e-01, -6.2612e-02,
         -3.6416e-03,  1.2990e-01, -1.0928e-01,  8.8871e-02, -2.8697e-02,
         -4.2639e-02,  1.9116e-02, -1.4369e-01, -5.7388e-02,  1.2343e-01,
          7.2237e-02, -5.2787e-02, -5.8901e-02,  5.2757e-04, -1.3031e-01,
          4.8420e-02, -8.9540e-02,  9.9273e-02, -1.7425e-02, -8.7863e-02,
          1.4287e-01,  1.3555e-01, -3.1532e-02,  4.1904e-02, -1.0398e-01,
         -1.3912e-01]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-8.1813e-02, -7.5916e-03,  5.1359e-03,  ..., -9.7796e-05,
          2.8649e-02,  3.1180e-03],
        [ 1.0893e-01, -7.4385e-02,  1.3421e-02,  ...,  7.6494e-02,
         -1.8905e-02,  4.8579e-02],
        [ 9.3738e-02,  8.6971e-02, -5.7982e-02,  ..., -6.2529e-02,
          5.7189e-02,  7.9929e-02],
        ...,
        [-6.5810e-02,  3.6103e-04, -8.4208e-02,  ...,  8.2032e-02,
         -3.3142e-02, -8.3316e-02],
        [-1.1053e-01,  7.0280e-02, -2.1805e-02,  ..., -7.4792e-02,
          7.0259e-02, -7.9215e-02],
        [-1.6976e-02,  1.1562e-01,  1.2364e-01,  ...,  3.4179e-02,
         -6.1157e-02,  3.7987e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0998, -0.0204, -0.0837,  ...,  0.0873, -0.0186, -0.0332],
        [ 0.1606,  0.0991,  0.1672,  ..., -0.0290, -0.0360, -0.1672],
        [-0.0600,  0.0184, -0.0768,  ...,  0.0370,  0.0306, -0.0363],
        ...,
        [-0.1653,  0.1287, -0.0224,  ...,  0.1474,  0.0228,  0.1716],
        [ 0.0806,  0.1646, -0.1545,  ...,  0.1164, -0.1060,  0.1458],
        [ 0.0535, -0.0907,  0.1229,  ..., -0.1740,  0.0746, -0.0396]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1459, -0.0781, -0.1910,  ...,  0.0159, -0.2029, -0.0300],
        [ 0.0566, -0.2466,  0.2287,  ...,  0.1932,  0.0568, -0.0904],
        [-0.1819, -0.2362, -0.0250,  ..., -0.0696,  0.1546,  0.1870],
        ...,
        [-0.0040,  0.0785, -0.1732,  ..., -0.2006, -0.0936, -0.2313],
        [-0.1004, -0.2056,  0.0502,  ..., -0.1058,  0.0453, -0.0443],
        [-0.0953, -0.0328,  0.0641,  ..., -0.2253,  0.0044,  0.0484]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4212],
        [-0.1942],
        [-0.1183],
        [-0.2854],
        [ 0.2535],
        [ 0.0892],
        [-0.0911],
        [-0.0330],
        [ 0.2788],
        [-0.1949],
        [-0.0483],
        [ 0.3558],
        [ 0.2891],
        [-0.0649],
        [ 0.2676],
        [ 0.0110],
        [ 0.3874],
        [ 0.2129],
        [-0.1891],
        [-0.3436],
        [ 0.0962],
        [-0.2255],
        [ 0.2564],
        [-0.1224],
        [ 0.0700],
        [-0.0925],
        [ 0.4040],
        [ 0.0901],
        [ 0.0047],
        [ 0.1599],
        [ 0.3670],
        [-0.0490]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-2.6664e-03,  3.8644e-02, -1.3687e-01,  1.2266e-01, -1.0474e-01,
         -2.3392e-02, -1.2854e-01,  1.0943e-01,  1.1584e-01,  1.1390e-01,
         -7.5646e-02,  5.7121e-02, -1.2723e-01,  8.9903e-02, -4.5657e-02,
         -1.2451e-01, -5.1709e-02, -6.6333e-02,  5.4502e-02, -1.2104e-01,
         -5.7485e-03,  5.0183e-02, -1.1610e-01,  3.5915e-02, -1.0012e-01,
          1.1200e-01,  7.6753e-02,  1.2758e-01,  1.8560e-02, -7.8992e-02,
         -1.9463e-02,  1.3991e-01, -6.9995e-02, -1.4514e-01, -3.1268e-02,
          2.9435e-02, -1.0019e-01,  3.8387e-02,  1.3252e-01,  1.8935e-02,
          5.1353e-02, -6.7764e-02,  6.6239e-02, -1.3145e-01, -3.7837e-02,
         -1.2310e-01, -3.9064e-02, -6.8005e-02, -2.3911e-02,  4.6267e-02,
         -6.9021e-02, -9.0955e-03,  6.2248e-02, -1.4661e-01, -5.7869e-02,
         -5.5670e-02,  8.4208e-02, -5.4088e-02, -1.1409e-01, -1.4683e-01,
         -9.5577e-02,  5.9888e-04, -1.3819e-01,  3.6600e-03,  2.8469e-02,
         -1.5975e-02,  4.0155e-02,  8.8752e-02,  1.3935e-01, -6.6856e-02,
         -1.0714e-02,  8.0260e-03,  5.2531e-02, -1.0205e-01, -3.2338e-02,
         -1.1193e-01,  1.0012e-01, -1.1769e-01, -7.2511e-02,  1.2602e-01,
          1.9764e-03, -2.4048e-02, -6.8004e-02,  7.3689e-02,  8.6505e-02,
         -9.0740e-02, -1.4228e-01, -3.9721e-02, -2.9083e-02,  1.2967e-01,
          1.1133e-01,  1.0398e-01, -8.9955e-02,  9.6115e-02,  9.6327e-03,
         -1.9869e-02, -2.2137e-02, -2.9675e-02, -1.3726e-01, -1.1123e-01,
         -5.5337e-02,  1.1664e-01,  5.6381e-02,  1.7232e-02,  1.2685e-01,
         -9.9337e-02, -7.4938e-02, -4.9271e-02, -8.6240e-03,  9.2985e-02,
          1.2195e-01,  3.3033e-02, -1.3363e-02,  1.0329e-01, -1.0063e-01,
         -5.2309e-02, -1.1162e-01, -5.9206e-02, -1.5462e-02,  4.1061e-02,
         -3.3035e-02, -4.9318e-02,  1.0411e-01,  7.2325e-02, -1.2569e-01,
         -3.0470e-02, -3.5986e-02,  1.3837e-01, -1.2320e-01,  2.8994e-02,
         -2.8382e-02,  4.9812e-02,  1.5012e-01, -4.7169e-02, -8.0310e-02,
         -7.3380e-02, -5.1698e-02,  4.1914e-02, -1.1898e-01,  7.7789e-02,
          8.1318e-02,  1.2110e-01,  7.1195e-02, -1.4758e-01, -6.9766e-02,
          4.3803e-02, -3.9603e-02,  5.1089e-02,  5.0244e-02, -1.1972e-01,
         -6.7046e-02,  4.6834e-02, -5.4793e-02, -5.1428e-03, -7.1078e-02,
          1.0446e-01,  2.2417e-02,  3.6138e-03, -3.0435e-02,  6.7166e-02,
          1.0024e-01, -7.9814e-02, -8.2008e-02,  1.1524e-01, -5.4170e-02,
          3.1774e-03,  9.1141e-02,  1.8074e-02, -8.0375e-02, -1.2840e-01,
         -1.2116e-01,  1.5106e-01, -1.3893e-01, -1.1287e-01, -3.8848e-02,
         -5.5827e-02, -1.2982e-01, -6.0394e-02, -5.9008e-02, -1.4778e-02,
          3.2016e-02, -1.1472e-01, -2.6230e-02,  7.8540e-02, -8.7101e-02,
         -1.0386e-01,  2.0701e-02, -3.8103e-02, -6.0642e-02, -6.1872e-02,
          1.2203e-01, -2.0700e-03,  1.1672e-01,  6.1606e-02,  1.1799e-01,
          1.3191e-01, -4.7037e-02,  4.2403e-02,  1.1679e-02, -9.9731e-02,
          6.4178e-02, -1.0741e-01,  1.2028e-01, -3.2859e-05,  1.4680e-01,
         -5.6358e-02,  5.3886e-02,  1.0996e-01, -1.7636e-02, -6.9447e-02,
          6.8915e-02,  4.6264e-02, -5.3519e-02,  1.1040e-02, -1.3321e-02,
          1.6082e-02,  8.3448e-02,  9.5224e-02, -1.1884e-01,  2.1815e-02,
          1.3310e-01, -8.3617e-03,  1.0250e-01,  1.2405e-02, -1.2073e-01,
         -8.2887e-02, -9.1213e-02,  5.5650e-02,  1.2013e-01, -6.2612e-02,
         -3.6416e-03,  1.2990e-01, -1.0928e-01,  8.8871e-02, -2.8697e-02,
         -4.2639e-02,  1.9116e-02, -1.4369e-01, -5.7388e-02,  1.2343e-01,
          7.2237e-02, -5.2787e-02, -5.8901e-02,  5.2757e-04, -1.3031e-01,
          4.8420e-02, -8.9540e-02,  9.9273e-02, -1.7425e-02, -8.7863e-02,
          1.4287e-01,  1.3555e-01, -3.1532e-02,  4.1904e-02, -1.0398e-01,
         -1.3912e-01]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-8.1813e-02, -7.5916e-03,  5.1359e-03,  ..., -9.7796e-05,
          2.8649e-02,  3.1180e-03],
        [ 1.0893e-01, -7.4385e-02,  1.3421e-02,  ...,  7.6494e-02,
         -1.8905e-02,  4.8579e-02],
        [ 9.3738e-02,  8.6971e-02, -5.7982e-02,  ..., -6.2529e-02,
          5.7189e-02,  7.9929e-02],
        ...,
        [-6.5810e-02,  3.6103e-04, -8.4208e-02,  ...,  8.2032e-02,
         -3.3142e-02, -8.3316e-02],
        [-1.1053e-01,  7.0280e-02, -2.1805e-02,  ..., -7.4792e-02,
          7.0259e-02, -7.9215e-02],
        [-1.6976e-02,  1.1562e-01,  1.2364e-01,  ...,  3.4179e-02,
         -6.1157e-02,  3.7987e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0998, -0.0204, -0.0837,  ...,  0.0873, -0.0186, -0.0332],
        [ 0.1606,  0.0991,  0.1672,  ..., -0.0290, -0.0360, -0.1672],
        [-0.0600,  0.0184, -0.0768,  ...,  0.0370,  0.0306, -0.0363],
        ...,
        [-0.1653,  0.1287, -0.0224,  ...,  0.1474,  0.0228,  0.1716],
        [ 0.0806,  0.1646, -0.1545,  ...,  0.1164, -0.1060,  0.1458],
        [ 0.0535, -0.0907,  0.1229,  ..., -0.1740,  0.0746, -0.0396]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1459, -0.0781, -0.1910,  ...,  0.0159, -0.2029, -0.0300],
        [ 0.0566, -0.2466,  0.2287,  ...,  0.1932,  0.0568, -0.0904],
        [-0.1819, -0.2362, -0.0250,  ..., -0.0696,  0.1546,  0.1870],
        ...,
        [-0.0040,  0.0785, -0.1732,  ..., -0.2006, -0.0936, -0.2313],
        [-0.1004, -0.2056,  0.0502,  ..., -0.1058,  0.0453, -0.0443],
        [-0.0953, -0.0328,  0.0641,  ..., -0.2253,  0.0044,  0.0484]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.4212],
        [-0.1942],
        [-0.1183],
        [-0.2854],
        [ 0.2535],
        [ 0.0892],
        [-0.0911],
        [-0.0330],
        [ 0.2788],
        [-0.1949],
        [-0.0483],
        [ 0.3558],
        [ 0.2891],
        [-0.0649],
        [ 0.2676],
        [ 0.0110],
        [ 0.3874],
        [ 0.2129],
        [-0.1891],
        [-0.3436],
        [ 0.0962],
        [-0.2255],
        [ 0.2564],
        [-0.1224],
        [ 0.0700],
        [-0.0925],
        [ 0.4040],
        [ 0.0901],
        [ 0.0047],
        [ 0.1599],
        [ 0.3670],
        [-0.0490]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1478.1433, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1478.1433, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0057, -0.0201,  ...,  0.0061, -0.0153, -0.0204],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2484.0400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(223.0171, device='cuda:0')



h[100].sum tensor(-79.9218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.8194, device='cuda:0')



h[200].sum tensor(92.6903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-144.7970, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0214, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        [0.0000, 0.0176, 0.0000,  ..., 0.0191, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(130628.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0827, 0.1335],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0708, 0.1144],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0569, 0.0919],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(622970.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(687.9539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-733.9581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2771.1807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1174.0417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.5800],
        [0.6276],
        [0.6925],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(60808.3867, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(62.3248, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1290.4813, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1290.4813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.0000e-04,  0.0000e+00,  ..., -1.0000e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2367.7761, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(194.7033, device='cuda:0')



h[100].sum tensor(-70.0265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.5117, device='cuda:0')



h[200].sum tensor(60.8458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.4139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(123687.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0003, 0.0079, 0.0123],
        [0.0000, 0.0004, 0.0000,  ..., 0.0005, 0.0026, 0.0029],
        [0.0000, 0.0008, 0.0000,  ..., 0.0009, 0.0012, 0.0005],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0009, 0.0011, 0.0005],
        [0.0000, 0.0008, 0.0000,  ..., 0.0009, 0.0011, 0.0005],
        [0.0000, 0.0008, 0.0000,  ..., 0.0009, 0.0011, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(598080.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(634.6954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.0526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1463.4783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-935.1045, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0526],
        [ 0.0253],
        [ 0.0094],
        ...,
        [-0.0025],
        [-0.0025],
        [-0.0025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(12003.2148, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1739.2471, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1739.2471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.7617e-04,  0.0000e+00,  ..., -6.6564e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.7617e-04,  0.0000e+00,  ..., -6.6564e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.7617e-04,  0.0000e+00,  ..., -6.6564e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.7617e-04,  0.0000e+00,  ..., -6.6564e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.7617e-04,  0.0000e+00,  ..., -6.6564e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.7617e-04,  0.0000e+00,  ..., -6.6564e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3150.5088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.2161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(262.4116, device='cuda:0')



h[100].sum tensor(-93.7151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.3350, device='cuda:0')



h[200].sum tensor(94.9610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-170.3744, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(161797.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.2826e-03, 0.0000e+00,  ..., 1.8824e-05, 1.5595e-03,
         1.3906e-03],
        [0.0000e+00, 8.2697e-04, 0.0000e+00,  ..., 1.7973e-05, 4.3079e-03,
         6.2647e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2157e-02,
         2.0159e-02],
        ...,
        [0.0000e+00, 1.6432e-03, 0.0000e+00,  ..., 3.8009e-05, 8.4884e-04,
         1.5698e-04],
        [0.0000e+00, 1.6432e-03, 0.0000e+00,  ..., 3.8009e-05, 8.4884e-04,
         1.5698e-04],
        [0.0000e+00, 1.6432e-03, 0.0000e+00,  ..., 3.8009e-05, 8.4884e-04,
         1.5698e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(785374.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(879.2147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-735.7798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1875.9783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1344.8154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0045],
        [ 0.0175],
        [ 0.0368],
        ...,
        [-0.0068],
        [-0.0068],
        [-0.0068]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(16312.5439, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1748.9614, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1748.9614, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6004e-04,  2.8164e-03, -9.1527e-03,  ...,  2.7293e-03,
         -6.9481e-03, -9.3032e-03],
        [-1.4955e-04,  2.6460e-03, -8.5526e-03,  ...,  2.5462e-03,
         -6.4925e-03, -8.6932e-03],
        [ 0.0000e+00,  2.1687e-04,  0.0000e+00,  ..., -6.2270e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  2.1687e-04,  0.0000e+00,  ..., -6.2270e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  2.1687e-04,  0.0000e+00,  ..., -6.2270e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  2.1687e-04,  0.0000e+00,  ..., -6.2270e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-3200.6255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.0867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(263.8772, device='cuda:0')



h[100].sum tensor(-94.1860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.0239, device='cuda:0')



h[200].sum tensor(102.5478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-171.3260, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0210, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(168471.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0486, 0.0900],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0284, 0.0528],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0312, 0.0581],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0004, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0004, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0004, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(855083.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.8456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-710.3937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2119.3755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1517.1970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1032],
        [ 0.0933],
        [ 0.0923],
        ...,
        [-0.0120],
        [-0.0119],
        [-0.0119]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-543.8474, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1448.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1448.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2517e-04,  2.3909e-03, -7.4405e-03,  ...,  2.2103e-03,
         -5.6470e-03, -7.5629e-03],
        [-1.5933e-04,  2.9672e-03, -9.4709e-03,  ...,  2.8288e-03,
         -7.1880e-03, -9.6267e-03],
        [ 0.0000e+00,  2.7902e-04,  0.0000e+00,  ..., -5.5992e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  2.7902e-04,  0.0000e+00,  ..., -5.5992e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  2.7902e-04,  0.0000e+00,  ..., -5.5992e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  2.7902e-04,  0.0000e+00,  ..., -5.5992e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2656.7288, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(218.5564, device='cuda:0')



h[100].sum tensor(-77.6232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.7228, device='cuda:0')



h[200].sum tensor(91.7143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-141.9009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0211, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0085, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(142749.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0421, 0.0821],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0280, 0.0545],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0145, 0.0278],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0014, 0.0018],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0066, 0.0119],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0175, 0.0336]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(742624., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(721.5862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-535.9331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1432.3245, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1297.4144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0591],
        [ 0.0475],
        [ 0.0305],
        ...,
        [-0.0063],
        [ 0.0089],
        [ 0.0271]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-34052.6602, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1411.6823, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.6823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.5355e-04,  0.0000e+00,  ..., -2.5601e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  3.5355e-04,  0.0000e+00,  ..., -2.5601e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  3.5355e-04,  0.0000e+00,  ..., -2.5601e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  3.5355e-04,  0.0000e+00,  ..., -2.5601e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  3.5355e-04,  0.0000e+00,  ..., -2.5601e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  3.5355e-04,  0.0000e+00,  ..., -2.5601e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2556.0146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(212.9897, device='cuda:0')



h[100].sum tensor(-76.4262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.1064, device='cuda:0')



h[200].sum tensor(92.9163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.2866, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(147935.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.7215e-03, 0.0000e+00,  ..., 1.4224e-05, 2.2586e-03,
         8.5242e-04],
        [0.0000e+00, 2.2133e-03, 0.0000e+00,  ..., 2.8449e-05, 1.8706e-03,
         2.6706e-05],
        [0.0000e+00, 2.2211e-03, 0.0000e+00,  ..., 2.8449e-05, 1.8820e-03,
         2.6706e-05],
        ...,
        [0.0000e+00, 2.1977e-03, 0.0000e+00,  ..., 2.8449e-05, 1.8478e-03,
         2.6706e-05],
        [0.0000e+00, 2.1977e-03, 0.0000e+00,  ..., 2.8449e-05, 1.8478e-03,
         2.6706e-05],
        [0.0000e+00, 2.1977e-03, 0.0000e+00,  ..., 2.8449e-05, 1.8478e-03,
         2.6706e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(800381.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(738.3126, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-510.3911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1292.8298, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1332.1960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0014],
        [-0.0132],
        [-0.0239],
        ...,
        [-0.0268],
        [-0.0267],
        [-0.0267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-50601.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1633.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1633.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.9253e-04,  5.7841e-03, -1.8866e-02,  ...,  5.7498e-03,
         -1.4312e-02, -1.9177e-02],
        [-3.3444e-04,  6.5512e-03, -2.1569e-02,  ...,  6.5711e-03,
         -1.6362e-02, -2.1924e-02],
        [-9.3659e-04,  1.7573e-02, -6.0403e-02,  ...,  1.8372e-02,
         -4.5822e-02, -6.1398e-02],
        ...,
        [ 0.0000e+00,  4.2972e-04,  0.0000e+00,  ...,  1.6715e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  4.2972e-04,  0.0000e+00,  ...,  1.6715e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  4.2972e-04,  0.0000e+00,  ...,  1.6715e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2873.0444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(246.4924, device='cuda:0')



h[100].sum tensor(-87.7381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.8529, device='cuda:0')



h[200].sum tensor(107.3648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-160.0387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.3516e-02, 0.0000e+00,  ..., 4.4820e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.9101e-02, 0.0000e+00,  ..., 5.0800e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0550e-02, 0.0000e+00,  ..., 3.0937e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.7189e-03, 0.0000e+00,  ..., 6.6859e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.7189e-03, 0.0000e+00,  ..., 6.6859e-05, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.7189e-03, 0.0000e+00,  ..., 6.6859e-05, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(172466.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1128, 0.2279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1210, 0.2445],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1003, 0.2022],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0004, 0.0030, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0004, 0.0030, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0004, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(931732.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(870.1520, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-592.0906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1467.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1535.7050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1288],
        [ 0.1296],
        [ 0.1194],
        ...,
        [-0.0351],
        [-0.0350],
        [-0.0349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-66497.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1654.8737, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1654.8737, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  5.1225e-04,  0.0000e+00,  ...,  6.9496e-05,
          0.0000e+00,  0.0000e+00],
        [-4.8167e-04,  9.7145e-03, -3.2431e-02,  ...,  9.9122e-03,
         -2.4597e-02, -3.2966e-02],
        [ 0.0000e+00,  5.1225e-04,  0.0000e+00,  ...,  6.9496e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  5.1225e-04,  0.0000e+00,  ...,  6.9496e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  5.1225e-04,  0.0000e+00,  ...,  6.9496e-05,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  5.1225e-04,  0.0000e+00,  ...,  6.9496e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2817.6177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(249.6816, device='cuda:0')



h[100].sum tensor(-88.7937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.3518, device='cuda:0')



h[200].sum tensor(112.4698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-162.1093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0165, 0.0000,  ..., 0.0157, 0.0000, 0.0000],
        [0.0000, 0.0200, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        [0.0000, 0.0407, 0.0000,  ..., 0.0416, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(177871.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0384, 0.0720],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0499, 0.0966],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0607, 0.1193],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0009, 0.0042, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0009, 0.0042, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0009, 0.0042, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(975497.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(930.1005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.6876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1271.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1566.2666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0242],
        [ 0.0262],
        [ 0.0246],
        ...,
        [-0.0417],
        [-0.0415],
        [-0.0414]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-80920.0234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1763.7823, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1763.7823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0000,  0.0006,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [-0.0001,  0.0034, -0.0099,  ...,  0.0031, -0.0075, -0.0101],
        ...,
        [ 0.0000,  0.0006,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0000,  0.0006,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0000,  0.0006,  0.0000,  ...,  0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2838.6370, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(266.1133, device='cuda:0')



h[100].sum tensor(-94.4353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-125.0749, device='cuda:0')



h[200].sum tensor(127.9985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-172.7779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(195738.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0195, 0.0283],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0195, 0.0289],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0259, 0.0433],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0013, 0.0057, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0013, 0.0057, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0013, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1100666.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1079.0750, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-595.6154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1479.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1697.6470, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0568],
        [ 0.0618],
        [ 0.0668],
        ...,
        [-0.0470],
        [-0.0469],
        [-0.0469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-85280.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1875.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1875.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0079, -0.0254,  ...,  0.0079, -0.0193, -0.0258],
        [-0.0002,  0.0057, -0.0176,  ...,  0.0055, -0.0133, -0.0179],
        [-0.0003,  0.0077, -0.0249,  ...,  0.0077, -0.0189, -0.0253],
        ...,
        [ 0.0000,  0.0007,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0007,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0000,  0.0007,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2838.7024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(282.9532, device='cuda:0')



h[100].sum tensor(-100.2084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-132.9897, device='cuda:0')



h[200].sum tensor(147.1198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-183.7114, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0206, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        [0.0000, 0.0228, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(208813.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0543, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0534, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0453, 0.0799],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0015, 0.0074, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0015, 0.0074, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0015, 0.0074, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1180790.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1207.4590, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-590.9231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1499.7683, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1785.6833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0662],
        [ 0.0692],
        [ 0.0721],
        ...,
        [-0.0535],
        [-0.0532],
        [-0.0532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-96165.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1562.0170, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(1562.0170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0008,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0008,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0000,  0.0008,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0002,  0.0053, -0.0159,  ...,  0.0051, -0.0121, -0.0162],
        [-0.0001,  0.0040, -0.0112,  ...,  0.0037, -0.0085, -0.0114],
        [-0.0002,  0.0053, -0.0159,  ...,  0.0051, -0.0121, -0.0162]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-2135.1350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.6722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(235.6717, device='cuda:0')



h[100].sum tensor(-82.7201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.7671, device='cuda:0')



h[200].sum tensor(140.2362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-153.0132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0100, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0221, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0000, 0.0183, 0.0000,  ..., 0.0173, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(179663.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0043, 0.0002,  ..., 0.0015, 0.0091, 0.0000],
        [0.0000, 0.0043, 0.0002,  ..., 0.0015, 0.0091, 0.0000],
        [0.0000, 0.0043, 0.0002,  ..., 0.0015, 0.0091, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0373, 0.0569],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0401, 0.0630]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1045856.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1122.9678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-341.8844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(875.2658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1512.0525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0862],
        [-0.0804],
        [-0.0704],
        ...,
        [ 0.0219],
        [ 0.0298],
        [ 0.0312]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-123534.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1436.0972, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1436.0972, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1951.8368, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(216.6733, device='cuda:0')



h[100].sum tensor(-76.4366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.8378, device='cuda:0')



h[200].sum tensor(132.9035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.6782, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(172150.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.2942e-04, 3.6772e-05,  ..., 3.6673e-04, 1.8485e-02,
         1.6493e-02],
        [0.0000e+00, 1.8249e-03, 3.6772e-05,  ..., 6.6387e-04, 1.2196e-02,
         3.8261e-03],
        [0.0000e+00, 2.5994e-03, 3.6772e-05,  ..., 9.2266e-04, 1.0249e-02,
         3.1992e-04],
        ...,
        [0.0000e+00, 4.2288e-03, 1.6665e-04,  ..., 1.5080e-03, 9.0106e-03,
         0.0000e+00],
        [0.0000e+00, 4.2288e-03, 1.6665e-04,  ..., 1.5080e-03, 9.0106e-03,
         0.0000e+00],
        [0.0000e+00, 4.2288e-03, 1.6665e-04,  ..., 1.5080e-03, 9.0106e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1021616.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1082.0977, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-299.6664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(660.4275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1443.7155, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0527],
        [ 0.0491],
        [ 0.0505],
        ...,
        [-0.0598],
        [-0.0596],
        [-0.0595]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-123822.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1459.6827, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1459.6827, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1785.2664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(220.2319, device='cuda:0')



h[100].sum tensor(-77.1718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.5103, device='cuda:0')



h[200].sum tensor(147.2933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.9886, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(183966., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.6444e-02,
         2.8332e-02],
        [0.0000e+00, 1.0458e-03, 3.8803e-05,  ..., 3.6565e-04, 1.7590e-02,
         1.0173e-02],
        [0.0000e+00, 2.1451e-03, 3.8803e-05,  ..., 6.6524e-04, 1.3532e-02,
         2.5037e-03],
        ...,
        [0.0000e+00, 4.7609e-03, 2.0150e-04,  ..., 1.5029e-03, 1.0550e-02,
         0.0000e+00],
        [0.0000e+00, 4.7609e-03, 2.0150e-04,  ..., 1.5029e-03, 1.0550e-02,
         0.0000e+00],
        [0.0000e+00, 4.7609e-03, 2.0150e-04,  ..., 1.5029e-03, 1.0550e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1127596., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1226.5164, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.4609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(906.1303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1540.9470, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0792],
        [ 0.0600],
        [ 0.0348],
        ...,
        [-0.0671],
        [-0.0668],
        [-0.0667]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-131008.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1485.6726, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.6726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1637.5607, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.3014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.1531, device='cuda:0')



h[100].sum tensor(-78.7479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.3533, device='cuda:0')



h[200].sum tensor(161.2046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.5346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0124, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(188428.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0010, 0.0138, 0.0014],
        [0.0000, 0.0024, 0.0000,  ..., 0.0006, 0.0201, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0359, 0.0426],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0015, 0.0120, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0015, 0.0120, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0015, 0.0120, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1157093.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1300.4476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-231.3684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(839.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1577.4099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0079],
        [ 0.0273],
        [ 0.0545],
        ...,
        [-0.0755],
        [-0.0751],
        [-0.0750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-159807.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1741.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1741.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1810.0889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(262.7603, device='cuda:0')



h[100].sum tensor(-91.8821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.4989, device='cuda:0')



h[200].sum tensor(187.6456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-170.6008, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(212751.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0001, 0.0292, 0.0255],
        [0.0000, 0.0014, 0.0000,  ..., 0.0003, 0.0228, 0.0126],
        [0.0000, 0.0015, 0.0000,  ..., 0.0001, 0.0203, 0.0072],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0013, 0.0134, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0013, 0.0134, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0013, 0.0134, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1286179.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1471.0822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-279.6796, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(978.8372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1804.6053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0539],
        [ 0.0515],
        [ 0.0509],
        ...,
        [-0.0842],
        [-0.0838],
        [-0.0836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-164763.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1642.3500, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1642.3500, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.1156e-03,  0.0000e+00,  ...,  4.4421e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.1156e-03,  0.0000e+00,  ...,  4.4421e-04,
          0.0000e+00,  0.0000e+00],
        [-9.1214e-05,  3.5990e-03, -8.7402e-03,  ...,  3.0886e-03,
         -6.6179e-03, -8.8850e-03],
        ...,
        [ 0.0000e+00,  1.1156e-03,  0.0000e+00,  ...,  4.4421e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.1156e-03,  0.0000e+00,  ...,  4.4421e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.1156e-03,  0.0000e+00,  ...,  4.4421e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1454.3369, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(247.7921, device='cuda:0')



h[100].sum tensor(-86.6350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.4637, device='cuda:0')



h[200].sum tensor(191.0562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-160.8825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(213004.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.5216e-03, 0.0000e+00,  ..., 4.4387e-04, 1.8942e-02,
         3.5579e-03],
        [0.0000e+00, 1.1373e-03, 0.0000e+00,  ..., 2.6930e-05, 2.6444e-02,
         1.6158e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.7140e-02,
         3.7248e-02],
        ...,
        [0.0000e+00, 7.1519e-03, 0.0000e+00,  ..., 9.4041e-04, 1.4601e-02,
         0.0000e+00],
        [0.0000e+00, 7.1519e-03, 0.0000e+00,  ..., 9.4041e-04, 1.4601e-02,
         0.0000e+00],
        [0.0000e+00, 7.1519e-03, 0.0000e+00,  ..., 9.4041e-04, 1.4601e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1332072.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1515.3271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-180.7605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(958.7226, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1803.3799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0218],
        [ 0.0611],
        [ 0.0870],
        ...,
        [-0.0923],
        [-0.0918],
        [-0.0917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-187243.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1593.8136, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1593.8136, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0012, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1179.7954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(240.4691, device='cuda:0')



h[100].sum tensor(-84.5888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.0219, device='cuda:0')



h[200].sum tensor(195.6110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-156.1279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(211712.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0007, 0.0158, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0007, 0.0158, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0007, 0.0159, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0007, 0.0157, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0007, 0.0157, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0006, 0.0158, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1334865.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1558.1414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-60.1789, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1010.4215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1779.9375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1444],
        [-0.1403],
        [-0.1298],
        ...,
        [-0.0944],
        [-0.0865],
        [-0.0755]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-212136.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1495.2291, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1495.2291, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9110e-05,  3.9849e-03, -9.6355e-03,  ...,  3.4190e-03,
         -7.2924e-03, -9.7954e-03],
        [ 0.0000e+00,  1.2471e-03,  0.0000e+00,  ...,  5.0304e-04,
          0.0000e+00,  0.0000e+00],
        [-8.9110e-05,  3.9849e-03, -9.6355e-03,  ...,  3.4190e-03,
         -7.2924e-03, -9.7954e-03],
        ...,
        [ 0.0000e+00,  1.2471e-03,  0.0000e+00,  ...,  5.0304e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.2471e-03,  0.0000e+00,  ...,  5.0304e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.2471e-03,  0.0000e+00,  ...,  5.0304e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-772.9672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.8257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.5950, device='cuda:0')



h[100].sum tensor(-78.4316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.0310, device='cuda:0')



h[200].sum tensor(194.9054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.4707, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0134, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0189, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(207920.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0417, 0.0407],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0412, 0.0398],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0315, 0.0186],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0002, 0.0167, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0002, 0.0167, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0002, 0.0167, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1342164.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1614.1057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(79.8600, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(733.2512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1768.9048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0953],
        [ 0.0927],
        [ 0.0830],
        ...,
        [-0.1024],
        [-0.1019],
        [-0.1018]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-201327.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1916.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1916.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3196e-05,  3.3736e-03, -7.2890e-03,  ...,  2.7343e-03,
         -5.5152e-03, -7.4100e-03],
        [-6.3196e-05,  3.3736e-03, -7.2890e-03,  ...,  2.7343e-03,
         -5.5152e-03, -7.4100e-03],
        [ 0.0000e+00,  1.3024e-03,  0.0000e+00,  ...,  5.2788e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.3024e-03,  0.0000e+00,  ...,  5.2788e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3024e-03,  0.0000e+00,  ...,  5.2788e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3024e-03,  0.0000e+00,  ...,  5.2788e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-1170.0056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(289.0978, device='cuda:0')



h[100].sum tensor(-100.7775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-135.8777, device='cuda:0')



h[200].sum tensor(227.9353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-187.7009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0184, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0208, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(252605.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0500, 0.0559],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0466, 0.0487],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0525, 0.0610],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0175, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0175, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0175, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1571640., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1915.7369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-64.4363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1557.8033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2202.9451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0960],
        [ 0.0954],
        [ 0.0915],
        ...,
        [-0.1075],
        [-0.1070],
        [-0.1069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-199460.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1644.5442, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1644.5442, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.3527e-03,  0.0000e+00,  ...,  5.4023e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3527e-03,  0.0000e+00,  ...,  5.4023e-04,
          0.0000e+00,  0.0000e+00],
        [-7.1823e-05,  3.8714e-03, -8.8628e-03,  ...,  3.2239e-03,
         -6.7045e-03, -9.0101e-03],
        ...,
        [ 0.0000e+00,  1.3527e-03,  0.0000e+00,  ...,  5.4023e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3527e-03,  0.0000e+00,  ...,  5.4023e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3527e-03,  0.0000e+00,  ...,  5.4023e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-549.8433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(248.1231, device='cuda:0')



h[100].sum tensor(-85.7888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.6193, device='cuda:0')



h[200].sum tensor(217.7878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-161.0974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(226378.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0208, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0268, 0.0077],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0324, 0.0156],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0182, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0182, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0182, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1456386., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1830.9036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(188.9207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(774.3365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1977.0247, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0360],
        [ 0.0185],
        [ 0.0611],
        ...,
        [-0.1139],
        [-0.1134],
        [-0.1133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-209011.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1516.4635, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1516.4635, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.3964e-03,  0.0000e+00,  ...,  5.4422e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3964e-03,  0.0000e+00,  ...,  5.4422e-04,
          0.0000e+00,  0.0000e+00],
        [-6.6368e-05,  3.8948e-03, -8.7904e-03,  ...,  3.2070e-03,
         -6.6481e-03, -8.9366e-03],
        ...,
        [ 0.0000e+00,  1.3964e-03,  0.0000e+00,  ...,  5.4422e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3964e-03,  0.0000e+00,  ...,  5.4422e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.3964e-03,  0.0000e+00,  ...,  5.4422e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-169.0363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.7987, device='cuda:0')



h[100].sum tensor(-79.1835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.5368, device='cuda:0')



h[200].sum tensor(217.5649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.5508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(222384.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0269, 0.0036],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0320, 0.0151],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0415, 0.0325],
        ...,
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0189, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0189, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0189, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1470276.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1867.5955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(309.5321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(606.6635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1956.4551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0470],
        [ 0.0653],
        [ 0.0793],
        ...,
        [-0.1203],
        [-0.1197],
        [-0.1196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-217642.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 600 loss: tensor(1124.2815, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1694.9645, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1694.9645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(-221.3969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.5652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(255.7303, device='cuda:0')



h[100].sum tensor(-88.3195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.1948, device='cuda:0')



h[200].sum tensor(235.5649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-166.0366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(236264.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0264, 0.0023],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0244, 0.0006],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0225, 0.0000],
        ...,
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0194, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0194, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0194, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1545857.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1990.8328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(314.5994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(868.8625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2086.7041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0750],
        [ 0.0737],
        [ 0.0725],
        ...,
        [-0.1268],
        [-0.1262],
        [-0.1260]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-251303.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1562.5605, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1562.5605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001,  0.0067, -0.0185,  ...,  0.0062, -0.0140, -0.0189],
        [ 0.0000,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0003,  0.0159, -0.0508,  ...,  0.0159, -0.0384, -0.0516],
        ...,
        [ 0.0000,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0000,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(184.5008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(235.7537, device='cuda:0')



h[100].sum tensor(-81.5242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.8056, device='cuda:0')



h[200].sum tensor(233.6595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-153.0664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0375, 0.0000,  ..., 0.0360, 0.0000, 0.0000],
        [0.0000, 0.0354, 0.0000,  ..., 0.0337, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(227946.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0465, 0.0385],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0816, 0.1115],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0985, 0.1462],
        ...,
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0199, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0199, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0199, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1522372.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1999.2070, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(426.4585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(714.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2023.9664, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0600],
        [ 0.0559],
        [ 0.0486],
        ...,
        [-0.1299],
        [-0.1304],
        [-0.1307]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-274016.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1498.5149, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1498.5149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.5747e-05,  6.0401e-03, -1.6034e-02,  ...,  5.4027e-03,
         -1.2118e-02, -1.6302e-02],
        [-1.0821e-04,  6.6335e-03, -1.8122e-02,  ...,  6.0359e-03,
         -1.3696e-02, -1.8424e-02],
        [-1.1067e-04,  6.7507e-03, -1.8534e-02,  ...,  6.1610e-03,
         -1.4007e-02, -1.8843e-02],
        ...,
        [ 0.0000e+00,  1.4819e-03,  0.0000e+00,  ...,  5.3871e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.4819e-03,  0.0000e+00,  ...,  5.3871e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.4819e-03,  0.0000e+00,  ...,  5.3871e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(509.6575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.1790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(226.0907, device='cuda:0')



h[100].sum tensor(-77.9180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.2640, device='cuda:0')



h[200].sum tensor(234.4183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.7926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0226, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0304, 0.0000,  ..., 0.0283, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(228623.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0884, 0.1251],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0817, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0745, 0.0959],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0203, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0203, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0203, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1558024., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2048.8403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(485.1375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(785.5591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2043.0570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0256],
        [ 0.0420],
        [ 0.0551],
        ...,
        [-0.1369],
        [-0.1363],
        [-0.1360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-294787., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1297.9454, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1297.9454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.4965e-03,  0.0000e+00,  ...,  5.4179e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.4965e-03,  0.0000e+00,  ...,  5.4179e-04,
          0.0000e+00,  0.0000e+00],
        [-5.2077e-05,  4.2006e-03, -9.5119e-03,  ...,  3.4289e-03,
         -7.1870e-03, -9.6706e-03],
        ...,
        [ 0.0000e+00,  1.4965e-03,  0.0000e+00,  ...,  5.4179e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.4965e-03,  0.0000e+00,  ...,  5.4179e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.4965e-03,  0.0000e+00,  ...,  5.4179e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1031.1211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.9367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(195.8295, device='cuda:0')



h[100].sum tensor(-67.4446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.0410, device='cuda:0')



h[200].sum tensor(225.7829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.1451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0207, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(214934.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0290, 0.0068],
        [0.0000, 0.0018, 0.0000,  ..., 0.0000, 0.0403, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0628, 0.0698],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0206, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0206, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0206, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1517138., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2026.3434, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(620.1733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(476.3692, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1951.7959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0487],
        [ 0.0512],
        [ 0.0445],
        ...,
        [-0.1400],
        [-0.1393],
        [-0.1391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-297854.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1355.1281, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1355.1281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.3824e-05,  4.0054e-03, -8.7744e-03,  ...,  3.2003e-03,
         -6.6281e-03, -8.9209e-03],
        [-4.2677e-05,  3.9402e-03, -8.5448e-03,  ...,  3.1306e-03,
         -6.4547e-03, -8.6875e-03],
        [-8.6501e-05,  6.4346e-03, -1.7319e-02,  ...,  5.7955e-03,
         -1.3083e-02, -1.7608e-02],
        ...,
        [ 0.0000e+00,  1.5110e-03,  0.0000e+00,  ...,  5.3546e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5110e-03,  0.0000e+00,  ...,  5.3546e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5110e-03,  0.0000e+00,  ...,  5.3546e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1182.4268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(204.4570, device='cuda:0')



h[100].sum tensor(-70.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.0960, device='cuda:0')



h[200].sum tensor(232.6392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.7466, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0229, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(222137.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.8044e-05, 0.0000e+00,  ..., 0.0000e+00, 4.6504e-02,
         3.5113e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.7825e-02,
         5.8536e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.3767e-02,
         4.9675e-02],
        ...,
        [0.0000e+00, 1.2074e-02, 0.0000e+00,  ..., 0.0000e+00, 2.1061e-02,
         0.0000e+00],
        [0.0000e+00, 1.2074e-02, 0.0000e+00,  ..., 0.0000e+00, 2.1061e-02,
         0.0000e+00],
        [0.0000e+00, 1.2074e-02, 0.0000e+00,  ..., 0.0000e+00, 2.1061e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1564441.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2106.6675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(641.6082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(625.6150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2042.7026, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0627],
        [ 0.0681],
        [ 0.0732],
        ...,
        [-0.1432],
        [-0.1425],
        [-0.1423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-307212.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1678.5708, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1678.5708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(938.5243, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.0050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(253.2569, device='cuda:0')



h[100].sum tensor(-87.2430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.0323, device='cuda:0')



h[200].sum tensor(256.5749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-164.4306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(250741.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0285, 0.0037],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0238, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0221, 0.0000],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0216, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0216, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0216, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1695973.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2295.4990, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(555.5146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1162.3199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2339.9951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0646],
        [ 0.0351],
        [-0.0071],
        ...,
        [-0.1455],
        [-0.1452],
        [-0.1452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-302852.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1653.8173, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1653.8173, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.1806e-05,  7.2387e-03, -2.0031e-02,  ...,  6.6118e-03,
         -1.5124e-02, -2.0366e-02],
        [-7.3706e-05,  6.6747e-03, -1.8047e-02,  ...,  6.0085e-03,
         -1.3626e-02, -1.8349e-02],
        [-8.0300e-05,  7.1338e-03, -1.9662e-02,  ...,  6.4996e-03,
         -1.4845e-02, -1.9991e-02],
        ...,
        [ 0.0000e+00,  1.5425e-03,  0.0000e+00,  ...,  5.1945e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5425e-03,  0.0000e+00,  ...,  5.1945e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5425e-03,  0.0000e+00,  ...,  5.1945e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1168.3384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(249.5222, device='cuda:0')



h[100].sum tensor(-85.3492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.2769, device='cuda:0')



h[200].sum tensor(257.7794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-162.0058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0258, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0000, 0.0263, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0000, 0.0257, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(248039.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0676, 0.0754],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0677, 0.0755],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0671, 0.0739],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0222, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0222, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0222, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1690314.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2318.3721, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(643.5192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(802.6030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2354.6541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1011],
        [ 0.1002],
        [ 0.1006],
        ...,
        [-0.1511],
        [-0.1504],
        [-0.1502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-286338.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1819.8193, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1819.8193, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1075.3650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.8766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(274.5680, device='cuda:0')



h[100].sum tensor(-94.1171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-129.0486, device='cuda:0')



h[200].sum tensor(271.3232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-178.2672, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(277718.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0228, 0.0000],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0228, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0229, 0.0000],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0227, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0227, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1910576.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2482.0688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(543.2173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1640.4148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2631.2134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2034],
        [-0.2220],
        [-0.2356],
        ...,
        [-0.1581],
        [-0.1574],
        [-0.1572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-298951.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1359.9628, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1359.9628, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1266e-05,  6.0635e-03, -1.5803e-02,  ...,  5.2676e-03,
         -1.1926e-02, -1.6067e-02],
        [ 0.0000e+00,  1.5686e-03,  0.0000e+00,  ...,  4.5426e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5686e-03,  0.0000e+00,  ...,  4.5426e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.5686e-03,  0.0000e+00,  ...,  4.5426e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5686e-03,  0.0000e+00,  ...,  4.5426e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5686e-03,  0.0000e+00,  ...,  4.5426e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1821.7854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.5790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(205.1864, device='cuda:0')



h[100].sum tensor(-69.9604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.4389, device='cuda:0')



h[200].sum tensor(244.3838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.2202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0153, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(230856.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 6.6405e-05, 0.0000e+00,  ..., 0.0000e+00, 5.0985e-02,
         3.8122e-02],
        [0.0000e+00, 3.1955e-03, 0.0000e+00,  ..., 0.0000e+00, 3.8468e-02,
         1.5522e-02],
        [0.0000e+00, 6.9767e-03, 0.0000e+00,  ..., 0.0000e+00, 2.8996e-02,
         9.6287e-04],
        ...,
        [0.0000e+00, 1.2554e-02, 0.0000e+00,  ..., 0.0000e+00, 2.3231e-02,
         0.0000e+00],
        [0.0000e+00, 1.2554e-02, 0.0000e+00,  ..., 0.0000e+00, 2.3231e-02,
         0.0000e+00],
        [0.0000e+00, 1.2554e-02, 0.0000e+00,  ..., 0.0000e+00, 2.3231e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1672991.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2216.3218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(861.3784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(437.8907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2161.1641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0544],
        [ 0.0494],
        [ 0.0327],
        ...,
        [-0.1624],
        [-0.1617],
        [-0.1615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-359671.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1772.9882, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1772.9882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.9448e-05,  9.5022e-03, -2.7860e-02,  ...,  8.9262e-03,
         -2.1020e-02, -2.8327e-02],
        [ 0.0000e+00,  1.5772e-03,  0.0000e+00,  ...,  4.3373e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5772e-03,  0.0000e+00,  ...,  4.3373e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.5772e-03,  0.0000e+00,  ...,  4.3373e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5772e-03,  0.0000e+00,  ...,  4.3373e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5772e-03,  0.0000e+00,  ...,  4.3373e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(1446.9746, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.6641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(267.5023, device='cuda:0')



h[100].sum tensor(-91.1816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-125.7277, device='cuda:0')



h[200].sum tensor(271.6269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-173.6797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0355, 0.0000,  ..., 0.0330, 0.0000, 0.0000],
        [0.0000, 0.0261, 0.0000,  ..., 0.0229, 0.0000, 0.0000],
        [0.0000, 0.0181, 0.0000,  ..., 0.0144, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(271116.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1124, 0.1619],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0971, 0.1308],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0821, 0.1000],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0239, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0239, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0239, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1881792.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2424.3132, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(696.3666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1503.1681, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2537.0151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0195],
        [ 0.0270],
        [ 0.0374],
        ...,
        [-0.1661],
        [-0.1653],
        [-0.1651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-367765.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 900 loss: tensor(570.6572, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1195.3223, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1195.3223, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9295e-05,  3.8001e-03, -7.7863e-03,  ...,  2.8059e-03,
         -5.8733e-03, -7.9169e-03],
        [-4.7078e-05,  6.9900e-03, -1.8998e-02,  ...,  6.2264e-03,
         -1.4330e-02, -1.9316e-02],
        [-2.5373e-05,  4.4980e-03, -1.0239e-02,  ...,  3.5542e-03,
         -7.7234e-03, -1.0411e-02],
        ...,
        [ 0.0000e+00,  1.5848e-03,  0.0000e+00,  ...,  4.3039e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5848e-03,  0.0000e+00,  ...,  4.3039e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5848e-03,  0.0000e+00,  ...,  4.3039e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2386.7764, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.3901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(180.3461, device='cuda:0')



h[100].sum tensor(-61.5731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.7637, device='cuda:0')



h[200].sum tensor(237.4221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-117.0922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0220, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0159, 0.0000,  ..., 0.0120, 0.0000, 0.0000],
        [0.0000, 0.0222, 0.0000,  ..., 0.0187, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(223280.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0515, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0519, 0.0353],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0593, 0.0503],
        ...,
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0246, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0246, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0246, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1682662.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2179.7661, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1023.3423, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(375.4714, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2110.5110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0084],
        [ 0.0339],
        [ 0.0459],
        ...,
        [-0.1691],
        [-0.1683],
        [-0.1681]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-375975.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1411.9515, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.9515, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0793e-05,  9.7444e-03, -2.8628e-02,  ...,  9.1848e-03,
         -2.1590e-02, -2.9109e-02],
        [-2.7310e-05,  5.2568e-03, -1.2861e-02,  ...,  4.3697e-03,
         -9.6987e-03, -1.3077e-02],
        [-3.5061e-05,  6.2956e-03, -1.6511e-02,  ...,  5.4843e-03,
         -1.2451e-02, -1.6788e-02],
        ...,
        [ 0.0000e+00,  1.5966e-03,  0.0000e+00,  ...,  4.4228e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5966e-03,  0.0000e+00,  ...,  4.4228e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.5966e-03,  0.0000e+00,  ...,  4.4228e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2321.5725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.3923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(213.0303, device='cuda:0')



h[100].sum tensor(-72.1734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.1255, device='cuda:0')



h[200].sum tensor(251.2127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.3130, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0298, 0.0000,  ..., 0.0269, 0.0000, 0.0000],
        [0.0000, 0.0299, 0.0000,  ..., 0.0270, 0.0000, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(241828.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1086, 0.1494],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0914, 0.1147],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0672, 0.0657],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0254, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0254, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0254, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1772677.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2297.3950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(975.1171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(646.5598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2323.5342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0119],
        [ 0.0012],
        [ 0.0125],
        ...,
        [-0.1714],
        [-0.1707],
        [-0.1704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-353160.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1753.9113, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1753.9113, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.6125e-03,  0.0000e+00,  ...,  4.4614e-04,
          0.0000e+00,  0.0000e+00],
        [-1.5554e-05,  4.0887e-03, -8.6980e-03,  ...,  3.1048e-03,
         -6.5579e-03, -8.8441e-03],
        [ 0.0000e+00,  1.6125e-03,  0.0000e+00,  ...,  4.4614e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.6125e-03,  0.0000e+00,  ...,  4.4614e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6125e-03,  0.0000e+00,  ...,  4.4614e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6125e-03,  0.0000e+00,  ...,  4.4614e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2077.0178, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.4086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(264.6240, device='cuda:0')



h[100].sum tensor(-89.1649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.3749, device='cuda:0')



h[200].sum tensor(272.9070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-171.8109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0226, 0.0000,  ..., 0.0191, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0220, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(274983.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0768, 0.0819],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0713, 0.0713],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0916, 0.1118],
        ...,
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0262, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0262, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1929293.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2488.6187, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(829.5262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(929.3658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2665.1868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0062],
        [-0.0176],
        [-0.0283],
        ...,
        [-0.1747],
        [-0.1740],
        [-0.1738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-319915.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1505.4855, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1505.4855, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2525.6362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.2889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(227.1424, device='cuda:0')



h[100].sum tensor(-76.4750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.7583, device='cuda:0')



h[200].sum tensor(259.0912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-147.4754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(251836.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0276, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0274, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0278, 0.0000],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0271, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0271, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1838908.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2363.9521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(987.3918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(724.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2422.9795, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0803],
        [-0.1131],
        [-0.1203],
        ...,
        [-0.1804],
        [-0.1797],
        [-0.1794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-368446.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1377.7816, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1377.7816, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.6485e-03,  0.0000e+00,  ...,  4.1478e-04,
          0.0000e+00,  0.0000e+00],
        [-2.1612e-05,  6.8825e-03, -1.8380e-02,  ...,  6.0427e-03,
         -1.3851e-02, -1.8690e-02],
        [-1.8738e-05,  6.1865e-03, -1.5936e-02,  ...,  5.2943e-03,
         -1.2009e-02, -1.6204e-02],
        ...,
        [ 0.0000e+00,  1.6485e-03,  0.0000e+00,  ...,  4.1478e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6485e-03,  0.0000e+00,  ...,  4.1478e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6485e-03,  0.0000e+00,  ...,  4.1478e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2812.8550, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.2111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(207.8749, device='cuda:0')



h[100].sum tensor(-69.8975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.7024, device='cuda:0')



h[200].sum tensor(252.7628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-134.9657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0154, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0342, 0.0000,  ..., 0.0314, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(243291.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.4834e-03, 0.0000e+00,  ..., 0.0000e+00, 4.3885e-02,
         1.6942e-02],
        [0.0000e+00, 6.1553e-05, 0.0000e+00,  ..., 0.0000e+00, 5.7359e-02,
         4.0110e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.4151e-02,
         9.3082e-02],
        ...,
        [0.0000e+00, 1.2366e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7963e-02,
         0.0000e+00],
        [0.0000e+00, 1.2366e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7963e-02,
         0.0000e+00],
        [0.0000e+00, 1.2366e-02, 0.0000e+00,  ..., 0.0000e+00, 2.7963e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1815100.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2316.2417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1059.1431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(454.9965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2321.2529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0695],
        [ 0.0734],
        [ 0.0730],
        ...,
        [-0.1868],
        [-0.1860],
        [-0.1857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-383563.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1737.2067, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1737.2067, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.1086e-05,  1.4688e-02, -4.5716e-02,  ...,  1.4403e-02,
         -3.4442e-02, -4.6485e-02],
        [-2.9513e-05,  1.1020e-02, -3.2838e-02,  ...,  1.0456e-02,
         -2.4741e-02, -3.3391e-02],
        [ 0.0000e+00,  1.6682e-03,  0.0000e+00,  ...,  3.9287e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.6682e-03,  0.0000e+00,  ...,  3.9287e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6682e-03,  0.0000e+00,  ...,  3.9287e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6682e-03,  0.0000e+00,  ...,  3.9287e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(2484.6982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.2048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(262.1037, device='cuda:0')



h[100].sum tensor(-88.6217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-123.1903, device='cuda:0')



h[200].sum tensor(277.1967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-170.1745, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0559, 0.0000,  ..., 0.0546, 0.0000, 0.0000],
        [0.0000, 0.0305, 0.0000,  ..., 0.0272, 0.0000, 0.0000],
        [0.0000, 0.0196, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(278334.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.6002e-01,
         2.4014e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0809e-01,
         1.3797e-01],
        [0.0000e+00, 2.2641e-04, 0.0000e+00,  ..., 0.0000e+00, 6.9945e-02,
         6.2597e-02],
        ...,
        [0.0000e+00, 1.2603e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8834e-02,
         0.0000e+00],
        [0.0000e+00, 1.2603e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8834e-02,
         0.0000e+00],
        [0.0000e+00, 1.2603e-02, 0.0000e+00,  ..., 0.0000e+00, 2.8834e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1986142., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2498.4631, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(879.7184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1012.2472, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2630.8608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0115],
        [ 0.0212],
        [ 0.0176],
        ...,
        [-0.1929],
        [-0.1921],
        [-0.1918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-383846.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1414.2482, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1414.2482, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9392e-05,  1.0302e-02, -3.0255e-02,  ...,  9.6575e-03,
         -2.2789e-02, -3.0764e-02],
        [ 0.0000e+00,  1.6860e-03,  0.0000e+00,  ...,  3.7811e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6860e-03,  0.0000e+00,  ...,  3.7811e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  1.6860e-03,  0.0000e+00,  ...,  3.7811e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6860e-03,  0.0000e+00,  ...,  3.7811e-04,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  1.6860e-03,  0.0000e+00,  ...,  3.7811e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3035.8682, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.1182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(213.3768, device='cuda:0')



h[100].sum tensor(-71.6196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.2884, device='cuda:0')



h[200].sum tensor(257.8000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.5379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0386, 0.0000,  ..., 0.0358, 0.0000, 0.0000],
        [0.0000, 0.0273, 0.0000,  ..., 0.0237, 0.0000, 0.0000],
        [0.0000, 0.0160, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(248117.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1453, 0.2095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1164, 0.1528],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0993, 0.1189],
        ...,
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0296, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0296, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0296, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1854334.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2339.9424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1068.8618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(390.8649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2325.1453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0308],
        [-0.0203],
        [-0.0114],
        ...,
        [-0.1979],
        [-0.1970],
        [-0.1968]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-428319.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1577.3439, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1577.3439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3042.2781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.0826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(237.9842, device='cuda:0')



h[100].sum tensor(-79.6269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.8540, device='cuda:0')



h[200].sum tensor(267.9831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-154.5146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(271039., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0317, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0321, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0325, 0.0000],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0303, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0303, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0303, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1981789.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2474.0591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(963.8518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(630.8045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2558.3301, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0671],
        [-0.0316],
        [ 0.0011],
        ...,
        [-0.2014],
        [-0.2005],
        [-0.2003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-397719.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1326.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1326.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3531.8071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.0314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(200.1218, device='cuda:0')



h[100].sum tensor(-66.7510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.0584, device='cuda:0')



h[200].sum tensor(252.4731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.9319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(248714.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0312, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0312, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0313, 0.0000],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0310, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0310, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0310, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1889826.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2356.1108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1115.4263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(517.8089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2342.1584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3023],
        [-0.3055],
        [-0.3059],
        ...,
        [-0.2049],
        [-0.2040],
        [-0.2038]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-421649.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1815.7734, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1815.7734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3200.9326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.0044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(273.9576, device='cuda:0')



h[100].sum tensor(-91.7716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-128.7617, device='cuda:0')



h[200].sum tensor(282.0172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-177.8708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(298634.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0341, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0322, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0333, 0.0000],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0316, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0316, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0316, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2131960.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2630.2041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(864.2350, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1419.5955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2846.0737, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0056],
        [-0.0530],
        [-0.0921],
        ...,
        [-0.2043],
        [-0.2035],
        [-0.2032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-381124.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 1200 loss: tensor(558.9811, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1665.8350, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1665.8350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3573e-05,  1.7199e-03,  0.0000e+00,  ...,  4.0634e-04,
          0.0000e+00,  0.0000e+00],
        [-6.3573e-05,  1.7199e-03,  0.0000e+00,  ...,  4.0634e-04,
          0.0000e+00,  0.0000e+00],
        [-6.3573e-05,  1.7199e-03,  0.0000e+00,  ...,  4.0634e-04,
          0.0000e+00,  0.0000e+00],
        ...,
        [-6.3573e-05,  1.7199e-03,  0.0000e+00,  ...,  4.0634e-04,
          0.0000e+00,  0.0000e+00],
        [-6.3573e-05,  1.7199e-03,  0.0000e+00,  ...,  4.0634e-04,
          0.0000e+00,  0.0000e+00],
        [-6.3573e-05,  1.7199e-03,  0.0000e+00,  ...,  4.0634e-04,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3587.1140, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(251.3354, device='cuda:0')



h[100].sum tensor(-83.7678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.1291, device='cuda:0')



h[200].sum tensor(271.1312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-163.1831, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(280077.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0323, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0323, 0.0000],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0325, 0.0000],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0321, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0321, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0321, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2021742.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2541.6521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1000.4993, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(682.5837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2714.0183, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1729],
        [-0.2297],
        [-0.2695],
        ...,
        [-0.2041],
        [-0.2032],
        [-0.2030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-354008.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1393.8000, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1393.8000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0001,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0001,  0.0062, -0.0158,  ...,  0.0053, -0.0119, -0.0160],
        [-0.0001,  0.0103, -0.0302,  ...,  0.0097, -0.0227, -0.0307],
        ...,
        [-0.0001,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0001,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0001,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4039.9287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(210.2917, device='cuda:0')



h[100].sum tensor(-69.9413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.8383, device='cuda:0')



h[200].sum tensor(253.3371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.5349, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0226, 0.0000,  ..., 0.0186, 0.0000, 0.0000],
        [0.0000, 0.0455, 0.0000,  ..., 0.0434, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(255808.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0546, 0.0300],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0846, 0.0862],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1279, 0.1711],
        ...,
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0327, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0327, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0327, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1921501.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2403.0437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1162.8188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(347.6468, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2467.5579, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0242],
        [ 0.0466],
        [ 0.0488],
        ...,
        [-0.2051],
        [-0.2047],
        [-0.2047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-388129.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1521.9532, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1521.9532, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(3986.7778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-35.7449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(229.6270, device='cuda:0')



h[100].sum tensor(-76.1919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.9261, device='cuda:0')



h[200].sum tensor(260.3456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-149.0886, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(270818.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0468, 0.0154],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0382, 0.0015],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0369, 0.0000],
        ...,
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0332, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0332, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0332, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2012696.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2449.0674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1097.5155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(811.4147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2561.1829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0852],
        [ 0.0686],
        [ 0.0553],
        ...,
        [-0.2139],
        [-0.2130],
        [-0.2127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-425468.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1588.5226, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1588.5226, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0002,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4004.2041, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-45.5943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(239.6708, device='cuda:0')



h[100].sum tensor(-79.2163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.6467, device='cuda:0')



h[200].sum tensor(263.3730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-155.6096, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(273253.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0140, 0.0000,  ..., 0.0000, 0.0340, 0.0000],
        [0.0000, 0.0140, 0.0000,  ..., 0.0000, 0.0340, 0.0000],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0341, 0.0000],
        ...,
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0337, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0337, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0337, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2016829.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2430.9873, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1101.4154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(592.3851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2562.9751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2871],
        [-0.3046],
        [-0.3126],
        ...,
        [-0.2175],
        [-0.2164],
        [-0.2160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-426159., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1511.5735, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1511.5735, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0094, -0.0269,  ...,  0.0087, -0.0202, -0.0274],
        [-0.0003,  0.0138, -0.0422,  ...,  0.0134, -0.0318, -0.0430],
        [-0.0004,  0.0154, -0.0480,  ...,  0.0152, -0.0361, -0.0488],
        ...,
        [-0.0003,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4209.6230, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-54.5233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.0609, device='cuda:0')



h[100].sum tensor(-75.0518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.1900, device='cuda:0')



h[200].sum tensor(257.5128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.0718, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0435, 0.0000,  ..., 0.0411, 0.0000, 0.0000],
        [0.0000, 0.0447, 0.0000,  ..., 0.0424, 0.0000, 0.0000],
        [0.0000, 0.0496, 0.0000,  ..., 0.0478, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(266307., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1483, 0.2115],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1450, 0.2050],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1398, 0.1943],
        ...,
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0343, 0.0000],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0343, 0.0000],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0343, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1992764.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2368.9937, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1159.2266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(328.7526, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2463.6560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0301],
        [ 0.0365],
        [ 0.0416],
        ...,
        [-0.2237],
        [-0.2227],
        [-0.2224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-457674.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1517.3489, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1517.3489, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0106, -0.0310,  ...,  0.0099, -0.0233, -0.0315],
        [-0.0004,  0.0076, -0.0205,  ...,  0.0067, -0.0154, -0.0208],
        [-0.0004,  0.0086, -0.0239,  ...,  0.0078, -0.0180, -0.0243],
        ...,
        [-0.0003,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4361.7598, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-62.6601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.9323, device='cuda:0')



h[100].sum tensor(-74.9256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.5995, device='cuda:0')



h[200].sum tensor(256.5711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.6376, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0315, 0.0000,  ..., 0.0281, 0.0000, 0.0000],
        [0.0000, 0.0351, 0.0000,  ..., 0.0320, 0.0000, 0.0000],
        [0.0000, 0.0325, 0.0000,  ..., 0.0292, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(269455.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1050, 0.1249],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1050, 0.1248],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0988, 0.1122],
        ...,
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0348, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2020180.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2372.6738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1163.0343, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(589.6843, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2482.3923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0774],
        [ 0.0774],
        [ 0.0773],
        ...,
        [-0.2185],
        [-0.2213],
        [-0.2230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-463951.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1486.4899, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1486.4899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0003,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0003,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4579.5244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-70.0531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.2764, device='cuda:0')



h[100].sum tensor(-73.8274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4112, device='cuda:0')



h[200].sum tensor(254.2301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.6146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(268515.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0460, 0.0090],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0474, 0.0094],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0475, 0.0107],
        ...,
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0354, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0354, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0354, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2015236.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2374.6313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1194.7113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(587.4967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2490.2407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0725],
        [ 0.0789],
        [ 0.0820],
        ...,
        [-0.2281],
        [-0.2271],
        [-0.2268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-466155.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1295.7305, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1295.7305, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0069, -0.0179,  ...,  0.0059, -0.0135, -0.0182],
        [-0.0004,  0.0047, -0.0103,  ...,  0.0036, -0.0078, -0.0105],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4989.7593, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-76.7098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(195.4953, device='cuda:0')



h[100].sum tensor(-64.0807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.8840, device='cuda:0')



h[200].sum tensor(241.2321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.9281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0240, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        [0.0000, 0.0163, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(253905.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0701, 0.0537],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0603, 0.0341],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0483, 0.0129],
        ...,
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0359, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0359, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0359, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(1962137.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2306.0037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1305.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(299.1064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2377.8098, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1281],
        [ 0.1231],
        [ 0.1119],
        ...,
        [-0.2293],
        [-0.2283],
        [-0.2280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-445029.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1563.3381, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1563.3381, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0096, -0.0275,  ...,  0.0089, -0.0207, -0.0280],
        [-0.0004,  0.0053, -0.0124,  ...,  0.0042, -0.0093, -0.0126],
        [-0.0004,  0.0057, -0.0137,  ...,  0.0046, -0.0103, -0.0139],
        ...,
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(4903.9932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-82.9242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(235.8710, device='cuda:0')



h[100].sum tensor(-77.1086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.8608, device='cuda:0')



h[200].sum tensor(256.5576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-153.1426, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0263, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0000, 0.0280, 0.0000,  ..., 0.0244, 0.0000, 0.0000],
        [0.0000, 0.0194, 0.0000,  ..., 0.0151, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(279720.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0776, 0.0684],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0781, 0.0694],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0705, 0.0541],
        ...,
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0364, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0364, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2078246.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2438.8328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1183.7850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(497.8987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2643.6353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1073],
        [ 0.1029],
        [ 0.0970],
        ...,
        [-0.2312],
        [-0.2303],
        [-0.2300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-408721.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1529.6823, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1529.6823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0005,  0.0063, -0.0160,  ...,  0.0053, -0.0120, -0.0162],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0004,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5068.5273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-88.4718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(230.7931, device='cuda:0')



h[100].sum tensor(-75.3949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.4741, device='cuda:0')



h[200].sum tensor(254.1452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-149.8457, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0236, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(275284.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0638, 0.0401],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.0532, 0.0189],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0588, 0.0299],
        ...,
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0368, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0368, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0368, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2058565.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2387.0491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1226.2075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(400.4659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2578.6030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0896],
        [ 0.0953],
        [ 0.0984],
        ...,
        [-0.2363],
        [-0.2353],
        [-0.2350]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-417662.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 1500 loss: tensor(560.6950, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1513.0721, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1513.0721, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0073, -0.0195,  ...,  0.0064, -0.0146, -0.0198],
        [-0.0005,  0.0067, -0.0174,  ...,  0.0058, -0.0131, -0.0177],
        [-0.0005,  0.0068, -0.0177,  ...,  0.0058, -0.0133, -0.0180],
        ...,
        [-0.0005,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5215.0420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-93.5187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.2871, device='cuda:0')



h[100].sum tensor(-74.5517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.2963, device='cuda:0')



h[200].sum tensor(253.1111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.2186, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0289, 0.0000,  ..., 0.0253, 0.0000, 0.0000],
        [0.0000, 0.0217, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0222, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(279166.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0970, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0793, 0.0703],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0727, 0.0569],
        ...,
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0374, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0374, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0374, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2102240., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2381.1450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1217.6825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(495.2446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2567.9116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0539],
        [ 0.0630],
        [ 0.0723],
        ...,
        [-0.2421],
        [-0.2411],
        [-0.2408]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-450105.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1535.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1535.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5313.2529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-98.1182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(231.6721, device='cuda:0')



h[100].sum tensor(-75.2357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.8873, device='cuda:0')



h[200].sum tensor(254.3351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-150.4164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(277227.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0382, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0382, 0.0000],
        [0.0000, 0.0154, 0.0000,  ..., 0.0000, 0.0383, 0.0000],
        ...,
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.0379, 0.0000],
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.0379, 0.0000],
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.0379, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2090558.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2341.1838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1239.7036, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(666.5743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2483.5432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2900],
        [-0.3062],
        [-0.3037],
        ...,
        [-0.2484],
        [-0.2474],
        [-0.2471]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-479461.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1687.9701, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1687.9701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5286.2637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-102.3672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(254.6751, device='cuda:0')



h[100].sum tensor(-82.8745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.6988, device='cuda:0')



h[200].sum tensor(264.1343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-165.3514, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(296666.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0446, 0.0054],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0397, 0.0000],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0388, 0.0000],
        ...,
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0385, 0.0000],
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0385, 0.0000],
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0385, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2208434.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2429.6333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1138.6442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(864.7100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2642.1963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0210],
        [-0.0611],
        [-0.1702],
        ...,
        [-0.2525],
        [-0.2514],
        [-0.2511]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-473784.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1605.5924, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1605.5924, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0064, -0.0161,  ...,  0.0054, -0.0121, -0.0164],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5541.2988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-106.1177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(242.2462, device='cuda:0')



h[100].sum tensor(-78.5493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.8571, device='cuda:0')



h[200].sum tensor(258.4393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-157.2818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0282, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0098, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(290543.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1015e-01,
         1.3060e-01],
        [0.0000e+00, 6.3484e-05, 0.0000e+00,  ..., 0.0000e+00, 8.7763e-02,
         8.5948e-02],
        [0.0000e+00, 3.6652e-05, 0.0000e+00,  ..., 0.0000e+00, 7.3264e-02,
         5.6521e-02],
        ...,
        [0.0000e+00, 1.5603e-02, 0.0000e+00,  ..., 0.0000e+00, 3.8960e-02,
         0.0000e+00],
        [0.0000e+00, 1.5603e-02, 0.0000e+00,  ..., 0.0000e+00, 3.8960e-02,
         0.0000e+00],
        [0.0000e+00, 1.5603e-02, 0.0000e+00,  ..., 0.0000e+00, 3.8960e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2181845., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2408.3145, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1187.9253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(611.4825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2597.6470, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0637],
        [ 0.0653],
        [ 0.0688],
        ...,
        [-0.2573],
        [-0.2563],
        [-0.2560]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-452231.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1566.0785, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1566.0785, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0039, -0.0074,  ...,  0.0026, -0.0056, -0.0075],
        ...,
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5737.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-109.5452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(236.2845, device='cuda:0')



h[100].sum tensor(-76.8406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.0551, device='cuda:0')



h[200].sum tensor(255.8796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-153.4110, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(287628.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0565, 0.0220],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0579, 0.0247],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0630, 0.0344],
        ...,
        [0.0000, 0.0156, 0.0000,  ..., 0.0000, 0.0395, 0.0000],
        [0.0000, 0.0156, 0.0000,  ..., 0.0000, 0.0395, 0.0000],
        [0.0000, 0.0156, 0.0000,  ..., 0.0000, 0.0395, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2164933.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2406.1743, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1223.7097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(427.1576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2570.7437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1155],
        [ 0.1192],
        [ 0.1202],
        ...,
        [-0.2612],
        [-0.2602],
        [-0.2598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-463130.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1749.1080, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1749.1080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0093, -0.0261,  ...,  0.0085, -0.0196, -0.0266],
        ...,
        [-0.0006,  0.0051, -0.0116,  ...,  0.0040, -0.0087, -0.0118],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0005,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(5743.5986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-112.7724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(263.8993, device='cuda:0')



h[100].sum tensor(-85.8061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-124.0343, device='cuda:0')



h[200].sum tensor(266.5891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-171.3404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0148, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0174, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(303144.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0456, 0.0053],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0589, 0.0266],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0667, 0.0420],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0795, 0.0685],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0597, 0.0306],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0459, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2243280.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2502.8237, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1156.4706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(840.7144, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2713.1040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2201],
        [-0.1328],
        [-0.0583],
        ...,
        [ 0.0628],
        [ 0.0191],
        [-0.0612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-460407.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1426.4529, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1426.4529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6223.2637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-115.4325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(215.2182, device='cuda:0')



h[100].sum tensor(-69.3972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.1539, device='cuda:0')



h[200].sum tensor(245.3147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.7335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(277501.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0494, 0.0097],
        [0.0000, 0.0143, 0.0000,  ..., 0.0000, 0.0431, 0.0006],
        [0.0000, 0.0154, 0.0000,  ..., 0.0000, 0.0414, 0.0000],
        ...,
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0403, 0.0000],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0403, 0.0000],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0403, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2135938.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2380.6069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1321.6941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(373.0330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2465.5359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0542],
        [-0.0063],
        [-0.0490],
        ...,
        [-0.2684],
        [-0.2674],
        [-0.2670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-472719.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1386.3345, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1386.3345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0047, -0.0100,  ...,  0.0035, -0.0075, -0.0102],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0106, -0.0309,  ...,  0.0100, -0.0232, -0.0314],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6402.8354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-117.9873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(209.1653, device='cuda:0')



h[100].sum tensor(-67.3461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.3089, device='cuda:0')



h[200].sum tensor(242.1400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-135.8036, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0191, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(273321., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0581, 0.0236],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0693, 0.0462],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0700, 0.0473],
        ...,
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0406, 0.0000],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0406, 0.0000],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0406, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2116847.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2368.9441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1358.5765, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(263.3136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2423.3684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1283],
        [ 0.1193],
        [ 0.1052],
        ...,
        [-0.2730],
        [-0.2719],
        [-0.2716]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-469890.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1715.8019, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1715.8019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6226.7471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-120.5085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(258.8742, device='cuda:0')



h[100].sum tensor(-82.7005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-121.6724, device='cuda:0')



h[200].sum tensor(261.3760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-168.0778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0177, 0.0000,  ..., 0.0125, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(300585.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0592, 0.0257],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0658, 0.0387],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0983, 0.1037],
        ...,
        [0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.0410, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0422, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0462, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2251144., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2514.8538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1216.2615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(868.5439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2644.7446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0947],
        [ 0.0774],
        [ 0.0561],
        ...,
        [-0.2540],
        [-0.2099],
        [-0.1365]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-495389.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1433.5391, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1433.5391, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6553.8789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-122.4796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(216.2874, device='cuda:0')



h[100].sum tensor(-69.5812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.6564, device='cuda:0')



h[200].sum tensor(244.7723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.4277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(279454.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.0418, 0.0000],
        [0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.0418, 0.0000],
        [0.0000, 0.0161, 0.0000,  ..., 0.0000, 0.0419, 0.0000],
        ...,
        [0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.0415, 0.0000],
        [0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.0415, 0.0000],
        [0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.0415, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2171649., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2412.7808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1350.4490, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(425.3070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2417.6243, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3885],
        [-0.3958],
        [-0.3978],
        ...,
        [-0.2842],
        [-0.2831],
        [-0.2827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-542175.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 1800 loss: tensor(552.7349, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1243.8391, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1243.8391, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0055, -0.0126,  ...,  0.0043, -0.0094, -0.0128],
        [-0.0006,  0.0043, -0.0085,  ...,  0.0030, -0.0064, -0.0087],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6820.3535, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-124.2946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(187.6661, device='cuda:0')



h[100].sum tensor(-60.2116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.2042, device='cuda:0')



h[200].sum tensor(233.0357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-121.8449, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0000, 0.0286, 0.0000,  ..., 0.0245, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(266392.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.1495e-03, 0.0000e+00,  ..., 0.0000e+00, 5.6817e-02,
         2.0253e-02],
        [0.0000e+00, 5.0060e-03, 0.0000e+00,  ..., 0.0000e+00, 6.8798e-02,
         4.3246e-02],
        [0.0000e+00, 8.4151e-05, 0.0000e+00,  ..., 0.0000e+00, 9.1519e-02,
         8.9063e-02],
        ...,
        [0.0000e+00, 1.5695e-02, 0.0000e+00,  ..., 0.0000e+00, 4.2054e-02,
         0.0000e+00],
        [0.0000e+00, 1.5695e-02, 0.0000e+00,  ..., 0.0000e+00, 4.2054e-02,
         0.0000e+00],
        [0.0000e+00, 1.5695e-02, 0.0000e+00,  ..., 0.0000e+00, 4.2054e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2132786.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2337.6499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1437.0006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(264.3009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2267.8682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1004],
        [ 0.0991],
        [ 0.0964],
        ...,
        [-0.2898],
        [-0.2887],
        [-0.2883]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-537708.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1467.1385, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1467.1385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0045, -0.0091,  ...,  0.0032, -0.0068, -0.0092],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6767.0215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-126.1749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(221.3568, device='cuda:0')



h[100].sum tensor(-70.7797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.0390, device='cuda:0')



h[200].sum tensor(246.5504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-143.7190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0155, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0216, 0.0000,  ..., 0.0167, 0.0000, 0.0000],
        [0.0000, 0.0192, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(282633.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0789, 0.0625],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0904, 0.0862],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0933, 0.0917],
        ...,
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0426, 0.0000],
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0426, 0.0000],
        [0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0426, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2193984., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2409.1689, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1360.9337, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(416.6576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2387.0371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0995],
        [ 0.0934],
        [ 0.0878],
        ...,
        [-0.2949],
        [-0.2938],
        [-0.2934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-527479.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1535.7112, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1535.7112, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0120, -0.0354,  ...,  0.0115, -0.0265, -0.0360],
        [-0.0008,  0.0155, -0.0478,  ...,  0.0153, -0.0358, -0.0486],
        [-0.0008,  0.0124, -0.0368,  ...,  0.0119, -0.0275, -0.0374],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(6886.5176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-127.8061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(231.7028, device='cuda:0')



h[100].sum tensor(-74.0966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.9017, device='cuda:0')



h[200].sum tensor(250.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-150.4363, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0533, 0.0000,  ..., 0.0517, 0.0000, 0.0000],
        [0.0000, 0.0589, 0.0000,  ..., 0.0579, 0.0000, 0.0000],
        [0.0000, 0.0579, 0.0000,  ..., 0.0568, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(289780.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1779, 0.2592],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1941, 0.2910],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1912, 0.2852],
        ...,
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.0430, 0.0000],
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.0430, 0.0000],
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.0430, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2223230., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2428.7383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1339.5264, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(393.9190, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2421.3867, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0237],
        [ 0.0178],
        [ 0.0119],
        ...,
        [-0.2918],
        [-0.2881],
        [-0.2865]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-511659.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1311.2579, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1311.2579, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0043, -0.0084,  ...,  0.0030, -0.0063, -0.0085],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7263.8623, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-129.1059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(197.8380, device='cuda:0')



h[100].sum tensor(-62.8727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.9851, device='cuda:0')



h[200].sum tensor(236.9782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.4492, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0158, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(271613.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0754, 0.0539],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0600, 0.0242],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0499, 0.0051],
        ...,
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0434, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0434, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0434, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2154250.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2317.2742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1466.6752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(240.2339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2214.0918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1060],
        [ 0.0978],
        [ 0.0784],
        ...,
        [-0.3000],
        [-0.2989],
        [-0.2986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-518684.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1362.2043, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.2043, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7313.5254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-130.4441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(205.5247, device='cuda:0')



h[100].sum tensor(-65.2564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.5978, device='cuda:0')



h[200].sum tensor(240.9862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.4398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(280387.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0442, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0445, 0.0000],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0462, 0.0000],
        ...,
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0439, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0439, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0439, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2210477., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2364.2041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1425.5154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(387.3358, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2292.8386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0720],
        [-0.0395],
        [ 0.0219],
        ...,
        [-0.3054],
        [-0.3043],
        [-0.3039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-516710.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1784.9873, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1784.9873, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0052, -0.0114,  ...,  0.0039, -0.0085, -0.0116],
        [-0.0008,  0.0140, -0.0425,  ...,  0.0137, -0.0318, -0.0433],
        ...,
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7002.4731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-131.8897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(269.3127, device='cuda:0')



h[100].sum tensor(-85.1078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-126.5786, device='cuda:0')



h[200].sum tensor(268.0607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-174.8551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0165, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0000, 0.0330, 0.0000,  ..., 0.0292, 0.0000, 0.0000],
        [0.0000, 0.0415, 0.0000,  ..., 0.0386, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(314173.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.7725e-03, 0.0000e+00,  ..., 0.0000e+00, 9.8417e-02,
         9.7020e-02],
        [0.0000e+00, 6.0102e-05, 0.0000e+00,  ..., 0.0000e+00, 1.4579e-01,
         1.9132e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8446e-01,
         2.6777e-01],
        ...,
        [0.0000e+00, 1.4922e-02, 0.0000e+00,  ..., 0.0000e+00, 4.4536e-02,
         0.0000e+00],
        [0.0000e+00, 1.4922e-02, 0.0000e+00,  ..., 0.0000e+00, 4.4536e-02,
         0.0000e+00],
        [0.0000e+00, 1.4922e-02, 0.0000e+00,  ..., 0.0000e+00, 4.4536e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2366318.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2539.6665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1230.3696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(786.9358, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2572.1055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0361],
        [ 0.0131],
        [-0.0112],
        ...,
        [-0.3131],
        [-0.3119],
        [-0.3116]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-551179.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1641.8480, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1641.8480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0050, -0.0107,  ...,  0.0037, -0.0080, -0.0109],
        [-0.0007,  0.0045, -0.0090,  ...,  0.0031, -0.0067, -0.0092],
        [-0.0007,  0.0076, -0.0197,  ...,  0.0065, -0.0147, -0.0200],
        ...,
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0006,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7179.3242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-132.8852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(247.7163, device='cuda:0')



h[100].sum tensor(-78.7618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.4281, device='cuda:0')



h[200].sum tensor(261.4048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-160.8333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0151, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0258, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0000, 0.0217, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(301838.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0730, 0.0455],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0893, 0.0789],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0929, 0.0859],
        ...,
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0451, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0451, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0451, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2304409., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2471.0483, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1297.8345, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(511.3165, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2447.8291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1166],
        [ 0.1131],
        [ 0.1096],
        ...,
        [-0.3191],
        [-0.3178],
        [-0.3174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-547195.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.6771, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.6771, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7476.8491, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-133.7171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(213.8942, device='cuda:0')



h[100].sum tensor(-67.7685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.5315, device='cuda:0')



h[200].sum tensor(248.3170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.8738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(285223.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0467, 0.0000],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0493, 0.0012],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0583, 0.0160],
        ...,
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0456, 0.0000],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0456, 0.0000],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0456, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2245996., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2384.9785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1401.2068, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(300.2323, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2275.6040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0934],
        [ 0.0086],
        [ 0.0783],
        ...,
        [-0.3233],
        [-0.3220],
        [-0.3215]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-548578., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1685.3417, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1685.3417, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0060, -0.0141,  ...,  0.0048, -0.0105, -0.0143],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0060, -0.0141,  ...,  0.0048, -0.0105, -0.0143],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7437.8828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-134.7742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(254.2785, device='cuda:0')



h[100].sum tensor(-80.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-119.5124, device='cuda:0')



h[200].sum tensor(265.2750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-165.0939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0113, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0225, 0.0000,  ..., 0.0174, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(308636.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0575, 0.0117],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0656, 0.0284],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0576, 0.0117],
        ...,
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0461, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0461, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0461, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2342501.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2502.5896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1277.7991, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(626.5072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2482.1309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1477],
        [-0.1529],
        [-0.2035],
        ...,
        [-0.3253],
        [-0.3241],
        [-0.3238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-527505.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1514.9167, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1514.9167, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0039, -0.0067,  ...,  0.0025, -0.0050, -0.0068],
        [-0.0009,  0.0159, -0.0490,  ...,  0.0158, -0.0366, -0.0499],
        [-0.0008,  0.0125, -0.0369,  ...,  0.0120, -0.0276, -0.0376],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7758.6855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-135.4625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.5654, device='cuda:0')



h[100].sum tensor(-71.8994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.4271, device='cuda:0')



h[200].sum tensor(254.1210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.3993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0551, 0.0000,  ..., 0.0536, 0.0000, 0.0000],
        [0.0000, 0.0372, 0.0000,  ..., 0.0338, 0.0000, 0.0000],
        [0.0000, 0.0530, 0.0000,  ..., 0.0514, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(298474.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1913, 0.2780],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1799, 0.2554],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1876, 0.2705],
        ...,
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0465, 0.0000],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0465, 0.0000],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0465, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2308367.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2446.5967, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1344.0421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(465.4338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2387.7139, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0273],
        [ 0.0249],
        [ 0.0278],
        ...,
        [-0.3266],
        [-0.3255],
        [-0.3251]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-492203.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 2100 loss: tensor(556.7560, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1619.7068, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1619.7068, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7789.1914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-136.2654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(244.3757, device='cuda:0')



h[100].sum tensor(-77.3126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.8580, device='cuda:0')



h[200].sum tensor(261.1988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-158.6644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(311689.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0472, 0.0000],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0473, 0.0000],
        [0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.0475, 0.0000],
        ...,
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0470, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0470, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0470, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2377336., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2515.3608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1275.5266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(482.0088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2497.5991, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3720],
        [-0.3143],
        [-0.2297],
        ...,
        [-0.3304],
        [-0.3292],
        [-0.3288]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-495516.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1487.3138, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1487.3138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0180, -0.0562,  ...,  0.0181, -0.0420, -0.0572],
        [-0.0009,  0.0134, -0.0401,  ...,  0.0130, -0.0299, -0.0408],
        [-0.0009,  0.0153, -0.0466,  ...,  0.0151, -0.0348, -0.0474],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(7951.9697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-136.8341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.4007, device='cuda:0')



h[100].sum tensor(-70.5288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4697, device='cuda:0')



h[200].sum tensor(252.4772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.6954, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0341, 0.0000,  ..., 0.0302, 0.0000, 0.0000],
        [0.0000, 0.0610, 0.0000,  ..., 0.0601, 0.0000, 0.0000],
        [0.0000, 0.0451, 0.0000,  ..., 0.0424, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(297203.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 8.7456e-04, 0.0000e+00,  ..., 0.0000e+00, 1.3706e-01,
         1.6796e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.6526e-01,
         2.2372e-01],
        [0.0000e+00, 2.0707e-04, 0.0000e+00,  ..., 0.0000e+00, 1.4851e-01,
         1.9068e-01],
        ...,
        [0.0000e+00, 1.4295e-02, 0.0000e+00,  ..., 0.0000e+00, 4.7399e-02,
         0.0000e+00],
        [0.0000e+00, 1.4295e-02, 0.0000e+00,  ..., 0.0000e+00, 4.7399e-02,
         0.0000e+00],
        [0.0000e+00, 1.4295e-02, 0.0000e+00,  ..., 0.0000e+00, 4.7399e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2317183., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2443.6575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1373.1393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(301.3870, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2331.8599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0931],
        [ 0.0850],
        [ 0.0805],
        ...,
        [-0.3361],
        [-0.3349],
        [-0.3345]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-533272.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1329.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1329.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0107, -0.0305,  ...,  0.0100, -0.0228, -0.0310],
        [-0.0008,  0.0108, -0.0307,  ...,  0.0100, -0.0229, -0.0312],
        [-0.0008,  0.0111, -0.0319,  ...,  0.0104, -0.0238, -0.0324],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8096.7427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-137.3256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(200.5477, device='cuda:0')



h[100].sum tensor(-62.7094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.2586, device='cuda:0')



h[200].sum tensor(242.3148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-130.2084, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0399, 0.0000,  ..., 0.0366, 0.0000, 0.0000],
        [0.0000, 0.0404, 0.0000,  ..., 0.0372, 0.0000, 0.0000],
        [0.0000, 0.0326, 0.0000,  ..., 0.0284, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(281829.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.4859e-04, 0.0000e+00,  ..., 0.0000e+00, 1.4013e-01,
         1.7422e-01],
        [0.0000e+00, 1.7286e-04, 0.0000e+00,  ..., 0.0000e+00, 1.4081e-01,
         1.7556e-01],
        [0.0000e+00, 2.8476e-03, 0.0000e+00,  ..., 0.0000e+00, 1.2328e-01,
         1.4021e-01],
        ...,
        [0.0000e+00, 1.4161e-02, 0.0000e+00,  ..., 0.0000e+00, 4.7832e-02,
         0.0000e+00],
        [0.0000e+00, 1.4161e-02, 0.0000e+00,  ..., 0.0000e+00, 4.7832e-02,
         0.0000e+00],
        [0.0000e+00, 1.4161e-02, 0.0000e+00,  ..., 0.0000e+00, 4.7832e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2259705., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2371.9971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1474.2983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(171.2560, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2168.9297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0885],
        [ 0.0842],
        [ 0.0779],
        ...,
        [-0.3424],
        [-0.3412],
        [-0.3408]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-568243., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1225.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1225.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0128, -0.0378,  ...,  0.0123, -0.0282, -0.0385],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0063, -0.0150,  ...,  0.0051, -0.0112, -0.0152],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8193.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-137.7993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(184.9135, device='cuda:0')



h[100].sum tensor(-57.7192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.9104, device='cuda:0')



h[200].sum tensor(235.7339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-120.0577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0236, 0.0000,  ..., 0.0183, 0.0000, 0.0000],
        [0.0000, 0.0356, 0.0000,  ..., 0.0316, 0.0000, 0.0000],
        [0.0000, 0.0168, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(273115.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0975, 0.0872],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.1047, 0.1017],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0851, 0.0622],
        ...,
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0482, 0.0000],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0482, 0.0000],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0482, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2239786.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2319.8809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1536.3718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(180.6905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2044.6080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0850],
        [ 0.1024],
        [ 0.1172],
        ...,
        [-0.3486],
        [-0.3473],
        [-0.3469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-632087.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1495.7053, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1495.7053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8087.6523, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-138.4655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.6668, device='cuda:0')



h[100].sum tensor(-70.2087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.0647, device='cuda:0')



h[200].sum tensor(250.9874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.5174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0175, 0.0000,  ..., 0.0114, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(295922.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0794, 0.0497],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0639, 0.0195],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0548, 0.0042],
        ...,
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0486, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0486, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0486, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2332574.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2446.1821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1416.3164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(326.3793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2276.7710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1019],
        [ 0.0923],
        [ 0.0513],
        ...,
        [-0.3546],
        [-0.3533],
        [-0.3529]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-589436.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1670.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1670.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0064, -0.0152,  ...,  0.0052, -0.0113, -0.0154],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0008,  0.0068, -0.0166,  ...,  0.0056, -0.0124, -0.0169],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8102.7549, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-139.0286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(252.0333, device='cuda:0')



h[100].sum tensor(-78.4902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.4571, device='cuda:0')



h[200].sum tensor(260.3487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-163.6362, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0160, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0329, 0.0000,  ..., 0.0287, 0.0000, 0.0000],
        [0.0000, 0.0347, 0.0000,  ..., 0.0306, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(313969.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.4748e-03, 0.0000e+00,  ..., 0.0000e+00, 1.0661e-01,
         1.0318e-01],
        [0.0000e+00, 1.6333e-03, 0.0000e+00,  ..., 0.0000e+00, 1.4541e-01,
         1.7996e-01],
        [0.0000e+00, 2.2885e-04, 0.0000e+00,  ..., 0.0000e+00, 1.7092e-01,
         2.2957e-01],
        ...,
        [0.0000e+00, 1.3135e-02, 0.0000e+00,  ..., 0.0000e+00, 4.8887e-02,
         0.0000e+00],
        [0.0000e+00, 1.3135e-02, 0.0000e+00,  ..., 0.0000e+00, 4.8887e-02,
         0.0000e+00],
        [0.0000e+00, 1.3135e-02, 0.0000e+00,  ..., 0.0000e+00, 4.8887e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2433855.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2553.0100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1334.9071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(601.6613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2448.3494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0900],
        [ 0.0718],
        [ 0.0533],
        ...,
        [-0.3599],
        [-0.3587],
        [-0.3583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-573383.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1463.7250, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1463.7250, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0008,  0.0095, -0.0261,  ...,  0.0086, -0.0194, -0.0265],
        [-0.0007,  0.0060, -0.0135,  ...,  0.0046, -0.0101, -0.0138],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8250.1162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-138.8874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(220.8417, device='cuda:0')



h[100].sum tensor(-68.4289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.7969, device='cuda:0')



h[200].sum tensor(247.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-143.3846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0284, 0.0000,  ..., 0.0236, 0.0000, 0.0000],
        [0.0000, 0.0184, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        [0.0000, 0.0191, 0.0000,  ..., 0.0132, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(294512.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0847, 0.0597],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0807, 0.0515],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0815, 0.0526],
        ...,
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0489, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0489, 0.0000],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0489, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2343251.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2451.0579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1449.1758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(404.4356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2262.6455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1455],
        [ 0.1493],
        [ 0.1528],
        ...,
        [-0.3600],
        [-0.3587],
        [-0.3583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-587378.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1668.0586, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1668.0586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8258.5801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-139.4258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(251.6709, device='cuda:0')



h[100].sum tensor(-77.8872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.2868, device='cuda:0')



h[200].sum tensor(258.6610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-163.4009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0211, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(316904.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0657, 0.0206],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0687, 0.0269],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0861, 0.0617],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0492, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0492, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0492, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2442258.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2554.8687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1329.7217, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(438.9890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2458.2202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1220],
        [ 0.1227],
        [ 0.1236],
        ...,
        [-0.3633],
        [-0.3620],
        [-0.3616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-542255.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1458.3992, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1458.3992, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8389.3027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-139.2978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(220.0382, device='cuda:0')



h[100].sum tensor(-68.8103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.4193, device='cuda:0')



h[200].sum tensor(246.7506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.8629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(297931.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0502, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0535, 0.0008],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0589, 0.0088],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0492, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0492, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0492, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2364173.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2455.1597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1443.7253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(445.3260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2268.5898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0395],
        [ 0.0480],
        [ 0.1036],
        ...,
        [-0.3633],
        [-0.3620],
        [-0.3616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-569546.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1389.6379, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1389.6379, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8525.9590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-139.6170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(209.6637, device='cuda:0')



h[100].sum tensor(-65.4268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.5432, device='cuda:0')



h[200].sum tensor(241.9595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.1272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(292043.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0501, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0525, 0.0006],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0600, 0.0114],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0495, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0495, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0495, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2336572.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2391.5415, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1483.0901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(228.5091, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2166.8223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2497],
        [-0.1291],
        [ 0.0085],
        ...,
        [-0.3689],
        [-0.3675],
        [-0.3671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-579561.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 2400 loss: tensor(439.2231, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1511.4043, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1511.4043, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0123, -0.0358,  ...,  0.0117, -0.0267, -0.0364],
        [-0.0009,  0.0122, -0.0354,  ...,  0.0116, -0.0264, -0.0360],
        [-0.0009,  0.0127, -0.0373,  ...,  0.0122, -0.0278, -0.0379],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8581.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-140.0157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.0354, device='cuda:0')



h[100].sum tensor(-70.0665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.1780, device='cuda:0')



h[200].sum tensor(247.7219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.0552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0528, 0.0000,  ..., 0.0510, 0.0000, 0.0000],
        [0.0000, 0.0509, 0.0000,  ..., 0.0489, 0.0000, 0.0000],
        [0.0000, 0.0421, 0.0000,  ..., 0.0390, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(301982.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.9118e-01,
         2.7106e-01],
        [0.0000e+00, 1.4351e-04, 0.0000e+00,  ..., 0.0000e+00, 1.8374e-01,
         2.5633e-01],
        [0.0000e+00, 9.3683e-04, 0.0000e+00,  ..., 0.0000e+00, 1.6688e-01,
         2.2274e-01],
        ...,
        [0.0000e+00, 1.2776e-02, 0.0000e+00,  ..., 0.0000e+00, 4.9654e-02,
         0.0000e+00],
        [0.0000e+00, 1.2776e-02, 0.0000e+00,  ..., 0.0000e+00, 4.9654e-02,
         0.0000e+00],
        [0.0000e+00, 1.2776e-02, 0.0000e+00,  ..., 0.0000e+00, 4.9654e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2395525.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2424.8840, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1439.2986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(462.2927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2215.2576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0515],
        [ 0.0541],
        [ 0.0581],
        ...,
        [-0.3734],
        [-0.3721],
        [-0.3717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-603967.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1620.7303, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1620.7303, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0064, -0.0151,  ...,  0.0051, -0.0112, -0.0153],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0008,  0.0064, -0.0151,  ...,  0.0051, -0.0112, -0.0153],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8595.2637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-140.3990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(244.5302, device='cuda:0')



h[100].sum tensor(-75.7717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.9306, device='cuda:0')



h[200].sum tensor(255.3275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-158.7647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0121, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0241, 0.0000,  ..., 0.0188, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(316918., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0626, 0.0128],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0716, 0.0315],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0628, 0.0128],
        ...,
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0499, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0499, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0499, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2480412.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2491.0569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1365.0620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(648.7186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2332.8027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1199],
        [-0.1127],
        [-0.1455],
        ...,
        [-0.3785],
        [-0.3772],
        [-0.3768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-598443.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1465.4219, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1465.4219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8806.5059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-140.5616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(221.0978, device='cuda:0')



h[100].sum tensor(-68.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.9173, device='cuda:0')



h[200].sum tensor(245.1493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-143.5509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(302490.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0533, 0.0007],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0508, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.0505, 0.0000],
        ...,
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0501, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0501, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0509, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2407073.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2414.3494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1462.3729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(316.3589, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2185.6792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1325],
        [-0.2222],
        [-0.2510],
        ...,
        [-0.3767],
        [-0.3504],
        [-0.2816]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-578687.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1372.7136, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1372.7136, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0045, -0.0081,  ...,  0.0029, -0.0061, -0.0083],
        [-0.0008,  0.0093, -0.0251,  ...,  0.0084, -0.0187, -0.0255],
        [-0.0008,  0.0093, -0.0251,  ...,  0.0084, -0.0187, -0.0255],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(8959.1816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-140.7484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(207.1103, device='cuda:0')



h[100].sum tensor(-63.7611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.3430, device='cuda:0')



h[200].sum tensor(239.7462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-134.4693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0293, 0.0000,  ..., 0.0247, 0.0000, 0.0000],
        [0.0000, 0.0239, 0.0000,  ..., 0.0185, 0.0000, 0.0000],
        [0.0000, 0.0247, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(293447.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1090, 0.1065],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.1041, 0.0962],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1018, 0.0913],
        ...,
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0503, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0503, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0503, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2366609.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2374.2422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1534.8286, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(180.9782, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2089.3328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1410],
        [ 0.1412],
        [ 0.1431],
        ...,
        [-0.3887],
        [-0.3873],
        [-0.3869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-604183.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1355.4871, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1355.4871, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0088, -0.0233,  ...,  0.0078, -0.0173, -0.0237],
        [-0.0007,  0.0040, -0.0065,  ...,  0.0024, -0.0049, -0.0066],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9069.7715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-140.9606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(204.5112, device='cuda:0')



h[100].sum tensor(-62.9029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.1215, device='cuda:0')



h[200].sum tensor(238.9619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.7818, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0200, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0000, 0.0183, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0184, 0.0000,  ..., 0.0124, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(293434.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0882, 0.0628],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0849, 0.0557],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0881, 0.0617],
        ...,
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0506, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0506, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0506, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2382356., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2391.8594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1567.8804, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(233.9303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2066.2578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1342],
        [ 0.1354],
        [ 0.1351],
        ...,
        [-0.3938],
        [-0.3925],
        [-0.3921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-658675.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1409.5927, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1409.5927, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0048, -0.0091,  ...,  0.0033, -0.0068, -0.0093],
        [-0.0008,  0.0072, -0.0176,  ...,  0.0060, -0.0131, -0.0179],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9144.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.1932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(212.6744, device='cuda:0')



h[100].sum tensor(-64.9105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.9582, device='cuda:0')



h[200].sum tensor(242.2042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.0819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0176, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        [0.0000, 0.0258, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(298063.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0637, 0.0159],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0787, 0.0429],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0912, 0.0682],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0509, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0509, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0509, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2402788.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2431.4695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1559.7198, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(206.6628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2142.2305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0052],
        [ 0.1012],
        [ 0.1433],
        ...,
        [-0.3989],
        [-0.3976],
        [-0.3972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-592112.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1370.2076, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1370.2076, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0067, -0.0159,  ...,  0.0054, -0.0118, -0.0161],
        [-0.0008,  0.0067, -0.0159,  ...,  0.0054, -0.0118, -0.0161],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9242.6348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.3530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(206.7322, device='cuda:0')



h[100].sum tensor(-63.2063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.1653, device='cuda:0')



h[200].sum tensor(240.8724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-134.2238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0205, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0196, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0223, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(294853.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.1063, 0.0985],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0989, 0.0834],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0951, 0.0754],
        ...,
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0512, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0512, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0512, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2393772., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2417.5820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1601.6702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(194.4624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2086.4370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1134],
        [ 0.1195],
        [ 0.1268],
        ...,
        [-0.4055],
        [-0.4041],
        [-0.4036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-626330.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1536.8972, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1536.8972, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0047, -0.0088,  ...,  0.0032, -0.0066, -0.0090],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9210.5752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.6344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(231.8817, device='cuda:0')



h[100].sum tensor(-71.0277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.9858, device='cuda:0')



h[200].sum tensor(252.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-150.5525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0207, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(311335.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0604, 0.0071],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0654, 0.0143],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0731, 0.0295],
        ...,
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0515, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0515, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0515, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2468077., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2492.5054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1517.0721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(370.9896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2227.6147, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0584],
        [ 0.1182],
        [ 0.1454],
        ...,
        [-0.4111],
        [-0.4096],
        [-0.4092]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-619492.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1283.8840, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1283.8840, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0022,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9479.7910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.6091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.7080, device='cuda:0')



h[100].sum tensor(-58.9191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.0439, device='cuda:0')



h[200].sum tensor(237.6154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.7676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(288988.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0525, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0526, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0529, 0.0000],
        ...,
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0518, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0518, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0518, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2384334.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2361.0632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1669.0070, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(267.8364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1970.8076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2031],
        [-0.2255],
        [-0.2299],
        ...,
        [-0.4158],
        [-0.4143],
        [-0.4138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-678029.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1454.0378, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1454.0378, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0008,  0.0066, -0.0156,  ...,  0.0054, -0.0116, -0.0159],
        [-0.0008,  0.0066, -0.0156,  ...,  0.0054, -0.0116, -0.0159],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9616.2949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.8591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(219.3802, device='cuda:0')



h[100].sum tensor(-66.5927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.1100, device='cuda:0')



h[200].sum tensor(248.2390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.4357, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0166, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0166, 0.0000,  ..., 0.0106, 0.0000, 0.0000],
        [0.0000, 0.0166, 0.0000,  ..., 0.0106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(304531.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0602, 0.0066],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0544, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0535, 0.0000],
        ...,
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0742, 0.0319],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0742, 0.0319],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0703, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2440724.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2441.0986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1597.8420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(297.8284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2120.8218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1961],
        [-0.2554],
        [-0.2333],
        ...,
        [ 0.0171],
        [ 0.0184],
        [-0.0522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-619067., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 2700 loss: tensor(544.7155, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1392.3887, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1392.3887, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0041, -0.0071,  ...,  0.0026, -0.0053, -0.0072],
        [-0.0008,  0.0082, -0.0214,  ...,  0.0073, -0.0159, -0.0218],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9821.3086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.9419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(210.0788, device='cuda:0')



h[100].sum tensor(-63.7429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.7383, device='cuda:0')



h[200].sum tensor(245.3228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.3966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0171, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0209, 0.0000,  ..., 0.0155, 0.0000, 0.0000],
        [0.0000, 0.0193, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(302974.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0792, 0.0405],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0888, 0.0605],
        [0.0000, 0.0151, 0.0000,  ..., 0.0000, 0.0941, 0.0711],
        ...,
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0523, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0523, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0523, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2436921.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2417.0403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1618.1731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(211.4822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2090.6240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1402],
        [ 0.1373],
        [ 0.1336],
        ...,
        [-0.4197],
        [-0.4184],
        [-0.4180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-596059., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1233.3845, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1233.3845, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9980.4932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-141.9429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(186.0888, device='cuda:0')



h[100].sum tensor(-56.1566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.4628, device='cuda:0')



h[200].sum tensor(236.3355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-120.8208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0122, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0143, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(286389.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0697, 0.0193],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0640, 0.0076],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0704, 0.0207],
        ...,
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0527, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0527, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0527, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2370280., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2316.0488, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1733.0667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(132.4579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1900.4099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1589],
        [ 0.1615],
        [ 0.1607],
        ...,
        [-0.4272],
        [-0.4258],
        [-0.4254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-645594., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1235.7814, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1235.7814, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0066, -0.0156,  ...,  0.0054, -0.0116, -0.0159],
        [-0.0008,  0.0072, -0.0180,  ...,  0.0062, -0.0134, -0.0184],
        [-0.0008,  0.0070, -0.0172,  ...,  0.0059, -0.0128, -0.0175],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10047.6113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.0468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(186.4504, device='cuda:0')



h[100].sum tensor(-56.2691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.6328, device='cuda:0')



h[200].sum tensor(237.2248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-121.0556, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0254, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0259, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0193, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(291590.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0881, 0.0570],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0906, 0.0621],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0838, 0.0476],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0531, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0531, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0531, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2417760.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2341.3613, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1724.5291, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(259.0168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1941.7479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1038],
        [ 0.1342],
        [ 0.1474],
        ...,
        [-0.4327],
        [-0.4310],
        [-0.4304]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-660039.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1693.0553, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1693.0553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9864.8848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.4541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(255.4423, device='cuda:0')



h[100].sum tensor(-77.4642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.0594, device='cuda:0')



h[200].sum tensor(266.5887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-165.8495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(331780.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0553, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0540, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0540, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0535, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0535, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0535, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2603406.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2558.0186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1507.8315, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(803.4921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2335.4966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0925],
        [-0.2061],
        [-0.2828],
        ...,
        [-0.4403],
        [-0.4387],
        [-0.4383]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-659788.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1622.2264, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1622.2264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0009,  0.0112, -0.0319,  ...,  0.0107, -0.0237, -0.0325],
        [-0.0008,  0.0083, -0.0219,  ...,  0.0074, -0.0163, -0.0223],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(9977.1318, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.4788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(244.7559, device='cuda:0')



h[100].sum tensor(-73.4207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.0367, device='cuda:0')



h[200].sum tensor(261.8388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-158.9112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0438, 0.0000,  ..., 0.0416, 0.0000, 0.0000],
        [0.0000, 0.0221, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0000, 0.0227, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(320237.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1437, 0.1659],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.1112, 0.1007],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0993, 0.0762],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0538, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0538, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0538, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2546959.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2494.0217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1585.7211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(483.7465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2228.2688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1208],
        [ 0.1198],
        [ 0.0975],
        ...,
        [-0.4462],
        [-0.4446],
        [-0.4442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-676970.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1556.0378, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1556.0378, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10006.9385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.4363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(234.7696, device='cuda:0')



h[100].sum tensor(-70.5619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.3431, device='cuda:0')



h[200].sum tensor(257.9673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-152.4275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(316157.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0675, 0.0164],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0568, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0546, 0.0000],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0538, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0538, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0538, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2518564., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2472.6343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1605.3690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(280.3918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2206.6353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0574],
        [-0.0445],
        [-0.1641],
        ...,
        [-0.4462],
        [-0.4446],
        [-0.4442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-631157.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1504.6777, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1504.6777, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0115, -0.0331,  ...,  0.0110, -0.0246, -0.0337],
        [-0.0008,  0.0091, -0.0247,  ...,  0.0083, -0.0183, -0.0251],
        [-0.0007,  0.0049, -0.0098,  ...,  0.0035, -0.0073, -0.0100],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10138.6289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.4809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(227.0205, device='cuda:0')



h[100].sum tensor(-68.4064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.7010, device='cuda:0')



h[200].sum tensor(255.4526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-147.3963, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0470, 0.0000,  ..., 0.0453, 0.0000, 0.0000],
        [0.0000, 0.0368, 0.0000,  ..., 0.0337, 0.0000, 0.0000],
        [0.0000, 0.0213, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(317966.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.1597, 0.1969],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.1352, 0.1479],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.1033, 0.0831],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0542, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0542, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0542, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2551432., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2463.3311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1606.4022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(347.3113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2199.0698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1277],
        [ 0.1305],
        [ 0.1236],
        ...,
        [-0.4516],
        [-0.4500],
        [-0.4495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-651472.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1458.2061, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1458.2061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0179, -0.0559,  ...,  0.0184, -0.0415, -0.0569],
        [-0.0010,  0.0171, -0.0529,  ...,  0.0174, -0.0393, -0.0539],
        [-0.0009,  0.0125, -0.0367,  ...,  0.0122, -0.0273, -0.0374],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10288.1133, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.5126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(220.0091, device='cuda:0')



h[100].sum tensor(-65.8690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.4056, device='cuda:0')



h[200].sum tensor(252.4810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.8440, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0705, 0.0000,  ..., 0.0722, 0.0000, 0.0000],
        [0.0000, 0.0631, 0.0000,  ..., 0.0638, 0.0000, 0.0000],
        [0.0000, 0.0432, 0.0000,  ..., 0.0411, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(307688.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.2503, 0.3766],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.2242, 0.3246],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1818, 0.2398],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0545, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0545, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0545, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2488560.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2377.2651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1674.9163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(184.0707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2069.4106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0629],
        [ 0.0742],
        [ 0.0897],
        ...,
        [-0.4571],
        [-0.4555],
        [-0.4550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-681195.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1430.3231, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1430.3231, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0068, -0.0165,  ...,  0.0057, -0.0123, -0.0168],
        [-0.0007,  0.0041, -0.0071,  ...,  0.0027, -0.0053, -0.0073],
        [-0.0007,  0.0048, -0.0094,  ...,  0.0034, -0.0070, -0.0096],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10459.9365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.5592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(215.8022, device='cuda:0')



h[100].sum tensor(-64.7672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.4283, device='cuda:0')



h[200].sum tensor(251.1345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.1126, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0170, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0217, 0.0000,  ..., 0.0166, 0.0000, 0.0000],
        [0.0000, 0.0143, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(313736.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0818, 0.0387],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0810, 0.0371],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0718, 0.0180],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0548, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0548, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0548, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2536657., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2377.0879, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1649.8679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(349.4821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2098.7026, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1554],
        [ 0.1385],
        [ 0.0924],
        ...,
        [-0.4620],
        [-0.4604],
        [-0.4599]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-660026.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1435.7964, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1435.7964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10644.5527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.6162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(216.6280, device='cuda:0')



h[100].sum tensor(-64.7439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.8164, device='cuda:0')



h[200].sum tensor(251.2547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.6488, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(314791.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0559, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0620, 0.0054],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0808, 0.0384],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0552, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0552, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0552, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2546723.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2357.7209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1657.9688, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(285.1499, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2075.9194, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0188],
        [ 0.0346],
        [ 0.0704],
        ...,
        [-0.4644],
        [-0.4631],
        [-0.4628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-667046.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 3000 loss: tensor(498.8407, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1516.6729, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1516.6729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10733.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.7190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.8303, device='cuda:0')



h[100].sum tensor(-68.1161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.5516, device='cuda:0')



h[200].sum tensor(256.3228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.5713, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(318275.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0564, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0584, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0641, 0.0055],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0555, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0555, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0555, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2557596.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2354.9666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1643.8214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(325.3249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2095.0515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5471],
        [-0.4695],
        [-0.3585],
        ...,
        [-0.4708],
        [-0.4692],
        [-0.4688]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-657819.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1423.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1423.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10850.6465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.7050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.8077, device='cuda:0')



h[100].sum tensor(-64.0523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.9609, device='cuda:0')



h[200].sum tensor(251.0846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.4670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(314736.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0562, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0571, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0592, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0559, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0559, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0559, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2559396.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2330.6396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1678.6591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(327.4084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2048.3811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2234],
        [-0.1157],
        [-0.0151],
        ...,
        [-0.4779],
        [-0.4763],
        [-0.4759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-694752.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1514.6685, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1514.6685, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0119, -0.0347,  ...,  0.0116, -0.0257, -0.0353],
        [-0.0009,  0.0115, -0.0332,  ...,  0.0111, -0.0246, -0.0338],
        [-0.0009,  0.0141, -0.0427,  ...,  0.0142, -0.0316, -0.0434],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10855.8760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.8036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(228.5279, device='cuda:0')



h[100].sum tensor(-67.7314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.4095, device='cuda:0')



h[200].sum tensor(256.7370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.3750, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0349, 0.0000,  ..., 0.0319, 0.0000, 0.0000],
        [0.0000, 0.0471, 0.0000,  ..., 0.0459, 0.0000, 0.0000],
        [0.0000, 0.0420, 0.0000,  ..., 0.0400, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(322459.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.1342, 0.1413],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1579, 0.1891],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.1586, 0.1906],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0563, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0563, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0563, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2617367., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2377.7769, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1647.7927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(702.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2133.5647, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1269],
        [ 0.1294],
        [ 0.1311],
        ...,
        [-0.4867],
        [-0.4851],
        [-0.4846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-701722.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1403.1371, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1403.1371, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(10932.1836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.7664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(211.7004, device='cuda:0')



h[100].sum tensor(-62.6953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.5005, device='cuda:0')



h[200].sum tensor(250.3977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-137.4495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(308088.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0584, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0587, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0589, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0567, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0567, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0567, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2543026., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2311.8833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1750.3889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(304.2808, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2000.2574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1833],
        [-0.1329],
        [-0.1060],
        ...,
        [-0.4949],
        [-0.4932],
        [-0.4927]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-774165.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1473.1974, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1473.1974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11000.9932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.8488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(222.2709, device='cuda:0')



h[100].sum tensor(-65.7785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.4686, device='cuda:0')



h[200].sum tensor(254.3054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-144.3125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(318401.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0574, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0585, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0603, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0571, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0571, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0571, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2594320., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2374.4866, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1708.5366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(328.2125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2126.0767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3917],
        [-0.2448],
        [-0.0762],
        ...,
        [-0.5011],
        [-0.4994],
        [-0.4988]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-718046.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1288.5269, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1288.5269, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0021,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11208.9980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.7550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(194.4084, device='cuda:0')



h[100].sum tensor(-57.4972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.3731, device='cuda:0')



h[200].sum tensor(242.0575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.2224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(304371.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0658, 0.0057],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0629, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0710, 0.0133],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0574, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0574, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0574, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2549711.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2285.7542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1807.8561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(228.9248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1962.3013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9720e-02],
        [-5.8847e-02],
        [ 2.6081e-04],
        ...,
        [-5.0572e-01],
        [-5.0397e-01],
        [-5.0333e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-776712.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1354.6653, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1354.6653, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11337.0605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.8272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(204.3872, device='cuda:0')



h[100].sum tensor(-60.3130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.0632, device='cuda:0')



h[200].sum tensor(245.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.7013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(308329.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0612, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0645, 0.0018],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0672, 0.0036],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0577, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0577, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0577, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2557018., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2281.4175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1789.5830, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(182.0725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1989.4755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0235],
        [ 0.0364],
        [ 0.0584],
        ...,
        [-0.5099],
        [-0.5082],
        [-0.5078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-729941.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1467.8224, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1467.8224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11428.2393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(221.4599, device='cuda:0')



h[100].sum tensor(-65.1516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.0875, device='cuda:0')



h[200].sum tensor(251.6096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-143.7860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0164, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(321781.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0653, 0.0053],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0810, 0.0291],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0580, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0580, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0585, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2631423.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2304.2769, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1740.2778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(370.9384, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2113.3760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0574],
        [ 0.0020],
        [ 0.0686],
        ...,
        [-0.5117],
        [-0.4957],
        [-0.4505]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-694314.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1351.8091, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1351.8091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11571.9502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.8725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(203.9563, device='cuda:0')



h[100].sum tensor(-59.9368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.8606, device='cuda:0')



h[200].sum tensor(244.1182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.4215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(314243.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0622, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0594, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0587, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0582, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0582, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0582, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2617623.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2249.5627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1774.3462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(336.4931, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1978.6333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2175],
        [-0.4091],
        [-0.5496],
        ...,
        [-0.5210],
        [-0.5193],
        [-0.5188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-737370.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1449.7013, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1449.7013, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0008,  0.0078, -0.0203,  ...,  0.0070, -0.0150, -0.0207],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11542.3145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(218.7259, device='cuda:0')



h[100].sum tensor(-64.1230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.8025, device='cuda:0')



h[200].sum tensor(249.9341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.0109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0289, 0.0000,  ..., 0.0255, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(323411.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1032, 0.0742],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0805, 0.0275],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0735, 0.0149],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0582, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0582, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0582, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2654957.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2299.1055, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1716.3218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(354.7872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2084.6990, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1134],
        [ 0.0056],
        [-0.1640],
        ...,
        [-0.5210],
        [-0.5193],
        [-0.5188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-677149.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 3300 loss: tensor(483.6737, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1399.3529, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1399.3529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0071, -0.0179,  ...,  0.0062, -0.0132, -0.0182],
        [-0.0008,  0.0067, -0.0165,  ...,  0.0057, -0.0122, -0.0168],
        [-0.0008,  0.0093, -0.0260,  ...,  0.0088, -0.0192, -0.0264],
        ...,
        [-0.0008,  0.0097, -0.0273,  ...,  0.0092, -0.0202, -0.0278],
        [-0.0008,  0.0058, -0.0133,  ...,  0.0047, -0.0099, -0.0136],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11616.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(211.1295, device='cuda:0')



h[100].sum tensor(-61.6441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.2321, device='cuda:0')



h[200].sum tensor(246.9577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-137.0788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0329, 0.0000,  ..., 0.0301, 0.0000, 0.0000],
        [0.0000, 0.0298, 0.0000,  ..., 0.0265, 0.0000, 0.0000],
        [0.0000, 0.0236, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0265, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        [0.0000, 0.0243, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0093, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(314368.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1455, 0.1590],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.1336, 0.1351],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.1195, 0.1062],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1145, 0.0975],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.1072, 0.0823],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0864, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2617314.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2226.0647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1783.6560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(274.5978, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1939.1837, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1286],
        [ 0.1344],
        [ 0.1393],
        ...,
        [ 0.1221],
        [ 0.0972],
        [-0.0117]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-760892.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1724.1261, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1724.1261, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11618.4014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(260.1302, device='cuda:0')



h[100].sum tensor(-76.4826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-122.2627, device='cuda:0')



h[200].sum tensor(267.2729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-168.8932, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(343954.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0626, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0599, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0594, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0588, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0588, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0588, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2745554., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2385.2688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1632.8124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(636.6838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2224.1641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1178],
        [-0.2825],
        [-0.4205],
        ...,
        [-0.5340],
        [-0.5323],
        [-0.5318]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-727411.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1434.8516, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1434.8516, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0202, -0.0643,  ...,  0.0214, -0.0476, -0.0655],
        [-0.0008,  0.0075, -0.0194,  ...,  0.0067, -0.0144, -0.0198],
        [-0.0008,  0.0082, -0.0219,  ...,  0.0075, -0.0162, -0.0223],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11821.3711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(216.4854, device='cuda:0')



h[100].sum tensor(-62.9853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.7494, device='cuda:0')



h[200].sum tensor(248.0175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.5562, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0614, 0.0000,  ..., 0.0632, 0.0000, 0.0000],
        [0.0000, 0.0507, 0.0000,  ..., 0.0509, 0.0000, 0.0000],
        [0.0000, 0.0185, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(317112.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.2319, 0.3264],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.1797, 0.2232],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1223, 0.1090],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0591, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0591, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0591, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2619205.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2249.0559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1811.7620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(209.8364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1979.8536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0808],
        [ 0.1050],
        [ 0.1285],
        ...,
        [-0.5385],
        [-0.5368],
        [-0.5363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-724931.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1492.6321, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1492.6321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0125, -0.0370,  ...,  0.0125, -0.0274, -0.0377],
        [-0.0009,  0.0136, -0.0410,  ...,  0.0138, -0.0304, -0.0418],
        [-0.0009,  0.0104, -0.0299,  ...,  0.0101, -0.0221, -0.0304],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11806.8633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.2031, device='cuda:0')



h[100].sum tensor(-65.2886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.8468, device='cuda:0')



h[200].sum tensor(251.2365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.2163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0487, 0.0000,  ..., 0.0487, 0.0000, 0.0000],
        [0.0000, 0.0435, 0.0000,  ..., 0.0426, 0.0000, 0.0000],
        [0.0000, 0.0400, 0.0000,  ..., 0.0385, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(323074.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1828, 0.2316],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1724, 0.2108],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1618, 0.1893],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0591, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0591, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0591, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2646510.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2280.9663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1778.5380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(308.1132, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2031.3433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1156],
        [ 0.1202],
        [ 0.1264],
        ...,
        [-0.5385],
        [-0.5368],
        [-0.5363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-729973.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1481.3411, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1481.3411, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0008,  0.0066, -0.0163,  ...,  0.0057, -0.0121, -0.0166],
        [-0.0008,  0.0059, -0.0140,  ...,  0.0050, -0.0104, -0.0142],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11912.8311, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(223.4996, device='cuda:0')



h[100].sum tensor(-64.6195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.0461, device='cuda:0')



h[200].sum tensor(250.3397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.1103, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0125, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0156, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0319, 0.0000,  ..., 0.0293, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(327123.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0772, 0.0188],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0887, 0.0416],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1104, 0.0853],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0594, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0594, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0594, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2682894., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2309.3896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1776.5911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(294.1053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2079.4109, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1454],
        [ 0.1388],
        [ 0.1246],
        ...,
        [-0.5435],
        [-0.5418],
        [-0.5413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-698143.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(2020.2089, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(2020.2089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0043, -0.0084,  ...,  0.0031, -0.0062, -0.0085],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0009,  0.0115, -0.0336,  ...,  0.0114, -0.0248, -0.0342],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(11818.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.4161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(304.8021, device='cuda:0')



h[100].sum tensor(-88.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-143.2588, device='cuda:0')



h[200].sum tensor(285.3044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-197.8971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0133, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0232, 0.0000,  ..., 0.0192, 0.0000, 0.0000],
        [0.0000, 0.0232, 0.0000,  ..., 0.0192, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(381795.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0991, 0.0611],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.1248, 0.1127],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1422, 0.1475],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0598, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0598, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0598, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2982326.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2606.3630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1492.6227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1465.2990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2570.8115, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1204],
        [ 0.1142],
        [ 0.1079],
        ...,
        [-0.5055],
        [-0.5344],
        [-0.5460]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-720677.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1290.0200, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1290.0200, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0020,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12064.4414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(194.6337, device='cuda:0')



h[100].sum tensor(-56.3034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.4790, device='cuda:0')



h[200].sum tensor(240.0918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.3687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(310874.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0606, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0617, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0632, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0603, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0603, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0603, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2628886.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2218.6445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1918.3485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(175.9009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1864.9202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6756],
        [-0.6311],
        [-0.5450],
        ...,
        [-0.5604],
        [-0.5586],
        [-0.5581]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-801203.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1878.3296, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1878.3296, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0104, -0.0299,  ...,  0.0102, -0.0221, -0.0304],
        [-0.0008,  0.0062, -0.0150,  ...,  0.0053, -0.0111, -0.0152],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12049.6494, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.3382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(283.3958, device='cuda:0')



h[100].sum tensor(-82.1109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-133.1977, device='cuda:0')



h[200].sum tensor(275.6941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-183.9988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0318, 0.0000,  ..., 0.0293, 0.0000, 0.0000],
        [0.0000, 0.0231, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        [0.0000, 0.0155, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(365759.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1317, 0.1257],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1120, 0.0858],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0897, 0.0409],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0605, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0605, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0605, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2871563.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2520.7114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1625.8234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(849.8293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2382.7896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1016],
        [ 0.0493],
        [-0.0552],
        ...,
        [-0.5645],
        [-0.5628],
        [-0.5623]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-699334.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1404.9718, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1404.9718, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0159, -0.0495,  ...,  0.0166, -0.0365, -0.0503],
        [-0.0011,  0.0205, -0.0660,  ...,  0.0220, -0.0487, -0.0671],
        [-0.0010,  0.0153, -0.0475,  ...,  0.0159, -0.0351, -0.0484],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12266.8496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(211.9773, device='cuda:0')



h[100].sum tensor(-61.5208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.6306, device='cuda:0')



h[200].sum tensor(245.9738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-137.6292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0678, 0.0000,  ..., 0.0713, 0.0000, 0.0000],
        [0.0000, 0.0788, 0.0000,  ..., 0.0840, 0.0000, 0.0000],
        [0.0000, 0.0798, 0.0000,  ..., 0.0852, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(324809.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.2670, 0.3914],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.3082, 0.4730],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.3082, 0.4728],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0609, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0609, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2699821., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2295.1543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1885.4155, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(416.2633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1941.2131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0493],
        [ 0.0416],
        [ 0.0437],
        ...,
        [-0.5693],
        [-0.5675],
        [-0.5670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-782426.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1503.0641, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1503.0641, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0039, -0.0070,  ...,  0.0027, -0.0052, -0.0071],
        [-0.0007,  0.0044, -0.0087,  ...,  0.0032, -0.0064, -0.0088],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12328.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(226.7771, device='cuda:0')



h[100].sum tensor(-64.9875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.5866, device='cuda:0')



h[200].sum tensor(250.5123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-147.2382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0237, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(330960.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.1020, 0.0640],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0880, 0.0354],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0749, 0.0132],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0617, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0664, 0.0020],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0772, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2724811.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2327.3013, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1871.0549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(363.0895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1973.5510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1515],
        [ 0.0986],
        [-0.0197],
        ...,
        [-0.4387],
        [-0.2718],
        [-0.0779]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-764994.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 3600 loss: tensor(488.4044, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1355.8199, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1355.8199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12427.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(204.5614, device='cuda:0')



h[100].sum tensor(-59.2182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.1451, device='cuda:0')



h[200].sum tensor(242.1163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.8144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0153, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(320752.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0810, 0.0215],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0682, 0.0030],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0643, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0615, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0615, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0615, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2690624., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2270.6748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1959.5559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(263.1087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1836.3223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2536],
        [-0.4307],
        [-0.5734],
        ...,
        [-0.5801],
        [-0.5782],
        [-0.5777]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-789158.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1321.7577, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.7577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0079, -0.0212,  ...,  0.0073, -0.0157, -0.0216],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0010,  0.0157, -0.0490,  ...,  0.0165, -0.0362, -0.0499],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12504.0332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(199.4222, device='cuda:0')



h[100].sum tensor(-57.0160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.7296, device='cuda:0')



h[200].sum tensor(237.7760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.4777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0310, 0.0000,  ..., 0.0288, 0.0000, 0.0000],
        [0.0000, 0.0374, 0.0000,  ..., 0.0362, 0.0000, 0.0000],
        [0.0000, 0.0360, 0.0000,  ..., 0.0346, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(316254.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1644, 0.1881],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1752, 0.2096],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1818, 0.2220],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0617, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0617, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0617, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2667967., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2261.8418, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2024.9403, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(228.0087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1795.0583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0889],
        [ 0.0847],
        [ 0.0824],
        ...,
        [-0.5841],
        [-0.5830],
        [-0.5833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-784113.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1487.7236, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1487.7236, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0131, -0.0396,  ...,  0.0134, -0.0292, -0.0403],
        [-0.0009,  0.0105, -0.0304,  ...,  0.0104, -0.0225, -0.0310],
        [-0.0008,  0.0081, -0.0221,  ...,  0.0077, -0.0163, -0.0225],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12500.8525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.4626, device='cuda:0')



h[100].sum tensor(-64.4005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4987, device='cuda:0')



h[200].sum tensor(247.4947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.7355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0615, 0.0000,  ..., 0.0644, 0.0000, 0.0000],
        [0.0000, 0.0530, 0.0000,  ..., 0.0545, 0.0000, 0.0000],
        [0.0000, 0.0546, 0.0000,  ..., 0.0564, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337591.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.2460, 0.3464],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.2361, 0.3273],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.2298, 0.3146],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0619, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0619, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0619, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2797249.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2404.0808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1944.2402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(502.5257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2012.3970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0864],
        [ 0.0873],
        [ 0.0906],
        ...,
        [-0.5951],
        [-0.5932],
        [-0.5927]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-769759.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1273.3923, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1273.3923, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12568.8730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(192.1250, device='cuda:0')



h[100].sum tensor(-54.8486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.2999, device='cuda:0')



h[200].sum tensor(233.5346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.7399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(311369.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0626, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0626, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0628, 0.0000],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0623, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0623, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0623, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2659930.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2268.0293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2127.7188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(139.3298, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1766.9663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5522],
        [-0.6483],
        [-0.7136],
        ...,
        [-0.6027],
        [-0.6008],
        [-0.6003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-804418.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1367.2321, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1367.2321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0042, -0.0083,  ...,  0.0031, -0.0061, -0.0085],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12600.8555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(206.2832, device='cuda:0')



h[100].sum tensor(-58.5028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.9543, device='cuda:0')



h[200].sum tensor(238.7215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.9323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0000, 0.0199, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(321690.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0768, 0.0134],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0986, 0.0522],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.1241, 0.1027],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0626, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0626, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0626, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2730490.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2305.5801, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2095.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(272.2601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1824.6038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1031],
        [ 0.1199],
        [ 0.1236],
        ...,
        [-0.6104],
        [-0.6084],
        [-0.6078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-851027.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1349.3761, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1349.3761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0042, -0.0084,  ...,  0.0031, -0.0062, -0.0085],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12691.1240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(203.5892, device='cuda:0')



h[100].sum tensor(-57.7752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.6881, device='cuda:0')



h[200].sum tensor(237.4619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.1832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0152, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(321655.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0908, 0.0348],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0779, 0.0136],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0690, 0.0010],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0630, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0630, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0630, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2730873., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2278.0442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2110.1128, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(201.5878, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1788.5032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1343],
        [ 0.0316],
        [-0.1385],
        ...,
        [-0.6144],
        [-0.6022],
        [-0.5586]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-824163.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1358.7882, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1358.7882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0036, -0.0062,  ...,  0.0024, -0.0046, -0.0063],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12814.6270, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(205.0092, device='cuda:0')



h[100].sum tensor(-57.9336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.3556, device='cuda:0')



h[200].sum tensor(237.4117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.1051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(323918.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0723, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0662, 0.0000],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0634, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0634, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0634, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2754196., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2254.9834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2106.0979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(227.2356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1735.4202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1442],
        [-0.2850],
        [-0.4142],
        ...,
        [-0.6217],
        [-0.6197],
        [-0.6191]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-847098.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1493.4629, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1493.4629, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0077, -0.0206,  ...,  0.0072, -0.0152, -0.0210],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0008,  0.0072, -0.0190,  ...,  0.0066, -0.0140, -0.0193],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(12925.1611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.3285, device='cuda:0')



h[100].sum tensor(-64.0743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.9057, device='cuda:0')



h[200].sum tensor(245.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.2977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0289, 0.0000,  ..., 0.0265, 0.0000, 0.0000],
        [0.0000, 0.0277, 0.0000,  ..., 0.0251, 0.0000, 0.0000],
        [0.0000, 0.0171, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(339400., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.1527, 0.1596],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1355, 0.1255],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1108, 0.0748],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0638, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0638, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0638, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2834595.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2318.3628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2029.9524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(489.3491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1838.3049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1155],
        [ 0.1276],
        [ 0.1404],
        ...,
        [-0.6244],
        [-0.6219],
        [-0.6208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-830026.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1780.2560, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1780.2560, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0115, -0.0340,  ...,  0.0116, -0.0251, -0.0347],
        [-0.0009,  0.0116, -0.0345,  ...,  0.0117, -0.0254, -0.0351],
        [-0.0008,  0.0090, -0.0254,  ...,  0.0087, -0.0187, -0.0258],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13000.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.3443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(268.5988, device='cuda:0')



h[100].sum tensor(-76.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-126.2430, device='cuda:0')



h[200].sum tensor(262.3177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-174.3916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0337, 0.0000,  ..., 0.0322, 0.0000, 0.0000],
        [0.0000, 0.0360, 0.0000,  ..., 0.0349, 0.0000, 0.0000],
        [0.0000, 0.0269, 0.0000,  ..., 0.0243, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(371666.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.1467, 0.1461],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1511, 0.1548],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1333, 0.1182],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0642, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0642, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0642, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3021264.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2481.9365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1869.6948, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1092.8378, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2109.6697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1433],
        [ 0.1437],
        [ 0.1477],
        ...,
        [-0.6317],
        [-0.6298],
        [-0.6292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-816110.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1272.1678, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1272.1678, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0042, -0.0082,  ...,  0.0031, -0.0060, -0.0083],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13112.0547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(191.9403, device='cuda:0')



h[100].sum tensor(-53.9586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.2131, device='cuda:0')



h[200].sum tensor(229.8602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.6199, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0149, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(317384.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0951, 0.0395],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0799, 0.0151],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0698, 0.0006],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0647, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0647, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0647, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2741369.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2193.9102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2208.7739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(196.8565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1600.5654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0891],
        [-0.0648],
        [-0.2872],
        ...,
        [-0.6389],
        [-0.6370],
        [-0.6364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-826962.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 3900 loss: tensor(533.6866, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.2212, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.2212, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13107.5918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(213.8254, device='cuda:0')



h[100].sum tensor(-60.2495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.4992, device='cuda:0')



h[200].sum tensor(240.1031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.8292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(334314.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0665, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0657, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0658, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0653, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0653, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0653, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2842203., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2291.5405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2139.2759, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(383.0069, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1754.7612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3457],
        [-0.5122],
        [-0.6388],
        ...,
        [-0.6470],
        [-0.6451],
        [-0.6448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-816171.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1316.2753, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1316.2753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13059.0176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.5950, device='cuda:0')



h[100].sum tensor(-55.7699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.3408, device='cuda:0')



h[200].sum tensor(236.1665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.9406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(322704.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0663, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0664, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0672, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0659, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0659, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0659, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2796770., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2216.3569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2224.6133, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(182.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1627.0121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7011],
        [-0.6145],
        [-0.4764],
        ...,
        [-0.6622],
        [-0.6600],
        [-0.6594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-895543.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1451.0240, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1451.0240, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0056, -0.0131,  ...,  0.0047, -0.0097, -0.0134],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13120.6973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(218.9255, device='cuda:0')



h[100].sum tensor(-61.3869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.8963, device='cuda:0')



h[200].sum tensor(245.6895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.1405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0160, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(337911.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.1073, 0.0578],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0888, 0.0210],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0882, 0.0191],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0667, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0667, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0667, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2890484., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2318.1753, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2137.1016, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(387.7093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1712.1550, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1566],
        [ 0.1672],
        [ 0.1740],
        ...,
        [-0.6706],
        [-0.6666],
        [-0.6557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-905498.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1608.3101, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1608.3101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0143, -0.0440,  ...,  0.0150, -0.0324, -0.0448],
        [-0.0009,  0.0111, -0.0327,  ...,  0.0112, -0.0241, -0.0333],
        [-0.0008,  0.0046, -0.0096,  ...,  0.0035, -0.0070, -0.0097],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13332.4863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(242.6562, device='cuda:0')



h[100].sum tensor(-67.4991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.0499, device='cuda:0')



h[200].sum tensor(252.6571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-157.5480, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0439, 0.0000,  ..., 0.0442, 0.0000, 0.0000],
        [0.0000, 0.0374, 0.0000,  ..., 0.0365, 0.0000, 0.0000],
        [0.0000, 0.0299, 0.0000,  ..., 0.0277, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0093, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(347442.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.1718, 0.1829],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.1571, 0.1538],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.1331, 0.1058],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0692, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0758, 0.0048],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0925, 0.0295]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2908739., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2422.0444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2078.3823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(400.0585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1745.2562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1511],
        [ 0.1648],
        [ 0.1791],
        ...,
        [-0.4370],
        [-0.2258],
        [-0.0157]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-853465.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1492.0873, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1492.0873, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0048, -0.0105,  ...,  0.0039, -0.0077, -0.0107],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13562.7402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.1209, device='cuda:0')



h[100].sum tensor(-62.7111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.8082, device='cuda:0')



h[200].sum tensor(244.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.1630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(343029.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0787, 0.0023],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0761, 0.0025],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0717, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0682, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0682, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0682, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2903048., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2445.7446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2099.0664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(326.3909, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1633.8794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2060],
        [-0.3703],
        [-0.4736],
        ...,
        [-0.6763],
        [-0.6743],
        [-0.6737]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-825368.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1578.5533, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1578.5533, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13744.2227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(238.1666, device='cuda:0')



h[100].sum tensor(-66.5210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.9397, device='cuda:0')



h[200].sum tensor(249.4041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-154.6331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(351366.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0694, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0694, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0696, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0720, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0701, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0691, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2941396.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2523.2822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2036.5876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(344.9437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1623.9049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8443],
        [-0.8523],
        [-0.8485],
        ...,
        [-0.4992],
        [-0.5897],
        [-0.6472]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-813873.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1319.2532, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1319.2532, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0108, -0.0316,  ...,  0.0109, -0.0232, -0.0322],
        [-0.0008,  0.0061, -0.0148,  ...,  0.0053, -0.0109, -0.0150],
        [-0.0007,  0.0038, -0.0070,  ...,  0.0027, -0.0051, -0.0071],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13849.3896, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(199.0443, device='cuda:0')



h[100].sum tensor(-55.4300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.5520, device='cuda:0')



h[200].sum tensor(235.6271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.2324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0237, 0.0000,  ..., 0.0205, 0.0000, 0.0000],
        [0.0000, 0.0273, 0.0000,  ..., 0.0247, 0.0000, 0.0000],
        [0.0000, 0.0201, 0.0000,  ..., 0.0163, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(334118.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1349, 0.1048],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1331, 0.1005],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.1180, 0.0693],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0701, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0701, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0701, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2896051., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2439.7578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2104.3174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(224.7579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1359.9819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1656],
        [ 0.1735],
        [ 0.1805],
        ...,
        [-0.6915],
        [-0.6895],
        [-0.6889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-852894.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1235.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1235.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(13897.3213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(186.3555, device='cuda:0')



h[100].sum tensor(-51.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.5882, device='cuda:0')



h[200].sum tensor(233.4045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-120.9940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0013, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(324358.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0776, 0.0022],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0748, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0790, 0.0025],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0707, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2860193.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2370.1560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2149.4194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(158.9813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1212.5881, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2748],
        [-0.2403],
        [-0.0933],
        ...,
        [-0.7025],
        [-0.7003],
        [-0.6997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-913134.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1676.6531, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1676.6531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0042, -0.0082,  ...,  0.0031, -0.0061, -0.0084],
        [-0.0007,  0.0038, -0.0068,  ...,  0.0026, -0.0050, -0.0069],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14019.5059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(252.9676, device='cuda:0')



h[100].sum tensor(-70.1985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-118.8963, device='cuda:0')



h[200].sum tensor(261.0417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-164.2428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0238, 0.0000,  ..., 0.0205, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(362425.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0840, 0.0098],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.1014, 0.0380],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1311, 0.0946],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0708, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0708, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0708, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3027171.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2558.8577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1950.7200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(628.6302, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1583.9127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0577],
        [ 0.0777],
        [ 0.0760],
        ...,
        [-0.7083],
        [-0.7065],
        [-0.7062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-872664.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1437.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1437.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14128.1074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(216.8890, device='cuda:0')



h[100].sum tensor(-59.9218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.9391, device='cuda:0')



h[200].sum tensor(245.8335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.8183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(341962.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0711, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0711, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0713, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0707, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2931190.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2428.0447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2093.9500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(250.2384, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1432.1901, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8880],
        [-0.8973],
        [-0.9000],
        ...,
        [-0.7149],
        [-0.7128],
        [-0.7122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-854447.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1602.0248, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1602.0248, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14133.3730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(241.7079, device='cuda:0')



h[100].sum tensor(-66.5875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.6042, device='cuda:0')



h[200].sum tensor(255.6426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-156.9323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(356570.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0777, 0.0003],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0737, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0731, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0707, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0707, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3001757., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2508.0679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2011.7552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(329.8274, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1569.8313, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0465],
        [-0.0264],
        [-0.0841],
        ...,
        [-0.6792],
        [-0.7045],
        [-0.7111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-854556.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1273.9204, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1273.9204, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14214.1367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(192.2047, device='cuda:0')



h[100].sum tensor(-52.9661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.3373, device='cuda:0')



h[200].sum tensor(236.6629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.7916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(331814.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0731, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0792, 0.0023],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0885, 0.0139],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0708, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0708, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0708, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2913581.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2342.8882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2170.0889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(143.2937, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1331.6758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2140],
        [-0.0330],
        [ 0.0880],
        ...,
        [-0.7216],
        [-0.7195],
        [-0.7189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-896354.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1596.3876, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1596.3876, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0063, -0.0155,  ...,  0.0056, -0.0114, -0.0158],
        [-0.0008,  0.0055, -0.0129,  ...,  0.0047, -0.0095, -0.0131],
        [-0.0010,  0.0187, -0.0592,  ...,  0.0202, -0.0435, -0.0603],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14316.8555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(240.8574, device='cuda:0')



h[100].sum tensor(-65.9792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.2044, device='cuda:0')



h[200].sum tensor(257.0705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-156.3801, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0206, 0.0000,  ..., 0.0170, 0.0000, 0.0000],
        [0.0000, 0.0509, 0.0000,  ..., 0.0526, 0.0000, 0.0000],
        [0.0000, 0.0403, 0.0000,  ..., 0.0401, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(355098.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.1529, 0.1384],
        [0.0000, 0.0029, 0.0000,  ..., 0.0000, 0.2156, 0.2616],
        [0.0000, 0.0025, 0.0000,  ..., 0.0000, 0.2310, 0.2915],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0709, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0709, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0709, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3008298.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2443.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2057.3909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(430.6104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1554.7288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0886],
        [ 0.0722],
        [ 0.0576],
        ...,
        [-0.7217],
        [-0.7186],
        [-0.7190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-903257.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1364.1368, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1364.1368, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0037, -0.0065,  ...,  0.0025, -0.0047, -0.0066],
        [-0.0007,  0.0037, -0.0065,  ...,  0.0025, -0.0047, -0.0066],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14366.9355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(205.8162, device='cuda:0')



h[100].sum tensor(-56.6459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.7348, device='cuda:0')



h[200].sum tensor(245.5523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.6291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0093, 0.0000, 0.0000],
        [0.0000, 0.0124, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(344537.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0940, 0.0198],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0909, 0.0149],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0854, 0.0085],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0712, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0712, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0712, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3006652., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2358.5505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2122.6758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(391.6470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1429.7531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1289],
        [ 0.0701],
        [-0.0733],
        ...,
        [-0.7385],
        [-0.7362],
        [-0.7356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-924659.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1309.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1309.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14363.2090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(197.6293, device='cuda:0')



h[100].sum tensor(-54.3713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.8870, device='cuda:0')



h[200].sum tensor(242.1846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.3136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(335194.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0737, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0721, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0718, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0712, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0712, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0712, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2943740.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2306.8984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2175.4519, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(262.6340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1340.2524, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7666],
        [-0.7859],
        [-0.7394],
        ...,
        [-0.7383],
        [-0.7360],
        [-0.7354]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-938641.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1390.0292, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1390.0292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0042, -0.0082,  ...,  0.0031, -0.0060, -0.0084],
        ...,
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0019,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14431.5762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(209.7228, device='cuda:0')



h[100].sum tensor(-57.4988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.5709, device='cuda:0')



h[200].sum tensor(248.1802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.1655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0077, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(341282.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0784, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0870, 0.0117],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0981, 0.0303],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0715, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0715, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0715, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2976521.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2322.9204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2155.6572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(326.8720, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1374.8594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1095],
        [ 0.1389],
        [ 0.1601],
        ...,
        [-0.7470],
        [-0.7447],
        [-0.7440]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-989756.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1471.2657, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1471.2657, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14567.9824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(221.9795, device='cuda:0')



h[100].sum tensor(-60.3230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.3317, device='cuda:0')



h[200].sum tensor(252.7845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-144.1233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0128, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(345424.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0906, 0.0187],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0781, 0.0015],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0743, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0717, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0717, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0717, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2982141., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2333.1487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2133.3982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(240.6628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1394.2001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3006],
        [-0.4346],
        [-0.4655],
        ...,
        [-0.7235],
        [-0.7003],
        [-0.6846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-917128.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1393.2491, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1393.2491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14561.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(210.2086, device='cuda:0')



h[100].sum tensor(-57.2600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.7993, device='cuda:0')



h[200].sum tensor(248.2310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.4809, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(338270.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0721, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0721, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0743, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0717, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0717, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0717, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(2960645.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2291.3950, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2175.7239, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(231.7248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1314.9465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9206],
        [-0.8725],
        [-0.7659],
        ...,
        [-0.7527],
        [-0.7503],
        [-0.7496]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-966306., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1538.0424, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1538.0424, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0054, -0.0124,  ...,  0.0045, -0.0091, -0.0127],
        [-0.0008,  0.0065, -0.0164,  ...,  0.0059, -0.0121, -0.0167],
        [-0.0008,  0.0089, -0.0250,  ...,  0.0087, -0.0183, -0.0254],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14728.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(232.0545, device='cuda:0')



h[100].sum tensor(-63.6490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-109.0670, device='cuda:0')



h[200].sum tensor(256.6226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-150.6647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0243, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        [0.0000, 0.0270, 0.0000,  ..., 0.0246, 0.0000, 0.0000],
        [0.0000, 0.0398, 0.0000,  ..., 0.0398, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0117, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0161, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0066, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(358252.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1437, 0.1208],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1547, 0.1432],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1780, 0.1898],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0956, 0.0254],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.1034, 0.0410],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0946, 0.0240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3069640., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2396.2036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2078.7346, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(562.9559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1474.7087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1545],
        [ 0.1495],
        [ 0.1453],
        ...,
        [-0.0192],
        [ 0.0393],
        [-0.0486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-951696.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1481.0806, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].sum tensor(1481.0806, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0039, -0.0075,  ...,  0.0029, -0.0055, -0.0077],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14868.5771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(223.4603, device='cuda:0')



h[100].sum tensor(-60.6742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.0277, device='cuda:0')



h[200].sum tensor(251.5279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.0848, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0106, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(351658.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0725, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0725, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0727, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0774, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0789, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0838, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3029459., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2360.1074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2119.9446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(291.8177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1397.0049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8876],
        [-0.8311],
        [-0.7399],
        ...,
        [-0.5913],
        [-0.5254],
        [-0.4888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-865504.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 4500 loss: tensor(898.3303, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1494.2887, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1494.2887, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0080, -0.0219,  ...,  0.0077, -0.0160, -0.0223],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(14925.7119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.4531, device='cuda:0')



h[100].sum tensor(-61.5726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.9643, device='cuda:0')



h[200].sum tensor(253.7298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.3786, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0185, 0.0000,  ..., 0.0147, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(354529.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.1097, 0.0521],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.1033, 0.0390],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0835, 0.0077],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0726, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0726, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0726, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3057108.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2374.3267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2112.5132, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(423.2928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1390.0728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0312],
        [ 0.0065],
        [-0.0249],
        ...,
        [-0.7700],
        [-0.7677],
        [-0.7670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-925305.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1653.5579, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1653.5579, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0008,  0.0081, -0.0223,  ...,  0.0078, -0.0163, -0.0227],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15015.0449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(249.4831, device='cuda:0')



h[100].sum tensor(-68.5258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.2585, device='cuda:0')



h[200].sum tensor(264.3533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-161.9804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0187, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0187, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(378043.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0836, 0.0079],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.1041, 0.0396],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.1106, 0.0525],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0729, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0729, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3207155., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2507.9441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1998.0281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1155.4797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1596.7944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4846],
        [-0.3029],
        [-0.1989],
        ...,
        [-0.7781],
        [-0.7758],
        [-0.7751]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-911500.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1460.9435, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1460.9435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0115, -0.0340,  ...,  0.0118, -0.0249, -0.0346],
        [-0.0010,  0.0150, -0.0462,  ...,  0.0159, -0.0339, -0.0471],
        [-0.0009,  0.0140, -0.0428,  ...,  0.0148, -0.0314, -0.0436],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15010.2012, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(220.4221, device='cuda:0')



h[100].sum tensor(-59.8378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.5997, device='cuda:0')



h[200].sum tensor(251.5592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-143.1122, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0544, 0.0000,  ..., 0.0572, 0.0000, 0.0000],
        [0.0000, 0.0577, 0.0000,  ..., 0.0612, 0.0000, 0.0000],
        [0.0000, 0.0666, 0.0000,  ..., 0.0716, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(355542.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.2478, 0.3250],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.2706, 0.3699],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.2874, 0.4025],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0733, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0733, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0733, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3082569.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2402.4165, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2151.4004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(318.2736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1410.1432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0970],
        [ 0.0919],
        [ 0.0900],
        ...,
        [-0.7879],
        [-0.7855],
        [-0.7848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-910487.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1373.7029, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1373.7029, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0044, -0.0092,  ...,  0.0035, -0.0067, -0.0094],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15032.8174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(207.2595, device='cuda:0')



h[100].sum tensor(-55.9595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.4132, device='cuda:0')



h[200].sum tensor(247.3709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-134.5662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0168, 0.0000,  ..., 0.0127, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(349832.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0822, 0.0013],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0832, 0.0010],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0891, 0.0046],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0739, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0739, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0739, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3085036., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2374.7759, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2198.4829, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(337.4933, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1330.0081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0286],
        [-0.0413],
        [-0.0480],
        ...,
        [-0.7984],
        [-0.7959],
        [-0.7952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-968429., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1508.3939, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1508.3939, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15136.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(227.5812, device='cuda:0')



h[100].sum tensor(-61.2808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.9645, device='cuda:0')



h[200].sum tensor(256.4434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-147.7603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(357159.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0748, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0748, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0750, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0744, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0744, 0.0000],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0744, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3116673.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2414.4102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2166.5845, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(435.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1355.2010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8871],
        [-0.9440],
        [-0.9880],
        ...,
        [-0.8074],
        [-0.8049],
        [-0.8042]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1019844.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1326.8253, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1326.8253, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0114, -0.0338,  ...,  0.0118, -0.0247, -0.0344],
        [-0.0010,  0.0185, -0.0585,  ...,  0.0201, -0.0428, -0.0596],
        [-0.0008,  0.0050, -0.0112,  ...,  0.0041, -0.0082, -0.0114],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15222.4961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(200.1868, device='cuda:0')



h[100].sum tensor(-53.7939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.0890, device='cuda:0')



h[200].sum tensor(245.7910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.9741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0450, 0.0000,  ..., 0.0462, 0.0000, 0.0000],
        [0.0000, 0.0366, 0.0000,  ..., 0.0363, 0.0000, 0.0000],
        [0.0000, 0.0525, 0.0000,  ..., 0.0551, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(342480.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.1883, 0.2034],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.1940, 0.2153],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.2095, 0.2458],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0750, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0750, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0750, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3056841.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2332.2925, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2245.9292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(236.3261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1167.5009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1445],
        [ 0.1411],
        [ 0.1387],
        ...,
        [-0.8146],
        [-0.8121],
        [-0.8114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1017898.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1307.4753, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1307.4753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0044, -0.0092,  ...,  0.0035, -0.0067, -0.0094],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15377.9492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(197.2673, device='cuda:0')



h[100].sum tensor(-52.7884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.7168, device='cuda:0')



h[200].sum tensor(244.8222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.0786, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(342975.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0786, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0856, 0.0049],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0944, 0.0155],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0755, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0755, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0755, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3074522.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2328.0630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2235.3936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(279.1237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1102.1562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2871],
        [-0.1930],
        [-0.0799],
        ...,
        [-0.8202],
        [-0.8177],
        [-0.8170]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-992458.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1315.9734, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1315.9734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15542.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.5495, device='cuda:0')



h[100].sum tensor(-53.5113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.3194, device='cuda:0')



h[200].sum tensor(246.6973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.9111, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(349013.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0911, 0.0145],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0804, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0772, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0760, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0760, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0760, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3108477., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2355.3276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2191.5522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(208.2879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1086.3640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3640],
        [-0.5578],
        [-0.6558],
        ...,
        [-0.8224],
        [-0.8203],
        [-0.8200]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-963667.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1424.9758, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1424.9758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15564.1162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.9954, device='cuda:0')



h[100].sum tensor(-57.7265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.0491, device='cuda:0')



h[200].sum tensor(253.0930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.5888, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(359487.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0764, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0764, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0766, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0760, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0760, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0760, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3171577.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2411.4480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2132.0337, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(598.0134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1184.4622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9963],
        [-0.9634],
        [-0.9063],
        ...,
        [-0.8254],
        [-0.8230],
        [-0.8223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-953913.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1456.9214, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.9214, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15719.2930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(219.8152, device='cuda:0')



h[100].sum tensor(-58.8911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.3145, device='cuda:0')



h[200].sum tensor(255.5635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.7182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(359944.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0768, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0768, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0770, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0764, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0764, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0764, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3159430., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2412.8826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2129.5171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(376.8738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1137.4310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7904],
        [-0.8949],
        [-0.9685],
        ...,
        [-0.8295],
        [-0.8272],
        [-0.8266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-944348.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 4800 loss: tensor(475.9129, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1438.4025, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1438.4025, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0036, -0.0065,  ...,  0.0026, -0.0048, -0.0067],
        [-0.0007,  0.0044, -0.0092,  ...,  0.0035, -0.0068, -0.0094],
        [-0.0008,  0.0084, -0.0233,  ...,  0.0082, -0.0170, -0.0237],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15775.2178, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(217.0212, device='cuda:0')



h[100].sum tensor(-57.7482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.0012, device='cuda:0')



h[200].sum tensor(256.0561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.9041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0219, 0.0000,  ..., 0.0191, 0.0000, 0.0000],
        [0.0000, 0.0192, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        [0.0000, 0.0222, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(361397.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1206, 0.0658],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1299, 0.0846],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1443, 0.1140],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0769, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0769, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0769, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3197834.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2415.1313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2126.0767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(515.9837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1108.1858, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1885],
        [ 0.1793],
        [ 0.1664],
        ...,
        [-0.8394],
        [-0.8370],
        [-0.8363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-958713.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1282.2228, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1282.2228, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0088, -0.0247,  ...,  0.0087, -0.0180, -0.0251],
        [-0.0009,  0.0107, -0.0314,  ...,  0.0110, -0.0230, -0.0320],
        [-0.0009,  0.0122, -0.0367,  ...,  0.0128, -0.0268, -0.0373],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15759.2783, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.4573, device='cuda:0')



h[100].sum tensor(-51.1985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.9261, device='cuda:0')



h[200].sum tensor(248.5860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.6049, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0369, 0.0000,  ..., 0.0368, 0.0000, 0.0000],
        [0.0000, 0.0419, 0.0000,  ..., 0.0429, 0.0000, 0.0000],
        [0.0000, 0.0408, 0.0000,  ..., 0.0415, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(341367.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1749, 0.1764],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1891, 0.2058],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1860, 0.1989],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0773, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0773, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0773, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3096672.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2301.3359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2255.7158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(212.9729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-889.4455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1629],
        [ 0.1625],
        [ 0.1651],
        ...,
        [-0.8449],
        [-0.8230],
        [-0.7480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1028011.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1277.3702, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1277.3702, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0018,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15792.5488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(192.7252, device='cuda:0')



h[100].sum tensor(-51.1100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.5820, device='cuda:0')



h[200].sum tensor(250.7198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.1296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(345174.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0791, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0780, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0782, 0.0000],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0776, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0776, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0776, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3131139.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2327.5947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2252.5356, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(159.2569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-927.0910, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9375],
        [-1.0314],
        [-1.0789],
        ...,
        [-0.8607],
        [-0.8581],
        [-0.8574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1018300.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1555.1263, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1555.1263, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0064, -0.0164,  ...,  0.0059, -0.0120, -0.0167],
        [-0.0008,  0.0075, -0.0202,  ...,  0.0072, -0.0148, -0.0206],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(15946.3789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(234.6320, device='cuda:0')



h[100].sum tensor(-62.1882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-110.2785, device='cuda:0')



h[200].sum tensor(269.6074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-152.3382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0340, 0.0000,  ..., 0.0336, 0.0000, 0.0000],
        [0.0000, 0.0182, 0.0000,  ..., 0.0148, 0.0000, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(372416.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.1558, 0.1357],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.1221, 0.0664],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.1007, 0.0262],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0778, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0778, 0.0000],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0778, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3279665., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2482.0557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2120.8176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(704.1113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1168.7109, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1734],
        [ 0.0737],
        [-0.1390],
        ...,
        [-0.8682],
        [-0.8652],
        [-0.8640]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1003416.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1302.0936, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1302.0936, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0046, -0.0100,  ...,  0.0038, -0.0073, -0.0102],
        [-0.0008,  0.0087, -0.0243,  ...,  0.0086, -0.0178, -0.0248],
        [-0.0008,  0.0046, -0.0100,  ...,  0.0038, -0.0073, -0.0102],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16005.0059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(196.4554, device='cuda:0')



h[100].sum tensor(-52.0108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.3352, device='cuda:0')



h[200].sum tensor(256.3063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.5514, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0345, 0.0000,  ..., 0.0343, 0.0000, 0.0000],
        [0.0000, 0.0230, 0.0000,  ..., 0.0206, 0.0000, 0.0000],
        [0.0000, 0.0162, 0.0000,  ..., 0.0125, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(348238.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.1309, 0.0849],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.1199, 0.0616],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.1076, 0.0364],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0780, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0780, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0780, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3154715., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2344.1545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2262.8176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(240.8922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-895.9638, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0792],
        [-0.0391],
        [-0.2481],
        ...,
        [-0.8199],
        [-0.8544],
        [-0.8646]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1006157., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1600.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1600.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0086, -0.0241,  ...,  0.0086, -0.0176, -0.0245],
        [-0.0008,  0.0085, -0.0236,  ...,  0.0084, -0.0173, -0.0241],
        [-0.0008,  0.0085, -0.0238,  ...,  0.0085, -0.0174, -0.0242],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16227.2217, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(241.5381, device='cuda:0')



h[100].sum tensor(-63.8413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.5243, device='cuda:0')



h[200].sum tensor(276.9802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-156.8220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0321, 0.0000,  ..., 0.0315, 0.0000, 0.0000],
        [0.0000, 0.0317, 0.0000,  ..., 0.0311, 0.0000, 0.0000],
        [0.0000, 0.0362, 0.0000,  ..., 0.0365, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(376909.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.2004, 0.2303],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.2003, 0.2299],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.2069, 0.2426],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0783, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0783, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0783, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3289953.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2491.5210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2101.1582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(541.3854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1105.9062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1101],
        [ 0.1136],
        [ 0.1161],
        ...,
        [-0.8779],
        [-0.8754],
        [-0.8747]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-969499.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1465.0439, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1465.0439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0064, -0.0165,  ...,  0.0060, -0.0121, -0.0168],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16314.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(221.0407, device='cuda:0')



h[100].sum tensor(-58.2980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.8905, device='cuda:0')



h[200].sum tensor(271.5935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-143.5138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(362314.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1078, 0.0361],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.1002, 0.0233],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0867, 0.0024],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0787, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0787, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0787, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3229703.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2397.6504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2182.7295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(354.0551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-900.3650, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1493],
        [ 0.0446],
        [-0.1469],
        ...,
        [-0.8511],
        [-0.8531],
        [-0.8589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1031917.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1407.5692, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1407.5692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0069, -0.0184,  ...,  0.0066, -0.0134, -0.0188],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0009,  0.0115, -0.0343,  ...,  0.0120, -0.0250, -0.0349],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16381.1855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(212.3691, device='cuda:0')



h[100].sum tensor(-55.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.8148, device='cuda:0')



h[200].sum tensor(270.5486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-137.8837, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0341, 0.0000,  ..., 0.0341, 0.0000, 0.0000],
        [0.0000, 0.0274, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(356853.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.1077, 0.0373],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.1405, 0.1057],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.1465, 0.1180],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0790, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0790, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0790, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3208564.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2362.9236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2224.8110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(188.7672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-822.6729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2107],
        [-0.0093],
        [ 0.1076],
        ...,
        [-0.8948],
        [-0.8917],
        [-0.8896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1019019.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1629.8264, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1629.8264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16518.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(245.9025, device='cuda:0')



h[100].sum tensor(-63.9484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-115.5757, device='cuda:0')



h[200].sum tensor(286.7523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-159.6557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(373313.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0807, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0852, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0963, 0.0156],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0793, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0793, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0793, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3298103.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2437.9263, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2143.9824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(736.2388, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-926.0193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4529],
        [-0.1813],
        [ 0.0297],
        ...,
        [-0.9044],
        [-0.9018],
        [-0.9011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1074416.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1336.0100, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.0100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16501.3691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(201.5725, device='cuda:0')



h[100].sum tensor(-52.5460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.7403, device='cuda:0')



h[200].sum tensor(271.7141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-130.8738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(357672.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0808, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0847, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0970, 0.0167],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0794, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0794, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0794, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3262921., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2349.6455, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2244.5127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(470.6158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-765.6857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7560],
        [-0.5017],
        [-0.1997],
        ...,
        [-0.9121],
        [-0.9096],
        [-0.9089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1086815.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 5100 loss: tensor(524.6353, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1497.1333, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1497.1333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0063, -0.0163,  ...,  0.0060, -0.0119, -0.0166],
        [-0.0008,  0.0071, -0.0191,  ...,  0.0069, -0.0140, -0.0195],
        [-0.0008,  0.0063, -0.0163,  ...,  0.0060, -0.0119, -0.0166],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16615.9648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.8823, device='cuda:0')



h[100].sum tensor(-58.4002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.1660, device='cuda:0')



h[200].sum tensor(283.8029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.6573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0564, 0.0000,  ..., 0.0611, 0.0000, 0.0000],
        [0.0000, 0.0377, 0.0000,  ..., 0.0387, 0.0000, 0.0000],
        [0.0000, 0.0234, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(370877.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.2333, 0.2985],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.2083, 0.2471],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1890, 0.2062],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0796, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0796, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0796, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3324024.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2428.0283, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2176.8840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(619.9044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-888.5618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1418],
        [ 0.1411],
        [ 0.1415],
        ...,
        [-0.9203],
        [-0.9177],
        [-0.9170]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1020952.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1376.5242, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1376.5242, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0039, -0.0079,  ...,  0.0031, -0.0057, -0.0080],
        [-0.0007,  0.0039, -0.0079,  ...,  0.0031, -0.0057, -0.0080],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0017,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16587.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(207.6852, device='cuda:0')



h[100].sum tensor(-54.1284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.6133, device='cuda:0')



h[200].sum tensor(281.7434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-134.8425, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0107, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(362769.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0968, 0.0087],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0937, 0.0053],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0904, 0.0024],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0801, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0801, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0801, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3301787.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2377.8174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2219.7559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(348.7847, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-787.8427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2604],
        [-0.4645],
        [-0.6794],
        ...,
        [-0.8867],
        [-0.9040],
        [-0.9188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1044930.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1584.7567, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1584.7567, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16721.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(239.1026, device='cuda:0')



h[100].sum tensor(-62.1042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.3796, device='cuda:0')



h[200].sum tensor(298.5146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-155.2407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(385286.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0827, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0810, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0812, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0807, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0807, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0807, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3437306.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2495.1960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2093.6580, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(703.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-952.3683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8903],
        [-1.0436],
        [-1.1291],
        ...,
        [-0.9417],
        [-0.9390],
        [-0.9382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1043605.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1194.7529, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1194.7529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0065, -0.0169,  ...,  0.0062, -0.0123, -0.0172],
        [-0.0008,  0.0065, -0.0169,  ...,  0.0062, -0.0123, -0.0172],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16657.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(180.2602, device='cuda:0')



h[100].sum tensor(-47.0068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.7234, device='cuda:0')



h[200].sum tensor(278.3685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-117.0365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0154, 0.0000,  ..., 0.0121, 0.0000, 0.0000],
        [0.0000, 0.0154, 0.0000,  ..., 0.0121, 0.0000, 0.0000],
        [0.0000, 0.0189, 0.0000,  ..., 0.0163, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(344599.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.1104, 0.0379],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1146, 0.0465],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.1187, 0.0543],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0811, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0811, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0811, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3236831.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2267.7583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2328.8423, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(212.3121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-507.6769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1234],
        [ 0.1969],
        [ 0.2110],
        ...,
        [-0.9490],
        [-0.9463],
        [-0.9454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1168129.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1442.1395, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1442.1395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16866.3945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(217.5850, device='cuda:0')



h[100].sum tensor(-55.9601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.2662, device='cuda:0')



h[200].sum tensor(294.7346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-141.2701, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(367951.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0826, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0819, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0823, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0814, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0814, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0814, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3344586.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2404.6140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2197.1968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(397.5606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-712.6135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8849],
        [-0.8495],
        [-0.7283],
        ...,
        [-0.9549],
        [-0.9524],
        [-0.9516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1045777.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1488.8794, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1488.8794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(16957.8809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.6369, device='cuda:0')



h[100].sum tensor(-57.9982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.5807, device='cuda:0')



h[200].sum tensor(301.5905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.8487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(369832.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0850, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0827, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0825, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0819, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0819, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0819, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3346272.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2411.5557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2179.9922, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(374.5343, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.2943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5502],
        [-0.7782],
        [-0.9416],
        ...,
        [-0.9645],
        [-0.9619],
        [-0.9611]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1059152., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1423.0590, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1423.0590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17026.7617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.7062, device='cuda:0')



h[100].sum tensor(-55.2692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.9132, device='cuda:0')



h[200].sum tensor(300.6445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.4010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(369727.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0868, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0869, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0883, 0.0000],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0824, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0824, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0824, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3377456.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2408.4368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2172.0615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(234.5316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-631.1168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1346],
        [-0.0756],
        [-0.0071],
        ...,
        [-0.9732],
        [-0.9706],
        [-0.9698]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1093415.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1221.1542, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1221.1542, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0098, -0.0284,  ...,  0.0102, -0.0207, -0.0289],
        [-0.0008,  0.0090, -0.0257,  ...,  0.0092, -0.0187, -0.0262],
        [-0.0009,  0.0098, -0.0284,  ...,  0.0102, -0.0207, -0.0289],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17020.5273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(184.2435, device='cuda:0')



h[100].sum tensor(-47.3351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.5955, device='cuda:0')



h[200].sum tensor(292.1337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.6227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0205, 0.0000,  ..., 0.0184, 0.0000, 0.0000],
        [0.0000, 0.0422, 0.0000,  ..., 0.0443, 0.0000, 0.0000],
        [0.0000, 0.0423, 0.0000,  ..., 0.0444, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(352993.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1419, 0.1007],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1649, 0.1494],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1647, 0.1484],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0855, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0835, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0829, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3324891.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2300.7629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2252.9736, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(271.7631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.9084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1939],
        [ 0.1892],
        [ 0.1788],
        ...,
        [-0.8158],
        [-0.9094],
        [-0.9562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1213744., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1368.4120, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1368.4120, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17172.6680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(206.4612, device='cuda:0')



h[100].sum tensor(-52.7052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.0380, device='cuda:0')



h[200].sum tensor(303.4205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-134.0479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(364529.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0842, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0837, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0839, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0833, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0833, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0833, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3381581., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2368.5669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2178.7134, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(359.7696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.3626, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8534],
        [-1.0275],
        [-1.1431],
        ...,
        [-0.9934],
        [-0.9907],
        [-0.9899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1190225.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1642.2017, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1642.2017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0009,  0.0095, -0.0273,  ...,  0.0098, -0.0198, -0.0278],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17439.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(247.7697, device='cuda:0')



h[100].sum tensor(-63.6801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-116.4532, device='cuda:0')



h[200].sum tensor(321.4290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-160.8680, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0201, 0.0000,  ..., 0.0181, 0.0000, 0.0000],
        [0.0000, 0.0223, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(388234.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1107, 0.0339],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1363, 0.0880],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1502, 0.1179],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0835, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0835, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0835, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3465198.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2527.3491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2056.4204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(454.5947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-698.1405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1986],
        [ 0.1935],
        [ 0.1886],
        ...,
        [-0.9992],
        [-0.9965],
        [-0.9957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1059681.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 5400 loss: tensor(533.0748, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1432.0946, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1432.0946, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0044, -0.0095,  ...,  0.0037, -0.0069, -0.0097],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0008,  0.0044, -0.0095,  ...,  0.0037, -0.0069, -0.0097],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17436.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(216.0695, device='cuda:0')



h[100].sum tensor(-55.2188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.5539, device='cuda:0')



h[200].sum tensor(310.2067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.2862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0000, 0.0164, 0.0000,  ..., 0.0137, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(377291.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0929, 0.0015],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0998, 0.0072],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0960, 0.0018],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0838, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0838, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0838, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3460260.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2470.4883, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2116.7393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(553.2803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-557.0723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5296],
        [-0.2976],
        [-0.1032],
        ...,
        [-1.0058],
        [-1.0032],
        [-1.0026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1078479.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1300.8605, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1300.8605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17389.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(196.2693, device='cuda:0')



h[100].sum tensor(-50.1840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.2477, device='cuda:0')



h[200].sum tensor(305.7602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.4306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0182, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(361176.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.1128, 0.0346],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.1132, 0.0353],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.1310, 0.0734],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0844, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0844, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0844, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3387770.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2376.8252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2207.1565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(230.0665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.8675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1560],
        [ 0.1615],
        [ 0.1681],
        ...,
        [-1.0206],
        [-1.0179],
        [-1.0172]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1171684.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1431.3396, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1431.3396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0008,  0.0058, -0.0144,  ...,  0.0054, -0.0105, -0.0147],
        [-0.0008,  0.0044, -0.0096,  ...,  0.0037, -0.0070, -0.0097],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17475.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(215.9555, device='cuda:0')



h[100].sum tensor(-55.1034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.5004, device='cuda:0')



h[200].sum tensor(316.9628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.2122, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0296, 0.0000,  ..., 0.0294, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(376669.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.1004, 0.0145],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.1160, 0.0420],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.1485, 0.1101],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0849, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0849, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0849, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3480123.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2455.4463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2122.1379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(431.8619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-468.0461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2299],
        [ 0.0100],
        [ 0.1543],
        ...,
        [-1.0341],
        [-1.0313],
        [-1.0305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1193582.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.7258, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.7258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0125, -0.0377,  ...,  0.0135, -0.0274, -0.0384],
        [-0.0009,  0.0110, -0.0326,  ...,  0.0117, -0.0237, -0.0332],
        [-0.0009,  0.0098, -0.0283,  ...,  0.0102, -0.0206, -0.0289],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17464.0039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(213.9015, device='cuda:0')



h[100].sum tensor(-54.1467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.5350, device='cuda:0')



h[200].sum tensor(315.4360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.8786, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0266, 0.0000,  ..., 0.0259, 0.0000, 0.0000],
        [0.0000, 0.0502, 0.0000,  ..., 0.0542, 0.0000, 0.0000],
        [0.0000, 0.0442, 0.0000,  ..., 0.0470, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(369127.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.1603, 0.1347],
        [0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.1948, 0.2083],
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.1931, 0.2053],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0849, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0849, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0849, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3428244.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2415.7222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2162.6367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(318.9533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-405.0184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1590],
        [ 0.1646],
        [ 0.1595],
        ...,
        [-1.0341],
        [-1.0313],
        [-1.0305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1162468.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1485.0706, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.0706, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0047, -0.0109,  ...,  0.0042, -0.0079, -0.0111],
        [-0.0008,  0.0089, -0.0251,  ...,  0.0091, -0.0183, -0.0256],
        [-0.0008,  0.0047, -0.0109,  ...,  0.0042, -0.0079, -0.0111],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17615.0332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.0623, device='cuda:0')



h[100].sum tensor(-56.7084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.3106, device='cuda:0')



h[200].sum tensor(321.1490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.4756, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0256, 0.0000,  ..., 0.0247, 0.0000, 0.0000],
        [0.0000, 0.0237, 0.0000,  ..., 0.0225, 0.0000, 0.0000],
        [0.0000, 0.0259, 0.0000,  ..., 0.0252, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(383021.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.1309, 0.0721],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.1292, 0.0689],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.1270, 0.0636],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0852, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0852, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0852, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3525193., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2499.5991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2069.6143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(509.0166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-485.8383, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2069],
        [ 0.1309],
        [-0.0259],
        ...,
        [-1.0416],
        [-1.0385],
        [-1.0373]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1140872.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1263.9075, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1263.9075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0035, -0.0066,  ...,  0.0027, -0.0048, -0.0067],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17640.5977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(190.6940, device='cuda:0')



h[100].sum tensor(-48.4451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.6273, device='cuda:0')



h[200].sum tensor(309.1196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-123.8108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0133, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(362824.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0143, 0.0000,  ..., 0.0000, 0.1081, 0.0208],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.1030, 0.0130],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0934, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0855, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0855, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0855, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3435549., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2398.4016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2179.6509, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(263.7722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.0443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1074],
        [ 0.0020],
        [-0.1932],
        ...,
        [-1.0487],
        [-1.0459],
        [-1.0451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1184070.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1390.0537, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1390.0537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0054, -0.0132,  ...,  0.0050, -0.0096, -0.0135],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0008,  0.0054, -0.0132,  ...,  0.0050, -0.0096, -0.0135],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17809.6211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(209.7265, device='cuda:0')



h[100].sum tensor(-53.4912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.5727, device='cuda:0')



h[200].sum tensor(318.4791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.1679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0202, 0.0000,  ..., 0.0184, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(373496.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0985, 0.0051],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.1074, 0.0196],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0987, 0.0051],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0858, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0858, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0858, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3483841.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2470.5308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2134.8386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(375.4157, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.5572, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5437],
        [-0.4988],
        [-0.5822],
        ...,
        [-1.0572],
        [-1.0544],
        [-1.0536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1202741.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1601.1881, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1601.1881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0038, -0.0077,  ...,  0.0032, -0.0056, -0.0079],
        [-0.0007,  0.0038, -0.0077,  ...,  0.0031, -0.0056, -0.0078],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18025.1406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(241.5817, device='cuda:0')



h[100].sum tensor(-60.7657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-113.5448, device='cuda:0')



h[200].sum tensor(330.9281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-156.8503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0184, 0.0000,  ..., 0.0164, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(395078.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.1116, 0.0273],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.1030, 0.0098],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.1013, 0.0051],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0861, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0861, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0861, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3609649.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2609.9883, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2036.7052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(835.9973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-514.5241, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2841],
        [-0.4212],
        [-0.5368],
        ...,
        [-1.0627],
        [-1.0599],
        [-1.0591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1147766.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1106.8348, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1106.8348, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(17826.4785, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(166.9954, device='cuda:0')



h[100].sum tensor(-41.9476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-78.4888, device='cuda:0')



h[200].sum tensor(302.3144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-108.4241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(350101.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0873, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0869, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0871, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0865, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0865, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0865, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3403422.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2376.5439, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2300.1687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(109.9987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-73.5634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0176],
        [-1.1673],
        [-1.2538],
        ...,
        [-1.0730],
        [-1.0701],
        [-1.0694]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1195130.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1610.4265, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1610.4265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0063, -0.0163,  ...,  0.0062, -0.0119, -0.0167],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0008,  0.0063, -0.0163,  ...,  0.0062, -0.0119, -0.0167],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18121.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(242.9756, device='cuda:0')



h[100].sum tensor(-60.9489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.2000, device='cuda:0')



h[200].sum tensor(336.4254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-157.7553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0101, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0235, 0.0000,  ..., 0.0226, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(390723.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.1072, 0.0164],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.1163, 0.0357],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.1041, 0.0109],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0871, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0871, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0871, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3584604.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2591.0852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2069.1846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(625.8484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.4093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1515],
        [ 0.0649],
        [-0.1244],
        ...,
        [-1.0890],
        [-1.0861],
        [-1.0852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1224738.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 5700 loss: tensor(469.5866, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1704.0009, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1704.0009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0008,  0.0061, -0.0157,  ...,  0.0060, -0.0114, -0.0160],
        [-0.0007,  0.0034, -0.0063,  ...,  0.0027, -0.0046, -0.0064],
        ...,
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0016,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18276.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.3219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(257.0937, device='cuda:0')



h[100].sum tensor(-64.4356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.8356, device='cuda:0')



h[200].sum tensor(344.1283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-166.9217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0123, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        [0.0000, 0.0372, 0.0000,  ..., 0.0390, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(407928.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.1094, 0.0241],
        [0.0000, 0.0179, 0.0000,  ..., 0.0000, 0.1288, 0.0612],
        [0.0000, 0.0241, 0.0000,  ..., 0.0000, 0.1805, 0.1699],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0875, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0875, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0875, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3705468.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2684.3354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1968.6771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(758.7798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-564.4531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2484],
        [ 0.0142],
        [ 0.1652],
        ...,
        [-1.0988],
        [-1.0957],
        [-1.0949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1227563.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1362.3632, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.3632, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010,  0.0143, -0.0437,  ...,  0.0157, -0.0317, -0.0446],
        [-0.0009,  0.0092, -0.0263,  ...,  0.0096, -0.0191, -0.0268],
        [-0.0008,  0.0073, -0.0198,  ...,  0.0074, -0.0144, -0.0202],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18235.9863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(205.5486, device='cuda:0')



h[100].sum tensor(-51.3717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.6091, device='cuda:0')



h[200].sum tensor(323.3685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.4554, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0581, 0.0000,  ..., 0.0641, 0.0000, 0.0000],
        [0.0000, 0.0417, 0.0000,  ..., 0.0445, 0.0000, 0.0000],
        [0.0000, 0.0332, 0.0000,  ..., 0.0342, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(375863.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0317, 0.0000,  ..., 0.0000, 0.2658, 0.3521],
        [0.0000, 0.0289, 0.0000,  ..., 0.0000, 0.2298, 0.2771],
        [0.0000, 0.0258, 0.0000,  ..., 0.0000, 0.1998, 0.2129],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0879, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0879, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0879, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3544117., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2507.2671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2139.9961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(326.2494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.6611, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1250],
        [ 0.1425],
        [ 0.1585],
        ...,
        [-1.1038],
        [-1.1008],
        [-1.1000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1194984.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1524.5901, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1524.5901, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18507.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(230.0249, device='cuda:0')



h[100].sum tensor(-57.6273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.1130, device='cuda:0')



h[200].sum tensor(334.5251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-149.3469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(393015.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0888, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0888, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0890, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0884, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0884, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0884, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3618130.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2598.1411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2017.4551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(448.5624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.1514, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2479],
        [-1.2936],
        [-1.3040],
        ...,
        [-1.1035],
        [-1.1026],
        [-1.1023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1138990., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1313.9302, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1313.9302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0040, -0.0084,  ...,  0.0034, -0.0061, -0.0086],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0040, -0.0084,  ...,  0.0034, -0.0061, -0.0086],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18482.1914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.2412, device='cuda:0')



h[100].sum tensor(-49.4602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.1745, device='cuda:0')



h[200].sum tensor(323.0202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.7109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0238, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        [0.0000, 0.0204, 0.0000,  ..., 0.0190, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(372452., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0230, 0.0000,  ..., 0.0000, 0.1643, 0.1345],
        [0.0000, 0.0208, 0.0000,  ..., 0.0000, 0.1429, 0.0877],
        [0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.1172, 0.0341],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0890, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0890, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0890, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3548836., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2469.8826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2127.7151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(318.3673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-132.0917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1938],
        [ 0.1890],
        [ 0.1563],
        ...,
        [-1.1143],
        [-1.1130],
        [-1.1123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1226262.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1463.4271, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1463.4271, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0068, -0.0182,  ...,  0.0068, -0.0132, -0.0185],
        [-0.0007,  0.0037, -0.0074,  ...,  0.0031, -0.0054, -0.0075],
        [-0.0007,  0.0036, -0.0072,  ...,  0.0030, -0.0052, -0.0073],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18682.6914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(220.7968, device='cuda:0')



h[100].sum tensor(-54.7671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.7758, device='cuda:0')



h[200].sum tensor(332.7548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-143.3555, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0183, 0.0000,  ..., 0.0165, 0.0000, 0.0000],
        [0.0000, 0.0187, 0.0000,  ..., 0.0171, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(390144.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0181, 0.0000,  ..., 0.0000, 0.1244, 0.0449],
        [0.0000, 0.0165, 0.0000,  ..., 0.0000, 0.1182, 0.0314],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.1048, 0.0098],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0895, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0895, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0895, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3643754., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2564.6157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2033.3833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(364.8182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-298.3870, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1705],
        [ 0.0449],
        [-0.1915],
        ...,
        [-1.1276],
        [-1.1246],
        [-1.1237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1184758.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1255.4686, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1255.4686, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18621.6152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(189.4207, device='cuda:0')



h[100].sum tensor(-47.1736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.0289, device='cuda:0')



h[200].sum tensor(321.5532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-122.9841, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(371983.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0937, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0910, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0907, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0900, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0900, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0900, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3562270.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2460.8833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2151.1433, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(249.1995, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-127.4610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5044],
        [-0.7872],
        [-1.0123],
        ...,
        [-1.1386],
        [-1.1355],
        [-1.1347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1205522., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1321.2183, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.2183, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18659.6758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(199.3408, device='cuda:0')



h[100].sum tensor(-49.4975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.6914, device='cuda:0')



h[200].sum tensor(325.3589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.4249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(376818.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0905, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0906, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0911, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0900, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0900, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0900, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3582127.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2486.7075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2124.2864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(327.6536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.9860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2041],
        [-1.0859],
        [-0.9202],
        ...,
        [-1.1386],
        [-1.1355],
        [-1.1347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1208782.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1582.4103, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1582.4103, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18813.4043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(238.7486, device='cuda:0')



h[100].sum tensor(-58.8289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-112.2132, device='cuda:0')



h[200].sum tensor(343.3057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-155.0109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(395214.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.1168, 0.0279],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1047, 0.0086],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0976, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0909, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0909, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0909, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3672640., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2571.1846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2017.8809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(539.4175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-309.4588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1917],
        [ 0.1560],
        [ 0.1309],
        ...,
        [-1.1532],
        [-1.1500],
        [-1.1491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1269459.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1285.0543, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1285.0543, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18733.2070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.8845, device='cuda:0')



h[100].sum tensor(-47.7884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.1269, device='cuda:0')



h[200].sum tensor(324.9344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.8823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(370931.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0944, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0928, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0922, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0913, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0913, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0913, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3565454.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2428.4180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2141.7705, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(178.9866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-49.2108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4300],
        [-0.5635],
        [-0.6627],
        ...,
        [-1.1610],
        [-1.1578],
        [-1.1569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1291518.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1170.2285, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1170.2285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(18862.2637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(176.5600, device='cuda:0')



h[100].sum tensor(-43.4751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.9843, device='cuda:0')



h[200].sum tensor(316.0973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-114.6341, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(366233.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0933, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0936, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0944, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0916, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0916, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0916, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3563126.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2388.2544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2139.7827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(246.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(48.9494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4983],
        [-0.5064],
        [-0.5141],
        ...,
        [-1.1616],
        [-1.1585],
        [-1.1578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1268886.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 6000 loss: tensor(467.0303, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1411.3531, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.3531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0008,  0.0062, -0.0159,  ...,  0.0061, -0.0115, -0.0162],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19223.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(212.9400, device='cuda:0')



h[100].sum tensor(-52.3277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.0831, device='cuda:0')



h[200].sum tensor(329.0085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.2543, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0096, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(391803.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0990, 0.0000],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.1126, 0.0187],
        [0.0000, 0.0175, 0.0000,  ..., 0.0000, 0.1199, 0.0287],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0918, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0918, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0918, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3690231.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2521.8765, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1978.0583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(579.2194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.7040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4825],
        [-0.1435],
        [ 0.0904],
        ...,
        [-1.1625],
        [-1.1595],
        [-1.1586]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1227592.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1419.7195, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1419.7195, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19379.3828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.2023, device='cuda:0')



h[100].sum tensor(-52.3237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.6764, device='cuda:0')



h[200].sum tensor(328.6037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.0739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(390610.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0948, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0980, 0.0000],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.1026, 0.0008],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0922, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0922, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0922, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3670465., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2509.9949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1970.4779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(426.7693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-104.1091, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2756],
        [-0.0735],
        [ 0.0741],
        ...,
        [-1.1621],
        [-1.1436],
        [-1.0871]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1148482.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1259.7955, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1259.7955, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19317.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(190.0736, device='cuda:0')



h[100].sum tensor(-46.3262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.3357, device='cuda:0')



h[200].sum tensor(319.9826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-123.4080, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(376034.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0955, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.1010, 0.0006],
        [0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.1157, 0.0223],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0927, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0927, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0927, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3612121., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2420.2773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2055.3420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(164.5077, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(72.4685, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5064],
        [-0.2352],
        [ 0.0153],
        ...,
        [-1.1796],
        [-1.1765],
        [-1.1757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1186976.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1318.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1318.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0009,  0.0096, -0.0277,  ...,  0.0102, -0.0200, -0.0282],
        [-0.0008,  0.0061, -0.0158,  ...,  0.0061, -0.0115, -0.0161],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19359.0039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.8905, device='cuda:0')



h[100].sum tensor(-48.6092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.4797, device='cuda:0')



h[200].sum tensor(323.7697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.1324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0178, 0.0000,  ..., 0.0163, 0.0000, 0.0000],
        [0.0000, 0.0206, 0.0000,  ..., 0.0197, 0.0000, 0.0000],
        [0.0000, 0.0481, 0.0000,  ..., 0.0527, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(380148.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0186, 0.0000,  ..., 0.0000, 0.1470, 0.0866],
        [0.0000, 0.0236, 0.0000,  ..., 0.0000, 0.1828, 0.1617],
        [0.0000, 0.0301, 0.0000,  ..., 0.0000, 0.2391, 0.2794],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0927, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0927, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0927, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3628175., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2442.6592, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2032.3053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(257.8475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(34.4133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1229],
        [ 0.1262],
        [ 0.1203],
        ...,
        [-1.1796],
        [-1.1765],
        [-1.1757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1193859.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1327.1001, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1327.1001, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19329.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(200.2282, device='cuda:0')



h[100].sum tensor(-48.9980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.1085, device='cuda:0')



h[200].sum tensor(327.3529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-130.0010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(379041.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0939, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0954, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0995, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0934, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0934, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0934, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3630334.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2427.8440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2055.8843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(246.7775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(81.2817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9717],
        [-0.7193],
        [-0.4291],
        ...,
        [-1.1979],
        [-1.1947],
        [-1.1938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1290104.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1134.3564, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1134.3564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19139.7129, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(171.1478, device='cuda:0')



h[100].sum tensor(-41.6929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.4405, device='cuda:0')



h[200].sum tensor(318.4936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-111.1201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(363264.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0945, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0945, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0947, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0941, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0941, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0941, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3597590., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2330.2849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2173.1602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(171.4411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(262.0766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2778],
        [-1.3831],
        [-1.4533],
        ...,
        [-1.2170],
        [-1.2137],
        [-1.2128]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1424407.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1280.6941, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1280.6941, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19277.5176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.2267, device='cuda:0')



h[100].sum tensor(-47.0732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.8177, device='cuda:0')



h[200].sum tensor(328.6632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.4552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(379142.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0949, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0949, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0951, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0944, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0944, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0944, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3669508.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2426.4133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2117.4194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(257.2169, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(108.2270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4763],
        [-1.4885],
        [-1.4910],
        ...,
        [-1.2279],
        [-1.2247],
        [-1.2237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1362209.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1157.0144, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1157.0144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19296.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(174.5663, device='cuda:0')



h[100].sum tensor(-42.1830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.0472, device='cuda:0')



h[200].sum tensor(320.6855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.3397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(364131.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0951, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0951, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0953, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0946, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0946, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0946, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3606864.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2339.7351, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2223.6313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(209.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.2226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1511],
        [-1.2830],
        [-1.3870],
        ...,
        [-1.2327],
        [-1.2294],
        [-1.2285]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1405513.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1257.8040, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1257.8040, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0072, -0.0193,  ...,  0.0074, -0.0140, -0.0197],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0008,  0.0046, -0.0107,  ...,  0.0043, -0.0077, -0.0109],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19487.0898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(189.7731, device='cuda:0')



h[100].sum tensor(-45.9456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.1945, device='cuda:0')



h[200].sum tensor(326.8977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-123.2129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0000, 0.0199, 0.0000,  ..., 0.0189, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(379440.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.1178, 0.0160],
        [0.0000, 0.0161, 0.0000,  ..., 0.0000, 0.1213, 0.0241],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.1079, 0.0038],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0952, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0949, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0949, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3677362.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2433.4080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2155.3557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(170.2632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(138.9677, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0398],
        [-0.2778],
        [-0.6120],
        ...,
        [-0.9893],
        [-1.1163],
        [-1.1919]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1265005.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1307.7239, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1307.7239, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0009,  0.0099, -0.0286,  ...,  0.0107, -0.0206, -0.0291],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19598.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(197.3048, device='cuda:0')



h[100].sum tensor(-47.7780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.7344, device='cuda:0')



h[200].sum tensor(330.7086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.1030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0143, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0104, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(382724.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1038, 0.0013],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.1221, 0.0291],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.1329, 0.0477],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0953, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0953, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0953, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3689167.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2433.2825, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2145.2363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(241.3396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(149.7865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1089],
        [ 0.0683],
        [ 0.1762],
        ...,
        [-1.2465],
        [-1.2432],
        [-1.2423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1323182.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 6300 loss: tensor(442.9907, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1107.0419, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1107.0419, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0059, -0.0149,  ...,  0.0058, -0.0108, -0.0152],
        [-0.0007,  0.0034, -0.0065,  ...,  0.0028, -0.0047, -0.0066],
        [-0.0008,  0.0074, -0.0201,  ...,  0.0076, -0.0145, -0.0204],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19481.2559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(167.0266, device='cuda:0')



h[100].sum tensor(-40.3411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-78.5035, device='cuda:0')



h[200].sum tensor(320.4963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-108.4444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0309, 0.0000,  ..., 0.0321, 0.0000, 0.0000],
        [0.0000, 0.0376, 0.0000,  ..., 0.0402, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(368329.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0204, 0.0000,  ..., 0.0000, 0.1418, 0.0666],
        [0.0000, 0.0271, 0.0000,  ..., 0.0000, 0.1834, 0.1552],
        [0.0000, 0.0322, 0.0000,  ..., 0.0000, 0.2201, 0.2314],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0958, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0958, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0958, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3649980., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2321.3799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2219.1360, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(143.6830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(333.7390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1502],
        [ 0.1996],
        [ 0.1949],
        ...,
        [-1.2594],
        [-1.2561],
        [-1.2552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1354499.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1242.5780, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1242.5780, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0039, -0.0083,  ...,  0.0034, -0.0060, -0.0084],
        [-0.0007,  0.0034, -0.0066,  ...,  0.0029, -0.0048, -0.0068],
        [-0.0008,  0.0059, -0.0149,  ...,  0.0058, -0.0108, -0.0152],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19649.9453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(187.4758, device='cuda:0')



h[100].sum tensor(-45.3171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.1148, device='cuda:0')



h[200].sum tensor(330.3427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-121.7214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0000, 0.0183, 0.0000,  ..., 0.0170, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(380109.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.1152, 0.0129],
        [0.0000, 0.0190, 0.0000,  ..., 0.0000, 0.1232, 0.0258],
        [0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.1175, 0.0150],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0962, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0962, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0962, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3700686., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2361.3923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2141.8999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(215.5890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(265.2114, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0846],
        [ 0.0365],
        [-0.0094],
        ...,
        [-1.2637],
        [-1.2621],
        [-1.2631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1351698.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1158.6014, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1158.6014, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19705.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(174.8058, device='cuda:0')



h[100].sum tensor(-41.7573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.1597, device='cuda:0')



h[200].sum tensor(324.1194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.4951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(371020.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0969, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0969, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0971, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0964, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0964, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0964, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3663848.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2300.7202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2199.0007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(78.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(379.9259, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2424],
        [-1.3724],
        [-1.4644],
        ...,
        [-1.2611],
        [-1.2603],
        [-1.2627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1323078.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1365.8707, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1365.8707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0038, -0.0080,  ...,  0.0034, -0.0058, -0.0081],
        [-0.0007,  0.0034, -0.0066,  ...,  0.0029, -0.0048, -0.0068],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19969.8086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(206.0778, device='cuda:0')



h[100].sum tensor(-48.9442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.8578, device='cuda:0')



h[200].sum tensor(335.8907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.7990, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0000, 0.0213, 0.0000,  ..., 0.0207, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(389120.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0143, 0.0000,  ..., 0.0000, 0.1102, 0.0079],
        [0.0000, 0.0182, 0.0000,  ..., 0.0000, 0.1220, 0.0249],
        [0.0000, 0.0219, 0.0000,  ..., 0.0000, 0.1328, 0.0463],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0966, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0966, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0966, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3752089., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2405.6624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2121.2839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(255.7798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(224.4895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2131],
        [ 0.0461],
        [ 0.1691],
        ...,
        [-1.2811],
        [-1.2778],
        [-1.2768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1333968.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1423.1918, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1423.1918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0053, -0.0131,  ...,  0.0052, -0.0094, -0.0133],
        [-0.0009,  0.0123, -0.0365,  ...,  0.0135, -0.0263, -0.0372],
        [-0.0008,  0.0085, -0.0238,  ...,  0.0090, -0.0172, -0.0242],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20091.0547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.7262, device='cuda:0')



h[100].sum tensor(-51.0067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.9226, device='cuda:0')



h[200].sum tensor(339.5106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.4140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0327, 0.0000,  ..., 0.0344, 0.0000, 0.0000],
        [0.0000, 0.0319, 0.0000,  ..., 0.0335, 0.0000, 0.0000],
        [0.0000, 0.0322, 0.0000,  ..., 0.0338, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(397486.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0253, 0.0000,  ..., 0.0000, 0.1612, 0.1070],
        [0.0000, 0.0273, 0.0000,  ..., 0.0000, 0.1685, 0.1230],
        [0.0000, 0.0288, 0.0000,  ..., 0.0000, 0.1713, 0.1287],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0969, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0969, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0969, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3800875.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2454.7136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2099.0740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(608.7905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(152.2188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2106],
        [ 0.2163],
        [ 0.2213],
        ...,
        [-1.2897],
        [-1.2864],
        [-1.2855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1313793.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1245.0311, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1245.0311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(19954.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(187.8460, device='cuda:0')



h[100].sum tensor(-44.8553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.2887, device='cuda:0')



h[200].sum tensor(329.0582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-121.9617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(380878.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0974, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0983, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.1027, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0969, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0969, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0969, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3707488.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2366.1323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2189.7441, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(128.7774, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(293.5796, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0925],
        [-0.7849],
        [-0.3810],
        ...,
        [-1.2897],
        [-1.2864],
        [-1.2855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1258275.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1421.0306, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.0306, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20079.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.4002, device='cuda:0')



h[100].sum tensor(-50.8648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.7693, device='cuda:0')



h[200].sum tensor(342.1624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.2023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(393508.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1045, 0.0012],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0985, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1004, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0975, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0975, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0975, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3782513., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2415.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2119.3047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(333.7078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(209.9802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1119],
        [-0.2835],
        [-0.3119],
        ...,
        [-1.3072],
        [-1.3038],
        [-1.3029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1365068.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1535.6432, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1535.6432, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20185.2520, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(231.6925, device='cuda:0')



h[100].sum tensor(-54.9688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-108.8969, device='cuda:0')



h[200].sum tensor(351.2405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-150.4296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(406411.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0992, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0999, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.1009, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0983, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0981, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0981, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3851437., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2479.1558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2046.4963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(473.1444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(93.0781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6868],
        [-0.6160],
        [-0.5523],
        ...,
        [-1.1805],
        [-1.2478],
        [-1.2906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1346363.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1274.6411, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1274.6411, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20051.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(192.3134, device='cuda:0')



h[100].sum tensor(-45.5939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.3885, device='cuda:0')



h[200].sum tensor(335.4777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.8622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(385594.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.1091, 0.0031],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.1055, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.1114, 0.0058],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0984, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0984, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0984, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3769700.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2356.9172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2170.7266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(188.4543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(307.3478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0236],
        [-0.0455],
        [ 0.0275],
        ...,
        [-1.3314],
        [-1.3279],
        [-1.3269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1377565.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1299.0590, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1299.0590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0039, -0.0083,  ...,  0.0035, -0.0060, -0.0084],
        [-0.0008,  0.0073, -0.0195,  ...,  0.0075, -0.0140, -0.0199],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20067.2305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(195.9975, device='cuda:0')



h[100].sum tensor(-46.2758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.1200, device='cuda:0')



h[200].sum tensor(336.6439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.2542, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0154, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0234, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(385381.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.1158, 0.0156],
        [0.0000, 0.0203, 0.0000,  ..., 0.0000, 0.1383, 0.0561],
        [0.0000, 0.0263, 0.0000,  ..., 0.0000, 0.1597, 0.1001],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0984, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0984, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0984, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3774448., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2351.0771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2174.5056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(375.0130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(329.3787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0430],
        [ 0.1573],
        [ 0.1997],
        ...,
        [-1.3314],
        [-1.3279],
        [-1.3269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1498337.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 6600 loss: tensor(406.6489, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1470.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1470.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20392.9980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(221.8237, device='cuda:0')



h[100].sum tensor(-52.5730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.2584, device='cuda:0')



h[200].sum tensor(345.6447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-144.0222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(406684.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0989, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0989, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0991, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0985, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0985, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0985, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3874722.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2467.6025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2063.1938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(499.3561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(126.0651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5888],
        [-1.5900],
        [-1.5764],
        ...,
        [-1.3326],
        [-1.3291],
        [-1.3282]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1356485.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1487.4423, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1487.4423, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20539.9180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.4201, device='cuda:0')



h[100].sum tensor(-53.2087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4788, device='cuda:0')



h[200].sum tensor(347.3255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.7079, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(408256.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0992, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0992, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0995, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0988, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0988, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0988, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3880795.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2457.7847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2040.7859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(381.5217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(129.5771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5790],
        [-1.5603],
        [-1.5188],
        ...,
        [-1.3396],
        [-1.3361],
        [-1.3352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1304142.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1266.9144, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1266.9144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20404.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(191.1476, device='cuda:0')



h[100].sum tensor(-44.9641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.8405, device='cuda:0')



h[200].sum tensor(335.0327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.1053, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(389027.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.1090, 0.0033],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.1024, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1006, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0994, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0994, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0994, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3815169., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2319.5103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2135.7139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(369.1093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(355.9786, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0255],
        [-1.2312],
        [-1.2971],
        ...,
        [-1.3523],
        [-1.3488],
        [-1.3479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1424741., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1650.8274, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1650.8274, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0015,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20847.0039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(249.0711, device='cuda:0')



h[100].sum tensor(-58.8760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-117.0649, device='cuda:0')



h[200].sum tensor(360.4788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-161.7129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(425006.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.1004, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.1004, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.1006, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3980032.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2496.4360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1918.9707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(814.8248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(32.4738, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5386],
        [-1.5963],
        [-1.6402],
        ...,
        [-1.3574],
        [-1.3529],
        [-1.3513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1360826.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1279.8850, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1279.8850, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20644.8926, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.1046, device='cuda:0')



h[100].sum tensor(-44.9210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.7603, device='cuda:0')



h[200].sum tensor(336.9302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.3759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(390825.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.1067, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1037, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.1035, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1004, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1004, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3825386.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2292.1382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2098.3667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(321.9608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(374.9489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1178],
        [-0.2768],
        [-0.3855],
        ...,
        [-1.3680],
        [-1.3645],
        [-1.3636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1351211.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1419.9579, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1419.9579, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20781.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.2383, device='cuda:0')



h[100].sum tensor(-50.3954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.6933, device='cuda:0')



h[200].sum tensor(346.3968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.0973, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(406372.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.1009, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1009, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1013, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1004, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1004, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3907135., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2377.9358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2012.8677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(569.2823, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(233.7633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4767],
        [-1.3551],
        [-1.1714],
        ...,
        [-1.3680],
        [-1.3645],
        [-1.3636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1363560.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1202.7402, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1202.7402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20714.1445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(181.4653, device='cuda:0')



h[100].sum tensor(-42.5306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.2897, device='cuda:0')



h[200].sum tensor(332.9675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-117.8189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(388246.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.1186, 0.0158],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.1062, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1035, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1009, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1009, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3814377., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2277.8718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2119.7446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(150.8629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(404.9053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0104],
        [-0.2217],
        [-0.3282],
        ...,
        [-1.3733],
        [-1.3702],
        [-1.3695]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1307790.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1431.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1431.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0038, -0.0080,  ...,  0.0034, -0.0058, -0.0081],
        [-0.0007,  0.0037, -0.0077,  ...,  0.0033, -0.0056, -0.0079],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20989.9551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(215.9223, device='cuda:0')



h[100].sum tensor(-50.5020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.4847, device='cuda:0')



h[200].sum tensor(348.4039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.1906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0175, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0073, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(405960.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0000, 0.1301, 0.0267],
        [0.0000, 0.0151, 0.0000,  ..., 0.0000, 0.1184, 0.0097],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.1112, 0.0019],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1015, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1015, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3902370., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2363.9717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2020.7029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(426.0843, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(258.1197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2170],
        [-0.3406],
        [-0.5106],
        ...,
        [-1.3863],
        [-1.3828],
        [-1.3818]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1327767.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1344.4049, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1344.4049, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20965.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(202.8391, device='cuda:0')



h[100].sum tensor(-47.4007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.3356, device='cuda:0')



h[200].sum tensor(345.2435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-131.6962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(404402.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1026, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1026, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1029, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1022, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1022, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3924107., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2340.9521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2026.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(464.9204, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(306.9038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6506],
        [-1.6452],
        [-1.6076],
        ...,
        [-1.4006],
        [-1.3971],
        [-1.3961]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1380707.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1178.3162, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1178.3162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0008,  0.0067, -0.0179,  ...,  0.0070, -0.0129, -0.0182],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(20925.9258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(177.7802, device='cuda:0')



h[100].sum tensor(-41.3685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.5578, device='cuda:0')



h[200].sum tensor(336.5833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-115.4263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0134, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0000, 0.0250, 0.0000,  ..., 0.0255, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(385343.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0219, 0.0000,  ..., 0.0000, 0.1527, 0.0708],
        [0.0000, 0.0216, 0.0000,  ..., 0.0000, 0.1523, 0.0707],
        [0.0000, 0.0248, 0.0000,  ..., 0.0000, 0.1700, 0.1073],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.1026, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.1026, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.1026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3836504.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2220.4790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2122.9277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(186.4504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(536.2871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2406],
        [ 0.2461],
        [ 0.2469],
        ...,
        [-1.4105],
        [-1.4068],
        [-1.4058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1450004.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 6900 loss: tensor(464.0290, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1209.0081, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1209.0081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21100.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(182.4109, device='cuda:0')



h[100].sum tensor(-42.1913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.7342, device='cuda:0')



h[200].sum tensor(339.5290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-118.4329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(388697.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1066, 0.0000],
        [0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.1107, 0.0000],
        [0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.1208, 0.0088],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1030, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1030, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3852945.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2234.4653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2092.3027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(137.7874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(534.4753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8148],
        [-0.5208],
        [-0.2596],
        ...,
        [-1.4180],
        [-1.4143],
        [-1.4133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1404489.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1221.5986, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1221.5986, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0068, -0.0180,  ...,  0.0070, -0.0130, -0.0184],
        [-0.0008,  0.0068, -0.0180,  ...,  0.0070, -0.0130, -0.0184],
        [-0.0007,  0.0032, -0.0062,  ...,  0.0028, -0.0045, -0.0063],
        ...,
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0014,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21273.0293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(184.3105, device='cuda:0')



h[100].sum tensor(-42.5790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.6271, device='cuda:0')



h[200].sum tensor(341.6879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.6662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0152, 0.0000,  ..., 0.0140, 0.0000, 0.0000],
        [0.0000, 0.0209, 0.0000,  ..., 0.0208, 0.0000, 0.0000],
        [0.0000, 0.0264, 0.0000,  ..., 0.0274, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(388963.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0251, 0.0000,  ..., 0.0000, 0.1571, 0.0799],
        [0.0000, 0.0280, 0.0000,  ..., 0.0000, 0.1716, 0.1100],
        [0.0000, 0.0309, 0.0000,  ..., 0.0000, 0.1855, 0.1390],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1033, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1033, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1033, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3854704.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2231.9236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2082.7520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(153.1349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(568.8806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2457],
        [ 0.2413],
        [ 0.2381],
        ...,
        [-1.4250],
        [-1.4214],
        [-1.4205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1400919.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1289.7423, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1289.7423, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0108, -0.0316,  ...,  0.0119, -0.0227, -0.0322],
        [-0.0010,  0.0147, -0.0447,  ...,  0.0167, -0.0321, -0.0455],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21472.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(194.5918, device='cuda:0')



h[100].sum tensor(-44.8203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.4593, device='cuda:0')



h[200].sum tensor(347.1569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.3415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0534, 0.0000,  ..., 0.0600, 0.0000, 0.0000],
        [0.0000, 0.0287, 0.0000,  ..., 0.0302, 0.0000, 0.0000],
        [0.0000, 0.0352, 0.0000,  ..., 0.0381, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(398736.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0439, 0.0000,  ..., 0.0000, 0.2668, 0.3038],
        [0.0000, 0.0338, 0.0000,  ..., 0.0000, 0.2121, 0.1918],
        [0.0000, 0.0284, 0.0000,  ..., 0.0000, 0.1827, 0.1310],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1037, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1037, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3910907.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2285.9141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2023.3485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(194.5350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(504.0072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1797],
        [ 0.1905],
        [ 0.1768],
        ...,
        [-1.4331],
        [-1.4295],
        [-1.4286]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1372360.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1437.6777, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1437.6777, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0091, -0.0258,  ...,  0.0098, -0.0185, -0.0263],
        [-0.0009,  0.0100, -0.0289,  ...,  0.0110, -0.0208, -0.0295],
        [-0.0008,  0.0079, -0.0220,  ...,  0.0085, -0.0158, -0.0225],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21662.5195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(216.9118, device='cuda:0')



h[100].sum tensor(-49.8528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.9498, device='cuda:0')



h[200].sum tensor(358.9944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.8331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0556, 0.0000,  ..., 0.0627, 0.0000, 0.0000],
        [0.0000, 0.0402, 0.0000,  ..., 0.0441, 0.0000, 0.0000],
        [0.0000, 0.0219, 0.0000,  ..., 0.0221, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(409867.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0461, 0.0000,  ..., 0.0000, 0.2745, 0.3178],
        [0.0000, 0.0387, 0.0000,  ..., 0.0000, 0.2322, 0.2315],
        [0.0000, 0.0287, 0.0000,  ..., 0.0000, 0.1808, 0.1251],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1043, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1043, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3967669.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2333.6050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1942.7086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(420.8639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(428.3348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1683],
        [ 0.1780],
        [ 0.1746],
        ...,
        [-1.4491],
        [-1.4453],
        [-1.4443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1375990., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1405.6693, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1405.6693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0033, -0.0066,  ...,  0.0029, -0.0047, -0.0067],
        [-0.0007,  0.0033, -0.0066,  ...,  0.0029, -0.0047, -0.0067],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21731.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(212.0825, device='cuda:0')



h[100].sum tensor(-48.7656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.6800, device='cuda:0')



h[200].sum tensor(358.7251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-137.6976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(411658.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.1164, 0.0005],
        [0.0000, 0.0170, 0.0000,  ..., 0.0000, 0.1188, 0.0005],
        [0.0000, 0.0173, 0.0000,  ..., 0.0000, 0.1190, 0.0005],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1047, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1047, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3999100.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2333.7708, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1918.5011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(499.6663, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(445.4362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2282],
        [-0.0877],
        [-0.0031],
        ...,
        [-1.4564],
        [-1.4524],
        [-1.4507]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1433408., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1085.7510, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1085.7510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21511.8789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(163.8143, device='cuda:0')



h[100].sum tensor(-37.5042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-76.9937, device='cuda:0')



h[200].sum tensor(339.7496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-106.3588, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(383393.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1055, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1055, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.1058, 0.0000],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1050, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1050, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3877730.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2169.3015, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2062.0933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(95.7533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(733.8235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7497],
        [-1.7291],
        [-1.6880],
        ...,
        [-1.4704],
        [-1.4666],
        [-1.4655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1471550.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1428.5687, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1428.5687, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0035, -0.0071,  ...,  0.0031, -0.0051, -0.0073],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21958.1309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(215.5375, device='cuda:0')



h[100].sum tensor(-49.1352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.3039, device='cuda:0')



h[200].sum tensor(360.9275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.9408, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(411651.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.1119, 0.0000],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.1139, 0.0000],
        [0.0000, 0.0158, 0.0000,  ..., 0.0000, 0.1167, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1054, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.1072, 0.0000],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.1138, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4000271., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2316.4653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1887.9890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(462.0493, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(497.9800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1999],
        [-0.1202],
        [ 0.0082],
        ...,
        [-1.3205],
        [-1.0651],
        [-0.6812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1485699.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1314.9845, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1314.9845, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21934.7363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.4003, device='cuda:0')



h[100].sum tensor(-45.2756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.2493, device='cuda:0')



h[200].sum tensor(354.5842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.8142, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(403421.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0156, 0.0000,  ..., 0.0000, 0.1171, 0.0008],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.1159, 0.0000],
        [0.0000, 0.0180, 0.0000,  ..., 0.0000, 0.1229, 0.0061],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1057, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1057, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3964429.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2268.6875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1919.4595, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(241.5009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(582.8690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0119],
        [ 0.0620],
        [ 0.1181],
        ...,
        [-1.4887],
        [-1.4850],
        [-1.4839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1418743.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1142.8589, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1142.8589, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0054, -0.0137,  ...,  0.0055, -0.0098, -0.0139],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(21863.4570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(172.4306, device='cuda:0')



h[100].sum tensor(-39.1533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.0434, device='cuda:0')



h[200].sum tensor(343.3588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-111.9530, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0231, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0170, 0.0000,  ..., 0.0164, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(390050.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0237, 0.0000,  ..., 0.0000, 0.1477, 0.0535],
        [0.0000, 0.0192, 0.0000,  ..., 0.0000, 0.1322, 0.0208],
        [0.0000, 0.0231, 0.0000,  ..., 0.0000, 0.1453, 0.0475],
        ...,
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1061, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1061, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3921351., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2175.1997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1972.6615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(191.1656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(753.2798, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.7813e-04],
        [ 1.3152e-03],
        [ 8.9741e-02],
        ...,
        [-1.4963e+00],
        [-1.4926e+00],
        [-1.4916e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1502360., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1248.3439, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1248.3439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0034, -0.0068,  ...,  0.0030, -0.0049, -0.0070],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22034.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(188.3458, device='cuda:0')



h[100].sum tensor(-42.6027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.5236, device='cuda:0')



h[200].sum tensor(350.1340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-122.2862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(399425.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.1112, 0.0000],
        [0.0000, 0.0161, 0.0000,  ..., 0.0000, 0.1184, 0.0005],
        [0.0000, 0.0175, 0.0000,  ..., 0.0000, 0.1210, 0.0005],
        ...,
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1066, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1066, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(3962686.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2212.6450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1901.1677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(235.1492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(684.3371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5610],
        [-0.3740],
        [-0.2736],
        ...,
        [-1.5084],
        [-1.5047],
        [-1.5037]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1452001., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 7200 loss: tensor(515.3913, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1453.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1453.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0036, -0.0076,  ...,  0.0033, -0.0055, -0.0078],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22281.1855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(219.2790, device='cuda:0')



h[100].sum tensor(-49.4650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-103.0624, device='cuda:0')



h[200].sum tensor(363.4373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.3700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0164, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(416735.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0209, 0.0000,  ..., 0.0000, 0.1338, 0.0200],
        [0.0000, 0.0183, 0.0000,  ..., 0.0000, 0.1268, 0.0090],
        [0.0000, 0.0185, 0.0000,  ..., 0.0000, 0.1262, 0.0037],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1071, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1071, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4054079.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2304.5994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1805.8628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(549.1599, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(533.6737, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1573],
        [ 0.1689],
        [ 0.1418],
        ...,
        [-1.5223],
        [-1.5185],
        [-1.5175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1471076.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1233.9087, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1233.9087, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22097.5527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(186.1678, device='cuda:0')



h[100].sum tensor(-42.1486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.5000, device='cuda:0')



h[200].sum tensor(351.3148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-120.8721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(403928.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.1174, 0.0008],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.1107, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.1090, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1077, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1077, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1077, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4021476., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2231.2695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1880.7029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(482.0992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(670.8426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3610],
        [-0.6874],
        [-0.9121],
        ...,
        [-1.5366],
        [-1.5327],
        [-1.5317]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1493001.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1394.6367, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1394.6367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22318.4023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(210.4179, device='cuda:0')



h[100].sum tensor(-47.5139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.8977, device='cuda:0')



h[200].sum tensor(361.5810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.6168, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(416486.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1091, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1119, 0.0000],
        [0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.1206, 0.0035],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1082, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1082, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1082, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4058175.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2289.8291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1795.9109, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(304.1321, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(563.2607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0264],
        [-0.7615],
        [-0.4168],
        ...,
        [-1.5498],
        [-1.5458],
        [-1.5448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1416003.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1356.9214, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1356.9214, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0036, -0.0078,  ...,  0.0034, -0.0056, -0.0079],
        [-0.0008,  0.0047, -0.0113,  ...,  0.0047, -0.0081, -0.0115],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22414.7246, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(204.7276, device='cuda:0')



h[100].sum tensor(-46.2905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.2232, device='cuda:0')



h[200].sum tensor(358.6898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.9223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0251, 0.0000,  ..., 0.0262, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(411364.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0271, 0.0000,  ..., 0.0000, 0.1634, 0.0774],
        [0.0000, 0.0203, 0.0000,  ..., 0.0000, 0.1388, 0.0312],
        [0.0000, 0.0169, 0.0000,  ..., 0.0000, 0.1270, 0.0109],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1086, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1086, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1086, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4055525.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2230.4343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1812.5834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(430.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(674.5828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2616],
        [ 0.2102],
        [ 0.1722],
        ...,
        [-1.5573],
        [-1.5534],
        [-1.5524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1570120.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1244.4573, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1244.4573, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0091, -0.0259,  ...,  0.0100, -0.0185, -0.0264],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0038, -0.0082,  ...,  0.0036, -0.0059, -0.0084],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22445.0566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(187.7594, device='cuda:0')



h[100].sum tensor(-42.3439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.2480, device='cuda:0')



h[200].sum tensor(350.4870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-121.9054, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0265, 0.0000,  ..., 0.0280, 0.0000, 0.0000],
        [0.0000, 0.0231, 0.0000,  ..., 0.0239, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(403823.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.3812e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9802e-01,
         1.4732e-01],
        [0.0000e+00, 2.7593e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6807e-01,
         8.5845e-02],
        [0.0000e+00, 1.9281e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3517e-01,
         2.3806e-02],
        ...,
        [0.0000e+00, 1.8363e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3358e-01,
         2.2461e-02],
        [0.0000e+00, 1.2701e-02, 0.0000e+00,  ..., 0.0000e+00, 1.1726e-01,
         1.1508e-04],
        [0.0000e+00, 1.0428e-02, 0.0000e+00,  ..., 0.0000e+00, 1.1059e-01,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4028607.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2163.8662, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1837.9475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(379.3821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(780.0454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2348],
        [ 0.2249],
        [ 0.1701],
        ...,
        [-0.2774],
        [-0.7255],
        [-1.1316]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1520434.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1397.9553, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1397.9553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0037, -0.0080,  ...,  0.0035, -0.0057, -0.0081],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22767.0039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(210.9186, device='cuda:0')



h[100].sum tensor(-47.1675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.1330, device='cuda:0')



h[200].sum tensor(357.5636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.9419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.0128, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(418045.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0166, 0.0000,  ..., 0.0000, 0.1236, 0.0000],
        [0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.1200, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.1223, 0.0020],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1091, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1091, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1091, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4091912.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2246.9399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1762.5731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(405.1202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(649.6061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0113],
        [ 0.0040],
        [ 0.0576],
        ...,
        [-1.5646],
        [-1.5608],
        [-1.5598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1438017.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1365.4856, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1365.4856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0071, -0.0191,  ...,  0.0076, -0.0137, -0.0195],
        [-0.0008,  0.0050, -0.0122,  ...,  0.0050, -0.0087, -0.0124],
        [-0.0008,  0.0067, -0.0178,  ...,  0.0071, -0.0128, -0.0182],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22879.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(206.0197, device='cuda:0')



h[100].sum tensor(-46.2256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.8305, device='cuda:0')



h[200].sum tensor(355.3173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.7612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0232, 0.0000,  ..., 0.0242, 0.0000, 0.0000],
        [0.0000, 0.0260, 0.0000,  ..., 0.0275, 0.0000, 0.0000],
        [0.0000, 0.0258, 0.0000,  ..., 0.0272, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(421853.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0305, 0.0000,  ..., 0.0000, 0.1749, 0.1005],
        [0.0000, 0.0329, 0.0000,  ..., 0.0000, 0.1848, 0.1210],
        [0.0000, 0.0367, 0.0000,  ..., 0.0000, 0.2005, 0.1525],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1094, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1094, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4111374., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2265.4414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1743.0519, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(268.1935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(622.4067, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2453],
        [ 0.2449],
        [ 0.2421],
        ...,
        [-1.5697],
        [-1.5660],
        [-1.5650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1333953., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1276.9055, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1276.9055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0013,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22802.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(192.6551, device='cuda:0')



h[100].sum tensor(-42.9544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.5490, device='cuda:0')



h[200].sum tensor(352.1187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.0840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(411508.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.1127, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.1105, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1107, 0.0000],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1100, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1100, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4081666.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2180.8110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1796.0992, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(217.2606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(775.2717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7924],
        [-1.0331],
        [-1.1033],
        ...,
        [-1.5881],
        [-1.5843],
        [-1.5833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1451653., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1344.4023, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1344.4023, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22876.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(202.8387, device='cuda:0')



h[100].sum tensor(-45.0975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.3354, device='cuda:0')



h[200].sum tensor(359.2833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-131.6959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(415878.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.1115, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1113, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.1125, 0.0000],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1107, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1107, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4112090., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2190.0151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1767.5006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(227.4671, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(770.2706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7664],
        [-0.7095],
        [-0.5496],
        ...,
        [-1.6087],
        [-1.6047],
        [-1.6036]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1490783.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1250.0739, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1250.0739, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22788.8574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(188.6068, device='cuda:0')



h[100].sum tensor(-41.9054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.6463, device='cuda:0')



h[200].sum tensor(355.6532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-122.4556, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        [0.0000, 0.0130, 0.0000,  ..., 0.0119, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(409609.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0182, 0.0000,  ..., 0.0000, 0.1320, 0.0128],
        [0.0000, 0.0224, 0.0000,  ..., 0.0000, 0.1459, 0.0356],
        [0.0000, 0.0239, 0.0000,  ..., 0.0000, 0.1515, 0.0457],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1113, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1113, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1113, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4119425.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2139.6104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1799.6462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(453.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(881.5968, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1598],
        [ 0.2127],
        [ 0.2188],
        ...,
        [-1.6229],
        [-1.6187],
        [-1.6174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1603058.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 7500 loss: tensor(503.7491, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1611.6145, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1611.6145, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0052, -0.0130,  ...,  0.0053, -0.0093, -0.0132],
        [-0.0008,  0.0059, -0.0152,  ...,  0.0062, -0.0109, -0.0155],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23294.3223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(243.1548, device='cuda:0')



h[100].sum tensor(-53.8358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-114.2842, device='cuda:0')



h[200].sum tensor(376.9341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-157.8717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0265, 0.0000,  ..., 0.0282, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(440279.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0184, 0.0000,  ..., 0.0000, 0.1336, 0.0168],
        [0.0000, 0.0238, 0.0000,  ..., 0.0000, 0.1502, 0.0435],
        [0.0000, 0.0296, 0.0000,  ..., 0.0000, 0.1680, 0.0784],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1115, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1115, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1115, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4238441., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2331.6982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1636.5063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(693.4563, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(594.1932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0744],
        [ 0.1988],
        [ 0.2611],
        ...,
        [-1.6348],
        [-1.6305],
        [-1.6294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1473513.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1322.5039, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1322.5039, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23115.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(199.5348, device='cuda:0')



h[100].sum tensor(-44.1265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.7825, device='cuda:0')



h[200].sum tensor(356.9290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.5508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(418329.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1120, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1120, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1123, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1115, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1115, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1115, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4155114.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2217.6599, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1762.0886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(356.6746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(818.6819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9131],
        [-1.8959],
        [-1.8450],
        ...,
        [-1.6382],
        [-1.6342],
        [-1.6331]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1471664.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1255.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1255.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0070, -0.0188,  ...,  0.0075, -0.0134, -0.0192],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23144.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(189.4666, device='cuda:0')



h[100].sum tensor(-41.6929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.0504, device='cuda:0')



h[200].sum tensor(351.3737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-123.0139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0153, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(411487.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0214, 0.0000,  ..., 0.0000, 0.1471, 0.0358],
        [0.0000, 0.0194, 0.0000,  ..., 0.0000, 0.1410, 0.0263],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.1229, 0.0004],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1118, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1118, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4123782.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2171.7678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1793.7323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(191.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(921.2196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3056],
        [-0.6413],
        [-1.0369],
        ...,
        [-1.6466],
        [-1.6374],
        [-1.6162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1504831.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1126.2045, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1126.2045, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(22995.5605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(169.9178, device='cuda:0')



h[100].sum tensor(-37.4356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.8624, device='cuda:0')



h[200].sum tensor(343.5491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-110.3215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(402593.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.1220, 0.0017],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.1165, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.1181, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1118, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1118, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4095334.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2121.2546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1842.9987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(138.9501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1005.6195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4287],
        [-0.5470],
        [-0.5339],
        ...,
        [-1.6474],
        [-1.6434],
        [-1.6423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1518590.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1211.4189, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1211.4189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23177.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(182.7747, device='cuda:0')



h[100].sum tensor(-40.2810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.9052, device='cuda:0')



h[200].sum tensor(348.7701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-118.6691, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(409774.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1127, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1127, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1129, 0.0000],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1122, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1122, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4135119.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2137.2632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1781.9028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(244.0336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(986.9034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6784],
        [-1.6421],
        [-1.5190],
        ...,
        [-1.6611],
        [-1.6570],
        [-1.6559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1580785., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1566.8169, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1566.8169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0048, -0.0119,  ...,  0.0049, -0.0085, -0.0121],
        [-0.0007,  0.0036, -0.0078,  ...,  0.0034, -0.0056, -0.0079],
        [-0.0007,  0.0033, -0.0069,  ...,  0.0031, -0.0049, -0.0070],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23733.3711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.2268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(236.3959, device='cuda:0')



h[100].sum tensor(-51.8259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-111.1075, device='cuda:0')



h[200].sum tensor(369.0024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-153.4834, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0143, 0.0000,  ..., 0.0136, 0.0000, 0.0000],
        [0.0000, 0.0158, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0000, 0.0272, 0.0000,  ..., 0.0291, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(441256.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0277, 0.0000,  ..., 0.0000, 0.1624, 0.0637],
        [0.0000, 0.0283, 0.0000,  ..., 0.0000, 0.1628, 0.0645],
        [0.0000, 0.0324, 0.0000,  ..., 0.0000, 0.1753, 0.0891],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1124, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1124, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1124, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4285173.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2298.5398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1588.7756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(751.1239, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(716.8586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2641],
        [ 0.2656],
        [ 0.2645],
        ...,
        [-1.6679],
        [-1.6638],
        [-1.6627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1485912., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1307.4423, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1307.4423, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23555.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(197.2623, device='cuda:0')



h[100].sum tensor(-43.1685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.7145, device='cuda:0')



h[200].sum tensor(352.1425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.0754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(419827., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1131, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1131, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.1134, 0.0000],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1126, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1126, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1126, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4185330.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2170.8984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1697.7065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(241.2836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(951.5576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9601],
        [-1.9600],
        [-1.9450],
        ...,
        [-1.6755],
        [-1.6715],
        [-1.6704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1474853.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1282.0542, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1282.0542, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0059, -0.0153,  ...,  0.0062, -0.0109, -0.0156],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0034, -0.0071,  ...,  0.0032, -0.0051, -0.0072],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23641.2383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.4319, device='cuda:0')



h[100].sum tensor(-42.1956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.9141, device='cuda:0')



h[200].sum tensor(350.1598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.5884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0168, 0.0000,  ..., 0.0167, 0.0000, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(418244.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0295, 0.0000,  ..., 0.0000, 0.1663, 0.0706],
        [0.0000, 0.0281, 0.0000,  ..., 0.0000, 0.1584, 0.0536],
        [0.0000, 0.0247, 0.0000,  ..., 0.0000, 0.1460, 0.0272],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1129, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1129, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1129, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4182809.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2164.4993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1710.9528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(391.0231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1002.2766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2615],
        [ 0.2607],
        [ 0.2503],
        ...,
        [-1.6827],
        [-1.6789],
        [-1.6779]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1514608.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1291.3230, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1291.3230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0036, -0.0077,  ...,  0.0034, -0.0055, -0.0079],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23717.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(194.8303, device='cuda:0')



h[100].sum tensor(-42.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.5714, device='cuda:0')



h[200].sum tensor(352.1855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.4964, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(422864.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0217, 0.0000,  ..., 0.0000, 0.1440, 0.0260],
        [0.0000, 0.0165, 0.0000,  ..., 0.0000, 0.1296, 0.0075],
        [0.0000, 0.0128, 0.0000,  ..., 0.0000, 0.1200, 0.0000],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1133, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1133, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1133, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4218929., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2199.9509, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1692.3867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(398.9517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(964.8102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0828],
        [-0.1805],
        [-0.5131],
        ...,
        [-1.6966],
        [-1.6925],
        [-1.6914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1461621.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1495.4237, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1495.4237, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0009,  0.0094, -0.0268,  ...,  0.0105, -0.0191, -0.0274],
        [-0.0009,  0.0131, -0.0388,  ...,  0.0149, -0.0277, -0.0396],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23972.0098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(225.6243, device='cuda:0')



h[100].sum tensor(-49.2269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-106.0448, device='cuda:0')



h[200].sum tensor(367.5175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-146.4898, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0188, 0.0000,  ..., 0.0192, 0.0000, 0.0000],
        [0.0000, 0.0348, 0.0000,  ..., 0.0385, 0.0000, 0.0000],
        [0.0000, 0.0597, 0.0000,  ..., 0.0684, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(439062.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0318, 0.0000,  ..., 0.0000, 0.1997, 0.1309],
        [0.0000, 0.0473, 0.0000,  ..., 0.0000, 0.2692, 0.2657],
        [0.0000, 0.0632, 0.0000,  ..., 0.0000, 0.3426, 0.4069],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1139, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1139, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1157, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4290012., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2292.8679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1601.5071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(412.2074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(839.9333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1604],
        [ 0.1533],
        [ 0.1399],
        ...,
        [-1.6646],
        [-1.5161],
        [-1.2030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1491457.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 7800 loss: tensor(462.2514, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1313.8314, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1313.8314, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0085, -0.0240,  ...,  0.0094, -0.0171, -0.0244],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0059, -0.0154,  ...,  0.0062, -0.0110, -0.0157],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23714.3730, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.2263, device='cuda:0')



h[100].sum tensor(-43.1590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.1675, device='cuda:0')



h[200].sum tensor(360.4675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.7012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0189, 0.0000,  ..., 0.0193, 0.0000, 0.0000],
        [0.0000, 0.0185, 0.0000,  ..., 0.0188, 0.0000, 0.0000],
        [0.0000, 0.0195, 0.0000,  ..., 0.0200, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(421851.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0344, 0.0000,  ..., 0.0000, 0.2070, 0.1420],
        [0.0000, 0.0318, 0.0000,  ..., 0.0000, 0.1907, 0.1109],
        [0.0000, 0.0324, 0.0000,  ..., 0.0000, 0.1896, 0.1081],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1146, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1146, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1146, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4233941., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2179.7905, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1685.0483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(240.4595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1052.1030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2380],
        [ 0.2439],
        [ 0.2454],
        ...,
        [-1.7418],
        [-1.7372],
        [-1.7359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1614167., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1336.7960, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.7960, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23847.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(201.6911, device='cuda:0')



h[100].sum tensor(-43.8443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.7960, device='cuda:0')



h[200].sum tensor(361.0969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-130.9508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(426644.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1165, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1157, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1156, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1148, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1148, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4270942., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2210.8491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1665.6858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(399.0826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1033.1082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7993],
        [-0.8784],
        [-0.8313],
        ...,
        [-1.7495],
        [-1.7450],
        [-1.7437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1647348.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1195.0143, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1195.0143, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011,  0.0208, -0.0639,  ...,  0.0242, -0.0455, -0.0652],
        [-0.0010,  0.0163, -0.0492,  ...,  0.0188, -0.0350, -0.0501],
        [-0.0009,  0.0120, -0.0351,  ...,  0.0136, -0.0250, -0.0358],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(23874.4805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(180.2996, device='cuda:0')



h[100].sum tensor(-38.9585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-84.7419, device='cuda:0')



h[200].sum tensor(347.9774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-117.0621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0544, 0.0000,  ..., 0.0622, 0.0000, 0.0000],
        [0.0000, 0.0598, 0.0000,  ..., 0.0688, 0.0000, 0.0000],
        [0.0000, 0.0462, 0.0000,  ..., 0.0523, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(415039.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0519, 0.0000,  ..., 0.0000, 0.2767, 0.2787],
        [0.0000, 0.0541, 0.0000,  ..., 0.0000, 0.2869, 0.2984],
        [0.0000, 0.0492, 0.0000,  ..., 0.0000, 0.2632, 0.2525],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1147, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1147, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4209519.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2148.5652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1725.3875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(139.0484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1156.9421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2013],
        [ 0.2115],
        [ 0.2225],
        ...,
        [-1.7390],
        [-1.7357],
        [-1.7352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1480883.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1232.4700, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1232.4700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24032.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(185.9508, device='cuda:0')



h[100].sum tensor(-40.0327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-87.3980, device='cuda:0')



h[200].sum tensor(349.0580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-120.7312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(419272.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1160, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.1183, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.1251, 0.0009],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1149, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1149, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1149, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4241099., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2146.0735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1686.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(275.6093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1170.4265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8790],
        [-1.6585],
        [-1.3329],
        ...,
        [-1.7529],
        [-1.7488],
        [-1.7477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1550264.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1183.9561, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1183.9561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24055.4883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(178.6312, device='cuda:0')



h[100].sum tensor(-38.3249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.9577, device='cuda:0')



h[200].sum tensor(345.5309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-115.9788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(417762.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1157, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.1158, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.1162, 0.0000],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1152, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1152, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.1152, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4253448., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2116.4136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1685.7607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(303.8983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1217.4561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8680],
        [-1.7225],
        [-1.4993],
        ...,
        [-1.7639],
        [-1.7598],
        [-1.7587]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1540671., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1480.5708, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1480.5708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0047, -0.0116,  ...,  0.0049, -0.0082, -0.0118],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24448.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(223.3834, device='cuda:0')



h[100].sum tensor(-47.7922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.9915, device='cuda:0')



h[200].sum tensor(365.1755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.0348, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0097, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(441411.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0244, 0.0000,  ..., 0.0000, 0.1547, 0.0429],
        [0.0000, 0.0200, 0.0000,  ..., 0.0000, 0.1459, 0.0290],
        [0.0000, 0.0174, 0.0000,  ..., 0.0000, 0.1414, 0.0249],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1158, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1158, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.1158, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4357953.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2231.8530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1553.1709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(408.5814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1011.2859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1882],
        [ 0.1636],
        [ 0.1341],
        ...,
        [-1.7826],
        [-1.7784],
        [-1.7773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1546671.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1396.5673, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1396.5673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0085, -0.0238,  ...,  0.0094, -0.0169, -0.0243],
        [-0.0008,  0.0086, -0.0240,  ...,  0.0095, -0.0171, -0.0245],
        [-0.0008,  0.0045, -0.0108,  ...,  0.0046, -0.0077, -0.0110],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24405.3301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(210.7092, device='cuda:0')



h[100].sum tensor(-45.0252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-99.0346, device='cuda:0')



h[200].sum tensor(360.3792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.8059, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0272, 0.0000,  ..., 0.0295, 0.0000, 0.0000],
        [0.0000, 0.0268, 0.0000,  ..., 0.0290, 0.0000, 0.0000],
        [0.0000, 0.0251, 0.0000,  ..., 0.0269, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(433820.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0310, 0.0000,  ..., 0.0000, 0.1786, 0.0849],
        [0.0000, 0.0312, 0.0000,  ..., 0.0000, 0.1791, 0.0859],
        [0.0000, 0.0289, 0.0000,  ..., 0.0000, 0.1710, 0.0693],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1162, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1162, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.1162, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4329276., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2194.1152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1607.8425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(336.7595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1080.3639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2390],
        [-0.2098],
        [-0.3189],
        ...,
        [-1.7940],
        [-1.7897],
        [-1.7872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1484909.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1148.0599, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1148.0599, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0031, -0.0064,  ...,  0.0030, -0.0046, -0.0065],
        [-0.0007,  0.0031, -0.0064,  ...,  0.0030, -0.0046, -0.0065],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24112.2852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(173.2153, device='cuda:0')



h[100].sum tensor(-37.1001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-81.4122, device='cuda:0')



h[200].sum tensor(346.2648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-112.4625, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0105, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(413942.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0193, 0.0000,  ..., 0.0000, 0.1388, 0.0102],
        [0.0000, 0.0219, 0.0000,  ..., 0.0000, 0.1441, 0.0151],
        [0.0000, 0.0209, 0.0000,  ..., 0.0000, 0.1409, 0.0109],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1167, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1167, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1167, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4263670., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2079.7998, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1733.7422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(166.6542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1295.5869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0115],
        [ 0.1064],
        [ 0.0291],
        ...,
        [-1.8098],
        [-1.8054],
        [-1.8043]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1683707.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1318.4075, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1318.4075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0068, -0.0183,  ...,  0.0074, -0.0130, -0.0186],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24388.3203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.9167, device='cuda:0')



h[100].sum tensor(-42.2739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.4920, device='cuda:0')



h[200].sum tensor(355.2180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.1495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0295, 0.0000,  ..., 0.0323, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0092, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(426088.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0282, 0.0000,  ..., 0.0000, 0.1748, 0.0737],
        [0.0000, 0.0198, 0.0000,  ..., 0.0000, 0.1493, 0.0280],
        [0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.1389, 0.0148],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1171, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1171, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1171, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4314490., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2152.1008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1675.0167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(300.0498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1190.7965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2130],
        [-0.0210],
        [-0.4364],
        ...,
        [-1.8214],
        [-1.8169],
        [-1.8157]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1652445.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1223.6938, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1223.6938, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0108, -0.0312,  ...,  0.0122, -0.0222, -0.0318],
        [-0.0008,  0.0082, -0.0227,  ...,  0.0091, -0.0161, -0.0232],
        [-0.0008,  0.0055, -0.0142,  ...,  0.0059, -0.0101, -0.0144],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24345.7539, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(184.6267, device='cuda:0')



h[100].sum tensor(-39.1861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.7756, device='cuda:0')



h[200].sum tensor(348.1901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.8715, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0274, 0.0000,  ..., 0.0298, 0.0000, 0.0000],
        [0.0000, 0.0295, 0.0000,  ..., 0.0323, 0.0000, 0.0000],
        [0.0000, 0.0229, 0.0000,  ..., 0.0244, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(422412.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0394, 0.0000,  ..., 0.0000, 0.2035, 0.1291],
        [0.0000, 0.0373, 0.0000,  ..., 0.0000, 0.1977, 0.1175],
        [0.0000, 0.0309, 0.0000,  ..., 0.0000, 0.1761, 0.0746],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1174, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1174, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1174, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4322882.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2121.4844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1691.6298, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(317.5264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1252.6550, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2836],
        [ 0.2856],
        [ 0.2819],
        ...,
        [-1.8327],
        [-1.8282],
        [-1.8270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1668560.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 8100 loss: tensor(508.5572, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1294.9067, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1294.9067, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0104, -0.0298,  ...,  0.0117, -0.0212, -0.0304],
        [-0.0009,  0.0097, -0.0278,  ...,  0.0109, -0.0197, -0.0283],
        [-0.0009,  0.0099, -0.0281,  ...,  0.0111, -0.0200, -0.0287],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24536.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(195.3710, device='cuda:0')



h[100].sum tensor(-41.1819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.8255, device='cuda:0')



h[200].sum tensor(350.6653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.8474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0441, 0.0000,  ..., 0.0500, 0.0000, 0.0000],
        [0.0000, 0.0441, 0.0000,  ..., 0.0499, 0.0000, 0.0000],
        [0.0000, 0.0411, 0.0000,  ..., 0.0463, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(427091.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0532, 0.0000,  ..., 0.0000, 0.2568, 0.2313],
        [0.0000, 0.0540, 0.0000,  ..., 0.0000, 0.2600, 0.2378],
        [0.0000, 0.0524, 0.0000,  ..., 0.0000, 0.2521, 0.2221],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1177, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1177, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.1177, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4345549.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2127.1956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1651.4835, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(273.3776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1243.4917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2226],
        [ 0.2157],
        [ 0.2137],
        ...,
        [-1.8273],
        [-1.7670],
        [-1.6228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1668137.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1250.9175, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1250.9175, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24561.9883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(188.7341, device='cuda:0')



h[100].sum tensor(-39.9379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.7061, device='cuda:0')



h[200].sum tensor(347.8812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-122.5383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(427614.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.1266, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.1219, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.1276, 0.0022],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1181, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1181, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1181, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4366833., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2095.8403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1623.2026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(272.4788, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1281.6915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6178],
        [-0.8607],
        [-0.8280],
        ...,
        [-1.8580],
        [-1.8535],
        [-1.8522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1647753., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1099.1678, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1099.1678, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0078, -0.0214,  ...,  0.0086, -0.0152, -0.0219],
        [-0.0008,  0.0042, -0.0100,  ...,  0.0043, -0.0071, -0.0101],
        [-0.0008,  0.0083, -0.0229,  ...,  0.0091, -0.0163, -0.0234],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24447.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(165.8386, device='cuda:0')



h[100].sum tensor(-35.0606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.9451, device='cuda:0')



h[200].sum tensor(336.9677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-107.6731, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0165, 0.0000,  ..., 0.0166, 0.0000, 0.0000],
        [0.0000, 0.0279, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        [0.0000, 0.0216, 0.0000,  ..., 0.0227, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(410941.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0336, 0.0000,  ..., 0.0000, 0.1758, 0.0732],
        [0.0000, 0.0343, 0.0000,  ..., 0.0000, 0.1816, 0.0846],
        [0.0000, 0.0305, 0.0000,  ..., 0.0000, 0.1700, 0.0613],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1183, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1183, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4290784., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1991.2306, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1710.5770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(121.6216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1469.2827, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2695],
        [ 0.2687],
        [ 0.2335],
        ...,
        [-1.8674],
        [-1.8629],
        [-1.8616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1725537., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1288.6707, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1288.6707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0058, -0.0151,  ...,  0.0062, -0.0108, -0.0154],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24822.0078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(194.4301, device='cuda:0')



h[100].sum tensor(-41.0176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.3833, device='cuda:0')



h[200].sum tensor(346.1629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.2365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(430385.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.1269, 0.0000],
        [0.0000, 0.0197, 0.0000,  ..., 0.0000, 0.1419, 0.0150],
        [0.0000, 0.0240, 0.0000,  ..., 0.0000, 0.1502, 0.0236],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1183, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1183, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4380027.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2120.8369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1611.7786, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(283.1519, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1269.1234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5365],
        [-0.1362],
        [ 0.1107],
        ...,
        [-1.8710],
        [-1.8667],
        [-1.8656]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1600693.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1184.4414, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1184.4414, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0090, -0.0253,  ...,  0.0101, -0.0180, -0.0258],
        [-0.0008,  0.0065, -0.0174,  ...,  0.0071, -0.0123, -0.0177],
        [-0.0008,  0.0061, -0.0159,  ...,  0.0065, -0.0113, -0.0162],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24797.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(178.7044, device='cuda:0')



h[100].sum tensor(-37.5400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.9921, device='cuda:0')



h[200].sum tensor(337.2794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-116.0264, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0207, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.0000, 0.0254, 0.0000,  ..., 0.0275, 0.0000, 0.0000],
        [0.0000, 0.0184, 0.0000,  ..., 0.0191, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(424626.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0329, 0.0000,  ..., 0.0000, 0.1781, 0.0771],
        [0.0000, 0.0351, 0.0000,  ..., 0.0000, 0.1840, 0.0887],
        [0.0000, 0.0305, 0.0000,  ..., 0.0000, 0.1696, 0.0594],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1183, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1183, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4365826., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2107.3711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1658.6676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(263.4066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1317.8376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0892],
        [ 0.0584],
        [-0.1750],
        ...,
        [-1.8745],
        [-1.8702],
        [-1.8691]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1563639.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1293.9238, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1293.9238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24979.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(195.2227, device='cuda:0')



h[100].sum tensor(-40.9704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.7558, device='cuda:0')



h[200].sum tensor(344.5131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-126.7511, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0087, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(433255.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0173, 0.0000,  ..., 0.0000, 0.1425, 0.0123],
        [0.0000, 0.0208, 0.0000,  ..., 0.0000, 0.1518, 0.0246],
        [0.0000, 0.0175, 0.0000,  ..., 0.0000, 0.1427, 0.0123],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1187, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1187, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4408774., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2149.2046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1611.7727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(318.1584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1256.6064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4475],
        [-0.3592],
        [-0.4448],
        ...,
        [-1.8901],
        [-1.8858],
        [-1.8848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1600301.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1208.6224, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1208.6224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0057, -0.0147,  ...,  0.0061, -0.0104, -0.0150],
        [-0.0007,  0.0033, -0.0068,  ...,  0.0031, -0.0048, -0.0069],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24889.6211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(182.3528, device='cuda:0')



h[100].sum tensor(-37.9973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.7069, device='cuda:0')



h[200].sum tensor(339.8676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-118.3951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0099, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0111, 0.0000, 0.0000],
        [0.0000, 0.0202, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(425082.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0202, 0.0000,  ..., 0.0000, 0.1449, 0.0166],
        [0.0000, 0.0241, 0.0000,  ..., 0.0000, 0.1527, 0.0262],
        [0.0000, 0.0279, 0.0000,  ..., 0.0000, 0.1625, 0.0425],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1194, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1194, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1194, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4399980., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2064.3435, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1633.8037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(297.0432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1400.1453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0087],
        [ 0.1812],
        [ 0.2133],
        ...,
        [-1.9115],
        [-1.9069],
        [-1.9057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1739896., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1157.8381, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1157.8381, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0036, -0.0080,  ...,  0.0036, -0.0057, -0.0081],
        [-0.0007,  0.0030, -0.0060,  ...,  0.0028, -0.0043, -0.0061],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24916.9492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(174.6906, device='cuda:0')



h[100].sum tensor(-36.4609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.1056, device='cuda:0')



h[200].sum tensor(336.9344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.4203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0000, 0.0188, 0.0000,  ..., 0.0194, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(423397.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0212, 0.0000,  ..., 0.0000, 0.1485, 0.0155],
        [0.0000, 0.0234, 0.0000,  ..., 0.0000, 0.1513, 0.0206],
        [0.0000, 0.0271, 0.0000,  ..., 0.0000, 0.1584, 0.0340],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.1199, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.1199, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.1199, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4396660., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2026.3893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1613.5303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(155.9627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1458.5342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2645],
        [ 0.2657],
        [ 0.2553],
        ...,
        [-1.9243],
        [-1.9197],
        [-1.9185]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1728012.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1083.6244, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1083.6244, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(24954.5977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.8996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(163.4935, device='cuda:0')



h[100].sum tensor(-34.0012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-76.8429, device='cuda:0')



h[200].sum tensor(330.7737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-106.1505, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(414992.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.1206, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.1211, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.1236, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1201, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1201, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1201, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4355422.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1972.9115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1654.9475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(113.2247, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1565.8058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4948],
        [-1.1653],
        [-0.7701],
        ...,
        [-1.9258],
        [-1.9211],
        [-1.9198]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1744408.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1335.8069, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1335.8069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0033, -0.0071,  ...,  0.0032, -0.0050, -0.0072],
        [-0.0007,  0.0034, -0.0073,  ...,  0.0033, -0.0052, -0.0075],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25454.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(201.5419, device='cuda:0')



h[100].sum tensor(-41.8780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.7259, device='cuda:0')



h[200].sum tensor(344.4229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-130.8539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0196, 0.0000,  ..., 0.0204, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(436589.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0169, 0.0000,  ..., 0.0000, 0.1387, 0.0035],
        [0.0000, 0.0218, 0.0000,  ..., 0.0000, 0.1490, 0.0194],
        [0.0000, 0.0274, 0.0000,  ..., 0.0000, 0.1608, 0.0373],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1202, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1202, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1202, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4438139., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2111.3350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1547.2214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(281.0241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1353.4099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2441],
        [ 0.2491],
        [ 0.2482],
        ...,
        [-1.9305],
        [-1.9261],
        [-1.9250]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1636240.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 8400 loss: tensor(504.1424, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1443.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1443.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25741.0859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(217.7267, device='cuda:0')



h[100].sum tensor(-45.3856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.3328, device='cuda:0')



h[200].sum tensor(349.9730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-141.3621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(449655.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.1243, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1214, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1261, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1204, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1204, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1204, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4500997., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2198.3828, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1492.4336, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(554.4734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1227.2727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1635],
        [-1.3747],
        [-1.2938],
        ...,
        [-1.9353],
        [-1.9309],
        [-1.9298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1566882.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1110.7438, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1110.7438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25252.6074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(167.5852, device='cuda:0')



h[100].sum tensor(-34.7771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-78.7660, device='cuda:0')



h[200].sum tensor(329.2140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-108.8070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(423822.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1246, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.1215, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1211, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1204, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1204, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1204, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4398111., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2053.0552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1634.3916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(128.1575, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1467.6018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0495],
        [-1.4505],
        [-1.7625],
        ...,
        [-1.9353],
        [-1.9309],
        [-1.9298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1566445., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1271.0343, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1271.0343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25516.0488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(191.7692, device='cuda:0')



h[100].sum tensor(-39.6968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.1327, device='cuda:0')



h[200].sum tensor(340.4445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.5089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(436152.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1225, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.1254, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.1331, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1209, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1209, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1209, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4466366.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2111.6631, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1567.0319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(334.2654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1374.3621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6958],
        [-1.3608],
        [-0.9389],
        ...,
        [-1.9548],
        [-1.9503],
        [-1.9492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1693993.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1156.3571, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1156.3571, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25355.7168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(174.4671, device='cuda:0')



h[100].sum tensor(-35.9894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.0006, device='cuda:0')



h[200].sum tensor(335.1788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.2753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(427476.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1220, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1220, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1223, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1215, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1215, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1215, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4445580.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2045.5490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1609.1062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(282.5530, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1475.3409, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2928],
        [-2.2609],
        [-2.1959],
        ...,
        [-1.9767],
        [-1.9720],
        [-1.9706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1730071.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1486.9175, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1486.9175, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0034, -0.0071,  ...,  0.0033, -0.0050, -0.0073],
        [-0.0007,  0.0034, -0.0071,  ...,  0.0033, -0.0050, -0.0073],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25935.3809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(224.3409, device='cuda:0')



h[100].sum tensor(-46.0808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-105.4416, device='cuda:0')



h[200].sum tensor(355.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.6565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0109, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(461230.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0198, 0.0000,  ..., 0.0000, 0.1463, 0.0083],
        [0.0000, 0.0209, 0.0000,  ..., 0.0000, 0.1491, 0.0122],
        [0.0000, 0.0186, 0.0000,  ..., 0.0000, 0.1447, 0.0080],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1219, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1219, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1219, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4632457., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2221.8167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1412.7780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1034.0746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1180.4495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0946],
        [ 0.0076],
        [-0.0603],
        ...,
        [-1.9886],
        [-1.9838],
        [-1.9826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1782955.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1275.1537, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1275.1537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25710.6816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(192.3907, device='cuda:0')



h[100].sum tensor(-39.6277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.4248, device='cuda:0')



h[200].sum tensor(341.1505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.9124, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(438214.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1251, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1235, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1231, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1223, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1223, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1223, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4500476., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2088.2188, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1537.8126, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(530.8289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1411.2678, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7135],
        [-1.1175],
        [-1.4627],
        ...,
        [-1.9963],
        [-1.9915],
        [-1.9903]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1744022.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1058.5835, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1058.5835, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0036, -0.0078,  ...,  0.0035, -0.0055, -0.0080],
        [-0.0007,  0.0031, -0.0063,  ...,  0.0030, -0.0044, -0.0064],
        [-0.0008,  0.0056, -0.0141,  ...,  0.0059, -0.0100, -0.0144],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25463.2539, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.8851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(159.7154, device='cuda:0')



h[100].sum tensor(-32.7831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-75.0672, device='cuda:0')



h[200].sum tensor(325.7903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-103.6975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0120, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0000, 0.0207, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(420534.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0200, 0.0000,  ..., 0.0000, 0.1524, 0.0193],
        [0.0000, 0.0254, 0.0000,  ..., 0.0000, 0.1649, 0.0385],
        [0.0000, 0.0219, 0.0000,  ..., 0.0000, 0.1550, 0.0216],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1225, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1225, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1225, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4430214., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1990.0457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1642.4686, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(205.3963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1592.1372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1301],
        [ 0.1872],
        [ 0.0441],
        ...,
        [-2.0022],
        [-1.9975],
        [-1.9962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1815061.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1417.0522, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.0522, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26087.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(213.7999, device='cuda:0')



h[100].sum tensor(-43.7201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.4872, device='cuda:0')



h[200].sum tensor(345.6094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-138.8126, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(453024.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1238, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1238, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1241, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.1227, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.1227, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.1227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4565007., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2180.6917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1463.2455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(449.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1270.4006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7261],
        [-1.6006],
        [-1.4065],
        ...,
        [-2.0072],
        [-2.0026],
        [-2.0014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1622196.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1199.8131, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1199.8131, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(25840.9082, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(181.0236, device='cuda:0')



h[100].sum tensor(-37.0633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.0822, device='cuda:0')



h[200].sum tensor(331.4335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-117.5322, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(435370.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.1355, 0.0011],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1274, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1250, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1231, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1231, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1231, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4497914., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2063.9785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1556.5696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(334.6288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1471.1941, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4556],
        [-0.8674],
        [-1.2070],
        ...,
        [-2.0165],
        [-2.0119],
        [-2.0107]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1667949.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1429.4496, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1429.4496, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0059, -0.0152,  ...,  0.0063, -0.0107, -0.0155],
        [-0.0008,  0.0066, -0.0172,  ...,  0.0071, -0.0122, -0.0176],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26246.0684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(215.6704, device='cuda:0')



h[100].sum tensor(-43.7875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.3664, device='cuda:0')



h[200].sum tensor(343.7454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.0271, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0325, 0.0000,  ..., 0.0359, 0.0000, 0.0000],
        [0.0000, 0.0207, 0.0000,  ..., 0.0216, 0.0000, 0.0000],
        [0.0000, 0.0292, 0.0000,  ..., 0.0319, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(455117.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0492, 0.0000,  ..., 0.0000, 0.2460, 0.1912],
        [0.0000, 0.0414, 0.0000,  ..., 0.0000, 0.2194, 0.1412],
        [0.0000, 0.0401, 0.0000,  ..., 0.0000, 0.2135, 0.1294],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1234, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1234, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1234, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4580262.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2166.7153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1446.7935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(466.2608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1298.8853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2007],
        [ 0.2174],
        [ 0.2280],
        ...,
        [-2.0257],
        [-2.0213],
        [-2.0203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1587486.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 8700 loss: tensor(452.2447, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1387.5514, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1387.5514, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0079, -0.0215,  ...,  0.0087, -0.0152, -0.0219],
        [-0.0008,  0.0042, -0.0096,  ...,  0.0042, -0.0068, -0.0098],
        [-0.0007,  0.0036, -0.0078,  ...,  0.0035, -0.0055, -0.0079],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26222.3496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(209.3489, device='cuda:0')



h[100].sum tensor(-42.6025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.3952, device='cuda:0')



h[200].sum tensor(341.7302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-135.9228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        [0.0000, 0.0203, 0.0000,  ..., 0.0212, 0.0000, 0.0000],
        [0.0000, 0.0177, 0.0000,  ..., 0.0180, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(454278.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0243, 0.0000,  ..., 0.0000, 0.1637, 0.0338],
        [0.0000, 0.0248, 0.0000,  ..., 0.0000, 0.1662, 0.0385],
        [0.0000, 0.0221, 0.0000,  ..., 0.0000, 0.1581, 0.0228],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1239, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1239, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1239, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4608482., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2139.7422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1448.2644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(535.4359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1347.3423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2637],
        [ 0.1845],
        [ 0.0435],
        ...,
        [-2.0410],
        [-2.0362],
        [-2.0347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1691023.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1385.7742, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1385.7742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26268.9570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(209.0808, device='cuda:0')



h[100].sum tensor(-42.4743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-98.2692, device='cuda:0')



h[200].sum tensor(341.0855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-135.7487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(452850.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.1279, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1248, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1251, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1242, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1242, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4594850., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2125.5791, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1465.1501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(544.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1384.2415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7931],
        [-1.9119],
        [-1.8276],
        ...,
        [-2.0529],
        [-2.0483],
        [-2.0470]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1702812.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1359.7651, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1359.7651, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0034, -0.0071,  ...,  0.0033, -0.0050, -0.0073],
        [-0.0007,  0.0034, -0.0071,  ...,  0.0033, -0.0050, -0.0073],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26248.0078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(205.1566, device='cuda:0')



h[100].sum tensor(-41.3450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-96.4248, device='cuda:0')



h[200].sum tensor(338.9573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-133.2009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0091, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(451057.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0196, 0.0000,  ..., 0.0000, 0.1494, 0.0089],
        [0.0000, 0.0208, 0.0000,  ..., 0.0000, 0.1523, 0.0128],
        [0.0000, 0.0181, 0.0000,  ..., 0.0000, 0.1467, 0.0084],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1245, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1245, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.1245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4602228., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2125.9453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1494.6431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(501.4989, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1404.9695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1523],
        [-0.0814],
        [-0.2534],
        ...,
        [-2.0645],
        [-2.0597],
        [-2.0585]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1684105.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1421.2061, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.2061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0037, -0.0081,  ...,  0.0037, -0.0057, -0.0083],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26397.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(214.4266, device='cuda:0')



h[100].sum tensor(-43.2968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-100.7818, device='cuda:0')



h[200].sum tensor(343.2301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.2195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0076, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(455999.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.1438, 0.0060],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.1360, 0.0006],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1312, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1248, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1248, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.1248, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4613859., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2156.0630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1480.9592, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(538.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1371.9949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8715],
        [-0.9373],
        [-0.8352],
        ...,
        [-2.0751],
        [-2.0702],
        [-2.0689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1659219., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1269.1302, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1269.1302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0129, -0.0371,  ...,  0.0147, -0.0262, -0.0378],
        [-0.0009,  0.0095, -0.0265,  ...,  0.0107, -0.0187, -0.0270],
        [-0.0008,  0.0056, -0.0141,  ...,  0.0060, -0.0099, -0.0144],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26166.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(191.4820, device='cuda:0')



h[100].sum tensor(-38.2696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.9977, device='cuda:0')



h[200].sum tensor(334.4048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.3224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0356, 0.0000,  ..., 0.0398, 0.0000, 0.0000],
        [0.0000, 0.0375, 0.0000,  ..., 0.0421, 0.0000, 0.0000],
        [0.0000, 0.0423, 0.0000,  ..., 0.0479, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(441762.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0537, 0.0000,  ..., 0.0000, 0.2560, 0.2051],
        [0.0000, 0.0513, 0.0000,  ..., 0.0000, 0.2488, 0.1914],
        [0.0000, 0.0523, 0.0000,  ..., 0.0000, 0.2522, 0.1971],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1251, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1251, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1251, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4568803., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2060.9526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1563.7518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(298.0759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1545.7474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2503],
        [ 0.2584],
        [ 0.2666],
        ...,
        [-2.0899],
        [-2.0850],
        [-2.0837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1763309.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1176.1317, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1176.1317, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26102.9316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(177.4507, device='cuda:0')



h[100].sum tensor(-35.4685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-83.4029, device='cuda:0')



h[200].sum tensor(329.0967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-115.2124, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(433646.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.1400, 0.0034],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.1305, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.1275, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1254, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1254, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1254, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4540223., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2000.1304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1604.4187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(224.1046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1670.9871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4019],
        [-0.8911],
        [-1.3272],
        ...,
        [-2.0991],
        [-2.0942],
        [-2.0929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1803932.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1124.3624, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1124.3624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0059, -0.0152,  ...,  0.0064, -0.0107, -0.0155],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26097.2617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(169.6399, device='cuda:0')



h[100].sum tensor(-33.9481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-79.7318, device='cuda:0')



h[200].sum tensor(326.6794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-110.1411, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(430339.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0250, 0.0000,  ..., 0.0000, 0.1641, 0.0354],
        [0.0000, 0.0225, 0.0000,  ..., 0.0000, 0.1586, 0.0277],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.1416, 0.0040],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1257, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1257, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1257, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4530666., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1972.1698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1617.6927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(150.8448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1733.6296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2473],
        [ 0.2038],
        [ 0.1090],
        ...,
        [-2.1083],
        [-2.1035],
        [-2.1022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1802663.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1522.8733, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1522.8733, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0029, -0.0055,  ...,  0.0027, -0.0039, -0.0056],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26763.9922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(229.7658, device='cuda:0')



h[100].sum tensor(-45.4460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-107.9913, device='cuda:0')



h[200].sum tensor(350.9310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-149.1787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(471930.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0172, 0.0000,  ..., 0.0000, 0.1412, 0.0000],
        [0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.1393, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.1332, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1260, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1260, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1260, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4761831., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2215.0347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1398.4478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1222.3257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1340.6700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2209],
        [-0.3502],
        [-0.5456],
        ...,
        [-2.1181],
        [-2.1133],
        [-2.1120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1731029.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1284.1445, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1284.1445, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26485.7090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.7473, device='cuda:0')



h[100].sum tensor(-38.6828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.0624, device='cuda:0')



h[200].sum tensor(337.4517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.7932, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(446514.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1310, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.1289, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1276, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1262, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1262, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4612542., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2079.1467, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1549.5481, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(448.5863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1588.4619, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3345],
        [-1.5254],
        [-1.7279],
        ...,
        [-2.1250],
        [-2.1202],
        [-2.1190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1709526., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1328.8671, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1328.8671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0033, -0.0068,  ...,  0.0032, -0.0048, -0.0069],
        [-0.0007,  0.0034, -0.0073,  ...,  0.0034, -0.0051, -0.0074],
        [-0.0008,  0.0056, -0.0140,  ...,  0.0060, -0.0099, -0.0143],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26570.3555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(200.4948, device='cuda:0')



h[100].sum tensor(-39.6329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.2338, device='cuda:0')



h[200].sum tensor(341.3278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-130.1741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0194, 0.0000,  ..., 0.0203, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(450376.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0238, 0.0000,  ..., 0.0000, 0.1582, 0.0222],
        [0.0000, 0.0296, 0.0000,  ..., 0.0000, 0.1706, 0.0420],
        [0.0000, 0.0264, 0.0000,  ..., 0.0000, 0.1627, 0.0292],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1266, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1266, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1266, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4642105., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2088.4844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1516.8169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(379.7161, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1583.4023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1543],
        [ 0.2467],
        [ 0.2597],
        ...,
        [-2.1404],
        [-2.1355],
        [-2.1343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1752136.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 9000 loss: tensor(503.8913, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1449.6060, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1449.6060, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0037, -0.0080,  ...,  0.0036, -0.0056, -0.0081],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26837.0195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(218.7115, device='cuda:0')



h[100].sum tensor(-43.1733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.7957, device='cuda:0')



h[200].sum tensor(349.5415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.0015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(462981.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1302, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.1368, 0.0000],
        [0.0000, 0.0168, 0.0000,  ..., 0.0000, 0.1416, 0.0024],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1269, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1269, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.1269, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4705431., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2158.3687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1436.2421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(565.0905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1479.4628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9868],
        [-0.4530],
        [-0.0605],
        ...,
        [-2.1452],
        [-2.1381],
        [-2.1352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1695307.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1248.6444, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1248.6444, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0034, -0.0070,  ...,  0.0033, -0.0049, -0.0071],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26573.6660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(188.3911, device='cuda:0')



h[100].sum tensor(-37.1145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-88.5449, device='cuda:0')



h[200].sum tensor(337.3675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-122.3156, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(445907.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.1403, 0.0000],
        [0.0000, 0.0182, 0.0000,  ..., 0.0000, 0.1476, 0.0067],
        [0.0000, 0.0245, 0.0000,  ..., 0.0000, 0.1598, 0.0233],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1272, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1272, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.1272, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4625686., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2065.6538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1531.7401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(239.5549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1649.0576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1235],
        [ 0.1874],
        [ 0.2520],
        ...,
        [-2.1596],
        [-2.1547],
        [-2.1535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1693061.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1369.3831, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1369.3831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0108, -0.0304,  ...,  0.0123, -0.0214, -0.0310],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0069, -0.0181,  ...,  0.0076, -0.0128, -0.0185],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26815.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(206.6078, device='cuda:0')



h[100].sum tensor(-40.7300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-97.1069, device='cuda:0')



h[200].sum tensor(345.4076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-134.1430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0294, 0.0000,  ..., 0.0323, 0.0000, 0.0000],
        [0.0000, 0.0326, 0.0000,  ..., 0.0362, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0081, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(454523.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0501, 0.0000,  ..., 0.0000, 0.2396, 0.1685],
        [0.0000, 0.0403, 0.0000,  ..., 0.0000, 0.2103, 0.1145],
        [0.0000, 0.0241, 0.0000,  ..., 0.0000, 0.1670, 0.0344],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1275, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1275, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1275, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4670478., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2120.5588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1492.5182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(504.6795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1572.9086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2448],
        [ 0.2063],
        [ 0.0612],
        ...,
        [-2.1703],
        [-2.1656],
        [-2.1645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1744617.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1284.0317, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1284.0317, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26672.7363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.7302, device='cuda:0')



h[100].sum tensor(-38.1456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-91.0544, device='cuda:0')



h[200].sum tensor(340.0883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.7821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(449869.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.1292, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1284, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.1284, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1275, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1275, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.1275, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4665533., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2092.3577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1521.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(369.4619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1627.5244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2892],
        [-1.6270],
        [-1.9259],
        ...,
        [-2.1719],
        [-2.1670],
        [-2.1658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1825140.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1441.6382, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1441.6382, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26999.8340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(217.5094, device='cuda:0')



h[100].sum tensor(-42.6869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-102.2307, device='cuda:0')



h[200].sum tensor(350.2499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-141.2210, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(463267.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.1391, 0.0000],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.1379, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.1352, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1281, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1281, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1281, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4713912.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2200.1826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1489.3116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(425.7074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1500.5540, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1699],
        [-0.3815],
        [-0.6392],
        ...,
        [-2.0273],
        [-2.1387],
        [-2.1702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1768846.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1160.8677, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1160.8677, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0058, -0.0147,  ...,  0.0062, -0.0103, -0.0150],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0009,  0.0089, -0.0243,  ...,  0.0100, -0.0171, -0.0248],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26601.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(175.1477, device='cuda:0')



h[100].sum tensor(-34.3015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-82.3204, device='cuda:0')



h[200].sum tensor(332.7030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.7171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        [0.0000, 0.0221, 0.0000,  ..., 0.0235, 0.0000, 0.0000],
        [0.0000, 0.0278, 0.0000,  ..., 0.0304, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(437534.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0279, 0.0000,  ..., 0.0000, 0.1717, 0.0381],
        [0.0000, 0.0405, 0.0000,  ..., 0.0000, 0.2091, 0.1075],
        [0.0000, 0.0535, 0.0000,  ..., 0.0000, 0.2520, 0.1860],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1282, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1282, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1282, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4610726., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2026.3066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1586.7618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(212.0690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1754.5830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3105],
        [ 0.2896],
        [ 0.2564],
        ...,
        [-2.1938],
        [-2.1888],
        [-2.1875]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1844413., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1282.5935, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1282.5935, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26900.9492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(193.5132, device='cuda:0')



h[100].sum tensor(-37.9027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-90.9524, device='cuda:0')



h[200].sum tensor(338.7128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.6412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(452015.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1290, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1290, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.1293, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1285, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1285, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1285, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4682037., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2106.5244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1494.1755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(477.9984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1618.5426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3339],
        [-2.4213],
        [-2.4822],
        ...,
        [-2.1970],
        [-2.1920],
        [-2.1908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1810710.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1201.1306, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1201.1306, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0031, -0.0061,  ...,  0.0029, -0.0043, -0.0062],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26828.9277, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(181.2224, device='cuda:0')



h[100].sum tensor(-35.2565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-85.1756, device='cuda:0')



h[200].sum tensor(332.1696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-117.6612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(444643.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.1368, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.1414, 0.0000],
        [0.0000, 0.0167, 0.0000,  ..., 0.0000, 0.1445, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1288, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1288, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1288, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4654184., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2054.8340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1520.2197, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(305.6814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1706.8965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5191],
        [-0.6370],
        [-0.7103],
        ...,
        [-2.2035],
        [-2.1987],
        [-2.1975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1804820.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1303.2213, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1303.2213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0037, -0.0080,  ...,  0.0037, -0.0056, -0.0082],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27052.8027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(196.6255, device='cuda:0')



h[100].sum tensor(-38.3829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-92.4152, device='cuda:0')



h[200].sum tensor(338.1239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.6619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(456218.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.1328, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.1377, 0.0000],
        [0.0000, 0.0140, 0.0000,  ..., 0.0000, 0.1417, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1297, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.1351, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.1474, 0.0084]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4710814., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2108.1865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1438.5334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(502.6662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1607.1560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1649],
        [-0.9459],
        [-0.5976],
        ...,
        [-1.8722],
        [-1.4053],
        [-0.7877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1773376.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1217.5273, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1217.5273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0095, -0.0260,  ...,  0.0106, -0.0183, -0.0265],
        [-0.0008,  0.0040, -0.0087,  ...,  0.0039, -0.0061, -0.0089],
        [-0.0008,  0.0071, -0.0184,  ...,  0.0077, -0.0129, -0.0187],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(26915.3848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(183.6963, device='cuda:0')



h[100].sum tensor(-35.7856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-86.3383, device='cuda:0')



h[200].sum tensor(332.3767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.2674, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0280, 0.0000,  ..., 0.0303, 0.0000, 0.0000],
        [0.0000, 0.0308, 0.0000,  ..., 0.0338, 0.0000, 0.0000],
        [0.0000, 0.0123, 0.0000,  ..., 0.0115, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(447872.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0370, 0.0000,  ..., 0.0000, 0.2073, 0.1015],
        [0.0000, 0.0337, 0.0000,  ..., 0.0000, 0.1987, 0.0862],
        [0.0000, 0.0226, 0.0000,  ..., 0.0000, 0.1677, 0.0322],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1296, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1296, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1296, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4677769., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2054.5264, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1481.3639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(283.9290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1695.2498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2932],
        [ 0.1470],
        [-0.1089],
        ...,
        [-2.2292],
        [-2.2242],
        [-2.2230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1818879., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 9300 loss: tensor(444.2423, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1353.2345, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1353.2345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27229.3242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(204.1713, device='cuda:0')



h[100].sum tensor(-39.4863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-95.9617, device='cuda:0')



h[200].sum tensor(337.5027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.5611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0177, 0.0000,  ..., 0.0181, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(466384.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0374, 0.0000,  ..., 0.0000, 0.2074, 0.1005],
        [0.0000, 0.0253, 0.0000,  ..., 0.0000, 0.1733, 0.0384],
        [0.0000, 0.0234, 0.0000,  ..., 0.0000, 0.1683, 0.0291],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1297, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1297, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1297, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4801671., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2163.7119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1381.7418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(882.4927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1519.4775, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2406],
        [ 0.2475],
        [ 0.2487],
        ...,
        [-2.2315],
        [-2.2267],
        [-2.2255]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1738928.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1426.0577, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1426.0577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0033, -0.0067,  ...,  0.0032, -0.0047, -0.0069],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27471.0820, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(215.1586, device='cuda:0')



h[100].sum tensor(-41.7142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-101.1258, device='cuda:0')



h[200].sum tensor(338.8941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-139.6948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0121, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(472299.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0227, 0.0000,  ..., 0.0000, 0.1635, 0.0235],
        [0.0000, 0.0158, 0.0000,  ..., 0.0000, 0.1480, 0.0059],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.1375, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1298, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1298, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1298, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4803368., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2209.1011, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1363.4324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(780.4573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1462.4731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0486],
        [-0.3013],
        [-0.7264],
        ...,
        [-2.2298],
        [-2.2271],
        [-2.2261]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1673437.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1339.0776, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1339.0776, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27351.9648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(202.0354, device='cuda:0')



h[100].sum tensor(-38.8661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-94.9578, device='cuda:0')



h[200].sum tensor(331.3137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-131.1743, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(458990.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1322, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.1361, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.1411, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1301, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1301, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.1301, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4749378., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2130.4644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1456.0850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(870.6612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1622.8751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7143],
        [-0.4104],
        [-0.1095],
        ...,
        [-2.2418],
        [-2.2370],
        [-2.2358]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1871454.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1472.8097, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1472.8097, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0009,  0.0093, -0.0253,  ...,  0.0104, -0.0177, -0.0258],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27649.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.1707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(222.2124, device='cuda:0')



h[100].sum tensor(-42.9953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-104.4411, device='cuda:0')



h[200].sum tensor(339.3421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-144.2746, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0342, 0.0000,  ..., 0.0380, 0.0000, 0.0000],
        [0.0000, 0.0161, 0.0000,  ..., 0.0162, 0.0000, 0.0000],
        [0.0000, 0.0224, 0.0000,  ..., 0.0238, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(473313.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0347, 0.0000,  ..., 0.0000, 0.2029, 0.0912],
        [0.0000, 0.0305, 0.0000,  ..., 0.0000, 0.1895, 0.0670],
        [0.0000, 0.0320, 0.0000,  ..., 0.0000, 0.1919, 0.0710],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1304, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1304, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.1304, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4812283., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2211.3442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1373.9185, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(908.5833, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1486.9755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3286],
        [ 0.3109],
        [ 0.1896],
        ...,
        [-2.2393],
        [-2.2306],
        [-2.2248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1712699.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1266.6885, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1266.6885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27241.6309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(191.1136, device='cuda:0')



h[100].sum tensor(-36.7486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-89.8245, device='cuda:0')



h[200].sum tensor(327.8289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.0832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0167, 0.0000,  ..., 0.0168, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0121, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(454126.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0345, 0.0000,  ..., 0.0000, 0.2027, 0.0885],
        [0.0000, 0.0204, 0.0000,  ..., 0.0000, 0.1623, 0.0174],
        [0.0000, 0.0233, 0.0000,  ..., 0.0000, 0.1654, 0.0212],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1310, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1310, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1310, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4728832., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2080.8604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1462.5309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(260.4955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1686.9382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2772],
        [ 0.2981],
        [ 0.3111],
        ...,
        [-2.2764],
        [-2.2713],
        [-2.2700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1729946.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1314.8080, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1314.8080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27294.3828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.3737, device='cuda:0')



h[100].sum tensor(-37.9184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.2368, device='cuda:0')



h[200].sum tensor(331.6798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.7969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(459515.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1320, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1320, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.1323, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1314, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1314, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.1314, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4772630.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2092.1992, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1422.2743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(356.6529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1662.0001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6136],
        [-2.6364],
        [-2.6421],
        ...,
        [-2.2960],
        [-2.2899],
        [-2.2877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1796827., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1699.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1699.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0060, -0.0149,  ...,  0.0064, -0.0104, -0.0152],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0008,  0.0038, -0.0083,  ...,  0.0038, -0.0058, -0.0084],
        ...,
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0012,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(28026.9961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.3107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(256.4281, device='cuda:0')



h[100].sum tensor(-48.6148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-120.5227, device='cuda:0')



h[200].sum tensor(352.8754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-166.4896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0207, 0.0000,  ..., 0.0217, 0.0000, 0.0000],
        [0.0000, 0.0247, 0.0000,  ..., 0.0266, 0.0000, 0.0000],
        [0.0000, 0.0175, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0109, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(492636.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0471, 0.0000,  ..., 0.0000, 0.2240, 0.1266],
        [0.0000, 0.0439, 0.0000,  ..., 0.0000, 0.2134, 0.1069],
        [0.0000, 0.0377, 0.0000,  ..., 0.0000, 0.1956, 0.0735],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.1332, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.1388, 0.0000],
        [0.0000, 0.0166, 0.0000,  ..., 0.0000, 0.1549, 0.0131]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4939148., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2294.8787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1258.0188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1265.3110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1355.8857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2603],
        [ 0.2809],
        [ 0.2856],
        ...,
        [-2.0428],
        [-1.7179],
        [-1.2855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1756872.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1315.5201, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1315.5201, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27450.2598, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-143.0564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(198.4811, device='cuda:0')



h[100].sum tensor(-37.7449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-93.2873, device='cuda:0')



h[200].sum tensor(328.2515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-128.8667, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(463351.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1322, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.1322, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.1325, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1316, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1316, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1316, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4816179., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2156.2314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1448.3120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(775.1939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1639.2601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6342],
        [-2.6179],
        [-2.5758],
        ...,
        [-2.3015],
        [-2.2978],
        [-2.2974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1803267.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet].sum tensor(1131.8799, device='cuda:0')



input graph: 
g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([203880, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1131.8799, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007,  0.0031, -0.0060,  ...,  0.0030, -0.0042, -0.0062],
        [-0.0007,  0.0031, -0.0060,  ...,  0.0030, -0.0042, -0.0062],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        ...,
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000],
        [-0.0007,  0.0011,  0.0000,  ...,  0.0006,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(27155.7480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-142.9331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(170.7741, device='cuda:0')



h[100].sum tensor(-32.4495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-80.2648, device='cuda:0')



h[200].sum tensor(316.8134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-110.8775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0068, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([203880, 256]) 
h.sum tensor(445510.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0165, 0.0000,  ..., 0.0000, 0.1468, 0.0000],
        [0.0000, 0.0158, 0.0000,  ..., 0.0000, 0.1456, 0.0000],
        [0.0000, 0.0143, 0.0000,  ..., 0.0000, 0.1432, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1319, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1319, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.1319, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([203880, 128]) 
h2.sum tensor(4733134.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(2054.7427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1551.0652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(308.0119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1828.0844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=203880, num_edges=2195580,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7552],
        [-0.8301],
        [-0.9437],
        ...,
        [-2.3183],
        [-2.3130],
        [-2.3117]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([203880, 1]) 
h5.sum tensor(-1835468.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([2195580, 1]) 
g.edata[efet].sum tensor(2195580., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 80, in <module>
    outi = net(batcheddglgraph, featbatch).reshape(BatchSize, 6796)#.to('cpu')#.type(torch.LongTensor)  # Perform a single forward pass.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/ModelBha.py", line 52, in forward
    h2 = self.conv2(g, h1)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 447, in forward
    rst = rst * norm
RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 31.75 GiB total capacity; 27.20 GiB already allocated; 19.75 MiB free; 27.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

real	3m4.847s
user	0m32.068s
sys	0m28.349s
